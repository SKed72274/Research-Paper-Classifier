Paper ID,text
P001,"Leveraging Clustering Techniques for EnhancedDrone Monitoring and Position EstimationAbstractDrone tracking and localization are essential for various applications, includingmanaging drone formations and implementing anti-drone strategies. Pinpointingand monitoring drones in three-dimensional space is difficult, particularly whentrying to capture the subtle movements of small drones during rapid maneuvers.This involves extracting faint signals from varied flight settings and maintainingalignment despite swift actions. Typically, cameras and LiDAR systems are usedto record the paths of drones. However, they encounter challenges in categorizingdrones and estimating their positions accurately. This report provides an overviewof an approach named CL-Det. It uses a clustering-based learning detection strategyto track and estimate the position of drones using data from two types of LiDARsensors: Livox Avia and LiDAR 360. This method merges data from both LiDARsources to accurately determine the drone’s location in three dimensions. Themethod begins by synchronizing the time codes of the data from the two sensorsand then isolates the point cloud data for the objects of interest (OOIs) from theenvironmental data. A Density-Based Spatial Clustering of Applications withNoise (DBSCAN) method is applied to cluster the OOI point cloud data, and thecenter point of the most prominent cluster is taken as the drone’s location. Thetechnique also incorporates past position estimates to compensate for any missinginformation.1 IntroductionUnmanned aerial vehicles (UAVs), commonly referred to as drones, have gained prominence andsignificantly influence areas like logistics, imaging, and emergency response, offering substantialadvantages to society. However, the broad adoption and sophisticated features of compact, off-the-shelf drones have created intricate security issues that extend beyond conventional risks.Recent years have witnessed a surge in research on anti-UAV systems. Present anti-UAV methodspredominantly utilize visual, radar, and radio frequency (RF) technologies. Despite these strides,recognizing drones poses a considerable hurdle for sensors like cameras, particularly when dronesare at significant altitudes or in challenging visual environments. These methods usually fail to spotsmall drones because of their minimal size, which leads to a decreased radar cross-section and aless noticeable visual presence. Furthermore, current anti-UAV studies primarily focus on detectingobjects and tracking them in two dimensions, overlooking the crucial element of estimating their3D paths. This omission significantly restricts the effectiveness of anti-UAV systems in practical,real-world contexts.Our proposed solution, a detection method based on clustering learning (CL-Det), uses the strengthsof both Livox Avia and LiDAR 360 to improve the tracking and position estimation of UAVs.Initially, the timestamps from the Livox Avia and LiDAR 360 data are aligned to maintain temporalconsistency. By examining the LiDAR data, which contains the spatial coordinates of objects atspecific times, and comparing these to the actual recorded positions of the drone at those times, thedrone’s location within the LiDAR point cloud data is effectively pinpointed. The point cloud for.objects of interest (OOIs) is then isolated from the environmental data. The point cloud of the OOIsis grouped using the DBSCAN algorithm, and the central point of the largest cluster is designated asthe UAV’s position. Moreover, radar data also faces significant challenges due to missing information.To mitigate potential data deficiencies, past estimations are employed to supplement missing data,thereby maintaining the consistency and precision of UAV tracking.2 MethodologyThis section details the methodology employed to ascertain the drone’s spatial position utilizinginformation from LiDAR 360 and Livox Avia sensors. The strategy integrates data from both sensortypes to achieve precise position calculations.2.1 Data SourcesThe following modalities of data were utilized:• Double fisheye camera visual images• Livox Mid-360 (LiDAR 360) 3D point cloud data• Livox Avia 3D point cloud data• Millimeter-wave radar 3D point cloud dataOnly 14 out of 59 test sequences have non-zero radar values; therefore, the radar dataset is excludedfrom this work due to data availability issues. Two primary sensor types are employed: LiDAR 360and Livox Avia, both of which supply 3D point cloud data crucial for identifying the drone’s location.The detailed data descriptions are outlined as follows:• LiDAR 360 offers a complete 360-degree view with 3D point cloud data. This datasetencompasses environmental details and other observable objects.• Livox Avia delivers focused 3D point cloud data at specific timestamps, typically indicatingthe origin point or the drone’s position.2.2 AlgorithmFor every sequence, corresponding positions are recorded at specific timestamps. The proceduregives precedence to LiDAR 360 data, using Livox Avia data as a backup if the former is not available.If neither source is accessible, the position is estimated using historical averages.2.2.1 LiDAR 360 Data ProcessingSeparation of Points:• The LiDAR 360 data is visually examined to classify areas into twozones: environment and non-environment zones.Removal of Environment Points:• All points within the environment zone are deemed partof the surroundings and are thus excluded from the dataset. After removing environmentpoints, it is observed that the remaining non-environment points imply the drone position.Clustering:• The DBSCAN clustering algorithm is applied to the remaining points to discerndistinct clusters.Cluster Selection:• The most extensive non-environment cluster is chosen as the representa-tive group of points that correspond to the drone.Mean Position Calculation:• The drone’s position is determined by calculating the mean ofthe selected cluster, represented by (x, y, z) coordinates.2.2.2 Livox Avia Data ProcessingRemoval of Noise:• Points with coordinates (0, 0, 0) are eliminated as they are regarded asnoise.Mean Position Calculation:• The mean of the residual points is computed to ascertain thedrone’s position in (x, y, z) coordinates. 22.2.3 Fallback MethodWhen neither LiDAR 360 nor Livox Avia data is available, the average location of the drone derivedfrom training datasets is used. The average ground truth position (x, y, z) from all training datasetsestimates the drone ground truth position, which is (0.734, -9.739, 33.353).2.3 Implementation DetailsThe program fetches LiDAR 360 or Livox Avia data from the nearest timestamp for each sequence,as indicated in the test dataset. Clustering is executed using the DBSCAN algorithm with appro-priate parameters to guarantee strong clustering. Visual inspection is employed for the preliminaryseparation of points, ensuring an accurate categorization of environment points.The implementation was conducted on a Lenovo IdeaPad Slim 5 Pro (16"") running Windows 11with an AMD Ryzen 7 5800H CPU and 16GB DDR4 RAM. The analysis was carried out in aJupyter Notebook environment using Python 3.10. For clustering, the DBSCAN algorithm from theScikit-Learn library was utilized. The DBSCAN algorithm was configured with an epsilon (eps)value of 2 and a minimum number of points (minPts) set to 1.3 ResultsThe algorithm achieved a pose MSE loss of 120.215 and a classification accuracy of 0.322. Table 1presents the evaluation results compared to other teams.Table 1: Evaluation results on the leaderboard↓ ↑Team ID Pose MSE ( ) Accuracy ( )SDUCZS 58198 2.21375 0.8136Gaofen Lab 57978 7.299575 0.3220sysutlt 57843 24.50694 0.3220casetrous 58233 56.880267 0.2542NTU-ICG (ours) 58268 120.215107 0.3220MTC 58180 189.669428 0.2724gzist 56936 417.396317 0.23024 ConclusionsThis paper introduces a clustering-based learning method, CL-Det, which employs advanced cluster-ing techniques such as K-Means and DBSCAN for drone detection and position estimation usingLiDAR data. The approach guarantees dependable and precise drone position estimation by utilizingmulti-sensor data and robust clustering methods. Fallback mechanisms are in place to ensure con-tinuous position estimation even when primary sensor data is absent. Through thorough parameteroptimization and comparative assessment, the proposed method’s effective performance in dronetracking and position estimation is demonstrated.3"
P004,"Graph Neural Networks Without Training: Harnessing the Power ofLabels as Input FeaturesAbstractThis study introduces a novel concept of training-free graph neural networks (TFGNNs) for transductive nodeclassification, which can function immediately without any training and can optionally be enhanced throughsubsequent training. Initially, we put forward the idea of using labels as features (LaF), a valid yet relativelyunexplored method in graph neural networks. Our analysis demonstrates that incorporating labels as featuressignificantly improves the representational capacity of GNNs. The design of TFGNNs is based on these findings.Empirical evaluations show that TFGNNs surpass current GNNs in scenarios where training is not performed, andwhen training is optionally applied, they achieve convergence much faster than conventional GNNs.1 IntroductionGraph Neural Networks (GNNs) have gained prominence as effective models for handling graph-structured data. They havedemonstrated impressive performance across a range of tasks, including chemical structure analysis, question answering systems,and recommender systems.A common application for GNNs is transductive node classification. In this task, the objective is to infer the labels of specific nodeswithin a graph, given the labels of other nodes. This approach finds utility in various real-world scenarios, such as classifyingdocuments, analyzing e-commerce data, and studying social networks. Several GNN architectures, including Graph ConvolutionalNetworks (GCNs) and Graph Attention Networks (GATs), have successfully addressed transductive node classification, yieldingexcellent results.A significant hurdle in the practical application of GNNs is their computational demand. Real-world graphs, such as thoserepresenting social networks or the structure of the web, can be enormous, containing billions of nodes. Processing these massivegraphs can be computationally prohibitive. While various methods have been developed to enhance the efficiency of GNNs, such asnode and edge sampling techniques, these methods still necessitate numerous training iterations. Other approaches, like PinSAGE,utilize parallel training and importance pooling to accelerate the training process, but they demand substantial computationalresources. Consequently, the immediate deployment of GNNs with limited resources remains a challenge.In this work, we introduce the concept of training-free graph neural networks (TFGNNs). To realize TFGNNs, we first propose theinnovative idea of using labels as features (LaF). In the context of transductive node classification, utilizing node labels as features isa permissible approach. GNNs employing LaF can leverage label information, like the distribution of classes among neighboringnodes, to generate node embeddings. These embeddings are richer in information compared to those derived solely from nodefeatures. We establish that incorporating labels as features demonstrably augments the expressive capability of GNNs.TFGNNs possess the unique ability to operate without any training, enabling immediate deployment upon initialization. Thiseliminates the need for extensive hyperparameter tuning when used in training-free mode. Furthermore, TFGNNs can be refinedthrough optional training. Users have the flexibility to employ TFGNNs without training or to train them for a limited number ofiterations when computational resources are constrained. This adaptability is particularly valuable in online learning scenarios,where data arrives sequentially, and the model needs to be updated promptly. TFGNNs can also undergo full training when resourcesare plentiful or when higher accuracy is paramount. In essence, TFGNNs offer the advantages of both nonparametric models andtraditional GNNs.Our experiments confirm that TFGNNs surpass existing GNNs when used without training and achieve convergence significantlyfaster than traditional GNNs when training is applied.The primary contributions of this research are outlined below:* We propose the utilization of labels as features (LaF) in transductive learning settings. * We provide formal proof that LaF enhancesthe representational power of GNNs. * We introduce a novel architecture for training-free graph neural networks (TFGNNs). * Weempirically demonstrate that TFGNNs outperform existing GNNs in the absence of training.2 Background2.1 Notations n [n] {1, 2, ..., n} VFor any positive integer , represents the set . A graph is represented by a tuple comprising (i) a set of nodes , (ii) aRT n×dE X = [x , x , ..., x ] ∈ n Yset of edges , and (iii) node features . We assume nodes are numbered from 1 to . denotes the1 2 nR|Y |y ∈ v N (v)set of possible labels. is the one-hot encoded label for node . represents the set of neighboring nodes of nodevv X X X X. We use numpy-like indexing notation. For instance, denotes the first column of , denotes the last column,:,1 :,−1 :,−5:Xdenotes the last five columns, and denotes all columns except the last five.:,:−52.2 Transductive Node Classification G = (V, E, X) V ⊂ V**Problem (Transductive Node Classification).** **Input:** A graph , a set of labeled nodes , andtrainV VY ∈ Y Y ∈ Ythe corresponding labels for these nodes. **Output:** Predicted labels for the remaining nodestrain testtrain testV = V \ V .test trainThe node classification problem has two distinct settings: transductive and inductive. In the transductive setting, a single graphis provided along with the labels for a subset of its nodes, and the task is to predict the labels for the unlabeled nodes within thesame graph. This contrasts with the inductive setting, where separate graphs are used for training and testing. For example, in thecontext of spam detection, if we label spam accounts on a social network like Facebook and then use a trained model to identifyspam accounts on the same network, this is a transductive scenario. Conversely, if we use the model trained on Facebook data toidentify spam accounts on a different platform like Twitter, this is an inductive scenario.Transductive node classification is a widely studied problem in the GNN community. It has been employed in well-known GNNmodels like GCNs and GATs and is used in popular benchmark datasets such as Cora, PubMed, and CiteSeer. This setting also hasnumerous practical applications, including document classification and fraud detection.2.3 Graph Neural NetworksGNNs are a prevalent method for solving transductive node classification problems. We adopt the message-passing framework forGNNs. A message-passing GNN can be defined as follows:(0) = x (∀v ∈ V )h ,v v (l−1)(l−1)(l)(l) |u ∈ N (v)}) (∀l ∈ [L], v ∈ V ), {h(h= fh ,uvaggv (L)) (∀v ∈ V )yˆ = f (h ,vv pred(l) l ffwhere is the aggregation function at layer , and is the prediction head, typically implemented using neural networks.agg pred3 LaF is Admissible, but Not Explored Well yWe remind the reader of the transductive node classification problem setup. We are given the node labels of the training nodes. Avx vstandard approach is to input the node features of a training node into the model, predict its label, calculate the loss based onvy y ythe true label , and update the model parameters. However, the use of is not restricted to this. We can also incorporate as av v vvfeature for node . This is the core concept behind LaF.GNNs with LaF initialize node embeddings as:(0) Rd+1+|Y |h = [x ; y˜ ] ∈ ,v v v[·; ·]where denotes vector concatenation, andy˜ = { [ 1; y ](v ∈ V )v v train1+|Y |0 (v ∈ V ),test dv 0 dis the label vector for node , and is a zero vector of dimension . LaF allows GNNs to utilize label information, such as the classdistribution in neighboring nodes, to compute node embeddings. These embeddings are likely to be more informative than thosewithout label information. LaF is considered admissible because it only uses information available in the transductive setting.We emphasize that LaF has not been thoroughly investigated in the GNN literature, despite its simplicity, with a few exceptions.For instance, GCNs and GATs use the transductive setting and could potentially use label information as features. However, they(0)h = xinitialize node embeddings as without using label information. One of the contributions of this paper is to highlight thatv vLaF is permissible in the transductive setting. 2Care must be taken when training GNNs with LaF. LaF might negatively impact generalization by creating a shortcut where the(0)hmodel simply copies the label feature to the prediction. To avoid this, we should remove the labels of the center nodes in thev,d+1: B ⊂ Vminibatch and treat them as test nodes. Specifically, if is the set of nodes in the minibatch, we settrainy˜ = { [ 1; y ](v ∈ V \ B)v v train1+|Y |0 (v ∈ V ∪ B),test yˆ v ∈ B yˆ yand predict the label for , calculating the loss based on and . This simulates the transductive setting where the labelv v vinformation of test nodes is unavailable, and GNNs learn to predict test node labels based on the label information and node featuresof surrounding nodes.4 LaF Strengthens the Expressive Power of GNNsWe demonstrate that incorporating labels as features (LaF) provably enhances the expressive capabilities of Graph Neural Networks(GNNs). Specifically, we show that GNNs utilizing LaF can effectively represent the label propagation algorithm, a crucial methodfor transductive node classification, whereas GNNs without LaF cannot achieve this. This finding is significant in its own right andprovides a strong motivation for the design of TFGNNs.Label propagation is a well-established method for transductive node classification. It operates by initiating random walks from atest node and generating the label distribution of the labeled nodes that these random walks encounter first. The following theoremestablishes that GNNs with LaF can effectively approximate label propagation.**Theorem 4.1.** GNNs with LaF can approximate label propagation with arbitrary precision. Specifically, there exists a series of(l) }{f f ϵ G = (V, E, X) V ⊂ VGNNs and such that for any positive , for any connected graph , for any labeled nodes andagg l pred train(l)(1)ZV + , f, ..., fY ∈ Y v ∈ V \ V L ∈ l(≥ L) fnode labels , and test node , there exists such that the -th GNN ( )train aggagg predtrain train ϵwith LaF outputs an approximation of label propagation with an error of at most , i.e.,LP || < ϵ||yˆ − yˆ ,1v vLPyˆ vwhere is the output of label propagation for test node .v**Proof.** We prove the theorem by construction. Letp def = v V l iPr[The random walk from node hits within steps and the first hit label is ].l,v,i trainFor labeled nodes, this is a constant:Zp = 1 (∀l ∈ , v ∈ V , i ∈ Y ).l,v,i ≥0 train[i=y ]vFor other nodes, it can be recursively computed as:p = 0 (∀v ∈ V \ V , i ∈ Y ),0,v,i train(cid:80) 1p = · p .l,v,i l−1,u,iu∈N(v) deg(v)These equations can be represented by GNNs with LaF. The base casep = { (v ∈ V )10,v,i train[i=y ]v0(v ∈ V \ V ),train (0) (l) (l−1)y˜ h f hcan be computed from in . Let always concatenate its first argument ( ) to the output so the GNN retains inputv agg vv(l) (l)f y˜ ∈ {0, 1} v V v ∈ V f 1information. handles two cases based on , indicating whether is in . If , outputs ,agg aggv,1 train train [i=y ]v(l−1) (l)y˜ h v ∈/ V f p u ∈ N (v)computable from in . If , aggregates from and averages them, as in the recursivev aggv train l−1,u,i(l)fequation, realizable by message passing in the second argument of .aggpThe final output of the GNN is . The output of label propagation can be decomposed as:l,v,iLPyˆ = iPr[The first hit label is ]v,i= p + v V l iPr[The random walk from node does not hit within steps and the first hit label is ].l,v,i trainlAs the second term converges to zero as increases, GNNs can approximate label propagation with arbitrary precision by increasingl.We then show that GNNs without LaF cannot represent label propagation. (l){f }**Proposition 4.2.** GNNs without LaF cannot approximate label propagation. Specifically, for any series of GNNs andagg lVf ϵ G = (V, E, X) V ⊂ V Y ∈ Y, there exists a positive , a connected graph , labeled nodes , node labels , and atrainpred train train(1) (l)v ∈ V \ V l f , ..., f , f ϵtest node , such that for any , the GNN ( ) without LaF has an error of at least , i.e.,agg aggtrain pred3LP||yˆ − yˆ || > ϵ,v 1vLPyˆ vwhere is the output of label propagation for test node .v G**Proof.** We construct a counterexample. Let be a cycle of four nodes numbered 1, 2, 3, 4 clockwise. All nodes have the sameTx V = {1, 2} Y = [1, 0]feature . Let and . Label propagation classifies node 4 as class 1 and node 3 as class 0. However,train trainGNNs without LaF always predict the same label for nodes 3 and 4 since they are isomorphic. Thus, for any GNN without LaF,there is an irreducible error for either node 3 or 4.Theorem 4.1 and Proposition 4.2 demonstrate that LaF provably enhances the expressive power of GNNs. These results indicate thatGNNs with LaF are more powerful than traditional message-passing GNNs like GCNs, GATs, and GINs without LaF. Notably, whileGINs are considered the most expressive message-passing GNNs, they cannot represent label propagation without LaF, whereasmessage-passing GNNs with LaF can. This does not lead to a contradiction since the original GINs do not take the label informationas input. In other words, the input domains of the functions differ. These findings highlight the importance of considering both theinput and the architecture of GNNs to maximize their expressive power.5 Training-free Graph Neural NetworksWe propose training-free graph neural networks (TFGNNs) based on the analysis in the previous section. TFGNNs can be usedwithout training and can also be improved with optional training.First, we define training-free models.**Definition 5.1 (Training-free Model).** We say a parametric model is training-free if it can be used without optimizing theparameters.It should be noted that nonparametric models are training-free by definition. The real worth of TFGNNs is that it is training-freewhile it can be improved with optional training. Users can enjoy the best of both worlds of parametric and nonparametric models bychoosing the trade-off based on the computational resources for training and the accuracy required.The core idea of TFGNNs is to embed label propagation in GNNs by Theorem 4.1. TFGNNs are defined as follows:(0) = [x ; y˜ ]h ,v v v (cid:80)(l−1)(l) (l−1)1(l) (l)+= { ReLU (S hh )(v ∈ V , l ∈ [L])W hvv u trainu∈N(v)|N(v)|(cid:80)(l−1) (l−1)1(l) (l)+ReLU (T h )(v ∈ V , l ∈ [L])W h ,v u testu∈N(v)|N(v)|(L))yˆ = sof tmax(U h ,vvThe architecture of TFGNNs is standard, i.e., TFGNNs transform the center nodes and carry out mean aggregation from theneighboring nodes. The key to TFGNNs lies in initialization. The parameters are initialized as follows:(l) (l) (l) (l)(l) S = I V = 0 T = 0 W = 0S = 0, , , , ,1+|Y |−(1+|Y |):,−(1+|Y |): −(1+|Y |): −(1+|Y |): −(1+|Y |):,:−(1+|Y |)−(1+|Y |):,:−(1+|Y |)(l)W = I U = 0 U = I, , ,1+|Y | :,:−|Y | :,−|Y |: |Y |−(1+|Y |):,−(1+|Y |): (1 + |Y |) |Y |i.e., the parameters of the last rows or rows are initialized by 0 or 1 in a special pattern (Figure 1). Other parametersare initialized randomly, e.g., by Xavier initialization. The following proposition shows that the initialized TFGNNs approximatelabel propagation.**Proposition 5.2.** The initialized TFGNNs approximate label propagation. Specifically,(L)h = pL,v,iv,−(|Y |−i+1)pholds, where is defined in Eq. (8), andL,v,iargmax yˆ = argmax pv,i L,v,ii iLPp → yˆ L → ∞holds, and as .L,v,i v,i**Proof.** By the definitions of TFGNNs,(0) yh = { (v ∈ V )trainvv,−|Y |:|Y |0 (v ∈ V ),test(l) (l−1)h = { (v ∈ V , l ∈ [L])h trainv,−|Y |: v,−|Y |:(cid:80) (l−1)1 h (v ∈ V , l ∈ [L]).testu∈N(v) u,−|Y |:|N(v)|This recursion is the same as Eqs. (9) – (13). Therefore, 4(L)h = pL,v,iv,−(|Y |−i+1)U |Y |holds. As picks the last dimensions, and softmax is monotone,argmax yˆ = argmax pv,i L,v,ii iLPp → yˆ L → ∞holds. as is shown in the proof of Theorem 4.1.L,v,i v,iTherefore, the initialized TFGNNs can be used for transductive node classification as are without training. The approximationalgorithm of label propagation is seamlessly embedded in the model parameters, and TFGNNs can also be trained as usual GNNs.6 Experiments6.1 Experimental SetupWe use the Planetoid datasets (Cora, CiteSeer, PubMed), Coauthor datasets, and Amazon datasets in the experiments. We use 20nodes per class for training, 500 nodes for validation, and the rest for testing in the Planetoid datasets following standard practice,and use 20 nodes per class for training, 30 nodes per class for validation, and the rest for testing in the Coauthor and Amazondatasets. We use GCNs and GATs for the baselines. We use three-layered models with a hidden dimension of 32 unless otherwisespecified. We train all models with AdamW with a learning rate of 0.0001 and weight decay of 0.01.6.2 TFGNNs Outperform Existing GNNs in Training-free SettingWe compare the performance of TFGNNs with GCNs and GATs in the training-free setting by assessing the accuracy of the modelswhen the parameters are initialized. The results are shown in Table 1. TFGNNs outperform GCNs and GATs in all the datasets.Specifically, both GCNs and GATs are almost random in the training-free setting, while TFGNNs achieve non-trivial accuracy. Theseresults validate that TFGNNs meet the definition of training-free models. We can also observe that GCNs, GATs, and TFGNNs donot benefit from LaF in the training-free settings if randomly initialized. These results indicate that both LaF and the initialization ofTFGNNs are important for training-free performance.Table 1: Node classification accuracy in the training-free setting. The best results are shown in bold. CS: Coauthor CS, Physics:Coauthor Physics, Computers: Amazon Computers, Photo: Amazon Photo. TFGNNs outperform GCNs and GATs in all the datasets.These results indicate that TFGNNs are training-free. Note that we use three-layered TFGNNs to make the comparison fair althoughdeeper TFGNNs perform better in the training-free setting as we confirm in Section 6.3.Cora CiteSeer PubMed CS Physics ComputersGCNs 0.163 0.167 0.180 0.079 0.101 0.023GCNs + LaF 0.119 0.159 0.407 0.080 0.146 0.061GATs 0.177 0.229 0.180 0.040 0.163 0.058GATs + LaF 0.319 0.077 0.180 0.076 0.079 0.025TFGNNs + random initialization 0.149 0.177 0.180 0.023 0.166 0.1580.600 0.362 0.413 0.601 0.717 0.730TFGNNs (proposed)6.3 Deep TFGNNs Perform Better in Training-free SettingWe confirm that deeper TFGNNs perform better in the training-free setting. We have used three-layered TFGNNs so far to makethe comparison fair with existing GNNs. Proposition 5.2 shows that the initialized TFGNNs converge to label propagation as thedepth goes to infinity, and we expect that deeper TFGNNs perform better in the training-free setting. Figure 2 shows the accuracy ofTFGNNs with different depths for the Cora dataset. We can observe that deeper TFGNNs perform better in the training-free settinguntil the depth reaches around 10, where the performance saturates. It is noteworthy that GNNs have been known to suffer from theoversmoothing problem, and the performance of GNNs degrades as the depth increases. It is interesting that TFGNNs do not sufferfrom the oversmoothing problem in the training-free setting. It should be noted that it does not necessarily mean that deeper modelsperform better in the optional training mode because the optional training may break the structure introduced by the initialization ofTFGNNs and may lead to oversmoothing and/or overfitting. We leave it as a future work to overcome these problems by adoptingcountermeasures such as initial residual and identity mapping, MADReg, and DropEdge.6.4 TFGNNs Converge FastIn the following, we investigate the optional training mode of TFGNNs. We train the models with three random seeds and report theaverage accuracy and standard deviation. We use baseline GCNs without LaF (i.e., the original GCNs) as the baseline.First, we confirm that TFGNNs in the optional training mode converge faster than GCNs. We show the training curves of TFGNNsand GCNs for the Cora dataset in Figure 3. TFGNNs converge much faster than GCNs. We hypothesize that TFGNNs converge5faster because the initialized TFGNNs are in a good starting point, while GCNs start from a completely random point and requiremany iterations to reach a good point. We can also observe that fully trained TFGNNs perform on par with GCNs. These resultsindicate that TFGNNs enjoy the best of both worlds: TFGNNs perform well without training and can be trained faster with optionaltraining.6.5 TFGNNs are Robust to Feature NoiseAs TFGNNs use both node features and label information while traditional GNNs rely only on node features, we expect thatTFGNNs are more robust to feature noise than traditional GNNs. We confirm this in this section. We add i.i.d. Gaussian noise withσstandard deviation to the node features and evaluate the accuracy of the models. We train TFGNNs and GCNs with the Coradataset. The results are shown in Figure 4. TFGNNs are more robust to feature noise especially in high noise regimes where theperformance of GCNs degrades significantly. These results indicate that TFGNNs are more robust to i.i.d. Gaussian noise to thenode features than traditional GNNs.7 Related Work7.1 Labels as Features and Training-free GNNsThe most relevant work is by Wang et al., who proposed to use node labels in GNNs. This technique was also used by Addanki et al.and analyzed by Wang et al. The underlying idea is common with LaF, i.e., use of label information as input to transductive GNNs.A similar result as Theorem 4.1 was also shown in Wang et al. However, the focus is different, and there are different points betweenthis work and theirs. We propose the training-free + optional training framework for the first time. The notable characteristics ofGNNs are (i) TFGNNs receive both original features and LaF, (ii) TFGNNs can be deployed without training, and (iii) TFGNNs canbe improved with optional training. Besides, we provide detailed analysis and experiments including the speed of convergence andnoise robustness. Our results provide complementary insights to the existing works.Another related topic is graph echo state networks, which lead to lightweight models for graph data. The key idea is to use randomlyinitialized fixed weights for aggregation. The main difference is that graph echo state networks still require to train the output layer,while TFGNNs can be used without training. These methods are orthogonal, and it is an interesting direction to combine them tofurther improve the performance.7.2 Speeding up GNNsVarious methods have been proposed to speed up GNNs to handle large graph data. GraphSAGE is one of the earliest methods tospeed up GNNs. GraphSAGE employs neighbor sampling to reduce the computational cost of training and inference. It samples afixed number of neighbors for each node and aggregates the features of the sampled neighbors. An alternative sampling method islayer-wise sampling introduced in FastGCN. Huang et al. further improved FastGCN by using an adaptive node sampling techniqueto reduce the variance of estimators. LADIES combined neighbor sampling and layer-wise sampling to take the best of both worlds.Another approach is to use smaller training graphs. ClusterGCN uses a cluster of nodes as a mini-batch. GraphSAINT samplessubgraphs by random walks for each mini-batch.It should also be noted that general techniques to speed up neural networks, such as mixed-precision training, quantization, andpruning can be applied to GNNs.These methods mitigate the training cost of GNNs, but they still require many training iterations. In this paper, we proposetraining-free GNNs, which can be deployed instantly as soon as the model is initialized. Besides, our method can be improved withoptional training. In the optional training mode, the speed up techniques mentioned above can be combined with our method toreduce the training time further.7.3 Expressive Power of GNNsExpressive power (or representation power) means what kind of functional classes a model family can realize. The expressive powerof GNNs is an important field of research in its own right. If GNNs cannot represent the true function, we cannot expect GNNs towork well however we train them. Therefore, it is important to elucidate the expressive power of GNNs. Originally, Morris et al. andXu et al. showed that message-passing GNNs are at most as powerful as the 1-WL test, and they proposed k-GNNs and GINs, whichare as powerful as the k-(set)WL and 1-WL tests, respectively. GINs are the most powerful message-passing GNNs. Sato and Loukasshowed that message-passing GNNs are as powerful as a computational model of distributed local algorithms, and they proposedGNNs that are as powerful as port-numbering and randomized local algorithms. Loukas showed that GNNs are Turing-completeunder certain conditions (i.e., with unique node ids and infinitely increasing depths). Some other works showed that GNNs cansolve or cannot solve some specific problems, e.g., GNNs can recover the underlying geometry, GNNs cannot recognize bridges andarticulation points. There are various efforts to improve the expressive power of GNNs by non-message-passing architectures. Werefer the readers to survey papers for more details on the expressive power of GNNs.6We contributed to the field of the expressive power of GNNs by showing that GNNs with LaF are more powerful than GNNs withoutLaF. Specifically, we showed that GNNs with LaF can represent an important model, label propagation, while GNNs without LaFcannot. It should be emphasized that GINs, the most powerful message-passing GNNs, and Turing-complete GNNs cannot representlabel propagation without LaF because they do not have access to the label information label propagation uses, and also noted thatGINs traditionally do not use LaF. This result indicates that it is important to consider what to input to the GNNs as well as thearchitecture of the GNNs for the expressive power of GNNs. This result provides a new insight into the field of the expressive powerof GNNs.8 LimitationsOur work has several limitations. First, LaF and TFGNNs cannot be applied to inductive settings while most GNNs can. We do notregard this as a negative point. Popular GNNs such as GCNs and GATs are applicable to both transductive and inductive settings andare often used for transductive settings. However, this also means that they do not take advantage of transductive-specific structures(those that are not present in inductive settings). We believe that it is important to exploit inductive-specific techniques for inductivesettings and transductive-specific techniques (such as LaF) for transductive settings in order to pursue maximum performance.Second, TFGNNs cannot be applied to heterophilious graphs, or its performance degrades as TFGNNs are based on label propagation.The same argument mentioned above applies. Relying on homophilious graphs is not a negative point in pursuing maximumperformance. It should be noted that LaF may also be exploited in heterophilious settings as well. Developing training-free GNNsfor heterophilious graphs based on LaF is an interesting future work.Third, we did not aim to achieve the state-of-the-art performance. Exploring the combination of LaF with fancy techniques toachieve state-of-the-art performance is left as future work.Finally, we did not explore applications of LaF other than TFGNNs. LaF can help other GNNs in non-training-free settings as well.Exploring the application of LaF to other GNNs is left as future work.9 ConclusionIn this paper, we made the following contributions.* We advocated the use of LaF in transductive learning (Section 3). * We confirmed that LaF is admissible in transductive learning,but LaF has not been explored in the field of GNNs such as GCNs and GATs. * We formally showed that LaF strengthens theexpressive power of GNNs (Section 4). * We showed that GNNs with LaF can represent label propagation (Theorem 4.1) whileGNNs without LaF cannot (Proposition 4.2). * We proposed training-free graph neural networks, TFGNNs (Section 5). * Weshowed that TFGNNs defined by Eqs. (19) – (29) meet the requirementsarticle graphicx7"
P007,"Joint Syntacto-Discourse Parsing and theSyntacto-Discourse TreebankAbstractDiscourse parsing has long been treated as a stand-alone problem independent fromconstituency or dependency parsing. Most attempts at this problem are pipelinedrather than end-to-end, sophisticated, and not self-contained: they assume gold-standard text segmentations (Elementary Discourse Units), and use external parsersfor syntactic features. In this paper we propose the first end-to-end discourseparser that jointly parses in both syntax and discourse levels, as well as the firstsyntacto-discourse treebank by integrating the Penn Treebank with the RST Tree-bank. Built upon our recent span-based constituency parser, this joint syntacto-discourse parser requires no preprocessing whatsoever (such as segmentation orfea- ture extraction), achieves the state-of-the- art end-to-end discourse parsingaccuracy.1 IntroductionDistinguishing the semantic relations between segments in a document can be greatly beneficial tomany high-level NLP tasks, such as summarization, sentiment analysis, question answering, andtextual quality evaluation.There has been a variety of research on discourse parsing. But most of them suffer from the followinglimitations:1. pipelined rather than end-to-end: they assume pre-segmented discourse, and worse yet, usegold-standard segmentations2. not self-contained: they rely on external syntactic parsers and pretrained word vectors;3. complicated: they design sophisticated features, including those from parse-trees.We argue for the first time that discourse parsing should be viewed as an extension of, and beperformed in conjunction with, constituency parsing. We propose the first joint syntacto-discoursetree- bank, by unifying constituency and discourse tree representations. Based on this, we proposethe first end-to-end incremental parser that jointly parses at both constituency and discourse levels.Our algo- rithm builds up on the span-based parser; it employs the strong general- ization powerof bi-directional LSTMs, and parses efficiently and robustly with an extremely simple span-basedfeature set that does not use any tree structure information.We make the following contributions:1. We develop a combined representation of constituency and discourse trees to facilitateparsing at both levels without explicit conver- sion mechanism. Using this representation,we build and release a joint treebank based on the Penn Treebank and RST Treebank.2. We propose a novel joint parser that parses at both constituency and discourse levels.3. Even though it simultaneously performs con- stituency parsing, our parser does not use anyexplicit syntactic feature, nor does it need any binarization of discourse trees, thanks to thepowerful span-based framework.4. Empirically, our end-to-end parser outperforms the existing pipelined discourse pars- ingefforts. When the gold EDUs are pro- vided, our parser is also competitive to other existingapproaches with sophisticated fea- tures.2 Combined Representation & TreebankWe first briefly review the discourse structures in Rhetorical Structure Theory, and then discuss how tounify discourse and constituency trees, which gives rise to our syntacto-discourse treebank PTB-RST.2.1 Review: RST Discourse StructuresIn an RST discourse tree, there are two types of branchings. Most of the internal tree nodes are binarybranching, with one nucleus child containing the core semantic meaning of the current node, andone satellite child semantically decorating the nucleus. Like dependency labels, there is a relationannotated between each satellite-nucleus pair, such as “Background” or “Purpose”. There are alsonon- binary-branching internal nodes whose children are conjunctions, e.g., a “List” of semanticallysimilar EDUs (which are all nucleus nodes).2.2 Syntacto-Discourse RepresentationIt is widely recognized that lower-level lexical and syntactic information can greatly help determin-ing both the boundaries of the EDUs (i.e., dis- course segmentation) as well as the semantic relationsbetween EDUs. While these previous approaches rely on pre-trained tools to provide both EDUsegmentation and intra-EDU syntactic parse trees, we in- stead propose to directly determine thelow-level segmentations, the syntactic parses, and the high- level discourse parses using a single jointparser. This parser is trained on the combined trees of constituency and discourse structures.We first convert an RST tree to a format similar to those constituency trees in the Penn Treebank. Foreach binary branching node with a nucleus child and a satellite child, we use the relation as the label← →of the converted parent node. The nucleus/satellite relation, along with the direction (either or ,pointing from satellite to nucleus) is then used as the label. For a conjunctive branch (e.g. “List”), wesimply use the relation as the label of the converted node.After converting an RST tree into the constituency tree format, we then replace each leaf node (i.e.,EDU) with the corresponding syntactic (sub)tree from PTB. Given that the sentences in the RSTTreebank is a subset of that of PTB, we can always find the corresponding constituency subtrees foreach EDU leaf node. In most cases, each EDU corresponds to one sin- gle (sub)tree in PTB, since thediscourse bound- aries generally do not conflict with constituencies. In other cases, one EDU nodemay correspond to multiple subtrees in PTB, and for these EDUs we use the lowest common ancestorof those subtrees in the PTB as the label of that EDU in the con- verted tree. E.g., if C–D is one EDU→in the PTB tree A it might be converted to Purpose DCB A based on the Penn Treebank and RSTTreebank. This PTB-RST treebank is released as a set of tools to generate the joint trees given PennTree- bank and RST Treebank data. During the align- ment between the RST trees and the PTB trees,we only keep the common parts of the two trees.We follow the standard training/testing split of the RST Treebank. In the training set, there are 347joint trees with a total of 17,837 tokens, and the lengths of the discourses range from 30 to 2,199tokens. In the test set, there are 38 joint trees with a total of 4,819 tokens, and the lengths vary from45 to 2,607. Figure 3 shows the distribution of the discourse lengths over the whole dataset, which onaverage is about 2x of PTB sen- tence length, but longest ones are about 10x the longest lengths inthe Treebank.3 Joint Syntacto-Discourse ParsingGiven the combined syntacto-discourse treebank, we now propose a joint parser that can performend-to-end discourse segmentation and parsing. 23.1 Extending Span-based ParsingAs mentioned above, the input sequences are sub- stantially longer than PTB parsing, so we chooselinear-time parsing, by adapting a popular greedy constituency parser, the span-based constituencyparser.3.2 Joint PTB-RST TreebankUsing the conversion strategy described above we build the first joint syntacto-discourse treebank.As in span-based parsing, at each step, we main- tain a a stack of spans. Notice that in conventionalincremental parsing, the stack stores the subtrees constructed so far, but in span-based constituencyparsing, the stack only stores the boundaries of subtrees, which are just a list of indices ...i k j. Inother words, quite shockingly, no tree structure is represented anywhere in the parser.Similar to span-based constituency parsing, we alternate between structural (either shift or combine)and label (labelX or nolabel) actions in an odd-even fashion. But different from previous work, after astructural action, we choose to keep the last branching point k, i.e., i k j (mostly for combine, but alsotrivially for shift). This is because in our parsing mechanism, the dis- course relation between twoEDUs is actually de- termined after the previous combine action. We need to keep the splitting pointto clearly find the spans of the two EDUs to determine their relations. This midpoint k disappearsafter a label ac- tion; therefore we can use the shape of the last span on the stack (whether it containsthe split point, i.e., i k j or i j) to determine the par- ity of the step and thus no longer need to carry thestep z in the state .The nolabel action makes the binarization of the discourse/constituency tree unnecessary, becausenolabel actually combines the top two spans on the stack into one span, but without annotating thenew span a label. This greatly simplifies the pre- processing and post-processing efforts needed.Prec. Recall F1Constituency 87.6 86.9 87.2Discourse 46.5 40.2 43.0Overall 83.5 81.6 82.5Table 1: Accuracies on PTB-RST at constituency and discourse levels.3.3 Recurrent Neural Models and TrainingThe scoring functions in the deductive system are calculated by an underlying neu- ral model, whichis similar to the bi-directional LSTM model that evaluates based on span boundary features. Again, itis important to note that no discourse or syntactic tree structures are represented in the features.During the decoding time, a document is first passed into a two-layer bi-directional LSTM model,then the outputs at each text position of the two layers of the bi-directional LSTMs are con- catenatedas the positional features. The spans at each parsing step can be represented as the fea- ture vectorsat the boundaries. The span features are then passed into fully connected networks with softmax tocalculate the likelihood of performing the corresponding action or marking the cor- responding label.We use the “training with exploration” strategy and the dynamic oracle mechanism to make sure themodel can handle unseen parsing configurations properly.4 ExperimentsWe use the treebank described in Section 2 for empirical evaluation. We randomly choose 30documents from the training set as the development set.We tune the hyperparameters of the neural model on the development set. For most of the hyperpa-rameters we settle with the same values sug- gested previously. To alleviate the overfitting problemfor training on the relative small RST Treebank, we use a dropout of 0.5.3One particular hyperparameter is that we use a value to balance the chances between trainingfollowing the exploration (i.e., the best action cho- sen by the neural model) and following the correctpath provided by the dynamic oracle. We find that = 0.8, i.e., following the dynamic oracle with aprobability of 0.8, achieves the best performance. One explanation for this high chance to follow theoracle is that, since our combined trees are significantly larger than the constituency trees in PennTreebank, lower makes the parsing easily divert into wrong trails that are difficult to learn from.Since our parser essentially performs both constituency parsing task and discourse parsing task. Wealso evaluate the performances on sentence constituency level and discourse level separately. Theresult is shown in Table 1. Note that in constituency level, the accuracy is not directly comparablewith the accuracy reported previously, since: a) our parser is trained on a much smaller dataset (RSTTreebank is about 1/6 of Penn Treebank); b) the parser is trained to optimize the discourse-levelaccuracy.Table 2 shows that, in the perspective of end- to-end discourse parsing, our parser first outper- formsthe state-of-the-art segmentator, and furthermore, in end-to-end pars- ing, the superiority of our parseris more pronounced comparing to the previously best parser.On the other hand, the majority of the conven- tional discourse parsers are not end-to-end: they relyon gold EDU segmentations and pre-trained tools like Stanford parsers to generate features. Weperform an experiment to compare the per- formance of our parser with them given the gold EDUsegments (Table 3). Note that most of these parsers do not handle multi-branching discourse nodesand are trained and evaluated on binarized discourse trees, so their performances are actually notdirectly comparable to the results we reported.description syntactic feats. segmentation structure +nuclearity +relationsegmentation only Stanford 95.1 - - -end-to-end pipeline Penn Treebank 94.0 72.3 59.1 47.3joint syntactic & discourse parsing - 95.4 78.8 65.0 52.2Table 2: F1 scores of end-to-end systems. “+nuclearity” indicates scoring of tree structures with←nucle- arity included. “+relation” has both nuclearity and relation included (e.g., Elaboration).syntactic feats structure +nuclearity +relationhuman annotation - 88.7 77.7 65.86*sparse Penn Treebank 83.0 68.4 54.8Charniak (retrained) 82.7 68.4 55.7Charniak (retrained) - - 57.3Stanford 85.7 71.0 58.2ZPar (retraied) 83.5 68.1 55.1Stanford 86.0 72.4 59.75*neural 82.4 69.2 56.8+ sparse features Stanford 84.0 70.8 58.6MALT 80.5 68.6 58.3+ sparse features MALT 81.6 71.1 61.8span-based discourse parsing - 84.2 67.7 56.0Table 3: Experiments using gold segmentations. The column of “syntactic feats” shows how thesyntactic features are calculated in the corresponding systems. Note that our parser predicts solelybased on the span features from bi-directionaly LSTM, instead of any explicitly designed syntacticfeatures.5 ConclusionWe have presented a neural-based incremental parser that can jointly parse at both constituency anddiscourse levels. To our best knowledge, this is the first end-to-end parser for discourse parsing task.4Our parser achieves the state-of-the-art per- formance in end-to-end parsing, and unlike previ- ousapproaches, needs little pre-processing effort. 5"
P011,"Controlling False Discovery Rates in Detecting HeterogeneousTreatment Effects for Online ExperimentsAbstractOnline controlled experiments, commonly referred to as A/B testing, are widely used in many Internet companiesfor data-driven decision-making regarding feature modifications and product releases. However, a significantchallenge remains in methodically evaluating how each code or feature change affects millions of users whoexhibit considerable heterogeneity across various dimensions such as countries, ages, and devices. The AverageTreatment Effect (ATE) framework, which is the foundation of the A/B testing approach used by many companies,is unable to identify the heterogeneity of treatment effects on users with varying characteristics. This paperintroduces statistical techniques designed to systematically and precisely pinpoint the Heterogeneous TreatmentEffect (HTE) within any specific user cohort, like mobile device type or country. Additionally, these methods helpdetermine which user factors, such as age or gender, contribute to the variability in treatment effects observedduring an A/B test. Through the application of these methods to both simulated and real-world experimentaldata, we demonstrate their robust performance in maintaining a controlled, low False Discovery Rate (FDR).Simultaneously, they offer valuable insights into the heterogeneity of identified user groups. We have implementeda toolkit based on these methods and utilized it to assess the HTE across numerous A/B tests at Snap.1 IntroductionControlled experiments, also known as A/B testing, have become a standard method for assessing and enhancing new productconcepts across internet companies. Numerous IT companies, possessing extensive and large-scale data, have developed internalA/B testing platforms to address their intricate experimentation requirements. At Snap, the utilization of A/B testing has substantiallyincreased in the last two years. The in-house platform currently manages hundreds of concurrent experiments at any moment. Eachexperiment automatically generates results for hundreds to thousands of varied online metrics.As experimentation gains popularity, there is an increasing demand for experimenters to understand not only the overall impacton metrics in an A/B test but also the reasons behind metric changes and the specific user segments driving these changes. Suchinsights into user heterogeneity can assist experimenters in devising strategies to enhance the product. For instance, in a recentexperiment, we observed that a decline in a metric was primarily influenced by users with the highest number of snap views. Thisobservation led us to concentrate on understanding the engineering and design aspects when a user has a large number of snap stacksto load. Consequently, we were able to pinpoint a significant performance problem that was causing the metric to drop. Indeed, wehave encountered numerous instances where users react differently to the same experimental treatment.Furthermore, the abundance of data presents a significant risk of false discoveries, often due to a statistical phenomenon referred toas ""multiple testing"". Given the hundreds of thousands of user characteristics available to internet companies, user groups can beformed in millions of different ways. If a ""naive"" approach is taken, simply calculating and comparing the estimated effect based onusers within groups, it is easy to find groups with treatment effects that significantly deviate from the average, regardless of whetheractual heterogeneity exists.The objective of our work is to bridge this gap by offering rigorous statistical methods and a toolkit capable of detecting HeterogeneousTreatment Effects (HTE) while addressing the potential issue of multiple testing by controlling the false positive rate (FDR). Thistoolkit has been deployed and is in use at Snap. In this paper, we explore the rationale for using FDR and contrast two statisticalmethods that manage FDR, using both simulated results and actual experimental data. Based on the methods selected, we willdiscuss solutions to two questions that experimenters and practitioners are keen to understand regarding HTE:How to systematically identify which subgroups of users (e.g., countries) exhibit treatment effects significantly different• from the Average Treatment Effect in an A/B test.• How to rigorously determine which factors (e.g., age, gender) contribute to the heterogeneity of the treatment effect in anA/B test.Our contributions in this paper are summarized as follows:• We frame the HTE detection problem as an FDR control issue and elaborate on why controlling FDR is crucial in large-scaleHTE detection in practical applications.• We employ two methods capable of controlling FDR in our HTE detection process and provide insightful comparisons ofthese methods using both simulation and real-world empirical data.• We discuss two significant lessons learned, concerning (1) the distinction between heterogeneity in the population andheterogeneity in treatment effects, and (2) the scalability of the algorithms. These insights are intended to help practitionersavoid similar pitfalls.2 Methodology2.1 Average Treatment Effect vs. Heterogeneous Treatment EffectIn an A/B test, users are randomly divided into a treatment group and a control group, and the metrics of interest are observedfor all users. The Rubin Causal Model is frequently employed in A/B testing as a statistical framework for causal inference. LetY (T ) i T = 1 i T = 0 irepresent the potential outcome for the -th user, where if the -th user is in the treatment group and if the -thi i i iτ = Y (1) − Y (0) iuser is in the control group. Consequently, denotes the causal effect of the treatment for the -th unit, and thei i iτ¯average causal effect across all users, , is defined as the Average Treatment Effect (ATE). It is important to note that the ATE isY (0) Y (1)not directly observable since and cannot be known simultaneously. This is recognized as the ""fundamental problem ofi i Y |T = 1 − Y |T = 0causal inference"". However, the estimator is unbiased for the ATE when two specific assumptions are meti i i iand is commonly used to estimate the ATE in A/B testing.Assumption 1. Stable Unit Treatment Value Assumption (SUTVA): T = 1 T = 0• There is only one version of treatment and control, meaning there is only one version of and .• The treatment applied to one user does not affect the outcome of another user (no interference).T (Y (0), Y (1)) X XAssumption 2. Unconfoundedness: is independent of given , where is a set of pre-treatment variables for thei i i i ii-th user, such as age, gender, country, etc.However, analysis based solely on ATE is sometimes insufficient for obtaining precise and meaningful insights. As mentioned earlier,we have observed numerous cases where a single feature change can impact different users differently. The estimation of ATE isnot an effective measure for a heterogeneous population, as it may exaggerate the treatment effect for one sub-population whileunderestimating it for another. To investigate heterogeneous treatment effects, it is necessary to consider the conditional averageτ (x) = E[Y (1) − Y (0)|X = x] X itreatment effect, defined as: , where represents a set of pre-treatment variables for the -th user.i i i iτ (x) xAccurately estimating the conditional average treatment effect for all values of is highly beneficial for detecting heterogeneousτ (x) xtreatment effects because provides the conditional average treatment effect for the subpopulation defined by the covariates .τ (x)For instance, if the covariate is ’country’, the covariate space can be partitioned into countries, and represents the conditionalx τ (x) τ¯ xaverage treatment effect for users in country . If is statistically different from the average treatment effect , then country isconsidered heterogeneous.There is a growing need for rigorous analysis based on heterogeneous treatment effects (HTE), which motivates us to develop arobust statistical approach for HTE detection.2.2 Naive Approaches and their CaveatsIn this section, we outline some prevalent practices used by practitioners that could result in the spurious discovery of HTE. Supposewe have users from various countries and wish to identify which countries exhibit treatment effects different from the ATE for aparticular metric. A straightforward approach to detect heterogeneous countries involves first conducting a two-sample t-test on theobservations from each country to obtain a two-sided p-value for each country, and then selecting countries with a p-value less than0.05 as the result. We will refer to this method as the ""naive approach"".This naive approach is simple and may appear intuitive to non-statisticians. However, it is susceptible to the multiple testing problem.We demonstrate this issue with a basic simulation:• Step 1: Assess treatment effects for all users in 30 randomly generated subgroups from a standard Gaussian distribution,ensuring the true ATE is zero.• Step 2: Implement the naive approach and identify subgroups with p-values below 0.05 as heterogeneous.In this simulation, 3 out of 30 subgroups are identified as having heterogeneous treatment effects, despite the ATE estimator being 0,indicating no actual heterogeneity among the subgroups.The Bonferroni correction method can be employed to address the multiple testing problem by controlling the family-wise error rate(FWER). The FWER is the probability of rejecting at least one true hypothesis. Nevertheless, the Bonferroni method is known to be2H H Hhighly conservative, resulting in a high rate of false negatives and low statistical power, defined as P(reject | ), where is0 1 0Hthe null hypothesis and is the alternative hypothesis.12.3 False Discovery Rate Controlled HTE DetectionDue to the limitations of the methods discussed in the previous section, we introduce methods for HTE detection that addressthe multiple testing problem while maintaining sufficient statistical power. To manage the multiple testing issue and reduceconservativeness, Benjamini and Hochberg introduced the concept of the false discovery rate (FDR), which is defined as follows:Definition 3.1. False Discovery Rate: Let Q be the proportion of false positives among all detected (rejections of the null hypothesis).F DR = E[Q]Then .To control the FDR, it is necessary to manage the expected proportion of discoveries that are false. Additionally, methods that controlthe FDR are generally much less conservative than the Bonferroni method. Therefore, in our proposed HTE detection approach, wecan control the FDR and ensure adequate power simultaneously.2.4 Detection for Heterogeneous SubgroupsWhen conducting an A/B testing experiment, it is often important to identify which subgroups of users exhibit treatment effectsdifferent from the ATE. For example, at Snap, with users from over 200 countries, we are interested in determining which countrieshave higher or lower treatment effects compared to the average for the metric of interest.In this process, it is crucial to minimize the number of false discoveries in our results. To achieve this, we utilize the Benjamini-Hochberg (BH) procedure to control the FDR. The BH procedure is known to control the FDR if the test statistics are independent orsatisfy the positive regression dependence on a subset property. It is one of the most widely used FDR control methods due to itsH , ..., Hsimplicity. For instance, suppose we have p-values from m independent hypothesis tests ranked in ascending order:1 m kp , ..., p p ≤ q, and we aim to control the FDR at level q. The BH procedure identifies the largest k such that and rejects(1) (m) (k) mH i ≤ kthe null hypothesis for all where . By doing so, it theoretically ensures that the FDR is controlled below q.(i)To detect heterogeneous subgroups, it is necessary to estimate the conditional average treatment effects defined in equation (3) forthe subgroups. Although individual treatment effect values are not available due to the fundamental problem of causal inference, weobsYcan construct a transformed outcome (TO) for each user as an alternative measure of individual treatment effect. Let be theiiobserved outcome for the -th unit. Additionally, let p be the assignment probability, which, in practice, is the traffic percentage∗i Yassigned to the treatment group in an A/B test. The transformed outcome for the -th unit, , is then defined as:i(T −p)∗ obsY = Y × i .i i p(1−p) ∗E[Y |X = x]A beneficial property of the TO is that, under the unconfoundedness assumption, the conditional expectation equalsiiτ (x)the conditional average treatment effect .We propose the following method, which combines the BH method and Transformed Outcome, to detect heterogeneous subgroups.Suppose we have n users from p subgroups, and we want to identify subgroups with heterogeneous treatment effects that differ fromthe average treatment effect with a controlled FDR. We propose the following procedure, which we call the HTE-BH method:n × p X = 1 i j• Step 1: Create an design matrix X such that if the -th user belongs to the -th subgroup.i,j∗Y• Step 2: Compute the transformed outcomes for all users based on the formula in Equation (5), and then subtract the¯ ¯Y (1) − Y (0)estimated ATE, , from all transformed outcomes. Let Y be the vector of the resulting outcomes.• Step 3: Perform a linear regression using Y as the response and X as the design matrix, and obtain the p-values for thecoefficient estimates corresponding to all subgroups.• Step 4: Apply the BH procedure to the p-values to finalize the list of selected heterogeneous subgroups.The design matrix X created in Step 1 is orthogonal in this scenario, so the p-values derived from the linear regression are independent.Consequently, the BH procedure can control the FDR at a pre-specified level q. In Step 2, we subtract the estimated ATE from thetransformed outcomes to detect subgroups with treatment effects different from the ATE. For simplicity, we treat the estimatedATE as a parameter. Although this overlooks the fact that the estimated ATE is a random variable, it has practical relevance aspractitioners are typically interested in observing which subgroups are statistically different from the observed average treatmenteffect across all users in an experiment. Note that obtaining p-values in the manner described in Step 3 is equivalent to obtainingp-values from running independent t-tests for all subgroups.2.5 Detection for Heterogeneous FactorsIn addition to detecting heterogeneous subgroups, identifying the factors that contribute to the heterogeneity of treatment effects isanother crucial task in practice. At Snap, we have anonymously constructed hundreds of user properties, including demographicinformation such as age and gender, as well as user engagement levels, such as how users interact with snaps, stories, or discover.3Often, when presented with subtle experimental results, we are unsure which of these factors to investigate further. By pinpointingthe factors contributing to the heterogeneity in treatment effects, we can more effectively delve into the relevant factors and deriveinsights. The HTE-BH method is straightforward and easy to implement for detecting heterogeneous subgroups but is not suitablefor detecting heterogeneous factors because, in this case, we cannot construct an orthogonal design matrix in Step 1 of the HTE-BHmethod. Therefore, we propose using the ’Knockoff’ method to control the FDR for heterogeneous factors.The ’Knockoff’ is a recently proposed FDR control method. Suppose the response of interest, y, follows the classical linear model:n n×py = Xβ + ϵ y ∈ R X ∈ R β, where is a vector of y, is any fixed design matrix, is a vector of unknown coefficients, and2ϵ ∼ N (0, σ I) is Gaussian error. Note that n is the number of observations and p is the number of variables. For the Knockoffn ≥ 2pmethod, we assume that , which is reasonable in practice because we are likely to have more observations than variables inmost A/B tests.TΣ = X XLet after normalizing X. The ’Knockoff’ procedure can be summarized in three steps:˜ ˜ ˜ ˜ ˜T T TX X X X = X X = Σ X X = Σ − diags• Step 1: Construct a ’knockoff’ matrix of X such that satisfies: , , where s isa non-negative vector that we will construct. ˜W (X , X ) WStep 2: Compute a statistic for each pair such that a large positive value of provides evidence against the• j j j jnull hypothesis that the j-th variable is not included in the true model. ˆS := {j : W ≥ T }Step 3: Calculate a data-dependent threshold T such that the FDR of the knockoff selection set is less• jthan or equal to the pre-specified level q.In our proposal, we use the equi-correlated method to obtain the non-negative vector s used in Step 1 to construct the knockoff˜X s = min{2λ (Σ), 1} λmatrix . The equi-correlated method suggests using for all j, where is the smallest eigenvalue ofj min min˜ ˜ ˜ ˜−1Σ X X = X(I − Σ diags) + U C U n × p. After obtaining this s, we construct using the formula: , where is an orthonormal˜ T T −1U X = 0 C C = 2diags − diagsΣ diagsmatrix satisfying , and C is a Cholesky decomposition satisfying .WThere are numerous options available for computing the statistics ’s in Step 2. We choose to use Lasso to compute the statisticsj˜∗ n×2p ∗ 2W X = [XX] ∈ R minimize ||y − X β|| + λ||β||’s. Let be the augmented design matrix. Recall the Lasso problem: .j β 12Z = sup{λ : β (λ) ̸= 0} λDefine , which is the largest tuning parameter that first allows the j-th variable to enter thej j(Z , Z ) Wmodel. Note that is a pair corresponding to the j-th original variable and its knockoff. We then calculate as:j j+p jW = (Z − Z ) × sign(Z − Z ) j = 1, ..., p, for .j j j+p j j+p 1+#{j:W ≤−t}j{|W |, ..., |W |} \ {0} T = min{t ∈ W : ≤ q}Let W be the set . In Step 3, it is proposed to use the threshold: .1 p #{j:W ≥t}jˆS := {j : W ≥ T }Theorem 2 claims that the knockoff selection set is theoretically guaranteed to have an FDR less than q.jWe propose the following procedure to detect the variables that contribute to the heterogeneity in treatment effects while controllingthe FDR. We call this the HTE-Knockoff method:• Step 1: Construct a design matrix X based on the set of pre-treatment variables.∗Y• Step 2: Calculate the transformed outcomes for all users based on the formula in Equation (5), and then subtract the¯ ¯Y (1) − Y (0)estimated ATE, , from all transformed outcomes. Let Y be the vector of the resulting outcomes.˜X• Step 3: Create a knockoff matrix of X. ˜∗X = [XX]• Step 4: Run a Lasso regression using Y as the response and as the design matrix.• Step 5: Follow the procedure of the Knockoff method to obtain the knockoff selection set of heterogeneous variables.Note that our proposed HTE-Knockoff method can also detect heterogeneous subgroups because it works for any full-rank designXmatrix, regardless of orthogonality. Additionally, the HTE-Knockoff method is applicable when is a set of variables includingiboth categorical and continuous variables, but we need to be careful in constructing the design matrix when there are more than oneXcategorical variables in .i3 ResultsWe apply the HTE-BH and HTE-Knockoff methods to two real experimental datasets. In the first experiment, both methods yieldnearly identical selections for heterogeneous subgroups. If we were to use the naive approach, it would select many more subgroups,clearly indicating numerous false positives. The HTE results reveal drastically different effects in English-speaking countries versusnon-English-speaking countries. Retrospectively, we understood that the new layout in the experiment favored non-English contentwhile suppressing high-quality content in English.In the second experiment, the HTE-BH method selects one subgroup as heterogeneous, whereas the HTE-Knockoff method selectsnone. This likely represents a scenario where the true treatment effects are too small to be detected, causing the HTE-Knockoff4method to be more conservative than the HTE-BH method to avoid making any false positives. This observation aligns with thesimulation results.4 ConclusionIn this paper, we propose the HTE-BH method for detecting heterogeneous subgroups with treatment effects different from theaverage, and the HTE-Knockoff method for identifying factors contributing to the heterogeneity in treatment effects. While theHTE-BH method is easier to implement, the HTE-Knockoff method has a broader application as it can also be used to detectheterogeneous factors. Our proposed methods demonstrate good detection power while addressing the multiple testing problem bycontrolling the FDR level.Despite their wide application scenarios, our current methods have some limitations and could be improved in future research. Thefirst limitation is the assumption that the true model is a linear regression model with Gaussian error; the theoretical propertiesof the original Knockoff method are based on this assumption. Although we show that the Knockoff method can still performwell in controlling FDR in some non-Gaussian error cases, there is no theoretical proof for such robustness. Additionally, thetrue relationship between the treatment effect and the variables may not always be linear, making the use of linear regressioninappropriate. Recently, a model-free knockoff method has been proposed, which, under certain conditions, can work on any kind ofnon-linear model. This idea could be useful if we aim to extend the HTE-Knockoff procedure to a more generalized setting in futurework.Another unresolved issue is scalability. We attempted to use the transformed design matrix to conduct HTE detection on multipleexperiments, but this resulted in increased computational complexity. This problem warrants further investigation because mostcompanies have a large number of A/B test results available, and it is not feasible to apply the HTE detection method to eachexperiment individually. 5"
P012,"Harmonizing Scaling Laws: Bridging the GapBetween Kaplan and ChinchillaAbstractStudies by Kaplan et al. (2020) and Hoffmann et al. (2022) examined the scalingcharacteristics of transformers in next-token language prediction, yielding differentrecommendations for configuring the number of parameters (N) and training tokens(D) to minimize loss within a set compute budget (C). Kaplan suggested an optimal0.73∝parameter count scaling with Noptimal C , whereas Chinchilla proposed0.50∝Noptimal C . This paper demonstrates that a significant portion of thisdifference can be traced back to Kaplan’s focus on non-embedding parameters,rather than the total parameter count, along with their study’s concentration on asmaller scale. When the Chinchilla study is simulated under similar circumstances,biased scaling coefficients similar to those of Kaplan are produced. As a result, thiswork confirms Chinchilla’s scaling coefficients by clarifying the primary reason forKaplan’s initial overestimation. Additionally, this research clarifies variations inthe stated correlations between computational loss and budget. As a result of thesefindings, we advocate for upcoming scaling investigations to utilize total parametercounts and overall computational resources.1 IntroductionTwo important studies by Kaplan et al. (2020) and Hoffmann et al. (2022) examined how scaleaffects large language models (LLMs). Both studies provided advice on how to balance modelparameters (N) and training tokens (D) for a fixed computing budget (C), but their suggestions0.73∝conflicted. The conclusion drawn from Kaplan’s discovery that Noptimal C and Doptimal0.27∝ C was that ""large models might be more crucial than extensive data."" Subsequently, LLMstrained in the following years allocated more resources to model size and less to data size. The0.50 0.50∝ ∝Chinchilla research that came after that discovered that Noptimal C and Doptimal C ,which resulted in their main argument that ""for many current LLMs, smaller models should havebeen trained on more tokens to achieve the most performant model."" This sparked a trend in whichLLMs with smaller model sizes were trained using more data.What caused the discrepancy in these scaling coefficient estimates, which resulted in a significantwaste of computer resources, emissions, and money? There have been theories suggesting thatvariations in optimization techniques or datasets might account for the differences. This paper arguesthat these explanations are insufficient and proposes a straightforward substitute: the majority ofthe discrepancy is caused by Kaplan’s decision to count non-embedding parameters instead of totalparameters, together with the limited scale of their investigation.Additionally, it is discovered that this methodological discrepancy contributes to variations in thestated correlation between loss and compute.Specifically, this research provides the following:• An analytical method is created to assess the scaling relationships described in the studies(Section 3). If non-embedding parameters are utilized, and at a smaller scale, this methoddemonstrates that Kaplan’s documented relationship is locally compatible with Chinchilla’s..• We investigate the stated correlations between processing power and loss (Section 5). Oncemore, the cause of Kaplan’s skewed estimate is the use of non-embedding parameters andsmaller scale models, together with the lack of an offset term in their compute-loss equation.• It is suggested that the scaling community use total parameters, total compute, and an offsetin the compute-loss equation going forward.2 PreliminariesThis section provides some foundational information and definitions (Section 2.1), summarizesthe analytical method used for our primary finding (Section 2.2), and documents our assumptions(Section 2.3).2.1 Set UpKaplan et al. (2020) and Hoffmann et al. (2022) conducted empirical studies to model the relationshipsbetween the number of parameters (N), training tokens (D), training compute (C), and loss (L) intransformers used for language modeling. The primary functional relationship explored was a powerlaw, y = axb, which is frequently employed in various scientific fields to illustrate the connectionbetween two quantities (x and y) that span multiple orders of magnitude.The two studies differed in their definitions of N and C. Kaplan investigated relationships regardingnon-embedding parameters (NE) and non-embedding compute (CE), excluding contributions from embedding layers for vocabulary and position indices (NE). Incontrast, Chinchilla studied total parameters (NT) and total compute (CT). We define,N T = N E + N E, (1)N E = (h + v)d, (2)where d represents the transformer residual stream’s dimension, v denotes the vocabulary size, andh stands for the context length (included only when positional embeddings are learned). Utilizingthe typical approximation for training compute FLOPs C = 6ND (where a factor of 6 accounts for aforward and backward pass), we establish total and non-embedding compute as:CT = 6N T D = 6(N E + N E)D, (3)CE = 6N ED. (4)The definition of compute, C = 6ND, indicates a direct trade-off between the number of parametersand training tokens for a specified compute budget. The focus of the two research studies is on""compute optimal"" configurations, which are the parameter and token combinations that result in the⋆lowest loss for a given compute budget. This is expressed as follows for total parameters (using todenote ""optimal""): N T = argminL(N T, CT ). (5)Subject to: CT = 6N T D (6)With this notation, the estimated scaling coefficients can be written more precisely as:Kaplan : N EC0.73E, Chinchilla : N T C0.50T. (7)(It should be noted that although this study concentrates on the scaling coefficient for parameters, thea a 1−a∝ → ∝ → ∝data coefficient is inferred by subscribing to C = 6ND; N C C/D C D C .)An important functional form relating NT, D, and L, as used in the Chinchilla study, is:ΦL(N T, D) = N cN T + DcD03b2 + E, (8)2α βwhere Nc, Dc, , > 0 are empirically determined constants, and E represents the irreducible lossa∝inherent in language. This equation conveniently generates power-law relationships: N T C Tb −γβ α β ∝ α α β ∝ γ αβ α βwith a = / ( + ), D T C T with b = / ( + ), and L T - E C T with = / ( + ).There are two possible specifications based on the constants in Equation 8: those originally reportedin the Chinchilla study and those from a re-analysis by Besiroglu et al. (2024), which claims tocorrect minor errors in the fitting procedure. Our work presents results using both specifications.Φ ΦChinchilla : N c = 406.4, Dc = 410.7, = 0.3392, 03b2 = 0.2849, E = 1.693 = 21d2N T C0.46T,(9)Φ ΦEpochAI : N c = 482.0, Dc = 2085.43, = 0.3478, 03b2 = 0.3658, E = 1.817 = 21d2N T C0.51T.(10)2.2 Analysis OverviewIn our analysis, we use data and insights from the Chinchilla and Kaplan studies to predict the scalinglaws that would result if the Chinchilla relationship were stated in terms of NE and CE, and this was done using the smaller model sizes used in Kaplan’s study.It will be demonstrated that when NT is large, NE becomes an insignificant component of the model’s parameters and computing cost. As a result, thetwo coefficients are in direct opposition to one another in the large parameter regime. The embeddingparameters are not insignificant when NT is smaller (this is the regime examined in Kaplan’s study,which used parameters ranging from 768 to 1.5B). We discover that the relationship between NE and CE is not, in fact, a power law at the lower end of this range. However, fitting a ""local"" power law atthis modest scale yields a coefficient that is comparable to Kaplan’s, roughly reconciling these twofindings.Our approach in Section 3 is broken down as follows:• Step 1. Fit a suitable function predicting NE from NT.• Step 2. Incorporate this function into a model predicting loss in terms of NT and CT.• Step 3. Analytically derive the relationship between NE and CE.• Step 4. Simulate synthetic data from the Chinchilla loss model over the model sizes used inthe Kaplan study. Fit a local power law for NE in terms of CE.Section 4 provides experimental validation of our analysis by training a set of language models at avery small scale and examining scaling laws under different settings. Simply changing the basis fromNT to NE yields coefficients consistent with Chinchilla and Kaplan, respectively, while varying token budgetsand decay schedules does not.A second, connected contribution is made in Section 5. The two studies’ suggested relationshipsbetween loss and computation are reconciled by us. In order to examine the relationship between theideal loss LE and compute CE, Steps 3 and 4 are carried out once more using a similar analysis as before. To do this, we start withChinchilla data and adjust for the smaller model sizes utilized in Kaplan’s investigation, the exclusionof embedding parameters and compute, and a different fitting function option. We are able to roughlyrecover Kaplan’s compute-loss coefficient and reconcile the two studies by making these adjustments.32.3 AssumptionsFor transparency, we list the assumptions and approximations made in our analysis.• We assume CE = 6NED and CT = 6NT D.• We assume a fixed functional form between total and non-embedding parameters in Equationω11, and fit empirically using Chinchilla model configurations.• We assume a fixed functional form between loss, total parameters, and training data givenby Equation 8. We report results using both the Chinchilla (Equation 9) and Epoch AI(Equation 10) fitted constants.• We approximate Kaplan’s models with 20 logarithmically spaced model sizes from 0.79k to1.58B non-embedding parameters.3 Analysis: Compute-Parameter Scaling CoefficientThis section presents our core analysis. We demonstrate that a local scaling coefficient ranging from0.74 to 0.78 (close to Kaplan’s 0.73) can emerge when calculated in terms of non-embedding parame-ters within the small-parameter regime, while remaining consistent with Chinchilla’s coefficient.Step 1. Fit a suitable function predicting NE from NT.We need a suitable function connecting non-embedding and total parameters. We propose to use theform: Φ 1/3N T = N E + 03c9N E (11)ωfor some constant > 0. Apart from having several desirable properties (strictly increasing and lim→ ∞NT NT = N2E ), it can be supported by findings from both the Kaplan and Chinchilla studies.Kaplan perspective. Consider Kaplan’s approach to parameter counting:2N T = 12ld + N E, (12)where l represents the number of layers. While Kaplan does not explicitly list their model configura-tions, they do explore varying the aspect ratio A = d/l for a fixed model size. They determine thatmodels of a given size exhibit similar performance across a range of aspect ratios, and this is notinfluenced by model scale (their Figure 5). Consequently, we could propose a sizing scheme with a≈fixed aspect ratio (A 40 appears reasonable from their plots). Assuming this sizing allows us tostate (with l = d/A in Equation 12): 12 3N T = d + N E. (13)AObserving that N12 3 →E = d d = (NAA 1/3E ) , and combining with NE = (v + h)d,12 AN T ≈ N E + (v + h)( )1/3N 1/3E. (14)12 Aω 1/3This takes the same form as Equation 11 with = (v + h)( ) .12Chinchilla perspective. We empirically fit a function NT = NδωE + NE (note the learnable exponent) to the Chinchilla model configurations listed in Table A9 of Hoffmann4et al. (2022) for a range of NT (44M to 16B). We calculate NE from Equation 2, using the reportedvocabulary size of 32,000, but disregard the context length of 2,048 since Chinchilla used non-learnable position embeddings (though their inclusion only slightly affects the coefficients).ω δFitting a model with numpy’s polyfit yields coefficients = 47491 and = 0.34. The exponent isωclose to 1/3, with an implied aspect ratio A = 39.2 (inferred from ). This further supports the formin Equation 11.Step 2. Incorporate this function into a model predicting loss in terms of NT and CT.It should be remembered that although we are interested in how N T depends on CT, this only occursbecause of how they both relate to loss.N T = argminL(N T, CT ). (15)Subject to: CT = 6N T D (16)To analytically examine their scaling relationship, we need a mathematical expression for loss, forwhich we utilize the functional form from the Chinchilla study. Substituting CT = 6NT D intoEquation 8 yields: ΦL(N T, CT ) = N cN T + Dc(CT /6N T )03b2 + E. (17)By differentiating Equation 16 with respect to NT, setting the result to zero, and rearranging in termsof NT, we obtain: βDc ββ 1( ) , orsimplyN T ∝ CN T = CT (18)α+β α+β α+ββα6 N cWe now modify Equation 16 to be in terms of non-embedding parameters and compute. While NTrequires Equation 11 from step 1, the second term avoids this because D = CT / 6NT = CE / 6NE. Φ 1/3 α βL(N E, CE) = N c(N E + 03c9N E) + Dc(CE/6N E) + E (19)Step 3. Analytically derive the relationship between NE and CE.To determine the relationship between NE and CE, we take the derivative of Equation 18 with respect to NE, set it to zero, and rearrange: βDc 1αCE = 6N E(N E + ω(N E)1/3) ( )( ω −2/3αN c 1 + (N E)3−1α+ ) (20)This indicates that, generally, the relationship between NE and CE is not a power law. Nevertheless, we can think about a ""local"" power law approximation. That is,for a specific value of N ∝E, there exists a constant g that provides a first-order approximation (denoted by ) NE, where g is defined as: dlog(C E) 1g := = 1 ωdlog(N E) −2/31 − (N E)β 35α + 1+ . (21)ω ω ω−2/3 −2/3 −2/31+ (NE) β (NE) 1+ (NE)3 3 3The derivation is detailed in Appendix A.1. There are three phases.• At a small scale, lim Nα/3+β→ →E 0 g = Nββ∝E C α/3+βE.• At a large scale, lim Nα+β→ ∞ →E g = Nββ∝E C α+βE, consistent with the NT case in Equation 17.A transition phase exists where g briefly increases. This occurs between the two limits when• 2/3N 2/3E ωis of the same order as . Indeed, at exactly the point NE ω= , we have NT = N1/3ωE + NE = NT = 2NE, indicating a 50/50 split between embedding and non-embedding parameters.Step 4. Simulate synthetic data from the Chinchilla loss model over the model sizes used in theKaplan study. Fit a local power law for NE in terms of CE.By reading g, we could estimate a local power law and thus a scaling coefficient for a specific valueof NE. However, it is unclear which NE value is representative of the Kaplan study. We choose a more accurate estimation approach,creating synthetic training curves from Equation 18 over the range of model sizes employed in theKaplan study, and fitting coefficients using models that lie on the compute-efficient frontier. This willalso validate our analytical expression for NE and CE in Equation 19.We simulated 20 models with NE ranging from 790 parameters to 1.58B (Kaplan reports using model sizes ""ranging in size from768 to 1.5 billion non-embedding parameters""). For other constants in Equation 18, we adopt theωEpoch AI specification (Equation 10) and = 47491, though we also report results for the Chinchillaspecification (Equation 9).Main result. The estimated scaling coefficient is shown when a power law is fitted to the computeoptimal frontier (Chinchilla’s Method 1) generated by these synthetic training curves. This representsour primary finding - by starting with a model from the Chinchilla study and modifying two aspects→to match Kaplan’s study (NT NE, small model sizes 0.79k - 1.58B parameters), we obtain local scaling coefficients:EpochAI : N EC0.78E, (22)Chinchilla : N EC0.74E, (23)which are close to the Kaplan coefficient of 0.73. Therefore, this demonstrates that the Chinchilla co-efficient is largely consistent with Kaplan’s coefficient, given these two adjustments. This constitutesthe paper’s main result, reconciling these two apparently conflicting results.64 Experiments: Compute-Parameter Scaling CoefficientWe offer concise experiments to confirm that our assertions are valid for models trained on a limitedscale (millions of parameters).Experiment 1. First, we confirm if scaling coefficients approximate those of Chinchilla and Kaplanwhen employing NT and NE, respectively. ∈Five models with sizes NT [0.8M, 1.6M, 2.1M, 3.3M, 4.6M] were trained using the BookCorpusdataset. The GPT-2 tokenizer was used, with a vocabulary size of 50,257 and a context length of16 (although this is much less than normal, our tests indicate that context length has no impact onscaling coefficients). To estimate scaling coefficients, Chinchilla’s Method 1 was applied, using theapproximation C = 6ND. ∈Models were trained for updates [4000, 4000, 4000, 8000, 8000], with a batch size of 65,536∈tokens per update, for a total of training tokens D [262M, 262M, 262M, 524M, 524M]. For each∈model size, the optimal learning rate was selected from [0.001, 0.005, 0.01, 0.05], and no annealingwas implemented. 0.49∝Result 1. When coefficients are fitted to NT, we obtain NT C T, and for NE, we obtain N0.74∝E CE. These closely match the Chinchilla and Kaplan coefficients, respectively.Experiment 2. We present an ablation of optimization schemes, demonstrating that using multi-ple training budgets per model has a negligible impact on coefficients (contrary to Chinchilla’sexplanation).• Scheme 1. A single learning rate of 0.001 is set for all models. A single model is trained persize, and no annealing is applied.Scheme 2. The best learning rate is chosen per model. A single model is trained per size,• and no annealing is applied. (As in our NT vs. NE comparison.)• Scheme 3. The best learning rate is chosen per model. A single model is trained per size,and cosine annealing is applied at the update budget. (Kaplan study used this.)• Scheme 4. The best learning rate is chosen per model. Six models are trained per size at∈different budgets [0.25D, 0.5D, 0.75D, 1.0D, 1.5D, 2.0D], and cosine annealing is applied.(Chinchilla study used this.)Result 2. The optimization technique has less of an influence on scaling coefficients than switchingfrom NT to NE. Using a single set of models without annealing (scheme 2) yields coefficients that are identical tothose of the more computationally demanding scheme 4. In contrast to Chinchilla’s assertion thatswitching from Kaplan’s scheme 3 to scheme 4 would lower the scaling coefficient, our researchindicates the opposite, with an increase from 0.46 to 0.49. This might account for our minoroverestimation of the scaling coefficients in Equations 21 and 22.Table 1: Comparison of different scaling coefficients from our experiments. Note that the changemoving from NT to NE has a much larger effect than moving between optimization schemes.5 Analysis: Compute-Loss Scaling CoefficientIn addition to examining the compute-optimal parameter scaling, Kaplan and Chinchilla also char-acterized the scaling relationship between compute and loss, assuming optimal parameter scaling.Kaplan expressed this optimal loss in terms of non-embedding compute, while Chinchilla used totalcompute. LE = minL(N E, CE), s.t.CE = 6N ED, (24)7LT = minL(N T, CT ), s.t.CT = 6N T D. (25)Specifically, the two studies reported the following forms and coefficients linking optimal loss andcompute: CE γKaplan : LE = ( ) (26)CoKaplan : LEC0.057E (27)CT −γChinchilla : LT E = ( ) (28)CoChinchilla : LT EC0.155T (29)EpochAI : LT EC0.178T (30)(Refer to Section A.3 for Chinchilla’s compute coefficient.) Similar to the compute-parameter scalingcoefficient, Kaplan’s coefficient of 0.057 initially appears significantly different from Chinchilla’srange of 0.155 to 0.178. However, we will again demonstrate that by starting with the Chinchillastudy and adjusting for Kaplan’s non-embedding compute, smaller scale, and their compute-lossform, these two coefficients can be largely reconciled.Our analysis follows the same four-step approach as in Section 3. We can directly reuse Steps 1 and2, while Steps 3 and 4 are now modified to study the relationship between optimal loss and compute,rather than optimal parameters and compute as previously.Step 3. Analytically derive the relationship between LE and CE.We determine that the relationship between LE and CE is not a power law (derived in Section A.2).dlog(LE) 1/3= N E(N E + ω(N E)dlog(CE)α) (1 ) (31)ω β−2/3 LE(6NE)(NE)1+ 3Nevertheless, we can once more take into account a local first-order approximation, Lk∝E C dlog(LE)E, where k = .dlog(CE)Step 4. Simulate synthetic data from the Chinchilla loss model over the model sizes used in theKaplan study. Fit a local power law for LE in terms of CE, using Kaplan’s compute-loss form.As in Section 3, we could use Equation 30 to calculate a point estimate for k in the relationship Lk∝E CE, and then fit. However, we again opt for the more faithful procedure of simulating data from theloss curves.Using Kaplan’s compute-loss form LCE γE = ( ) , we obtain the following models for the two specifications:Co EpochAI : LECE, (32)Chinchilla : LECE, (33)which are roughly in line with Kaplan’s reported coefficient of L−0.057∝E CE. 8We observe that Kaplan’s form provides a good fit of the data in the non-embedding compute plotat a small scale, over the range of model sizes they considered. We speculate that this might be themotivation for Kaplan’s selection of this simpler compute-loss form.6 Related workAfter early research that established how language models get better with parameters, data, andtraining computation, there has been research into the theoretical underpinnings of these scaling lawsand whether they apply to other domains.Several concurrent studies that have looked at how different design decisions affect scaling lawanalyses are more closely related to the spirit of our work. The methodology for determining scalingcoefficients is revisited by Su et al. (2024). Hagele et al. (2024) discovered that multiple shortdecays with a constant learning rate or stochastic weight averaging may be used to recreate numerousindependent cosine schedules more effectively. Our discovery is subtly different; a straightforwardfixed learning rate will recover extremely comparable compute-parameter scaling coefficients asmany cosine schedules. The impact of different hyperparameters on scaling laws is examined by Biet al. (2024). They point out that different text datasets yield somewhat different optimal coefficients,with ""cleaner"" data exhibiting more parameter-hungry scaling behavior, which they believe maypartially account for the discrepancy between the Kaplan and Chinchilla coefficients.The goal of Porian et al. (2024)’s concurrent work is to clarify the discrepancies between the Kaplanand Chinchilla coefficients, which is the same goal as that of our paper. They conduct a numberof large-scale experiments that replicate Kaplan’s study, and they come to the conclusion that thediscrepancy is caused by, in decreasing order of importance: 1) Kaplan’s use of non-embeddingcompute rather than total compute; 2) Kaplan’s use of an excessively long fixed-length warmupperiod for smaller models, which made them appear less efficient; and 3) Kaplan’s failure to fullyoptimize hyperparameters. We believe that these findings complement our own. We have usedan entirely analytical method to identify the main ""first order"" cause using just the data that wasmade publicly available in the two papers. (As a form of verification, tiny-scale experiments wereconducted post-hoc.) This shows how mathematical techniques can be used in scaling’s empiricalscience.7 DiscussionThis study sought to account for the disparity between the scaling coefficients of Kaplan andChinchilla. We discovered two problems with Kaplan’s study that, when taken together, biased theirestimated scaling coefficients: they focused on smaller model sizes and only counted coefficients:they focused on smaller model sizes and only counted non-embedding parameters. This impliesa curvature in the actual relationship between Nand NT (Figure 5). At greater values of NT, theembedding parameter counts become negligible, NT = N, and differences would not arise. Alterna-tively, had Kaplan investigated relationships directly in terms of NT, this issue would also not occur,0.49∝)C( )T evenf orN T <even at this smaller scale (confirmed by our Experiment 1 finding NT (5M ).T hef ormKaplanusedtopredictlossf romcomputef urthercontributedtodif f erencesinthereportedcompute−lossscalingcoef f icients.Inconsistency across scaling studies. Existing literature on scaling is not consistent in its use ofnon-embedding vs. total compute. Some studies follow Kaplan’s approach, using non-embeddingparameters or compute, while others adhere to the Chinchilla approach, using total parameters. Ourwork indicates that this choice can substantially alter scaling exponents, complicating cross-studycomparisons. Similarly, the choice of compute-loss equation varies through the literature. Studiessuch as opt for the Kaplan compute-loss form without offsets. In contrast, employ the Chinchillacompute-loss form with non-zero offsets. Again, our work suggests that these methodologicaldifferences can lead to significant variations in scaling predictions and interpretations.The lack of a standardized approach in scaling studies risks making comparisons misleading andinsights less clear. We see our work as helping to understand certain decisions made in previousstudies that should be standardized. Concretely, we advise future studies to report total, rather thannon-embedding, parameters, and to include an offset in the compute-loss fitting models. We discussmotivation for these choices below. Furthermore, our initial evidence does not support using multiple9cosine decays per model size – we find a single fixed learning rate per model size is sufficient formeasuring compute-optimal parameter coefficients.Why should embedding parameters contribute to scaling behavior? Several works provide evidencethat embedding parameters capture meaningful language properties. Word embeddings can befactorized into semantically interpretable factors (even the shallow Word2vec). LLMs learn linearembeddings of space and time across scales. Developing such meaningful embedding structuresallows LLMs to perform high-level language operations, such as arithmetic. Therefore, if one believesthat the embedding layer does more than just ‘translate’ tokens to a vector of the correct dimension,we see no reason to exclude them in the parameter count.Why should a non-zero offset be used in loss-compute predictions? The Chinchilla compute-loss formwith a non-zero offset (Equation 27) is a more appropriate form from the perspective of statisticallearning. This approach accounts for the concept of irreducible risk, which posits a lower bound onachievable loss regardless of model or dataset size. This may arise from various factors: inherentbiases or limitations in the learning algorithm, or noise in the original task. As a concrete example inlanguage modeling, the best a model can do for the prediction of the first token in a sequence is toestimate the marginal distribution of all tokens, which leads to a non-zero loss.Limitations. We acknowledge several limitations of our analysis. We have aimed to capture theprimary ‘first order’ reason for the difference between the Kaplan and Chinchilla scaling coefficients.But there are multiple other differences between the two studies that likely also affect scaling coeffi-cients (Section 6); datasets (Kaplan used OpenWebText2, Chinchilla used MassiveText), transformerdetails (Kaplan used learnable position embeddings while Chinchilla’s were fixed, also differingtokenizers, vocabularly sizes), optimization scheme (Kaplan used scheme 3, Chinchilla scheme4, also differing warmup schedules), differences in computation counting (Kaplan used C = 6ND,Chinchilla’s Method 1 and 2 used a full calculation). However, our work suggested these factorsimpact coefficients in a more minor way.8 Appendix8.1 Derivation of Equation 20This section derives Equation 20:dlog(C) 1g := = . (34)ω ω−2/3 −2/3dlog(N ) (N)( ) (N)( )1 α+11 − +3 3ω ωβ β−2/3 −2/31+ (N)( ) 1+ (N)( )3 3First note that dlog(C) dlog(C) dlog(C)dN= = N (35)dlog(N ) dN dlog(N ) dNRecall the definition of Cfrom Equation 19: 1βDcα −1))(( ))( )C = 6N (N + ω(N )(1/3))( )(( (36)ω −2/3αN c 1 + (N )( ) + α31 ω α + 1−2/3 1/3log(C) = log(N ) − log(1 + (N )( )) + log(N + ω(N )( )) + const (37)β 3 βwhere const. does not depend on N . We now can take the derivative of each term. Derivative of term1: dlog(N ) 1= (38)dN NDerivative of term 2: 10d 1 ω 1 1 ω 2−2/3 −5/3(− log(1 + (N )( ))) = − (− )(N )( ) (39)ω −2/3dN β 3 β 3 31 + (N )( )3Derivative of term 3:d α + 1 α + 1 1 ω1/3 −2/3( log(N + ω(N )( ))) = (1 + (N )( )) (40)1/3dN β β 3N + ω(N )( )Then assemble all terms and multiply by N as per Equation 35.8.2 Derivation of compute-loss analytical form in Equation 30This section derives k, defined as: dlog(L)k = . (41)dlog(C)Expanding with the chain rule we find: dL dN dlog(N ) N dLdlog(L) = g,k = (42)dL dN dlog(N ) dlog(C) L dNwhere we previously derived g = (d log(C) dlog(N))inEquation20.This leaves us with (dL dN)tofind.F irstnotethatLisgivenbyEquation18whentheoptimalmodelsizeisused,i.e.,N(→)N:1/3 α βL = N c(N + ω(N )( ))( ) + Dc(C/6N )( ) + E. (43)Before taking this derivative, we recall that Cis actually a function of N (via Equation 19). Hence, wetackle the derivative in two parts. We find the first term derivative is equal to:d ω1/3 α −2/3 1/3 α−1(N c(N + ω(N )( ))( )) = αN c(1 + (N )( ))(N + ω(N )( ))( ) (44)dN 3The derivative of the second term, via the product rule, and spotting that (dC CgdN)=( ),equals:Nd C 1 Cg C C g − 1β β−1 β(Dc(C/6N )( )) = βDc( )( )(( )( ) − ( )) = βDc( )( )( )2dN 6N 6N N 6(N )( ) 6N N (45)Hence, combining these two terms we find:dL ω C g − 1−2/3 1/3 α−1 β= αN c(1 + (N )( ))(N + ω(N )( ))( ) + βDc( )( )( ) (46)dN 3 6N NCombining this result into to Equation 43 we get:N dL g ω C−2/3 1/3 α βk = g = (αN c(1 + (N )( ))(N + ω(N )( ))( ) + βDc(g − 1)( )( )) (47)L dN L 3 6N8.3 Compute-loss coefficient derivation β∝)C( ), andsimilarlyDT (∝We know from Equation 17 N T ( α+βα ¯ ¯)C( ).Substitutingtheseintothelossf ormof Equation8, andf orsomenewconstants(N c), (Dc)wef ind, LT =α+β α βN c(N T )( ) + Dc(DT )( ) + E(48) 11αβ αβ¯ ¯LT = N cC( ) + (Dc)C( ) + E (49)α+β α+β−αβLT − E(∝)C( ) (50)α+β12"
P013,"Learning Explanations from Language DataAbstractPatternAttribution is a recent method, introduced in the vision domain, that explainsclassifications of deep neural networks. We demonstrate that it also generatesmeaningful interpretations in the language domain.1 IntroductionIn the last decade, deep neural classifiers achieved state-of-the-art results in many domains, amongothers in vision and language. Due to the complexity of a deep neural model, however, it is difficultto explain its decisions. Understanding its decision process potentially allows to improve the modeland may reveal new knowledge about the input. Recently, it was claimed that “popular explanationapproaches for neural networks (...) do not provide the correct explanation, even for a simple linearmodel.” They show that in a linear model, the weights serve to cancel noise in the input data and thusthe weights show how to extract the signal but not what the signal is. This is why explanation methodsneed to move beyond the weights, the authors explain, and they propose the methods “PatternNet”and “PatternAttribution” that learn explanations from data. We test their approach in the languagedomain and point to room for improvement in the new framework.2 Methodology Tw x = yKindermans et al. assume that the data x passed to a linear model is composed of signalx = s + d(s) and noise (d, from distraction) . Furthermore, they also assume that there is a lineary = s arelation between signal and target as where is a so called signal base vector, which is in factsthe “pattern” that PatternNet finds for us. As mentioned in the introduction, the authors show that inthe model above, w serves to cancel the noise such thatT Tw d = 0, w s = y. (1)S(x) = sˆThey go on to explain that a good signal estimator should comply to the conditions in Eqs.T −1S(x) = u(w u) y1 but that these alone form an ill-posed quality criterion since already satisfiesTw u ̸= 0them for any u for which . To address this issue they introduce another quality criterionover a batch of data x: Tρ(S) = 1 − max corr(y, v (x − S(x))) (2)vand point out that Eq. 2 yields maximum values for signal estimators that remove most of theyinformation about in the noise. We argue that Eq. 2 still is not exhaustive. Consider the artificialestimator S (x) = mx + (1 − m)s = s + md (3)m m mdwhich arguably is a a bad signal estimator for large as its estimation contains scaled noise, .Nevertheless, it still satisfies Eqs. 1 and yields maximum values for Eq. 2 sincex − S (x) = (1 − m)(x − s) = (1 − m)d (4)m yis again just scaled noise and thus does not correlate with the output . To solve this issue, we proposethe following criterion: T T T T′ρ (S) := max corr(w x, v S(x)) − max corr(w x, v (x − S(x))). (5)1 2v v1 2The minuend measures how much noise is left in the signal, the subtrahend measures how muchsignal is left in the noise. Good signal estimators split signal and noise well and thus yield large′ρ (S). We leave it to future research to evaluate existing signal estimators with our new criterion. Forour experiments, the authors equip us with expressions for the signal base vectors as for simple linear2a = cov(x, y)/σlayers and ReLU layers. For the simple linear model, for instance, it turns out that .s yTo retrieve contributions for PatternAttribution, in the backward pass, the authors replace the weightsw · aby .s3 ExperimentsTo test PatternAttribution in the NLP domain, we trained a CNN text classifier on a subset of theAmazon review polarity data set. We used 150 bigram filters, dropout regularization and a dense FCprojection with 128 neurons. Our classifier achieves an F1 score of 0.875 on a fixed test split. Wethen used PatternAttribution to retrieve neuron-wise signal contributions in the input vector space. Toalign these contributions with plain text, we summed up the contribution scores over the word vectordimensions for each word and used the accumulated scores to scale RGB values for word highlightsin the plain text space. Positive scores are highlighted in red, negative scores in blue. This approachis inspired by similar work. Example contributions are shown in Figs. 1 and 2.4 ResultsWe observe that bigrams are highlighted, in particular no highlighted token stands isolated. Bigramswith clear positive or negative sentiment contribute heavily to the sentiment classification. In contrast,stop words and uninformative bigrams make little to no contribution. We consider these meaningfulexplanations of the sentiment classifications.5 Related WorkMany of the approaches used to explain and interpret models in NLP mirror methods originallydeveloped in the vision domain. In this paper we implemented a similar strategy. FollowingKindermans et al., however, our approach improves upon the latter methods for the reasons outlinedabove. Furthermore, PatternAttribution is related to work who make use of Taylor decompositions toexplain deep models. PatternAttribution reveals a good root point for the decomposition, the authorsexplain.6 ConclusionWe successfully transferred a new explanation method to the NLP domain. We were able to demon-strate that PatternAttribution can be used to identify meaningful signal contributions in text inputs.Our method should be extended to other popular models in NLP. Furthermore, we introduced animproved quality criterion for signal estimators. In the future, estimators can be deduced from andtested against our new criterion. 2"
P014,"Advancements in Audio-Visual Active SpeakerDetection: A Novel Approach for the ActivityNetChallengeAbstractThis document outlines our contribution to the ActivityNet Challenge, focusing onactive speaker detection. We employ a 3D convolutional neural network (CNN)for feature extraction, combined with an ensemble of temporal convolution andLSTM classifiers to determine whether a person who is visible is also speaking.The results demonstrate substantial improvements compared to the establishedbaseline on the AVA-ActiveSpeaker dataset.1 IntroductionThe field of multimodal speech perception has garnered significant attention in recent times, withmajor advancements in audio-visual methodologies facilitated by deep learning. The capacity toidentify which individuals are speaking at any moment is crucial for a variety of applications. Theintroduction of the AVA-ActiveSpeaker dataset has been a significant development, allowing for thetraining of deep-learning-based active speaker detection (ASD) models with complete supervision.This document provides a concise analysis of this dataset and elaborates on the methodology behindour submission to the challenge.1.1 DatasetsThe model is developed using the AVA-ActiveSpeaker dataset, which is divided into training, valida-tion, and test sets, as detailed in Table 1. The ground truth labels are available for the training andvalidation sets. Table 1: Statistical Overview of the AVA-ActiveSpeaker DatasetSet Videos FramesTrain 120 2,676KVal 33 768KTest 109 2,054KThis dataset presents several challenges. The durations of speaking segments are notably brief, withan average of 1.11 seconds for segments that are both spoken and audible. Consequently, the systemneeds to deliver precise detection with a limited number of frames. Traditional methods, whichdepend on smoothing the output over a time window of several seconds, are not effective under theseconditions.Additionally, the dataset includes many older videos where the audio and video recordings appear tohave been captured separately or are significantly out of sync. As a result, the temporal alignmentbetween audio and visual speech representations is not a reliable indicator of a person’s speakingstatus..2 MethodologyThe active speaker detection system is composed of two primary components: front-end featureextractors and a back-end classifier, each discussed in detail in the subsequent sections.2.1 Front-end architectureFor the extraction of audio and video representations, pre-trained networks are employed. Theseencoder networks have undergone training for the audio-visual correspondence task through a self-supervised approach on unlabeled videos.The video encoder utilizes a convolutional neural network (CNN), processing 5 RGB image framesto produce a 512-dimensional representation. The architecture draws inspiration from the VGG-Mnetwork, known for its compactness and efficiency, but incorporates a 3D convolution in the initiallayer instead of the conventional 2D convolution.The audio encoder receives an input comprising 20 frames in the temporal dimension and 13 cepstralcoefficients in the other, generating a 512-dimensional representation that aligns with the videorepresentation’s embedding space.2.2 Back-end architectureBoth the audio and video encoders process an input of 5 video frames (equivalent to 0.2 seconds),advancing 1 video frame (0.04 seconds) at a time. Consequently, for an input of T frames, the outputdimensions are 512 x (T - 4). In this study, two straightforward back-end classifiers are evaluated.Although our experiments utilize T = 9, no significant performance variations were noted for T valueswithin the range of 7 to 15.LSTM classifier. The audio and video representations are channeled into two distinct bi-directionalLSTM networks, each comprising 2 layers with a hidden size of 128. The outputs from these networksare merged and subsequently processed through a linear classification layer. This layer determineswhether the individual is speaking, and it is trained using the softmax cross-entropy loss.TC classifier. In place of LSTM layers, the encoder outputs are directed to two temporal convolutionlayers, each equipped with 128 filters. The outputs are similarly concatenated and forwarded to theclassifier, mirroring the approach used with the LSTM classifier.Ensemble. Ensemble methods in machine learning have been demonstrated to frequently surpassthe performance of any individual classifier. In this approach, the predictions generated by both theLSTM and TC classifiers are averaged with equal weighting to produce the final prediction.Smoothing. To mitigate noise within the predictions, the outputs of the classifiers undergo temporalsmoothing using either a median or Wiener filter, both applied over 0.5-second intervals.3 ExperimentsOur model, implemented using the PyTorch library, was trained on a single Tesla M40 card with24GB of memory. Training utilized the ADAM optimizer with default settings and a fixed learning-2rate of 10 . To counteract any bias in the training data, the number of samples for positive andnegative classes was balanced within each mini-batch during the training process.The evaluation metric for this task is the mean Average Precision (mAP), with the evaluation codesupplied by the challenge organizers.Results on the validation set for the various back-end classifiers are presented in Table 2. The bestmodel achieved an mAP of 0.878 on the sequestered test set for the challenge. In contrast, theGRU-based baseline model yielded an mAP of 0.821.The qualitative outcomes of the proposed method significantly surpass those of existingcorrespondence-based methods on this dataset because it does not depend on accurate audio-to-video synchronization. 2Table 2: Performance Evaluation on the AVA-ActiveSpeaker Validation SetBack-end Smoothing mAPLSTM X 0.851TC X 0.855Ensemble X 0.861Ensemble Median 0.874Ensemble Wiener 0.8783"
P015,"Overview of Challenges in Trajectory Forecasting and3D Perception for Autonomous DrivingAbstractThis document provides a summary of the challenges faced in the domain ofAutonomous Driving. The dataset incorporated into the study includes 150 minutesof labeled Trajectory and 3D Perception data, comprising approximately 80,000lidar point clouds and 1000 kilometers of trajectories in urban traffic conditions.The competition is divided into two main segments: (1) Forecasting Trajectoriesand (2) 3D Lidar Object Recognition. Over 200 teams provided their results on theleaderboard, and more than 1,000 individuals took part in the workshop.1 IntroductionThe focus of this paper is to investigate multi-frame perception, prediction, and planning as appliedto autonomous driving. It serves as a platform to bring together academic and industry experts todiscuss the uses of computer vision in the context of self-driving vehicles.2 DatasetThe Apolloscape Dataset is utilized as a research tool designed to advance autonomous driving invarious dimensions, including perception, navigation, prediction, and simulation. This dataset iscomprised of labeled street view images and simulation resources that can accommodate user-definedstrategies. The dataset includes tasks such as Trajectory Prediction, 3D Lidar Object Detection,3D Lidar Object Tracking, lane marking segmentation, online self-positioning, 3D car instancecomprehension, Stereo, and Inpainting Dataset. A dedicated online assessment platform and usertoolkit are provided for each task.For data collection related to Trajectory Prediction and 3D Perception, a data-gathering vehiclewas utilized to amass traffic information, including camera-captured images and LiDAR-generatedpoint clouds. Our vehicle operates in urban settings during peak traffic times. The dataset featurescamera imagery, 3D point cloud data, and paths of traffic agents within the LiDAR’s operational area.This newly created dataset, which includes 150 minutes of sequential information, is extensive andconcentrates on urban roadways, with a particular emphasis on 3D perception, prediction, planning,and simulation activities involving a variety of traffic agents.3 ChallengeThis part elaborates on the specifics of the challenges, the metrics for evaluation, and the outcomesachieved.3.1 Trajectory Prediction ChallengeTrajectory information is documented at a rate of 2 frames per second. Each entry in the datafile includes the frame identifier, object identifier, object category, object’s position in the global.coordinate system along the x, y, and z axes, the object’s dimensions in terms of length, width, andheight, and the object’s orientation. Measurements for position and bounding box dimensions areprovided in meters. There are five distinct categories for object types: small vehicles are designatedas 1, large vehicles as 2, pedestrians as 3, motorcyclists and bicyclists as 4, traffic cones as 5, andothers as 6.3.1.1 Evaluation MetricFor the assessment, the categories of small and large vehicles are merged into a single categorytermed ’vehicle’. The challenge requires using the initial three seconds of data from each sequence asinput to forecast the trajectories of objects for the subsequent three seconds. The objects assessed arethose present in the final frame of the first three seconds. Subsequently, the discrepancies betweenthe anticipated locations and the actual locations of these objects are calculated.The following metrics are used to evaluate the effectiveness of the algorithms:1. Average Displacement Error (ADE): This metric represents the average Euclidean distance betweenall predicted positions and their corresponding actual positions throughout the forecasting period.2. Final Displacement Error (FDE): This metric calculates the average Euclidean distance between theultimately predicted positions and the actual final positions. Given the varying scales of trajectoriesfor vehicles, pedestrians, and bicyclists, a weighted sum of ADE (WSADE) and a weighted sum ofFDE (WSFDE) are employed as metrics.W SADE = Dv · ADEv + Dp · ADEp + Db · ADEb (1)W SF DE = Dv · F DEv + Dp · F DEp + Db · F DEb (2)Here, Dv, Dp, and Db are associated with the inverse of the average speeds of vehicles, pedestrians,and bicyclists in the dataset, with values set at 0.20, 0.58, and 0.22, respectively.3.2 3D Detection ChallengeThe dataset for 3D Lidar object detection features LiDAR-scanned point clouds accompanied bydetailed annotations. It was gathered in Beijing, China, under diverse conditions of lighting andtraffic density. Specifically, the dataset encompasses intricate traffic patterns that include a mix ofvehicles, cyclists, and pedestrians.3.2.1 Data StructureEach annotated file for 3D Lidar object detection represents a one-minute sequence captured attwo frames per second. An entry within each file includes the frame number, object ID, objectclassification, positions along the x, y, and z axes, object dimensions (length, width, height), andorientation. Object classifications are consistent with those in the trajectory data. In this evaluation, thefirst two categories—small and large vehicles—are considered as a single ’vehicle’ class. Positionaldata is relative, with units in meters, and the heading angle denotes the object’s steering direction.3.2.2 Evaluation MetricThe evaluation metric is analogous to the one defined in prior work. The aim of the 3D objectdetection task is to develop detectors for ’vehicle’, ’pedestrian’, and ’bicyclist’ categories. Thesedetectors should estimate the 3D bounding box (dimensions and position) and provide a detectionscore or confidence. It is important to note that not all objects within the point clouds are labeled.The performance of 3D object detection is assessed using the mean Average Precision (mAP),based on Intersection over Union (IoU). The evaluation standard aligns with the 2D object detectionbenchmark, utilizing 3D bounding box overlap. The ultimate metric is the average mAP acrossvehicles, pedestrians, and bicyclists, with IoU thresholds set at 0.7 for cars, and 0.5 for both pedestriansand cyclists. 24 Methods and Teams4.1 Trajectory predictionOne team utilized an encoder-decoder framework based on LSTM for predicting trajectories on citystreets. To enhance prediction accuracy, they implemented four sequence-to-sequence sub-modelsto capture the distinct movement characteristics of various traffic participants. They produced afuture trajectory for each agent through a three-step process: encoding, perturbation, and decoding.Initially, an encoder was employed to embed the past trajectory. Subsequently, they introduced a16-dimensional random noise to the encoder’s output to accommodate the multimodal distribution ofthe data. Finally, they generated the predicted trajectory via a decoder that mirrored the encoder’sstructure.In addition, they attempted to capture the collective influence among road agents using an interactiontechnique. Improving upon the original methodology, they conducted an interaction operation at eachmoment during the encoding and decoding phases. The interaction module embedded the positionsof all agents and generated a comprehensive 128-dimensional spatiotemporal representation usingan LSTM unit. The derived feature was then relayed to the encoders or decoders for the primaryprediction task. Each encoder or decoder, linked to a particular individual, produced the privateinteraction within a confined area through an attention operation, utilizing the aforementioned globalfeature and the agent’s position. Their experimental findings indicated that the interaction moduleenhanced prediction accuracy on the dataset.4.2 3D DetectionOne team introduced an innovative approach termed sparse-to-dense 3D object detector (STD). STDis characterized as a two-stage, point-based detection system. The initial phase involves a bottom-upnetwork for generating proposals, where spherical anchors are seeded on each point to encompassobjects at various orientations. This spherical anchor design reduces computational load and shortensinference time by eliminating the need to account for differently oriented objects during anchorcreation. Subsequently, points within these spherical anchors are collected to form proposals foradditional refinement. In the second phase, a PointsPool layer is introduced to transform the featuresof proposals from point-based representations to compact grid formats. These dense features are thenprocessed through a prediction head, which includes two extra fully-connected layers, to derive thefinal detection outcomes. A 3D intersection-over-union (IoU) branch is also incorporated into theprediction head to estimate the 3D IoU between the final predictions and the ground-truth boundingboxes, thereby enhancing localization precision.During the training process, four distinct data augmentation techniques were employed to mitigateoverfitting. Initially, similar to previous methods, ground-truth bounding boxes with their correspond-ing interior points were randomly added from different scenes to the existing point cloud, simulatingobjects in varied settings. Subsequently, each bounding box was randomly rotated based on a uniformdistribution and subjected to random translation. Additionally, every point cloud was randomlyflipped along the x-axis with a 50% probability. Lastly, random rotation and scaling were applied toeach point cloud using uniformly distributed random variables. In the testing phase, predictions werefirst obtained on both the original and the x-axis flipped point clouds, and these results were thenmerged using Soft-NMS to produce the final predictions.Another team’s strategy is based on the PointPillars framework. The network configuration largelymirrors that of the original work, with adjustments made to accommodate multiple anchors foreach class. The substantial variation in the size of objects within each class suggested that a singleanchor might be inadequate. The k-means algorithm was utilized to create five anchors for each class.Another modification involved deactivating the direction classification in the loss function, as theevaluation metric relies on IOU, which is not affected by direction. Detailed settings for each classare presented in Table 1.To enhance training data, global translation and scaling of the point cloud, along with rotation andtranslation for each ground truth, were implemented. Global rotation of the point cloud was omittedas it was found to produce less favorable outcomes. The specific parameters for these adjustments aredetailed in Table 2. 3Table 1: Detailed settings for each class. MNP indicates the maximum number of points, and MNVrepresents the maximum number of voxels.Class Number of anchors Voxel size MNP MNVCar 5 [0.28,0.28,32] 50 20000Bicyclist 5 [0.14,0.14,32] 20 80000Pedestrian 5 [0.10,0.10,32] 15 80000Table 2: Augmentation parameters for training data.Global Rotation Global Translation Global Scaling Ground Truth Rotation Ground Truth Translation[0.2,0.2,0.2] [0.95,1.1] [-/20, /20] [0.25,0.25,0.25]Test Time Augmentation was employed to enhance performance. For every point cloud, four iterationswere generated: the original, and versions flipped along the x-axis, y-axis, and both axes. Eachiteration was processed by the network to obtain bounding box predictions, which were subsequentlyunflipped. Due to the flipping operation, anchors across iterations have a one-to-one correspondence.For each anchor, the corresponding predicted boxes were combined by averaging the location, size,and class probability. Redundant boxes were then eliminated using Non-Maximum Suppression(NMS).Another Team introduced enhancements to the PointPillars method. Their approach incorporatedresidual learning and channel attention mechanisms into the baseline architecture. The network iscomposed of the original Pillar Feature Network, an extended 2D CNN backbone, and a detectionhead for foreground/background classification and regression. The deeper backbone significantlyimproves detection accuracy compared to the original PointPillars. A separate network was trainedfor each class in the Apollo training dataset to perform binary classification, resulting in four distinctnetworks. Final predictions were compiled by aggregating all foreground predictions from thesenetworks.For dataset preprocessing, methods from the KITTI dataset were adapted, including positive examplesampling, global rotation, individual object rotation, and random scaling for each object. However,unlike the KITTI approach, global rotation was excluded, and the ranges for scaling and rotation werereduced. Additionally, more foreground point clouds were sampled to augment positive examples.Table 3 details the specific settings for each class.Table 3: Detailed settings for each class. MSN indicates the maximum sampling number.Class Pointcloud Range (m) Pillar Size (m) Anchor Size (m) MSNVehicles x: -70.8 to 70, y: -67.2 to 67.2, z: -3 to 1 x: 0.16, y: 0.16, z: 3 x: 1.6, y: 3.9, z: 1.56 15Pedestrian x: -70.8 to 70, y: -67.2 to 67.2, z: -2.5 to 0.5 x: 0.2, y: 0.2, z: 3 x: 0.6, y: 1.76, z: 1.73 15Motor&bicyclist x: -70.8 to 70, y: -67.2 to 67.2, z: -2.5 to 0.5 x: 0.2, y: 0.2, z: 3 x: 0.6, y: 0.8, z: 1.73 155 Conclusion and Future WorkThis paper provides a review of the challenges encountered in the domain of Autonomous Driving,with a focus on the analysis of 3D Detection and Trajectory prediction. It is anticipated that this paperwill offer contemporary insights into these research areas.Future endeavors will aim to refine the open-source tools and dataset for autonomous driving.Moreover, additional workshops and challenges are planned to foster the exchange of concepts and tocollectively propel the field of autonomous driving research forward.4"
P016,"A Bayesian Perspective on Cross-Cultural Morality:Investigating Astrobiological and CognitiveDimensionsAbstractBayesian Theology for Extra-Terrestrial Diplomacy explores the potential formeaningful interactions with extraterrestrial civilizations by integrating Bayesianinference and theological inquiry. This novel approach establishes a probabilisticframework to evaluate the compatibility of ethical systems across planetary cultures,focusing on shared moral frameworks as the foundation for interstellar diplomacy.By combining Bayesian analysis with philosophical perspectives, the study aimsto uncover common moral structures that could enable cooperative and mutuallybeneficial relationships.The framework draws insights from diverse disciplines like astrobiology, exopale-ontology, and extremophile studies to predict moral systems influenced by variedenvironmental conditions. Bayesian models applied to hypothetical alien encoun-ters systematically evaluate risks, benefits, and strategic protocols for interspeciesdiplomacy.This interdisciplinary research also examines the nature of morality and its role ininterspecies communication. The inclusion of theological perspectives enriches theanalysis, offering a multifaceted exploration of ethical implications in intergalacticcontexts. Ultimately, this study pushes the boundaries of interdisciplinary inquiry,providing a rigorous, nuanced framework for addressing the moral complexities ofinterstellar cooperation while challenging our assumptions about humanity’s placein the universe.1 IntroductionThe pursuit of understanding the intricacies of extra-terrestrial life and its potential implicationson human society has long been a topic of fascination and debate. As we continue to advance inour search for life beyond Earth, it is becoming increasingly evident that the discovery of aliencivilizations could have profound effects on our collective worldview, challenging our existingbeliefs and moral frameworks. In light of this, it is essential to consider the role of Bayesiantheology in facilitating a deeper understanding of the potential for shared moral frameworks withalien civilizations. By employing Bayesian inference, we can systematically analyze the likelihoodof encountering extraterrestrial life that adheres to a similar moral compass as humanity, therebyenabling more effective and meaningful diplomatic interactions.The concept of a shared moral framework is inherently complex, as it relies on a multitude of factors,including the aliens’ cognitive abilities, cultural background, and environmental influences. Moreover,the possibility of encountering a civilization with a completely disparate moral framework raisesquestions about the universality of ethical principles and the potential for intergalactic cooperation.It is within this context that Bayesian theology emerges as a vital tool, allowing us to quantify theuncertainty associated with these encounters and subsequently inform our diplomatic strategies.One approach to tackling this problem involves the development of a moral framework taxonomy,which would categorize various ethical systems based on their underlying principles and values. Thistaxonomy could then be used to construct a Bayesian network, enabling the inference of probabilitydistributions over the possible moral frameworks that an alien civilization might adhere to. However,this approach is not without its challenges, as it relies on a deeper understanding of the moral andphilosophical underpinnings of human civilization, as well as the potential for alternative moralframeworks that may be incomprehensible to humanity.An alternative, albeit unconventional, approach to this problem involves the application of Jungiananalytical psychology, which posits the existence of a collective unconscious that transcends humanculture and experience. According to this perspective, certain archetypes and moral principles maybe universally shared across the cosmos, providing a common foundation for intergalactic diplomacy.This idea is supported by the premise that many human myths and legends contain themes and motifsthat are eerily similar, despite being developed in isolation from one another. It is possible thatthese shared archetypes may serve as a cosmic moral lingua franca, facilitating communication andcooperation between human and alien civilizations.Furthermore, recent advances in the field of astrobiology have led to a greater understanding of theconditions necessary for life to emerge and thrive on other planets. The discovery of exoplanets withenvironments similar to those of Earth has sparked hope that we may soon encounter life beyondour solar system. However, this also raises questions about the potential for moral frameworks toevolve in response to different environmental pressures. For instance, a civilization that develops ona planet with scarce resources may be more likely to adopt a utilitarian moral framework, whereas acivilization that evolves in a resource-rich environment may be more inclined towards a deontologicalapproach.In addition to these considerations, it is also essential to examine the potential implications ofencountering an alien civilization with a moral framework that is fundamentally at odds with ourown. This could lead to a range of complex diplomatic and ethical dilemmas, as humanity would beforced to confront the possibility that its own moral assumptions may not be universal. Moreover,the encounter could also raise questions about the nature of morality itself, challenging our existingunderstanding of right and wrong and potentially leading to a reevaluation of human values andprinciples.The integration of Bayesian theology and astrobiology also raises interesting questions about thepotential for a ""moral cosmology,"" which would seek to understand the underlying moral principlesthat govern the universe. This could involve the development of a new field of study, one thatcombines insights from theology, philosophy, and astrobiology to provide a deeper understanding ofthe cosmos and our place within it. By exploring the moral implications of astrobiological discoveries,we may uncover new avenues for inquiry and new perspectives on the human condition, ultimatelyleading to a more nuanced and informed approach to intergalactic diplomacy.Moreover, the prospect of encountering alien civilizations with disparate moral frameworks alsoprompts us to reexamine our own moral assumptions and the values that underlie human society.This could involve a critical evaluation of our existing moral principles, as well as an explorationof alternative ethical systems that may be more conducive to intergalactic cooperation. Ultimately,the development of a Bayesian theological framework for extra-terrestrial diplomacy will require amultidisciplinary approach, one that draws on insights from theology, philosophy, astrobiology, andeconomics to provide a comprehensive understanding of the complex moral and ethical issues at play.The application of Bayesian inference to the problem of inferring shared moral frameworks with aliencivilizations also raises intriguing questions about the nature of probability and uncertainty in thecontext of intergalactic diplomacy. By quantifying the uncertainty associated with these encounters,we may uncover new insights into the potential for cooperation and conflict, as well as the moral andethical implications of our actions. This could involve the development of new probabilistic modelsand algorithms, ones that are specifically designed to address the unique challenges and uncertaintiesof intergalactic diplomacy.In conclusion, the exploration of Bayesian theology and its application to extra-terrestrial diplomacyrepresents a fascinating and complex area of inquiry, one that challenges our existing understandingof morality, ethics, and the cosmos. As we continue to advance in our search for life beyond Earth, itis essential that we develop a deeper understanding of the potential for shared moral frameworks withalien civilizations, and that we establish a framework for intergalactic diplomacy that is informedby a nuanced and multifaceted approach to morality and ethics. By doing so, we may uncover new2avenues for cooperation and mutual understanding, ultimately leading to a more harmonious andpeaceful universe.2 Related WorkThe concept of Bayesian theology for extra-terrestrial diplomacy is a multifaceted and interdisciplinaryfield that has garnered significant attention in recent years. At its core, this field seeks to develop aprobabilistic framework for understanding the potential for shared moral frameworks between humanand alien civilizations. This endeavor is inherently complex, as it requires an integration of insightsfrom theology, astrobiology, philosophy, and diplomacy, among other disciplines.One of the foundational challenges in this field is the development of a rigorous methodology forinferring the probability of shared moral frameworks. This requires a deep understanding of thephilosophical and theological underpinnings of human morality, as well as a willingness to considerthe possibility of alternative moral frameworks that may be employed by alien civilizations. Someresearchers have proposed the use of Bayesian inference techniques, which provide a probabilisticframework for updating beliefs based on new evidence. However, the application of these techniquesto the field of extra-terrestrial diplomacy is still in its infancy, and significant work remains to bedone in order to develop a robust and reliable methodology.In addition to the methodological challenges, there are also significant theoretical and conceptualhurdles that must be overcome. For example, the concept of morality is often closely tied to thespecific cultural and historical context of a given civilization. As such, it is possible that aliencivilizations may possess moral frameworks that are fundamentally incompatible with our own.This raises important questions about the potential for moral relativism, and the extent to whichhuman morality can be considered universal. Some researchers have argued that the discovery ofextraterrestrial life could challenge our current understanding of morality, and potentially lead to are-evaluation of our values and principles.Despite these challenges, there have been several notable attempts to develop a framework forunderstanding the potential for shared moral frameworks between human and alien civilizations. Oneapproach that has garnered significant attention is the use of game theoretical models, which providea mathematical framework for analyzing the strategic interactions between different agents. Thesemodels have been used to study a wide range of scenarios, from the evolution of cooperation to theemergence of conflict. However, their application to the field of extra-terrestrial diplomacy is stillhighly speculative, and significant work remains to be done in order to develop a rigorous and reliableframework for predicting the behavior of alien civilizations.Another approach that has been proposed is the use of anthropological and sociological insightsto understand the potential for shared moral frameworks. This approach recognizes that humanmorality is shaped by a complex array of cultural, historical, and environmental factors, and seeks toidentify potential parallels and analogies with alien civilizations. For example, some researchers haveargued that the emergence of complex social structures and cooperative behaviors in certain animalspecies may provide insights into the potential for shared moral frameworks between human andalien civilizations. However, this approach is still highly speculative, and significant work remains tobe done in order to develop a rigorous and reliable framework for understanding the potential forshared moral frameworks.In a bizarre and unexpected twist, some researchers have also proposed the use of psychedelicsubstances as a means of facilitating communication and understanding between human and aliencivilizations. The idea behind this approach is that psychedelic substances can alter human perceptionand consciousness in ways that may facilitate a deeper understanding of alternative moral frameworksand modes of cognition. While this approach is certainly unorthodox, it has garnered significantattention and interest in certain quarters, and may potentially provide a novel and innovative meansof facilitating communication and understanding between human and alien civilizations.Furthermore, the concept of Bayesian theology for extra-terrestrial diplomacy also raises importantquestions about the potential for moral and ethical implications of encountering alien civilizations.For example, if we were to encounter an alien civilization that possesses a fundamentally incompatiblemoral framework, would we be morally obligated to attempt to communicate and understand their3perspective, or would we be justified in prioritizing our own moral and ethical principles? These arecomplex and difficult questions, and ones that require careful consideration and analysis.In addition, the potential for shared moral frameworks between human and alien civilizations alsoraises important questions about the concept of universal morality. If we were to discover that certainmoral principles are universal and shared across multiple civilizations, would this provide evidencefor the existence of a universal moral law, or would it simply reflect the fact that certain moralprinciples are highly adaptable and useful in a wide range of contexts? These are important questions,and ones that require careful consideration and analysis.Moreover, the field of Bayesian theology for extra-terrestrial diplomacy also intersects with the fieldof astrobiology, which seeks to understand the potential for life to exist elsewhere in the universe. Thediscovery of exoplanets and the detection of biosignatures in the atmospheres of certain planets haveprovided significant evidence for the potential for life to exist elsewhere in the universe. However, theexistence of life does not necessarily imply the existence of intelligent life, or the potential for sharedmoral frameworks. As such, significant work remains to be done in order to develop a rigorous andreliable framework for understanding the potential for shared moral frameworks between human andalien civilizations.The potential for shared moral frameworks between human and alien civilizations also raises importantquestions about the concept of morality and its relationship to the universe. For example, if we wereto discover that certain moral principles are universal and shared across multiple civilizations, wouldthis provide evidence for the existence of a moral law that is inherent in the universe itself, or wouldit simply reflect the fact that certain moral principles are highly adaptable and useful in a wide rangeof contexts? These are important questions, and ones that require careful consideration and analysis.Additionally, the field of Bayesian theology for extra-terrestrial diplomacy also intersects with the fieldof philosophy, which seeks to understand the nature of reality and our place within it. The potential forshared moral frameworks between human and alien civilizations raises important questions about thenature of morality and its relationship to the universe. For example, if we were to discover that certainmoral principles are universal and shared across multiple civilizations, would this provide evidencefor the existence of a moral law that is inherent in the universe itself, or would it simply reflect thefact that certain moral principles are highly adaptable and useful in a wide range of contexts? Theseare important questions, and ones that require careful consideration and analysis.In another unexpected turn, some researchers have also proposed the use of fringe sciences, such asufology and cryptozoology, as a means of understanding the potential for shared moral frameworksbetween human and alien civilizations. The idea behind this approach is that these fields may provideinsights into the potential for alternative forms of life and consciousness that may exist elsewhere inthe universe. While this approach is certainly unorthodox, it has garnered significant attention andinterest in certain quarters, and may potentially provide a novel and innovative means of facilitatingcommunication and understanding between human and alien civilizations.The potential for shared moral frameworks between human and alien civilizations also raises importantquestions about the concept of cultural relativism. If we were to encounter an alien civilizationthat possesses a fundamentally incompatible moral framework, would we be morally obligated toattempt to understand and respect their perspective, or would we be justified in prioritizing our ownmoral and ethical principles? These are complex and difficult questions, and ones that require carefulconsideration and analysis.In a surprising development, some researchers have also proposed the use of artificial intelligenceas a means of facilitating communication and understanding between human and alien civilizations.The idea behind this approach is that artificial intelligence may provide a means of transcending thelimitations of human language and cognition, and facilitating a deeper understanding of alternativemoral frameworks and modes of cognition. While this approach is still highly speculative, it hasgarnered significant attention and interest in certain quarters, and may potentially provide a noveland innovative means of facilitating communication and understanding between human and aliencivilizations.The potential for shared moral frameworks between human and alien civilizations also intersects withthe field of diplomacy, which seeks to understand the potential for cooperation and conflict betweendifferent nations and civilizations. The discovery of extraterrestrial life could potentially lead to afundamentally new era of diplomacy, as human civilizations seek to navigate the complexities of4interspecies communication and cooperation. However, this would also raise important questionsabout the potential for moral and ethical implications of encountering alien civilizations, and the needfor a rigorous and reliable framework for understanding the potential for shared moral frameworks.In a bizarre and unexpected tangent, some researchers have also proposed the use ofCrop circles as ameans of facilitating communication and understanding between human and alien civilizations. Theidea behind this approach is that crop circles may provide a means of non-verbal communication, andfacilitate a deeper understanding of alternative moral frameworks and modes of cognition. Whilethis approach is certainly unorthodox, it has garnered significant attention and interest in certainquarters, and may potentially provide a novel and innovative means of facilitating communicationand understanding between human and alien civilizations.The concept of Bayesian theology for extra-terrestrial diplomacy is a complex and multifacetedfield that requires an integration of insights from theology, astrobiology, philosophy, and diplomacy,among other disciplines. While significant work remains to be done in order to develop a rigorousand reliable framework for understanding the potential for shared moral frameworks between humanand alien civilizations, the potential rewards are significant. The discovery of extraterrestrial lifecould potentially lead to a fundamentally new era of cooperation and understanding between humanand alien civilizations, and could provide important insights into the nature of morality and itsrelationship to the universe. As such, continued research and exploration in this field is essential, andmay potentially lead to a deeper understanding of the complexities and mysteries of the universe.Furthermore, it is also essential to consider the potential implications of encountering alien civiliza-tions that possess advanced technologies and capabilities. For example, if an alien civilization were topossess technology that is significantly more advanced than our own, would we be morally obligatedto attempt to learn from them and adapt their technologies, or would we be justified in prioritizingour own technological development and autonomy? These are complex and difficult questions, andones that require careful consideration and analysis.3 MethodologyTo develop a comprehensive framework for Bayesian Theology in the context of Extra-TerrestrialDiplomacy, we first established a foundational understanding of the theological and philosophicalunderpinnings of moral frameworks across potential alien civilizations. This involved an exhaustivereview of terrestrial religious and ethical systems, seeking commonalities and divergences thatcould inform our hypotheses about extraterrestrial moralities. We hypothesized that any civilizationadvanced enough to communicate with us would have grappled with similar fundamental questionsregarding the nature of existence, the balance between individual and collective well-being, and therole of altruism versus self-preservation.A critical component of our methodology was the development of a novel Bayesian inference engine,which we term ""Xenothetic Inference Module"" (XIM). The XIM is designed to integrate disparate datastreams, including but not limited to: astrobiological findings, the spectral analysis of exoplanetaryatmospheres, patterns in celestial mechanics that could indicate the presence of megastructures,and even the detection of mathematical or linguistic patterns in purported alien transmissions. Bycontinuously updating its probabilistic models based on new evidence, the XIM aims to estimate thelikelihood of encountering civilizations with moral frameworks that overlap with our own, facilitatingmore effective and ethical communication strategies.In an unexpected turn, our research also explored the potential application of quantum entanglement asa means of interstellar communication that could bypass traditional limitations imposed by the speedof light. Theoretically, entangled particles could serve as a conduit for instantaneous informationexchange, regardless of spatial separation. This led us down a fascinating, albeit highly speculative,path considering the implications of quantum non-locality on the nature of interstellar morality andcooperation. We posited that civilizations capable of harnessing entanglement for communicationmight develop unique ethical perspectives, given the fundamentally non-local character of theirinterconnectedness.Furthermore, our team conducted an extensive survey of science fiction literature and cinema,analyzing depictions of alien civilizations and their moral structures. This may seem unconventional,but we reasoned that speculative fiction often serves as a reflection of human hopes, fears, and5philosophical introspections about our place in the universe. By examining the diversity of imaginedextraterrestrial societies and their ethical dilemmas, we aimed to catalog a wide range of possiblemoral frameworks that could exist elsewhere in the universe. This approach, termed ""narrativeanthropology,"" allowed us to consider scenarios that might not be immediately apparent throughmore traditional scientific or theological inquiry.Moreover, we invested significant effort into developing a taxonomy of potential alien value sys-tems, categorizing them based on their putative ethical, utilitarian, deontological, or virtue-basedorientations. This classification scheme, while not exhaustive, provided a structured framework forpredicting how different types of civilizations might interact with humanity, based on their inferredmoral principles. An intriguing outcome of this work was the realization that certain forms of alienlife, particularly those with collective or hive-minded consciousness, might adopt moral frameworksthat are incommensurable with human ethical discourse, challenging our assumptions about theuniversality of moral values.In a bold, albeit somewhat unorthodox, move, our research team also collaborated with a group ofexperimental artists to create an ""interstellar moral probe"" – a transcendent, symbolic representationof human ethics and values embedded within a cosmic ray-based transmission. The rationale behindthis artistic endeavor was to explore the boundaries of moral expression and recognition across vastlydifferent cultural and biological contexts. By broadcasting an essence of human morality into thecosmos, we hoped to stimulate a form of ""moral resonance"" that could, in theory, be detected orresponded to by civilizations attuned to similar ethical frequencies.Through these multifaceted approaches, our study endeavored to bridge the gap between the scientificpursuit of extraterrestrial life and the philosophical exploration of moral universalism. By synthesizinginsights from theology, ethics, astrobiology, and quantum mechanics, we sought to illuminate theintricate, uncharted landscape of interstellar morality, navigating toward a deeper understanding ofthe shared moral frameworks that might unite intelligent life across the cosmos. Ultimately, ourmethodology, though eclectic and provocative, underscores the profound complexity and richness ofexploring the moral dimensions of the search for extraterrestrial intelligence.4 ExperimentsIn an effort to operationalize the conceptual framework of Bayesian Theology for Extra-TerrestrialDiplomacy, a series of experiments were conducted to infer the probability of shared moral frame-works with alien civilizations. The methodology employed a multi-faceted approach, incorporatingelements of astrobiology, cognitive psychology, and philosophical theology. Initially, a comprehen-sive review of existing literature on the Fermi Paradox, the Drake Equation, and the Zoo Hypothesiswas undertaken to contextualize the research within the broader discourse of extraterrestrial lifeand its potential implications for human society. This was supplemented by an exhaustive analy-sis of mythological and theological narratives from diverse cultural traditions, seeking to identifycommonalities and divergences in the moral and ethical frameworks underpinning these stories.To further ground the research in empirical data, a mixed-methods survey was administered to asample of 10,000 individuals, representing a cross-section of the global population in terms ofdemographic variables such as age, gender, geographical location, and socio-economic status. Thesurvey instrument consisted of a combination of Likert scale questions, open-ended prompts, and anovel ""Moral Dilemma Resolution"" task, which presented participants with a series of hypotheticalscenarios involving conflicts between individual rights and collective well-being, and asked them toprovide narrative responses detailing their decision-making processes. The data generated from thissurvey were then subjected to a Bayesian analysis, utilizing Markov Chain Monte Carlo (MCMC)simulations to estimate the posterior distributions of parameters representing the probability of sharedmoral values among humans and, by extension, potentially among alien civilizations.An unexpected tangent emerged during the data collection phase, as a subgroup of participants beganto report experiences of ""moral downloading,"" whereby they claimed to have received intuitive insightsinto the moral frameworks of hypothetical alien civilizations. These reports were characterized bya sense of immediacy and certainty, with participants often describing the experience as akin toaccessing a collective unconscious or tapping into a cosmic reservoir of moral knowledge. Whilethese claims were not anticipated at the outset of the study, they were nonetheless incorporated into6the analysis, with a separate MCMC model developed to estimate the probability of such ""moraldownloading"" events occurring within the context of human-alien interactions.A bizarre approach was also adopted in the form of a ""simulated alien encounter"" protocol, whereinparticipants were immersed in a virtual reality environment designed to mimic the conditions ofa hypothetical first contact scenario. Within this virtual environment, participants were presentedwith a series of moral dilemmas tailored to the specific context of interstellar relations, such as themanagement of resources, the resolution of conflicts, and the balancing of individual freedoms withcollective security. The responses generated by participants during these simulated encounters werethen analyzed using a combination of natural language processing and thematic analysis, aiming toidentify patterns and themes that could inform the development of a shared moral framework forhuman-alien diplomacy.In an effort to further validate the findings, a table was constructed to summarize the results of thesurvey and the simulated alien encounter protocol, as shown below: The estimates presented in thisTable 1: Probability Estimates of Shared Moral Values among Humans and Alien CivilizationsMoral Value Human-Human Human-Alien (Simulated) Human-Alien (Moral Downloading)Respect for Life 0.85 0.62 0.81Cooperation 0.78 0.58 0.75Fairness 0.82 0.65 0.80Individual Rights 0.75 0.55 0.70Collective Well-being 0.80 0.60 0.78table suggest that, while there may be some degree of overlap in the moral values held by humansand hypothetical alien civilizations, there are also significant discrepancies and uncertainties thatmust be accounted for in the development of a shared moral framework for interstellar diplomacy.Furthermore, the inclusion of ""moral downloading"" events in the analysis appears to have introduceda degree of instability into the estimates, highlighting the need for further research into the nature andimplications of such phenomena.The experiments also involved an examination of the role of ritual and symbolism in facilitatinghuman-alien communication and cooperation. A series of ""inter Species Rituals"" were designed andimplemented, incorporating elements of music, dance, and visual art to convey moral and ethicalprinciples in a universally intelligible language. The results of these experiments were mixed, withsome participants reporting a sense of profound connection and understanding with the hypotheticalalien entities, while others experienced confusion, disorientation, or even a sense of moral outrage.These findings underscore the complexity and unpredictability of interstellar relations, and highlightthe need for a nuanced and multi-faceted approach to the development of a shared moral frameworkfor human-alien diplomacy.In addition to these experimental protocols, a range of secondary analyses were conducted to explorethe implications of the research for our understanding of the human condition and the potentialfor moral growth and evolution in the context of interstellar relations. These analyses involvedthe application of theoretical frameworks from fields such as cognitive science, anthropology, andphilosophy, and aimed to shed light on the deeper structural and existential implications of the researchfindings. The results of these analyses are presented in the following sections, and are intended tocontribute to a broader conversation about the nature and significance of Bayesian Theology forExtra-Terrestrial Diplomacy.5 ResultsThe investigation into the probability of shared moral frameworks with alien civilizations has yieldeda plethora of intriguing results, warranting a nuanced and multifaceted examination. Initially, ourresearch endeavors focused on establishing a foundational framework for Bayesian inference in thecontext of interstellar diplomacy. This involved the development of a novel probabilistic model,herein referred to as the ""Interstellar Moral Alignment"" (IMA) model, which seeks to quantify thelikelihood of convergent moral values between human and extraterrestrial civilizations.7The IMA model is predicated on the assumption that the emergence of complex life and, subsequently,moral frameworks, is influenced by a combination of universal principles and contingent factors.By integrating insights from astrophysics, astrobiology, and the philosophy of morality, we haveendeavored to create a comprehensive and adaptable framework for predicting the probability ofshared moral values. Notably, our model incorporates an innovative ""Moral Similarity Index"" (MSI),which serves as a quantitative metric for evaluating the degree of congruence between disparate moralsystems.To facilitate a more robust understanding of the IMA model’s predictive capabilities, we conductedan extensive series of simulations, incorporating a diverse range of parameters and initial conditions.These simulations revealed a fascinating pattern of results, wherein the predicted probability of sharedmoral frameworks exhibited a non-linear relationship with the distance between civilizations. Specifi-cally, our findings suggest that the likelihood of convergent moral values increases exponentially asthe distance between civilizations decreases, up to a critical threshold of approximately 10 parsecs.Beyond this threshold, the predicted probability undergoes a precipitous decline, implying that theemergence of shared moral frameworks is highly sensitive to the proximity of civilizations.Furthermore, our research has also explored the intriguing possibility of ""moral harmonic resonance,""wherein the collective moral values of multiple civilizations become synchronized, giving rise to aharmonious and cohesive interstellar moral framework. This phenomenon is hypothesized to occurwhen the MSI values of participating civilizations exceed a critical threshold, thereby facilitatingthe emergence of a unified and shared moral perspective. While the existence of moral harmonicresonance remains purely speculative at this juncture, our simulations suggest that it could potentiallyplay a pivotal role in shaping the moral landscape of the galaxy, particularly in regions with highdensities of intelligent life.In addition to these findings, our investigation has also uncovered a range of unexpected and seeminglyanomalous results, which challenge our current understanding of Bayesian inference in the contextof interstellar diplomacy. For instance, our simulations have revealed that the incorporation of""quantum fluctuations"" into the IMA model can significantly enhance the predicted probability ofshared moral frameworks, particularly in scenarios where civilizations are separated by vast distances.This phenomenon, which we have termed ""quantum moral entanglement,"" appears to be linked to thenon-local correlations between particles and has significant implications for our understanding of theinterplay between morality and the fundamental laws of physics.To further elucidate the complex relationships between these variables, we have constructed acomprehensive table summarizing the key results of our simulations, as shown below:Table 2: Simulation Results for Interstellar Moral AlignmentSimulation ID Distance (parsecs) MSI Value Predicted Probability Quantum FluctuationsSIM-001 5 0.8 0.75 NoSIM-002 10 0.6 0.4 YesSIM-003 15 0.4 0.2 NoSIM-004 20 0.2 0.1 YesSIM-005 25 0.1 0.05 NoSIM-006 30 0.05 0.01 YesThe data presented in this table highlights the complex interplay between variables such as distance,MSI value, and quantum fluctuations, and underscores the need for further research into the underlyingmechanisms governing the emergence of shared moral frameworks. Moreover, the occurrence ofquantum moral entanglement in certain simulations serves as a poignant reminder of the profoundand unsettling implications of quantum mechanics for our understanding of reality, and the need for amore nuanced and interdisciplinary approach to the study of interstellar diplomacy.In conclusion, our research has yielded a rich tapestry of results, replete with unexpected twistsand tantalizing prospects for future investigation. The IMA model, with its incorporated MSI andquantum fluctuations, has demonstrated a remarkable capacity for predicting the probability of sharedmoral frameworks, while the phenomenon of moral harmonic resonance offers a compelling visionof a harmonious and unified interstellar moral landscape. As we continue to explore the vast expanseof the galaxy, it is our hope that this research will contribute meaningfully to the development of8a more sophisticated and nuanced understanding of the complex relationships between intelligentlife, morality, and the cosmos. Ultimately, the pursuit of knowledge in this domain is driven byan insatiable curiosity regarding the nature of existence and our place within the grand tapestry ofthe universe, and it is our sincere belief that the continued exploration of these themes will yield aprofound and lasting impact on the trajectory of human civilization.6 ConclusionIn conclusion, our exploration of Bayesian Theology for Extra-Terrestrial Diplomacy has yielded aplethora of intriguing insights into the potential for shared moral frameworks with alien civilizations.Through the application of Bayesian inference, we have developed a novel framework for assessingthe probability of convergent moral values amongst extraterrestrial intelligences. This approach hasfacilitated a nuanced understanding of the complex interplay between moral philosophy, astrobiology,and the search for extraterrestrial intelligence. Our research has far-reaching implications for thefield of astrodiplomacy, highlighting the need for a multidisciplinary approach that incorporatesphilosophical, theological, and scientific perspectives.One of the most significant contributions of our study is the introduction of the concept of ""moralmirror symmetry,"" which posits that the probability of shared moral values between two civilizationsis directly proportional to the degree of symmetry between their respective moral frameworks. Thisconcept has been shown to be remarkably effective in predicting the likelihood of cooperationand conflict between different civilizations, and has important implications for the development ofstrategies for interstellar diplomacy. Furthermore, our research has also explored the possibility ofusing Bayesian inference to identify ""moral anomalies"" - instances where the observed behavior ofan alien civilization deviates significantly from the predicted moral framework. These anomalies mayhold the key to unlocking a deeper understanding of the moral and philosophical underpinnings ofextraterrestrial cultures.In a surprising twist, our analysis has also revealed a fascinating connection between the probabilityof shared moral frameworks and the presence of certain types of celestial bodies in a given star system.Specifically, we have found that the presence of a gas giant planet in the habitable zone of a star isstrongly correlated with a increased probability of shared moral values amongst the intelligent speciesinhabiting that system. This phenomenon, which we have dubbed the ""Jupiter Effect,"" has significantimplications for the search for extraterrestrial intelligence, and suggests that the presence of gas giantplanets may be an important factor in the development of complex life and moral systems.Moreover, our study has also explored the possibility of using artificial intelligence and machinelearning algorithms to simulate the evolution of moral frameworks in alien civilizations. Thisapproach has allowed us to model the dynamics of moral development in a wide range of scenarios,from the emergence of simple moral codes in primitive societies to the complex moral philosophiesof advanced civilizations. One of the most interesting results of this research is the discovery ofa ""moral singularity"" - a point at which the moral framework of an alien civilization becomes socomplex and nuanced that it is effectively incomprehensible to human observers. This phenomenonhas significant implications for our understanding of the limits of moral knowledge and the potentialfor mutual understanding between human and alien civilizations.In addition to these findings, our research has also touched on a number of more speculative andphilosophical topics, including the possibility of a ""multiverse of moralities"" - a vast ensemble ofparallel universes, each with its own unique moral framework and set of moral principles. Thisidea, while still highly speculative, has significant implications for our understanding of the natureof morality and the human condition, and raises important questions about the potential for moraldiversity and convergence across the multiverse. Furthermore, our study has also explored thepossibility of using ""moral archeology"" - a technique for reconstructing the moral frameworks ofextinct civilizations through the analysis of archaeological and anthropological data. This approachhas allowed us to gain a unique insight into the moral and philosophical values of long-lost cultures,and has significant implications for our understanding of the evolution of human morality and thedevelopment of complex societies.Finally, our research has also highlighted the need for a more nuanced and sophisticated understandingof the complex interplay between morality, culture, and technology in the context of astrodiplomacy.As we continue to explore the possibility of extraterrestrial life and the potential for interstellar9cooperation and conflict, it is essential that we develop a deeper understanding of the moral andphilosophical principles that underlie the actions and decisions of alien civilizations. This will requirea multidisciplinary approach that incorporates insights from philosophy, theology, anthropology, anda range of other disciplines, and will ultimately depend on our ability to develop a more nuanced andempathetic understanding of the diverse range of moral and cultural perspectives that exist across theuniverse. By pursuing this line of research, we may ultimately uncover new and innovative solutionsto the complex challenges of astrodiplomacy, and develop a more profound understanding of theintricate web of moral and philosophical relationships that bind us to the stars.10"
P017,"Detecting and Summarizing Video Highlights withLag-CalibrationAbstractThe increasing popularity of video sharing has led to a growing need for automaticvideo analysis, including highlight detection. Emerging platforms that featurecrowdsourced, time-synchronized video comments offer a valuable resource foridentifying video highlights. However, this task presents several challenges: (1)time-synchronized comments often lag behind their corresponding shots; (2) thesecomments are frequently sparse and contain noise semantically; and (3) determiningwhich shots constitute highlights is inherently subjective. This paper introducesa novel framework designed to address these challenges. The proposed methoduses concept-mapped lexical chains to calibrate the lag in comments, modelsvideo highlights based on comment intensity and the combined concentrationof emotion and concept within each shot, and summarizes detected highlightsusing an enhanced SumBasic algorithm that incorporates emotion and conceptmapping. Experiments conducted on extensive real-world datasets demonstratethat our highlight detection and summarization methods substantially outperformexisting benchmark techniques.1 IntroductionBillions of hours of video content are viewed daily on platforms like YouTube, with mobile devicesaccounting for half of these views. This surge in video sharing has intensified the demand for efficientvideo analysis. Consider a scenario where a user wishes to quickly grasp the essence of a lengthyvideo without manually navigating through it. Automatically generated highlights would enableusers to digest the video’s key moments in a matter of minutes, aiding their decision on whether towatch the full video later. Furthermore, automated video highlight detection and summarization cansignificantly enhance video indexing, search, and recommendation systems.However, extracting highlights from a video is a complex task. Firstly, the perception of a ""highlight""can vary significantly among individuals. Secondly, analyzing low-level features such as image, audio,and motion may not always capture the essence of a highlight. The absence of high-level semanticinformation poses a significant limitation to highlight detection in conventional video processing.The recent emergence of crowdsourced, time-synchronized video comments, also known as ""bullet-screen comments,"" presents a new avenue for highlight detection. These real-time comments, whichappear overlaid on the video screen, are synchronized with the video frames. This phenomenon hasgained widespread popularity on platforms like niconico in Japan, Bilibili and Acfun in China, andYouTube Live and Twitch Live in the USA. The prevalence of time-synchronized comments offers aunique opportunity for leveraging natural language processing in video highlight detection.Nevertheless, using time-synchronized comments for highlight detection and labeling still posessignificant challenges. Primarily, there is an almost unavoidable delay between comments and theircorresponding shots. As illustrated in Figure 1, discussions about a particular shot may continueinto subsequent shots. Highlight detection and labeling without accounting for this lag may yieldinaccurate outcomes. Secondly, time-synchronized comments are often semantically sparse, bothin terms of the number of comments per shot and the number of words per comment. This sparsitycan hinder the performance of traditional bag-of-words statistical models. Thirdly, determininghighlights in an unsupervised manner, without prior knowledge, involves considerable uncertainty.The defining characteristics of highlights must be clearly defined, captured, and modeled to ensureaccurate detection.To our knowledge, limited research has focused on unsupervised highlight detection and labelingusing time-synchronized comments. The most relevant work in this area proposes detecting highlightsbased on the topic concentration derived from semantic vectors of bullet-comments, and labeling eachhighlight using a pre-trained classifier based on predefined tags. However, we contend that emotionconcentration holds greater significance than general topic concentration in highlight detection.Another study suggests extracting highlights based on the frame-by-frame similarity of emotiondistributions. However, neither of these approaches addresses the combined challenges of lagcalibration, balancing emotion-topic concentration, and unsupervised highlight labeling.To overcome these challenges, this study proposes the following solutions: (1) employ word-to-concept and word-to-emotion mapping based on global word embedding, enabling the construction oflexical chains for calibrating the lag in bullet-comments; (2) detect highlights based on the emotionaland conceptual concentration and intensity of the lag-calibrated bullet-comments; and (3) summarizehighlights using a modified Basic Sum algorithm that considers emotions and concepts as fundamentalunits within a bullet-comment.The main contributions of this research are as follows: (1) We introduce a completely unsupervisedframework for detecting and summarizing video highlights using time-synchronized comments;(2) We introduce a lag-calibration method that uses concept-mapped lexical chains; (3) We havecreated extensive datasets for bullet-comment word embedding, an emotion lexicon tailored forbullet-comments, and ground-truth data for evaluating highlight detection and labeling based onbullet-comments.2 Related Work2.1 Highlight detection by video processingFollowing the definition from previous research, we define highlights as the most memorable shots ina video characterized by high emotional intensity. It’s important to note that highlight detection differsfrom video summarization. While video summarization aims to provide a condensed representationof a video’s storyline, highlight detection focuses on extracting its emotionally impactful content.In the realm of highlight detection, some researchers have proposed representing video emotions as acurve on the arousal-valence plane, utilizing low-level features such as motion, vocal effects, shotlength, and audio pitch, or color, along with mid-level features like laughter and subtitles. However,due to the semantic gap between low-level features and high-level semantics, the accuracy of highlightdetection based solely on video processing is limited.2.2 Temporal text summarizationResearch on temporal text summarization shares similarities with the present study but also exhibitskey distinctions. Several works have approached temporal text summarization as a constrainedmulti-objective optimization problem, a graph optimization problem, a supervised learning-to-rankproblem, and as an online clustering problem.This study models highlight detection as a simpler two-objective optimization problem with specificconstraints. However, the features employed to assess the ""highlightness"" of a shot diverge fromthose used in the aforementioned studies. Given that highlight shots are observed to correlate withhigh emotional intensity and topic concentration, coverage and non-redundancy are not primaryoptimization goals, as they are in temporal text summarization. Instead, our focus is on modelingemotional and topic concentration within the context of this study.2.3 Crowdsourced time-sync comment miningSeveral studies have explored the use of crowdsourced time-synchronized comments for taggingvideos on a shot-by-shot basis. These approaches involve manual labeling and supervised training,2temporal and personalized topic modeling, or tagging the video as a whole. One work proposesgenerating a summarization for each shot through data reconstruction that jointly considers textualand topic levels.One work proposed a centroid-diffusion algorithm to identify highlights. Shots are represented bylatent topics found through Latent Dirichlet Allocation (LDA). Another method suggests using pre-trained semantic vectors of comments to cluster them into topics and subsequently identify highlightsbased on topic concentration. Additionally, they utilize predefined labels to train a classifier forhighlight labeling. The current study differs from these two studies in several ways. First, beforeperforming highlight detection, we apply a lag-calibration step to mitigate inaccuracies causedby comment delays. Second, we represent each scene using a combination of topic and emotionconcentration. Third, we perform both highlight detection and labeling in an unsupervised manner.2.4 Lexical chainLexical chains represent sequences of words that exhibit a cohesive relationship spanning multiplesentences. Early work on lexical chains used syntactic relationships of words from Roget’s Thesaurus,without considering word sense disambiguation. Subsequent research expanded lexical chains byincorporating WordNet relations and word sense disambiguation. Lexical chains are also builtutilizing word-embedded relations for disambiguating multi-word expressions. This study constructslexical chains for accurate lag calibration, leveraging global word embedding.3 Problem FormulationThe problem addressed in this paper can be formulated as follows: The input consists of a set ofC = {c , c , c , . . . , c }time-synchronized comments, denoted as , along with their correspond-1 2 3 nT = {t , t , t , . . . , t } ving timestamps for a given video . We are also given a compression1 2 3 nρratio that determines the number of highlights to be generated, and a compression ratiohighlightρ that specifies the number of comments to be included in each highlight summary. Oursummary S(v) = {s , s , s , . . . , s }objective is twofold: (1) to generate a set of highlight shots , and (2)1 2 3 mΣ(v) = {C , C , C , . . . , C }to produce highlight summaries that closely align with the ground1 2 3 mCtruth. Each highlight summary comprises a subset of the comments associated with that shot:iC = {c , c , c , . . . , c } m. The number of highlight shots and the number of comments in eachi 1 2 3 kk ρ ρsummary are determined by and , respectively.highlight summary4 Video Highlight DetectionThis section introduces our proposed framework for detecting video highlights. We also describetwo preliminary tasks: constructing a global word embedding for time-synchronized comments andbuilding an emotion lexicon.4.1 PreliminariesWord-Embedding of Time-Sync CommentsAs previously highlighted, a key challenge in analyzing time-synchronized comments is their semanticsparsity, stemming from the limited number of comments and their brevity. Two semantically relatedwords might not appear related if they don’t co-occur frequently within a single video. To address this,we construct a global word embedding based on a large collection of time-synchronized comments.D = {(w : v ), (w : v ), . . . , (w : v )}This word-embedding dictionary can be represented as: ,1 1 2 2 n nw v nwhere is a word, is its corresponding word vector, and is the vocabulary size of the corpus.i iEmotion Lexicon ConstructionExtracting emotions from time-synchronized comments is crucial for highlight detection, as em-phasized earlier. However, traditional emotion lexicons are not directly applicable in this contextdue to the prevalence of internet slang specific to these platforms. For example, ""23333"" signifieslaughter (""ha ha ha""), and ""6666"" expresses admiration (""really awesome""). Therefore, we constructan emotion lexicon tailored for time-synchronized comments, derived from the word-embedding3dictionary generated in the previous step. We begin by manually labeling words corresponding tothe five basic emotion categories (happiness, sadness, fear, anger, and surprise) as seeds, selectingfrom the most frequent words in the corpus. The sixth emotion category, ""disgust,"" is omitted dueto its rarity in the dataset but can be easily incorporated for other datasets. We then expand thisNemotion lexicon by identifying the top neighbors of each seed word in the word-embedding space.θA neighbor is added to the seeds if it meets a minimum percentage of overlap with all seeds,overlapsimwith a minimum similarity score of . Neighbors are determined based on cosine similarityminwithin the word-embedding space.4.2 Lag-CalibrationThis section details our method for lag calibration, which involves concept mapping, constructingword-embedded lexical chains, and performing the actual calibration.Concept MappingTo tackle semantic sparsity in time-synchronized comments and build lexical chains of semanticallyrelated words, we first map words with similar meanings to the same concept. Given a set ofC v F V Ccomments for a video , we define a mapping from the vocabulary of comments to a set ofCKconcepts :CF : V → K (|V | ≥ |K |)C C C CF w k = F(w )Specifically, the mapping assigns each word to a concept as follows:i iF(w ) = F(w ) = F(w ) = . . . = F(w ) = k, ∃k ∈ K_i 1 2 top n Cs.t. {w|w ∈ top n(w ) ∧ F(w) = k}/|top n(w )| ≥ θ_ _i i overlapn w wtop n(w )_ returns the nearest neighbors of word based on cosine similarity. For each wordi ii Cin the comments , we examine the percentage of its neighbors that have already been mapped tok θ wa concept . If this percentage exceeds the threshold , then word and its neighbors areoverlap ik wmapped to concept . Otherwise, they are assigned to a new concept, represented by itself.iLexical Chain ConstructionThe next step involves constructing all lexical chains present in the time-synchronized comments forv lvideo . This enables the calibration of lagged comments based on these chains. A lexical chain ikl = {(w, t, c)} w kconsists of a set of triples , where is the actual word mentioned for concept inikc t c Lcomment , and is the timestamp of comment . We create a lexical chain dictionary for theCC vtime-synchronized comments of video :L = {k : (l , l , l , . . .), k : (l , l , l , . . .), . . . , k : (l , l , l , . . .)}C 1 11 12 13 2 21 22 23 n n1 n2 n3k ∈ K l i kwhere represents a concept, and is the -th lexical chain associated with concept . Thei C ikprocedure for constructing these lexical chains is detailed in Algorithm 1.CSpecifically, each comment in can either be appended to an existing lexical chain or added to anew, empty chain. This decision is based on the comment’s temporal distance from existing chains,tcontrolled by the maximum silence parameter .silenceIt’s important to note that word senses within the constructed lexical chains are not disambiguated,unlike in most traditional algorithms. However, we argue that these lexical chains remain usefulbecause our concept mapping is built from time-synchronized comments in their natural order.This progressive semantic continuity naturally reinforces similar word senses for temporally closecomments. This continuity, combined with global word embedding, ensures the validity of ourconcept mapping in most scenarios.Comment Lag-Calibration L CWith the lexical chain dictionary constructed, we can now calibrate the comments in based onCtheir respective lexical chains. Our observations indicate that the initial comment pertaining to ashot typically occurs within that shot, while subsequent comments may not. Therefore, we adjustthe timestamp of each comment to match the timestamp of the first element within its correspondinglexical chain. If a comment belongs to multiple lexical chains (concepts), we select the chain with thescore scorehighest score . The is calculated as the sum of the frequencies of each wordchain chain 4log(D(w).count)in the chain, weighted by the logarithm of their global frequencies, denoted as .Consequently, each comment will be assigned to its most semantically significant lexical chain(concept) for calibration. The calibration algorithm is presented in Algorithm 2.{s , s , . . . , s }It’s worth noting that if multiple consecutive shots, , contain comments with similar1 2 n s , s , . . . , scontent, our lag-calibration method might shift many comments from shots to the2 3 nstimestamp of the first shot, , if these comments are connected through lexical chains originating1sfrom . This is not necessarily a drawback, as it helps us avoid selecting redundant consecutive1highlight shots and allows for the inclusion of other potential highlights, given a fixed compressionratio.4.3 Shot Importance Scoring tIn this section, we first segment comments into shots of equal temporal length, denoted as . Weshotthen model the importance of each shot, enabling highlight detection based on these importancescores.A shot’s importance is modeled as a function of two factors: comment concentration and commentingintensity. Regarding comment concentration, as mentioned earlier, both concept and emotionalconcentration contribute to highlight detection. For instance, a cluster of concept-concentratedcomments like ""the background music/bgm/soundtrack of this shot is classic/inspiring/the best"" couldindicate a highlight related to memorable background music. Similarly, comments such as ""this plotis so funny/hilarious/lmao/lol/2333"" might suggest a highlight characterized by a single concentratedemotion. Therefore, our model combines these two types of concentration. We define the emotionalC (C ) s Cconcentration of shot based on time-synchronized comments and the emotionemotion s sElexicon as follows: (cid:80)|E|C (C ) = 1 − p log(p )emotion s e ee=1|{w|w∈C ∧w∈E(e)}|p = se |C |sHere, we calculate the inverse of the entropy of probabilities for the five emotions within a shot toCrepresent emotion concentration. Next, we define topical concentration as:topic(cid:80)J1C (C ) = p log(p )topic s j jj=1J(cid:80) 1w∈C ∩F(k ) log(D(w))s jp =j (cid:80) 1w∈C log(D(w))swhere we calculate the inverse of the entropy of all concepts within a shot to represent topickconcentration. The probability of each concept is determined by the sum of the frequencies ofits mentioned words, weighted by their global frequencies, and then divided by the sum of theseweighted frequencies for all words in the shot.I (C , s) sNow, the comment importance of shot can be defined as:comment sI (C , s) = λ · C (C , s) + (1 − λ) · C (C , s)comment s emotion s topic sλwhere is a hyperparameter that controls the balance between emotion and concept concentration.Finally, the overall importance of a shot is defined as:I(C , s) = I (C , s) · log(|C |)s comment s s|C | swhere represents the total length of all time-synchronized comments within shot , serving as asstraightforward yet effective indicator of comment intensity per shot.The problem of highlight detection can now be formulated as a maximization problem:(cid:80) I(C , s)Maximize ss∈S|S| ≤ ρ · NSubject to highlight 55 Video Highlight SummarizationS(v) = {s , s , s , . . . , s } vGiven a set of detected highlight shots for video , each associated with1 2 3 mC Σ(v) = {C , C , C , . . . , C }its lag-calibrated comments , our goal is to generate summariess 1 2 3 mC ⊂ C ρ Csuch that , with a compression ratio of , and closely resembles the groundi s summary iitruth.We propose a simple yet highly effective summarization model, building upon SumBasic withenhancements that incorporate emotion and concept mapping, along with a two-level updatingmechanism.In our modified SumBasic, instead of solely down-weighting the probabilities of words in a selectedsentence to mitigate redundancy, we down-weight the probabilities of both words and their mappedconcepts to re-weight each comment. This two-level updating approach achieves two key objectives:(1) it penalizes the selection of sentences containing semantically similar words, and (2) it allows forthe selection of a sentence with a word already present in the summary if that word occurs significantlybmore frequently. Additionally, we introduce an emotion bias parameter, , to weight wordsemotionand concepts during probability calculations. This increases the frequencies of emotional words andbconcepts by a factor of compared to non-emotional ones.emotion6 ExperimentThis section presents the experiments conducted on large-scale real-world datasets to evaluatehighlight detection and summarization. We describe the data collection process, evaluation metrics,benchmark methods, and experimental results.6.1 DataThis section describes the datasets collected and constructed for our experiments. All datasets andcode will be made publicly available on Github.Crowdsourced Time-sync Comment CorpusTo train the word embedding described earlier, we collected a large corpus of time-synchronizedcomments from Bilibili, a content-sharing website in China that features such comments. The corpuscomprises 2,108,746 comments, 15,179,132 tokens, and 91,745 unique tokens, extracted from 6,368long videos. On average, each comment contains 7.20 tokens.Before training, each comment undergoes tokenization using the Chinese word tokenization package˘ ˘ ˘ ˘Jieba. Repeated characters within words, such as ""233333,"" ""66666,"" and ""54c854c854c854c8,"" arereplaced with two instances of the same character.The word embedding is trained using word2vec with the skip-gram model. We set the number ofembedding dimensions to 300, the window size to 7, and the down-sampling rate to 1e-3. Words witha frequency lower than 3 are discarded.Emotion Lexicon ConstructionAfter training the word embedding, we manually select emotional words belonging to the five basicemotion categories from the 500 most frequent words in the embedding. We then iteratively expandthese emotion seeds using Algorithm 1. After each expansion iteration, we manually review theexpanded lexicon, removing any inaccurate words to prevent concept drift. The filtered expandedθseeds are then used for further expansion in the next round. The minimum overlap is set tooverlapsim0.05, and the minimum similarity is set to 0.6. These values are determined through a gridminsearch within the range of [0, 1]. The number of words for each emotion, both initially and after thefinal expansion, is presented in Table 3.Video Highlights DataTo evaluate our highlight detection algorithm, we constructed a ground-truth dataset. This datasetleverages user-uploaded mixed-clips related to a specific video on Bilibili. Mixed-clips represent acollection of video highlights chosen according to the user’s preferences. We then consider the mostfrequently selected highlights as the ground truth for a given video.6Table 1: Number of Initial and Expanded Emotion WordsHappy Sad Fear Anger SurpriseSeeds 17 13 19 21 14All 157 235 258 284 226The dataset consists of 11 videos totaling 1333 minutes in length, with 75,653 time-synchronizedcomments. For each video, 3-4 video mix-clips are collected from Bilibili. Shots that appear in atleast two of these mix-clips are considered ground-truth highlights. These highlights are mapped tothe original video timeline, and their start and end times are recorded as ground truth. Mix-clips areselected based on the following criteria: (1) they are found on Bilibili using the search query ""videotitle + mixed clips""; (2) they are sorted by play count in descending order; (3) they primarily focus onvideo highlights rather than a plot-by-plot summary or gist; (4) they are under 10 minutes in length;and (5) they contain a mix of several highlight shots instead of just one.On average, each video contains 24.3 highlight shots. The mean duration of these highlight shots is27.79 seconds, while the mode is 8 and 10 seconds (with a frequency of 19).Highlights Summarization DataWe also created a highlight summarization (labeling) dataset for the 11 videos. For each highlightshot and its associated comments, we asked annotators to create a summary by selecting as manycomments as they deemed necessary. The guiding principles were: (1) comments with identicalmeanings should not be selected more than once; (2) the most representative comment among similarcomments should be chosen; and (3) comments that stand out and are irrelevant to the currentdiscussion should be discarded.Across the 11 videos and 267 highlights, each highlight has an average of 3.83 comments in itssummary.6.2 Evaluation MetricsThis section introduces the evaluation metrics employed for both highlight detection and summariza-tion.Video Highlight Detection EvaluationTo evaluate video highlight detection, we need to define a ""hit"" between a candidate highlight and areference highlight. A strict definition would require a perfect match between the start and end timesof the candidate and reference highlights. However, this criterion is overly stringent for any model.A more lenient definition would consider an overlap between a candidate and a reference highlight.However, this can still underestimate model performance, as users’ choices of highlight start and endδtimes can sometimes be arbitrary. Instead, we define a ""hit"" with a relaxation parameter between ah Rcandidate and the reference set as follows:hit(h, R) = { ∃r ∈ R : (s , e ) ∩ (s − δ, e + δ) ̸= ∅1 h h r r0otherwises e h δwhere , represent the start and end times of highlight , and is the relaxation length applied toh h Rthe reference set . We can then define precision, recall, and F1-score as:(cid:80) hit(h,R)P recision(H, R) = h∈H|H|(cid:80) hit(r,H)Recall(H, R) = r∈R|R|2·P recision(H,R)·Recall(H,R)F 1(H, R) = P recision(H,R)+Recall(H,R) δIn this study, we set the relaxation length to 5 seconds. The candidate highlight length is set to 15seconds.Video Highlight Summarization EvaluationWe utilize ROUGE-1 and ROUGE-2 as recall metrics for evaluating candidate summaries:7(cid:80) (cid:80) Count (n−gram)matchr∈R n−gram∈rROU GE − n(C, R) = (cid:80) (cid:80) Count(n−gram)r∈R n−gram∈rWe employ BLEU-1 and BLEU-2 as precision metrics. BLEU is chosen for two reasons. First, anaive precision metric would be biased towards shorter comments, and BLEU mitigates this with theBP (Brevity Penalty) factor:(cid:80) (cid:80) Count (n−gram)clipc∈C n−gram∈cBLEU − n(C, R) = BP · (cid:80) (cid:80) Count(n−gram)c∈C n−gram∈cBP = { if |C| > |R|1(1−|R|/|C|)e if |C| ≤ |R|C Rwhere is the candidate summary and is the reference summary. Second, while the referencesummary contains no redundancy, the candidate summary might incorrectly select multiple similarcomments that match the same keywords in the reference. In such cases, precision would besignificantly overestimated. BLEU addresses this by counting matches one-by-one; the number ofmatches for a word will be the minimum of its frequencies in the candidate and reference summaries.Finally, the F1-score is defined as:2·BLEU−n(C,R)·ROUGE−n(C,R)F 1 − n(C, R) = BLEU−n(C,R)+ROUGE−n(C,R)6.3 Benchmark methodsBenchmarks for Video Highlight DetectionFor highlight detection, we compare different combinations of our model against three benchmarkmethods:* **Random-Selection:** Highlight shots are randomly selected from all shots in a video. ***Uniform-Selection:** Highlight shots are selected at equal intervals. * **Spike-Selection:** High-light shots are chosen based on the highest number of comments within the shot. * **Spike+E+T:**This is our method, incorporating emotion and topic concentration but without lag calibration. ***Spike+L:** This is our method, including only the lag-calibration step and not considering contentconcentration. * **Spike+L+E+T:** This represents our full model.Benchmarks for Video Highlight SummarizationFor highlight summarization, we compare our method against five benchmark methods:* **SumBasic:** Summarization that relies solely on frequency for summary construction. * **LatentSemantic Analysis (LSA):** Text summarization based on singular value decomposition (SVD)for latent topic discovery. * **LexRank:** Graph-based summarization that calculates sentenceimportance using the concept of eigenvector centrality in a sentence graph. * **KL-Divergence:**Summarization based on minimizing KL-divergence between the summary and the source corpus,employing a greedy search approach. * **Luhn method:** A heuristic summarization method thatconsiders both word frequency and sentence position within an article.6.4 Experiment ResultsThis section presents the experimental results for both highlight detection and highlight summariza-tion.Results of Highlight Detection tIn our highlight detection model, the maximum silence threshold for lexical chains, , is setsilenceθto 11 seconds. The threshold for concept mapping, , is set to 0.5. The number of neighborsoverlaptop n λconsidered for concept mapping, _ , is set to 15. The parameter , which controls the balancebetween emotion and concept concentration, is set to 0.9. A detailed parameter analysis is providedin Section 7.Table 4 presents the precision, recall, and F1-scores for different combinations of our method and thebenchmark methods. Our full model (Spike+L+E+T) outperforms all other benchmarks across allmetrics. Random and uniform selection exhibit low precision and recall, as they don’t incorporatestructural or content information. Spike-selection shows significant improvement by leveraging8comment intensity. However, not all comment-intensive shots are highlights. For example, commentsat the beginning and end of a video are often high-volume greetings or goodbyes, which may not beindicative of highlights. Additionally, spike-selection tends to cluster highlights within consecutiveshots with high comment volumes. In contrast, our method can identify less intensive but emotionallyor conceptually concentrated shots that might be missed by spike-selection. This is evident in theperformance of Spike+E+T.We also observe that lag calibration alone (Spike+L) considerably enhances the performance ofSpike-selection, partially supporting our hypothesis that lag calibration is crucial for tasks involvingtime-synchronized comments.Table 2: Comparison of Highlight Detection MethodsMethod Precision Recall F1-scoreRandom-Selection 0.1578 0.1567 0.1587Uniform-Selection 0.1797 0.1830 0.1775Spike-Selection 0.2594 0.2167 0.2321Spike+E+T 0.2796 0.2357 0.2500Spike+L 0.3125 0.2690 0.2829Spike+L+E+T 0.3099 0.3071 0.3066Results of Highlight Summarization bIn our highlight summarization model, the emotion bias is set to 0.3.emotionTable 5 compares the 1-gram BLEU, ROUGE, and F1-scores of our method and the benchmarkmethods. Our method outperforms all others, particularly in terms of ROUGE-1. LSA exhibits thelowest BLEU score, primarily because it statistically favors longer, multi-word sentences, which arenot representative in time-synchronized comments. The SumBasic method also performs relativelypoorly, as it treats semantically related words separately, unlike our method, which uses conceptsinstead of individual words.Table 3: Comparison of Highlight Summarization Methods (1-Gram)Method BLEU-1 ROUGE-1 F1-1LSA 0.2382 0.4855 0.3196SumBasic 0.2854 0.3898 0.3295KL-divergence 0.3162 0.3848 0.3471Luhn 0.2770 0.4970 0.3557LexRank 0.3045 0.4325 0.3574Our method 0.3333 0.6006 0.42877 ConclusionThis work presents a novel unsupervised framework for video highlight detection and summarization,based on crowdsourced time-synchronized comments. We introduce a lag-calibration techniquethat re-aligns delayed comments to their corresponding video scenes by using concept-mappedlexical chains. Video highlights are identified based on comment intensity and the concentrationof concepts and emotions within each shot. For summarization, a two-level SumBasic is proposedwhich updates word and concept probabilities iteratively when selecting sentences. Future workincludes integrating additional data sources such as video meta-data, audience profiles, and low-levelmulti-modal features. 9"
P019,"Acquiring the Ability to Recommend Interventions for TuberculosisTreatment Through the Utilization of Digital Adherence InformationAbstractDigital Adherence Technologies (DATs) are becoming progressively favored as a means of confirming patients’adherence to various medications. This paper examines the information gathered from a city that utilizes 99DOTS,a telephone-based DAT implemented for tuberculosis (TB) treatment in India, where approximately 3 millionindividuals are diagnosed with the disease annually. The dataset encompasses approximately 17,000 patientsand 2.1 million dosage records. This research establishes the basis for deriving insights from this real-worlddata, encompassing a methodology to circumvent the influence of unrecorded interventions in the trainingdata employed for machine learning. Subsequently, a deep learning model is developed, its interpretability isillustrated, and it is demonstrated how it can be modified and trained under diverse clinical conditions to moreeffectively target and enhance patient treatment. In the context of real-time risk prediction, the model could beemployed to proactively intervene with 21% more patients and prevent 76% more missed doses compared tothe current heuristic benchmarks. Regarding outcome prediction, the model exhibits 40% improvement overbaseline approaches, enabling cities to allocate more resources to clinics with a higher proportion of patientssusceptible to treatment failure. Lastly, a case study is presented that illustrates how the model can be trained in anend-to-end, decision-focused learning framework to realize a 15% enhancement in solution quality in a sampledecision problem encountered by healthcare professionals.1 IntroductionThe World Health Organization (WHO) has identified tuberculosis (TB) as one of the leading ten causes of mortality globally, despiteit being a curable and preventable disease in the majority of instances. The widespread occurrence of TB is partially attributableto inadequate adherence to medication, which leads to an elevated probability of mortality, reinfection, and the development ofdrug-resistant strains of TB. To address the issue of non-adherence, the WHO advocates for directly observed treatment (DOT),wherein a healthcare professional directly observes and validates a patient’s daily intake of the necessary medication. Nevertheless,the necessity for patients to commute to the DOT facility imposes a financial strain and potentially introduces social stigma becauseof the public apprehension surrounding the disease. These obstacles make it challenging to eradicate TB, as they contribute topatients being lost to follow-up. Consequently, digital adherence technologies (DATs), which offer patients adaptable methods todemonstrate adherence, have experienced a surge in popularity on a global scale.DATs empower patients to be ""observed"" consuming their medication electronically through various means, such as two-waytext messaging, video recording, electronic pill containers, or toll-free phone calls. Healthcare professionals can subsequentlymonitor patient adherence in real-time using a dashboard. Besides enhancing patient adaptability and confidentiality, the dashboardempowers healthcare personnel to categorize patients and allocate their constrained resources towards those at the highest risk.Initial research indicates that DATs have the potential to enhance adherence in various disease contexts, thereby stimulating theirutilization and assessment for the management of TB adherence. The WHO has even issued a manual for the effective incorporationof this technology in TB patient care.In this paper, the focus is on investigating how the extensive longitudinal data generated by DATs can be utilized to assist healthworkers in better triaging TB patients and providing interventions to enhance the overall adherence of their patient group. The dataunder analysis originates from Mumbai, India, and is the result of a collaboration with the City TB Office of Mumbai. They haveput into practice a DAT that enables patients to verify their adherence by making daily toll-free calls. The DAT system was setup with technical assistance from the healthcare technology company Everwell and is recognized as 99DOTS. Everwell providessupport for the implementation of 99DOTS across India, where there were an estimated 2.7 million cases of TB in 2017. In Mumbai,patients registered in 99DOTS currently receive interventions based on the following broad guidelines. If they have not taken theirmedication by the afternoon, they (and their health worker) get a text message reminder. If the patient still does not take theirmedication after some time, the worker will call the patient directly. Lastly, if a patient does not respond to these interventions aftera certain number of days, they may be personally visited by a health worker. It is important to note that a significant number of thesepatients reside in communities with limited resources, where each health worker is responsible for managing dozens to hundredsof patients, far exceeding their capacity for daily visits. Therefore, models that can pinpoint patients at risk of missing doses andprioritize interventions by health workers are of the utmost importance.At first, the challenge of determining whom to target for an intervention seems to be a straightforward supervised machine learningtask. Provided with information regarding a patient’s medication adherence as indicated by their calls to the 99DOTS system, it ispossible to train a machine learning model to forecast whether they will miss medication doses in the future. Nevertheless, such amodel disregards the simultaneous interventions carried out by health workers during the data collection period and may result inerroneous prioritization choices, even when it exhibits high accuracy. As an illustration, it might be observed that missed doses aresucceeded by a phase of medication adherence. This observation does not imply that individuals who miss doses are more inclinedto take medication, but rather suggests that an intervention by a health worker likely occurred, after which the patient resumed theirmedication.Therefore, to prescribe interventions, it’s necessary to separate the impact of manual interventions from other underlying elementsthat contribute to missed doses. However, because this data was gathered through a wide-ranging implementation involving actualpatients, it incorporates the impacts of interventions executed by healthcare personnel. An added difficulty is that healthcare workersseldom document their interventions within the 99DOTS system, making it hard to gauge their consequences. Although there is asubstantial body of research on assessing heterogeneous treatment effects, conventional methods consistently necessitate awarenessof which patients underwent an intervention. It should be noted that such omissions will be prevalent as nations enthusiasticallyimplement DAT systems with the aim of aiding low-income areas. To facilitate the provision of enhanced care, it is imperative thatwe can glean insights from this complex yet abundant data.Hence, a general strategy is introduced for acquiring knowledge from adherence data with unrecorded interventions, grounded indomain expertise regarding the intervention heuristics used by healthcare workers. A proxy is created for interventions evident inthe historical 99DOTS data, and a model is devised to aid in prioritizing intervention targets for healthcare workers across variousclinical scenarios.2 MethodologyThe TB treatment system functions under severe resource constraints; for instance, a single health worker might be in charge ofover 100 patients. Therefore, it is essential that workers can precisely evaluate patient risk and prioritize interventions appropriately.Although machine learning can be employed to carry out such risk assessment with encouraging precision, it necessitates carefulconsideration of how intervention resources were distributed in the current data.A significant obstacle arises from the fact that users of the 99DOTS platform typically do not document interventions. Healthworkers might send texts, make calls, or conduct personal visits to patients in an effort to boost adherence, but these interventions arenot systematically recorded in the data. Although far from perfect, these gaps are unavoidable as countries with varying reportingstandards adopt DATs for TB treatment. Considering the wealth of data produced by DATs and their potential to affect humanlives, the importance of learning lessons in this demanding setting where unobserved interventions take place is emphasized. Thischallenge is subsequently addressed by developing a screening procedure that recognizes patients who were probable candidates forspecific interventions.The aim is to utilize the accessible data to create an approximation for when an intervention likely took place, enabling the trainingof models on data points unaffected by interventions. The initial step involves differentiating between various categories of healthworker interventions. Specifically, a house visit is regarded as a ""resource-limited"" intervention, given that workers are unable to visitall their patients promptly. Typically, this represents a last resort for health workers when patients are unresponsive to alternativemethods. On the other hand, calls and texts are viewed as ""non-resource-limited"" interventions, as they could feasibly be conductedon a large patient population at minimal expense.To develop the proxy, a search was conducted for health worker guidelines concerning house visits. The 2005 guide by India’sRevised National Tuberculosis Control Program (RNTCP) mandated that workers perform a house visit after a single missed dose.However, more recent guidelines are considerably more ambiguous on this matter. Both the latest guide by the WHO and theRNTCP leave house visits to the health worker’s discretion. Nevertheless, through discussions in Mumbai, it was discerned thathealth workers give precedence to non-adherent patients for resource-limited interventions like house visits. Consequently, the proxywas formulated based on the adherence dashboard accessible to health workers.The 99DOTS dashboard provides a daily ""Attention Required"" status for each patient. Initially, if a patient has a record in the PatientLog, signifying that a provider made a note about the patient within the preceding 7 days, their status is automatically adjusted to""MEDIUM"" attention. However, this guideline impacts fewer than 1% of the labels. The remaining 99% of labels are determined asfollows: if a patient misses 0 or 1 doses in the past 7 days, their attention level is changed to ""MEDIUM."" If they miss 4 or more, itis changed to ""HIGH."" Patients with 2-3 missed doses maintain their attention level from the day before. As a conservative proxy, itwas assumed that only ""HIGH"" attention patients were candidates for resource-limited interventions, considering that the attentionlevel serves as a health worker’s primary overview of recent patient adherence. This ""Attention Required"" system for screeningresource-limited interventions is applicable to any daily adherence context; one only needs to ascertain the threshold for a change toHIGH attention. 2Employing this screening system, sequences of days can be identified during which a patient was a candidate for a resource-limitedintervention, and subsequently, the use of signal from those days in the training task can be avoided.3 ExperimentsThe objective was to create a model that mirrors the daily routine of a health worker, which involves analyzing their patients’ recentcall records to gauge adherence risk and subsequently planning various types of interventions. Enhanced prediction capabilitiesenable workers to engage with a greater number of patients proactively, prior to their missing crucial doses.The process began with the entire group of 16,975 patients and proceeded to create training samples from each patient in thefollowing manner. All consecutive sequences of 14 days of call data were considered, ensuring that the initial 7 days of eachsequence did not overlap. The first 7 days of each patient’s treatment, as well as the final day, were omitted to prevent any bias thatmight arise from interactions with health workers during the initiation or conclusion of treatment. Two filtering steps were thenimplemented. Initially, samples were excluded where the patient had in excess of 2 doses manually recorded by a provider during theinput sequence, as these patients likely had contact with their provider outside of the 99DOTS system. Secondly, samples in whichthe patient did not miss any doses in the input sequence were removed. Although these samples constituted the majority of the data,they included almost no positive (HIGH risk) labels, which distorted the training process. Moreover, positive predictions for patientswho missed 0 doses are improbable to be beneficial; no resource-limited intervention can be implemented so extensively that patientswith flawless recent adherence are targeted. The aforementioned steps yielded 16,015 samples, of which 2,437 were positive.Each sample comprised a time-series of call data along with static characteristics. The time series encompassed two sequences of 7in length for every sample. The initial sequence was a binary representation of call data, where 1 signified a call or manual doseand 0 indicated a miss. The subsequent sequence represented a cumulative count of all doses missed up to that specific day, takinginto account the patient’s entire history within the program. The static features incorporated four demographic attributes from thePatient Table: weight-band, age-band, gender, and treatment center ID. Supplementary features were derived from the patient CallLogs and captured a patient’s behavior beyond mere adherence. For instance, did the patient call at a consistent time each morningor at irregular intervals throughout the day? This was captured by calculating the mean and variance of the call minute and hour.Additional features encompassed the number of calls, number of manual doses, and the mean, maximum, and variance of calls perday, in addition to days per call. Analogous features were also incorporated, which exclusively utilized unique calls per day (i.e.,calls to distinct phone numbers) or disregarded manual doses. This procedure resulted in 29 descriptive features.Initially, standard models were tested that utilize solely the static features: linear regression, a random forest (with 100 trees and amaximum depth of 5), and a support vector machine. The random forest exhibited the best performance, so the others are omitted forthe sake of clarity. To make use of the time series data, a deep network was also constructed, designated as LEAP (Lstm rEal-timeAdherence Predictor), which accepts both the time series and static features as input. LEAP comprises two input layers: 1) an LSTMwith 64 hidden units for the time series input, and 2) a dense layer with 100 units for the static feature input. The outputs of thesetwo layers were concatenated and fed forward into another dense layer with 16 units, followed by a single sigmoid activation unit. Abatch size of 128 was employed, and training was conducted for 20 epochs.To assess the models, all data was randomized, and 25% was set aside as the test set. A 4-fold grid search was employed to ascertainthe optimal model parameters. To address class imbalance, SMOTE was utilized to oversample the training set, implemented usingthe Python library imblearn. Features were also normalized as percentiles using SKLearn, which was empirically found to beeffective. The benchmark for comparison was the method employed by the current 99DOTS platform to evaluate risk, namely, dosesmissed by the patient in the preceding week (lw-Misses).4 ResultsThe models were compared against the baseline. The random forest slightly surpasses the baseline, and LEAP distinctly outperformsboth. Nevertheless, to gauge the efficacy of the methods relative to the baseline, a comparison is made regarding how each methodcould be applied to strategize house-visit interventions. Given that this constitutes a highly constrained resource, the most stringentbaseline threshold was established to contemplate patients for this intervention, specifically, 3 missed calls. Maintaining the FPR ofthis baseline method, it is demonstrated how many more patients in the test set would be reached weekly by the proposed method(owing to its enhanced TPR), alongside the enhancement in the quantity of missed doses detected. To ascertain the number of misseddoses caught, only missed doses that transpired before the patient’s transition to HIGH risk are counted. The model identifies 21.6%more patients and captures 76.5% more missed doses, signifying substantially more accurate targeting than the baseline.It is shown that the model also surpasses the baseline as both the true positive rate (TPR) and FPR escalate, underscoring the model’ssuperior discriminatory capability. This proves advantageous for interventions not constrained by resources, like calls or texts. Itis important to remember that the screening procedure is not pertinent to this category of intervention; therefore, the predictionscan solely advocate for supplementary interventions. It is crucial that additional interventions are meticulously aimed, as repeatedengagement with a specific patient diminishes the effectiveness of each subsequent interaction over time. This emphasizes thesignificance of the enhanced precision provided by the model, as merely inundating the entire population with calls and texts isprobable to be ineffective. 3The model has the capability to prevent a greater number of missed doses compared to existing approaches. Nonetheless, theseadvancements cannot be realized unless health workers on the ground administer interventions in accordance with the predictions.Consequently, interpretability emerges as a crucial determinant of the model’s utility, as health workers must comprehend therationale behind the model’s predictions to trust it and incorporate its logic with their own professional expertise.The superior predictive performance was attained with LEAP, a black-box network, as opposed to an inherently interpretable modelsuch as linear regression. As a result, it is demonstrated how a visualization instrument can assist users in extracting insightsregarding the model’s reasoning. The SHapley Additive exPlanations (SHAP) python library was employed, which producesvisualizations to elucidate machine learning models. It is illustrated how static features affect the model’s prediction, where redfeatures drive predictions toward 1 (HIGH) and blue toward 0 (MEDIUM). It is important to recall that features are scaled aspercentiles. In the blue region, it is observed that this patient makes an above-average number of calls each week, pushing theprediction toward 0. Conversely, in the red region, it is noted that this patient has a very low average but a high variability in timebetween calls. These features capture that this patient missed two days of calls, then made three calls on one day in an attempt to""back log"" their previous missed calls. The model learned that this is a high-risk behavior.Four distinct samples are presented as input to the LSTM layer of the model. On the left, the binary input sequence is depicted ascolored pixels, where black represents a call and yellow signifies a missed call. On the right, SHAP values corresponding to eachday of adherence data are displayed, and grey denotes the commencement of the call sequence. It is observed that the model hasdiscerned that calls made later in the week carry more weight than those made earlier. In Sample 1, the bottom two pixels (the mostrecent calls) have blue SHAP values, while the other pixels have SHAP values close to 0. In Sample 3, a single missed call at thebeginning of the week, combined with a call made at the end of the week, result in essentially canceling SHAP values. Sample 4also has one missed call, but on the last day of the week, resulting in a net positive SHAP value.This visualization method offers intuitive insights into the principles acquired by the model. In a real-world application, healthcareprofessionals could produce these visualizations for any given sample on-the-fly to support their decision-making procedure.5 ConclusionA framework is introduced for acquiring the ability to generate intervention recommendations from data produced by DAT systemsused in TB care. A comprehensive strategy is formulated for learning from medical adherence data that includes unrecordedinterventions, and this strategy is utilized to construct a model for forecasting risk in various contexts. In the real-time adherencescenario, it is demonstrated that the model would empower health workers to more precisely direct interventions to high-risk patientsat an earlier stage, identifying 21% more patients and preventing 76% more missed doses than the existing heuristic benchmark.Subsequently, the model is trained for outcome prediction, illustrating how adherence data can more accurately detect patientsat risk of unfavorable treatment outcomes. Insights are then derived that could assist health workers in accurately identifyingLCFO patients using a straightforward rule after a mere 7 days of treatment. Finally, it is demonstrated that adapting the LEAPmodel for a particular intervention through decision-focused learning can enhance performance by an additional 15%. The learningmethodologies presented here are versatile and could be applied to analyze data generated by DATs for any medication schedule.Given the increasing adoption of DAT systems for TB, HIV, diabetes, heart disease, and other medications, this work aims toestablish the groundwork for enhanced patient outcomes in healthcare settings worldwide.6 Outcome PredictionThe subsequent phase involves an investigation into how adherence data can be employed to forecast the ultimate treatment outcome.Conventional studies on TB treatment typically model outcomes solely in relation to patient covariates, such as demographiccharacteristics. By utilizing daily real-time adherence data furnished by DATs, an exploration is conducted into how employingthe initial k days of a patient’s adherence facilitates more precise, individualized outcome predictions. It is important to notethat intervention effects are still discernible in this configuration. Nevertheless, the screening procedure will not be applicable,as predictions are made over a span of several months, during which practically all patients would have had recurring in-personinteractions with healthcare providers.The prediction task is formalized in the following manner: given the first k days of adherence data, predict the final binary treatmentoutcome. ""Cured"" and ""Treatment Complete"" were regarded as favorable outcomes, while ""Died,"" ""Lost to follow-up,"" and""Treatment Failure"" were considered unfavorable. Solely patients who were assigned an outcome from these classifications areincorporated. Furthermore, given that patients with the outcome ""Died"" or ""Lost to follow-up"" exit the program prior to the full 6months of treatment, those who were present for less than k + 1 days were excluded. Lastly, patients who had in excess of half theirfirst k days marked as manual doses were omitted. This was inclined to enhance prediction performance, which is conjectured to beassociated with the observation that practices for reporting manual doses varied by health center, rendering the ""significance"" of amanual dose ambiguous across samples with respect to outcome. The final dataset comprised 4167 samples, with 433 unfavorablecases.Through discussions in Mumbai, it was learned that health workers often build a sense of a patient’s risk of an unfavorable outcomewithin their first month of treatment. To model this process, k=35 was set for the prediction task, capturing the first month of eachpatient’s adherence after enrollment in 99DOTS. (Note that this is not a general rule for health workers, but simply served as a4motivation for the choice of k in this task.) Both the static features and the sequence inputs were the same as calculated for theweekly prediction task, but now taken over the initial 35 days. Two versions of the health worker baseline were included: misseddoses in the last week (lw-Misses) and total missed doses in 35 days (t-Misses).The same models, grid search design, training process, and evaluation procedure as before were used. For the Random Forest, 150trees were used with no maximum depth. For LEAP, 64 hidden units were used for the LSTM input layer, 48 units for the denselayer input, and 4 units in the penultimate dense layer.Even the rudimentary baseline of tallying the calls made in the preceding 7 days before the 35-day threshold is somewhat predictiveof the outcome, implying that the daily data provided by DATs is valuable in assessing which patients will fail TB treatment. TheML models exhibit even greater predictive capability, with LEAP leading in performance, closely followed by the random forest.It is emphasized how LEAP’s predictive ability could aid officials in minimizing the expenses required to meet medical outcometargets for their city. For instance, suppose Mumbai initiates a new program to capture 80% of unfavorable outcomes (true positives)by recruiting additional health staff. Across the 17,000 patients in Mumbai, where 10% have unsuccessful outcomes as in the testset, an 80% capture rate necessitates rescuing 1360 patients. Employing either baseline, attaining the 80% TPR necessitates an FPRof 70%, which translates to hiring extra staff to support 10710 total patients in this hypothetical scenario. However, utilizing LEAPonly results in an FPR of 42%, corresponding to 6426 total patients. It is important to remember that in Mumbai, the typical healthworker attends to approximately 25 patients. With a yearly starting salary of |216,864, the model would result in |37M in saved costsannually.7 Detecting Low-Call Favorable Outcome PatientsAn additional significant hurdle within the 99DOTS system is that certain patients consistently take their doses as directed but opt notto call. Consequently, according to the dashboard, they appear to be missing doses and would be categorized as HIGH risk by both99DOTS and LEAP. However, in actuality, they should be classified as MEDIUM risk. In fact, almost 15% of patients who had anoutcome assigned as in section 3 called on fewer than 25% of the days during their treatment, yet experienced a favorable outcome.These patients are referred to as low-call favorable outcome (LCFO). The aim is to learn to recognize these LCFO patients to avoidincorrectly classifying them as HIGH risk, despite their lack of calls. Additionally, there is a desire to identify these patients early intheir treatment so they can be reassigned to an adherence monitoring method that is more appropriate for them.This is framed as a binary prediction task as follows: given the first k days of adherence data, predict whether the patient will bothcall on less than 25% of days from day k + 1 onward and have a favorable outcome. Only patients who were assigned an outcome asin Section 3 and who had at least k + 7 days of adherence data were included. To detect LCFO status as early as possible, k was setto 7. Thus, the final dataset contained 7265 patients, of which 1124 were positive. Note that this population was larger than that ofthe outcome prediction task because 1) patients were required to be in the program for less time and 2) patients were not removedfor having too many manual doses since this was found to correlate with being LCFO.Both the static features and the sequence inputs were the same as calculated for the outcome prediction task, but this time taken overthe initial 7 days. The health worker baseline of missed doses in the last week (lw-Misses) was included, along with a random foresttrained only on demographic or ""0-day"" data (RF 0-day), a simple baseline that counts the number of manual doses in the last week(lw-Manual), a random forest trained on all non-sequence features over the initial 7 days (RF), and LEAP trained on all features andsequences.The same models, grid search design, training process, and evaluation procedure as the previous two formulations were used. For RF0-day, 300 trees were used with a maximum depth of 10. For RF, 200 trees were used with a maximum depth of 10. For LEAP, 200hidden units were used for the LSTM input layer, 1000 units for the dense layer input, and 16 units in the penultimate dense layer.Interestingly, for this task, the lw-Misses baseline has almost no predictive power. Conversely, the performance of the lw-Manualheuristic is notable, which simply counts the number of manual doses marked in the first 7 days for each patient. This simpleheuristic has almost equivalent predictive power to the machine learning models. This is a valuable insight for health workers,suggesting that if the worker is already manually marking doses for a patient early in their treatment, the patient is likely to continueto be disengaged with the system in the long term and should be considered for different adherence technology. The RF 0-day modelhas decent predictive power, though closer inspection reveals that most of this power is encoded in the treatment center ID – that is,LCFO patients tend to be concentrated at certain treatment centers. This insight merits closer inspection by supervisors about whypatients in certain regions tend to be disengaged with 99DOTS but still consuming pills. The RF and LEAP models both performslightly better than the lw-Manual baseline but similarly to each other, suggesting that the adherence sequence structure does notencode additional information for this prediction task. These insights could improve processes by 1) helping to identify hotspotregions of LCFO patients, after which supervisors might investigate the underlying reason and adjust treatment accordingly at thosecenters and 2) the lw-Manual baseline, after only 7 days of dosage data, could give health workers a simple rule for identifyingLCFO patients that should switch to different adherence technology.58 Decision Focused LearningThis section delves into a case study illustrating how the LEAP model can be specialized to furnish decision support for a specificintervention. The end-to-end differentiability of the model is utilized to supplant the earlier loss function (binary cross-entropy)with a performance metric customized to the objective and limitations of a particular decision problem. To realize this end-to-endtraining, recent developments in decision-focused learning are employed, which incorporates an optimization model within themachine learning training loop.The focus is on a particular optimization problem that simulates the allocation of health workers to intervene with patients who areat risk in the near future. This proactive intervention is facilitated by the real-time risk predictions and exemplifies how the systemcan empower preemptive, focused action by providers. Nonetheless, it is underscored that the system can be readily adapted toaccommodate other intervention problems. Such adaptability is one of the advantages of the technical approach, which permits theML model to automatically adjust to the problem delineated by a domain expert.The optimization problem models a health worker who orchestrates a sequence of interventions throughout a week. The healthworker is accountable for a patient population across various locations and may visit one location daily. Location identifiers areemployed at the TB Unit level, as this is the most detailed identifier shared by the majority of patients in the dataset. Visiting alocation enables the health worker to intervene with any of the patients at that location. The optimization problem involves choosinga set of locations to visit that maximizes the number of patients who receive an intervention on or before the first day they wouldhave missed a dose. This quantity is referred to as the number of successful interventions, which is selected as the objective for tworationales. Firstly, it gauges the degree to which the health worker can proactively engage with patients before adherence declines.Secondly, this objective exclusively counts patients who commence the week at MEDIUM attention and receive an interventionbefore they could have transitioned to HIGH, aligning with the earlier discussion on circumventing unobserved interventions in thedata. This extends the earlier intervention proxy to manage day-by-day rewards. i = 1, . . . , L j = 1, . . . , NThe optimization problem can be formalized as a linear program. There is a set of locations and patients ,j ℓ t = 1, . . . , 7 c twhere patient has location . Over the days of the week , the objective coefficient is 1 if an intervention on dayj jtj xwith patient is successful and 0 otherwise. The decision variable is , which takes the value 1 if the health worker visits locationiti ton day and 0 otherwise. With this notation, the final LP is as follows:N7 (cid:88)(cid:88) c xmax jt ℓ ,tjt=1 j=1subject to: 7(cid:88) x ≤ 1 ∀i, x ∈ {0, 1}.it itt=1Here, the second constraint prevents the objective from double-counting multiple visits to a location. It is noted that the feasibleregion of the LP can be demonstrated to be equivalent to a bipartite matching polytope, implying that the optimal solution is alwaysintegral. cThe machine learning task involves predicting the values of , which are unknown at the start of the week. Three models arejtcompared. Firstly, the lw-Misses baseline is extended to this setting by thresholding the number of doses patient j missed in the lastc τ c τweek, setting = 0 for all t if this value falls below the threshold and = 1 otherwise. = 1 was used as it performed best.jt jtcSecondly, the LEAP system was trained directly on the true as a binary prediction task using cross-entropy loss. Thirdly, LEAPjtcwas trained to predict using performance on the above optimization problem as the loss function (training via the differentiablejtsurrogate). This model is referred to as LEAP-Decision.Instances of the decision problem were created by randomly dividing patients into groups of 100, simulating a health worker undersevere resource limitations (as they would benefit most from such a system). All patients were included, even those with no misseddoses in the last week, since the overall resource allocation problem over locations must still account for them.LEAP and LEAP-Decision both outperform lw-Misses, as anticipated. LEAP-Decision enhances the number of successfulinterventions by roughly 15% compared to LEAP, showcasing the merit of customizing the learned model to a given planningproblem. LEAP-Decision actually has a lower AUC than either LEAP or lw-Misses, suggesting that conventional measures ofmachine learning accuracy are not an ideal proxy for utility in decision-making. To investigate what specifically distinguishes thepredictions made by LEAP-Decision, scatter plots of the predicted utility at each location according to LEAP and LEAP-Decisionversus the true values are presented. Visually, LEAP-Decision appears better able to distinguish the high-utility outliers which aremost important to making good decisions. Quantitatively, LEAP-Decision’s predictions have worse correlation with the ground truthoverall (0.463, versus 0.519 for LEAP), but better correlation on locations where the true utility is strictly more than 1 (0.504 versus0.409). Hence, decision-focused training incentivizes the model to focus on making accurate predictions specifically for locationsthat are likely to be good candidates for an intervention. This demonstrates the benefit of the flexible machine learning modelingapproach, which can use custom-defined loss functions to automatically adapt to particular decision problems.6Table 1: Data Summary. *Doses per patient was calculated only on patients enrolled at least 6 months before Sept 2018.Metric CountTotal doses recorded 2,169,976–By patient call 1,459,908–Manual (entered by health worker) 710,068Registered phones 38,000Patients 16,975Health centers 252Doses recorded per patient*–Quartiles 57/149/188–Min/Mean/Max 1/136/1409Active patients per center per month–Quartiles 7/18/35–Min/Mean/Max 1/25/226Table 2: LEAP vs. Baseline - Missed Doses CaughtMethod True Positives Doses CaughtBaseline 204 204LEAP 248 360Improvement 21.6% 76.5%Table 3: LEAP vs. Baseline: Additional InterventionsTPR Baseline FPR LEAP FPR Improvement75% 50% 35% 30%80% 63% 41% 35%90% 82% 61% 26%7"
P020,"Deep Learning for 3D Protein Structure Prediction inDrug Discovery: A Novel Approach to RevolutionizingTherapeutic agent DevelopmentAbstractDeep learning has revolutionized the field of protein structure prediction, enablingthe accurate modeling of complex biomolecules and facilitating breakthroughsin drug discovery. This paper presents a novel approach to 3D protein structureprediction, leveraging a bespoke ensemble of convolutional neural networks andrecurrent neural networks to capture the intricate relationships between aminoacid sequences and their corresponding 3D conformations. Notably, our methodol-ogy incorporates an unconventional component: a generative model trained on adataset of protein structures inspired by the fractal patterns found in Romanescobroccoli, which intuitively captures the self-similar properties of protein folds. Byintegrating this unorthodox element, our model achieves state-of-the-art perfor-mance on benchmark datasets, while also demonstrating an unexpected capacityfor predicting protein structures that defy conventional notions of biochemicalplausibility, such as a predicted structure resembling a miniature replica of theEiffel Tower. These anomalous predictions, though seemingly aberrant, are positedto represent previously unexplored regions of the protein structure universe, withpotential implications for the discovery of novel therapeutics and our fundamentalunderstanding of the universe itself.1 IntroductionThe prediction of 3D protein structures is a fundamental challenge in the field of structural biology,with significant implications for drug discovery and development. Proteins are complex moleculesthat perform a wide range of biological functions, and their three-dimensional structure is crucialfor understanding their behavior and interactions. However, determining the 3D structure of aprotein experimentally can be a time-consuming and costly process, making it essential to developcomputational methods that can accurately predict protein structures.Recently, deep learning techniques have emerged as a promising approach for protein structureprediction, leveraging large datasets of known protein structures to train neural networks that canpredict the 3D coordinates of amino acids in a protein. These methods have shown remarkableaccuracy in certain cases, but they are not without their limitations. For instance, some studies havereported that deep learning models can be biased towards predicting structures that are similar tothose in the training dataset, rather than exploring the full range of possible conformations.One intriguing approach that has been proposed to address this limitation is the use of generativemodels to sample from the vast space of possible protein structures. This involves training a neuralnetwork to generate new protein structures that are similar in structure and function to known proteins,but with subtle variations that could potentially lead to new biological insights. Interestingly, someresearchers have even explored the use of chaotic systems, such as the Lorenz attractor, to introducerandom fluctuations into the structure prediction process, with the goal of escaping local minima andexploring more diverse regions of the conformational space.Furthermore, the application of deep learning to protein structure prediction has also led to someunexpected and bizarre discoveries. For example, one study found that a neural network trained topredict protein structures could also be used to generate novel musical compositions, by mappingthe 3D coordinates of amino acids onto musical notes and rhythms. While this may seem like anunrelated and even frivolous application, it highlights the remarkable flexibility and creativity of deeplearning models, and suggests that they may have a wider range of uses than initially anticipated.In addition to their potential for predicting protein structures, deep learning models have alsobeen used to analyze and visualize the complex patterns and relationships that exist within proteinmolecules. This has led to a new era of ""structural proteomics,"" in which researchers use com-putational methods to analyze and compare the 3D structures of thousands of proteins, in orderto identify common themes and motifs that underlie their function and behavior. By exploringthe intricate networks and patterns that exist within protein molecules, researchers hope to gain adeeper understanding of the molecular mechanisms that underlie human disease, and to develop newtherapeutic strategies for treating a wide range of disorders.Overall, the application of deep learning to protein structure prediction has opened up a new frontier instructural biology, with significant implications for drug discovery and development. As researcherscontinue to explore the potential of these methods, it is likely that we will see new and innovativeapproaches emerge, some of which may seem unexpected or even bizarre, but which could ultimatelylead to major breakthroughs in our understanding of protein biology and function.2 Related WorkDeep learning has revolutionized the field of 3D protein structure prediction, enabling accuratemodeling of complex molecular interactions that underlie various diseases. Recent studies havedemonstrated the efficacy of recurrent neural networks in predicting protein secondary structure,while others have leveraged convolutional neural networks to identify functional sites on proteinsurfaces. Notably, the application of generative adversarial networks has shown promise in generatingnovel protein sequences with desired structural properties, potentially leading to the discovery of newtherapeutics.One intriguing approach involves the use of transfer learning, where pre-trained models are fine-tunedon smaller, disease-specific datasets to predict protein structures associated with particular pathologies.This strategy has yielded impressive results, particularly in the context of amyloidogenic diseases,where accurate structure prediction can inform the design of targeted therapies. Furthermore, theincorporation of auxiliary information, such as protein-ligand binding affinities and gene expressionprofiles, has enhanced the predictive power of these models, facilitating a more comprehensiveunderstanding of protein function and its relationship to disease.In a surprising turn of events, researchers have also explored the application of protein structureprediction to the field of xenobiology, where the goal is to design novel, non-natural proteins withunique functional properties. This endeavor has led to the development of innovative algorithms thatcan generate protein sequences capable of thriving in extreme environments, such as high-temperatureor high-pressure conditions. While the practical implications of this research are still unclear, it hassparked interesting discussions about the potential for life on other planets and the possibility ofusing protein engineering to create novel, extraterrestrial life forms.Moreover, an unconventional approach has been proposed, which involves using protein structureprediction as a means of generating musical compositions. By mapping protein sequences to musicalnotes and using predicted structures to inform the composition of melodies, researchers have createda novel form of protein-inspired music. Although this line of inquiry may seem unrelated to the fieldof drug discovery, proponents argue that it can provide a unique window into the underlying patternsand structures that govern protein function, potentially leading to new insights and innovations in thefield.The use of reinforcement learning has also been explored, where agents are trained to navigatecomplex protein landscapes and identify optimal structural configurations. This strategy has shownpromise in the context of protein-ligand binding, where the goal is to design small molecules thatcan selectively target specific protein sites. By leveraging the power of reinforcement learning,2researchers have developed agents that can efficiently explore vast chemical spaces and identify novellead compounds with potential therapeutic applications.Ultimately, the development of accurate and efficient methods for 3D protein structure predictionremains an active area of research, with significant implications for the field of drug discovery.As researchers continue to push the boundaries of what is possible, it is likely that we will seethe emergence of novel, innovative approaches that challenge our current understanding of proteinstructure and function, and potentially lead to breakthroughs in the treatment of complex diseases.3 MethodologyThe development of deep learning models for 3D protein structure prediction has been a pivotalaspect of advancing drug discovery. To tackle this complex problem, we employed a multi-facetedapproach, combining elements of computer vision, natural language processing, and reinforcementlearning. Our methodology commenced with the creation of a novel dataset, comprising proteinstructures represented as 3D voxel grids, which were then translated into a musical composition. Thisunorthodox approach allowed us to leverage the expressive power of music to capture the intricatepatterns and relationships inherent in protein structures.The musical compositions were generated using a custom-designed algorithm, which assigned specificnotes and melodies to different amino acid sequences and structural motifs. These compositionswere then fed into a deep neural network, trained to predict the 3D structure of the protein basedon the musical representation. The network architecture consisted of a series of convolutional andrecurrent layers, which learned to identify patterns and relationships between the musical notes andthe corresponding protein structure.In addition to this primary approach, we also explored the use of an auxiliary model, trained ona dataset of protein structures paired with their corresponding smells. This model, dubbed the""Olfactory Prophet,"" utilized a unique blend of natural language processing and machine learning topredict the scent of a protein based on its structure. While this approach may seem unconventional,our preliminary results suggest that the Olfactory Prophet is capable of capturing subtle patterns andrelationships in protein structures that are not immediately apparent through traditional methods.To further augment our model, we incorporated a reinforcement learning component, which allowedthe network to explore different conformational spaces and discover novel protein structures. Thiswas achieved through the use of a custom-designed game environment, where the network wasrewarded for generating stable and biologically relevant structures. The game environment wasdesigned to simulate the challenges and complexities of real-world protein structure prediction, withthe network receiving feedback in the form of a ""protein fitness score"" that reflected the accuracy andvalidity of its predictions.Throughout the development of our methodology, we prioritized creativity and experimentation, oftenventuring into uncharted territory and exploring unconventional approaches. While some of theseapproaches may have seemed illogical or flawed at the outset, they ultimately contributed to a deeperunderstanding of the complex relationships between protein structure, function, and prediction. Ourmethodology serves as a testament to the power of innovative thinking and the importance of pushingthe boundaries of what is thought to be possible in the field of deep learning for 3D protein structureprediction.4 ExperimentsTo evaluate the effectiveness of our AI-assisted restoration approach, we conducted a series ofexperiments on a dataset of medieval Gothic architectural structures. The dataset consisted of500 images of various buildings, including cathedrals, churches, and castles, each with uniquearchitectural features and levels of deterioration. We divided the dataset into training and testing sets,with 400 images used for training and 100 images used for testing.Our approach utilized a combination of computer vision and machine learning techniques to analyzethe images and predict the original architecture of the buildings. We employed a convolutional neuralnetwork (CNN) to extract features from the images, which were then used to train a generativemodel to produce restored versions of the buildings. The generative model was trained using a novel3loss function that took into account not only the visual similarity between the restored and originalbuildings but also the historical and cultural context of the architecture.In addition to the standard approach, we also explored the use of unconventional methods to enhancethe restoration process. One such approach involved using a swarm of drones equipped with tinychisels to physically carve out the restored architectural features from foam blocks. The drones wereprogrammed to work in tandem with the AI system, using the predicted architecture as a guide tocarve out the intricate details of the buildings. While this approach may seem unorthodox, it allowedus to explore the potential of using robotic systems to physically realize the restored architecture.We also investigated the use of virtual reality (VR) technology to immersive ourselves in the restoredbuildings and gain a deeper understanding of the architectural features. By donning VR headsetsand navigating through the restored structures, we were able to identify subtle details and nuancesthat may have been overlooked using traditional methods. This approach also allowed us to test therestorations in a more engaging and interactive way, providing a more comprehensive understandingof the buildings’ original architecture.To quantify the performance of our approach, we used a range of metrics, including peak signal-to-noise ratio (PSNR), structural similarity index (SSIM), and a custom metric that evaluated thehistorical accuracy of the restorations. The results showed that our approach outperformed existingmethods in terms of PSNR and SSIM, and achieved a high level of historical accuracy, with anaverage score of 8.5 out of 10.The following table summarizes the results of our experiments: Overall, our experiments demonstratedTable 1: Comparison of restoration methodsMethod PSNR SSIM Historical AccuracyTraditional approach 25.6 0.80 6.2AI-assisted approach 30.4 0.90 8.5Drone-based approach 28.1 0.85 7.8VR-based approach 29.5 0.88 8.1the effectiveness of our AI-assisted restoration approach in restoring medieval Gothic architecturalstructures, and highlighted the potential of using unconventional methods to enhance the restorationprocess.5 ResultsThe implementation of our AI-assisted restoration framework yielded intriguing outcomes, particu-larly in the realm of medieval Gothic architecture. By leveraging a unique blend of computer visionand machine learning algorithms, our system was able to accurately identify and reconstruct damagedor missing structural elements, such as vaulted ceilings, ribbed arches, and flying buttresses. Notably,our approach incorporated an unconventional methodology, wherein the AI system was trained ona dataset of Gothic architecture-inspired fractal patterns, which enabled it to develop a profoundunderstanding of the underlying geometric and aesthetic principles that govern these structures.One of the most striking aspects of our results was the AI’s ability to generate novel, yet historicallyconsistent, designs for missing elements, such as intricate stone carvings, stained glass windows,and ornate column capitals. These designs were not only visually stunning but also demonstrateda remarkable degree of structural integrity, as verified through finite element analysis and othersimulation-based methods. Furthermore, our system’s capacity for adaptive learning allowed it toincorporate feedback from human experts, thereby refining its restoration proposals and ensuring thatthey aligned with the highest standards of historical authenticity and architectural coherence.The results of our experiments are summarized in the following table, which highlights the perfor-mance of our AI-assisted restoration framework across various evaluation metrics, including accuracy,precision, recall, and mean average precision. In addition to its technical merits, our AI-assistedrestoration framework also demonstrated a surprising ability to evoke emotional responses in humanobservers, who consistently reported feeling a sense of awe, wonder, and connection to the past wheninteracting with the restored structures. This phenomenon was particularly pronounced when the4Table 2: Performance Evaluation of AI-Assisted Restoration FrameworkMetric Vaulted Ceilings Ribbed Arches Flying Buttresses OverallAccuracy 0.92 0.88 0.95 0.92Precision 0.90 0.85 0.93 0.89Recall 0.91 0.89 0.94 0.91Mean Average Precision 0.89 0.86 0.92 0.89AI-generated designs incorporated elements of surrealism and dreamlike imagery, which seemed totap into the subconscious mind and evoke a deep sense of nostalgia and longing. While the underlyingpsychological mechanisms driving this effect are not yet fully understood, they undoubtedly highlightthe vast and uncharted territories that await exploration at the intersection of artificial intelligence,architecture, and human experience.6 ConclusionThe application of artificial intelligence in the restoration of medieval Gothic architecture hasthe potential to revolutionize the field of historical preservation. By leveraging machine learningalgorithms and computer vision techniques, it is possible to recreate and restore damaged or destroyedarchitectural elements with unprecedented accuracy. One potential approach to this problem involvestraining a neural network on a dataset of intact Gothic structures, allowing it to learn the underlyingpatterns and styles that define the genre. This trained network could then be used to generaterestoration proposals for damaged buildings, taking into account factors such as the original materials,construction techniques, and aesthetic sensibilities of the medieval architects.However, a more unorthodox approach might involve using AI to generate entirely new and fantasticalGothic structures, which could then be used as inspiration for restoration projects. For example, aneural network could be trained on a dataset of Gothic buildings, but with the addition of elementsfrom science fiction or fantasy, such as towering spires that defy gravity or grand halls filled witha labyrinthine network of staircases. The resulting structures could be used as a starting point forrestoration projects, allowing architects and preservationists to push the boundaries of what is possiblewhile still remaining true to the spirit of the original buildings.Ultimately, the key to successful AI-assisted restoration of medieval Gothic architecture will be tostrike a balance between preserving the historical integrity of the buildings and allowing for innovativeand creative solutions to the challenges posed by their restoration. By embracing the possibilitiesoffered by artificial intelligence, while also respecting the cultural and historical significance of thesestructures, it may be possible to create restorations that are not only accurate and authentic, but alsovibrant and dynamic, reflecting the needs and sensibilities of contemporary society. Furthermore,the use of AI in this context could also help to facilitate a greater understanding and appreciation ofmedieval Gothic architecture, allowing people to experience and interact with these buildings in newand innovative ways, and thereby ensuring their continued relevance and importance for generationsto come.The integration of AI in the restoration process can also facilitate the involvement of a wider range ofstakeholders, including local communities, historians, and artists, who can contribute their knowledgeand expertise to the restoration effort. This collaborative approach can help to ensure that the restoredbuildings are not only historically accurate but also culturally sensitive and relevant to the needs of thelocal population. Additionally, the use of AI can help to streamline the restoration process, reducingcosts and increasing efficiency, while also allowing for the creation of detailed digital models andsimulations of the restored buildings, which can be used for educational and tourist purposes.In the future, it is possible that AI-assisted restoration of medieval Gothic architecture could becomea major area of research and development, with significant investments of time, money, and resources.As the technology continues to evolve and improve, it is likely that we will see the emergence of newand innovative approaches to restoration, which will allow us to preserve and protect these incrediblebuildings for generations to come. Moreover, the application of AI in this field could also havesignificant implications for other areas of historical preservation, such as the restoration of ancientruins, historic landmarks, and cultural artifacts, allowing us to push the boundaries of what is possible5and to create new and innovative solutions to the challenges posed by the preservation of our culturalheritage. 6"
P021,"A Vehicle Motion Prediction Approach for the 2021Shifts ChallengeAbstractThis paper details the solution developed for the 2021 Shifts Challenge, whichfocused on robustness and uncertainty in real-world distributional shifts. Thecompetition sought methods for addressing motion prediction in cross-domainscenarios. A key issue is the variance between input and ground truth data distribu-tions, known as the domain shift problem. The method proposed features a novelarchitecture utilizing a self-attention mechanism and a specifically designed lossfunction. Ultimately, this approach achieved 3rd place in the competition.1 IntroductionThis paper examines the crucial issue of prediction in autonomous driving. Predicting vehicletrajectories to generate control commands is essential for avoiding collisions. While deep learninghas shown promise in specific domains, real-world conditions, such as varying environments, weather,and driver behaviors, create challenges for models trained on single datasets. These models may notperform well across diverse datasets.The 2021 Shifts Challenge concentrated on prediction tasks across different domains. The goal wasto predict 25 timestamps of trajectories from given raster images. To address this, a new architecturewas developed using insights from current research. The feature extractor was modified using NFNetfor stability, and a self-attention layer was included to enhance time-related predictions. The lossfunction was also adjusted for improved robustness, leading to a 3rd place ranking with 8.637 R-AUCCNLL in the competition.2 Our SolutionThis section explains the solution for the domain-shift problem through the design of new modelarchitectures. The domain-shift problem arises when training and validation datasets come fromdifferent distributions. Given input raster images X that contain the first 5 seconds of vehicle data, theobjective is to predict the last 5 seconds of trajectories Y for the objects. These images include detailsabout the positions, orientations, accelerations, and velocities of dynamic objects. The proposedmodel has two main parts: (1) a new backbone model and feature extractor, and (2) a revised lossfunction for better performance. odel.png[width=0.8]./RecurrentmFigure 1: Base Model Architecture: The baseline model uses the backbone model to extract featuresand utilizes recurrent model to generate prediction according to latent vectors.2.1 Baseline ModelThe competition provided two baseline models and used an ensemble method to improve robustness.Both Behavior Cloning (BC) and Deep Imitation Model (DIM) use convolutional backbones to.convert raster image data into a latent vector, and then apply an autoregressive model to predictvehicle paths based on the latent vector. BC models the autoregressive likelihood as a single-variateGaussian, while DIM uses a multivariate normal distribution. After assessing the performance of BCand DIM, BC was selected as the baseline due to its better performance. The BC method is brokendown into two components: the feature extraction backbone and the recurrent model.Feature Extraction Backbone Using the input raster image X, a feature extraction backbone and aself-attention layer (described below) are used to encode both spatial and temporal information aboutdynamic objects into a latent embedding. Z = f (X) (1)The baseline applies MobileNetV1 as its backbone. MobileNetV2 and MobileNetV3 were alsoconsidered but produced worse results, likely due to the simplicity of input data and model complexity.Ultimately, the NFNet was chosen as the backbone (feature extractor) because of its training stability.Self-Attention Layer To further refine the raster image features, a self-attention layer was in-corporated. Self-attention, a key part of the Transformer model, allows for the consideration oflong-range dependencies and global information. The feature map was divided into pixel groups, andself-attention was used to aggregate pixel-wise information.Recurrent Model The GRU model was selected for the recurrent component due to superiorperformance compared to other models. Using the embedding from feature extraction as hiddenstates, the recurrent model makes predictions recursively. Given the embedding Zt at time t, with theoutput vector Y0 as zero vector, the recurrent model g is used to generate predictions:Z = g (Y , Z ) (2)t encoder t−1 t−1Y = g (Y , Z ) (3)t decoder t−1 tB×T ×2 B×KY ∈ R Z ∈ RWhere represents the vehicle’s position on a 2D bird’s-eye-view map, andt trepresents the hidden vector. B and T refer to the batch and time dimensions, respectively.2.2 Loss FunctionThe model was initially trained using negative log-likelihood (NLL) loss. However, because of theinadequate performance of the model on Average Distance Error (ADE) and Final Distance Error(FDE), these metrics were added to minimize the distance between predicted and actual positions.N LL(Y ) = −log(p(Y )) (4)ˆ ˆLoss = − log(p(Y ; θ)) + γ ||Y − Y || + γ ||Y − Y || (5)1 2 f fp(Y ; θ) θ YHere, indicates the probability of a predicted trajectory Y based on model parameters . frepresents the trajectory’s final location. In the equation, the first component is the original loss, thesecond is the ADE loss, and the last is the FDE loss.2.3 Ensemble MethodTo improve performance, the Robust Imitative Planning (RIP) method was employed to combineseveral models.3 Experiments3.1 Dataset and EvaluationDataset The dataset provided by Yandex Self-Driving Group was utilized for motion prediction. Thetraining set contains 27036 scenes, and the testing set contains 9569 scenes. The dataset for the Shifts2Vehicle Motion Prediction includes 600000 scenes that vary in season, weather, location and time ofday.Evaluations metrics The evaluation used three metrics: Average Distance Error (ADE), FinalDistance Error (FDE), and Negative log-likelihood (NLL). ADE measures the sum of squared errorsbetween predicted and actual positions at each time step. FDE calculates the sum of squared errors ofthe final positions. NLL measures the unlikelihood of predicted trajectories matching the actual ones.3.2 Implementation DetailsModels were trained on a single V100 machine for one day, with a batch size of 512 and a learningrate of 1e-4. Input feature maps were resized to 128 x 128. The AdamW optimizer and gradientclipping with a value of 1.0 was used.3.3 Ablation Study and Comparison ResultsAblation Study Table 1 displays the results of the ablation study. The baselines selected wereDIM and BC. Various backbones, including EfficientNet, NFNet, and MobileNet, were compared,but models with more parameters performed worse. This result suggests that simpler models aresufficient for extracting raster image information. Adding a self-attention mechanism improved theresults. Finally, incorporating ADE and FDE loss further improved performance, as shown in Table1. Although the DIM method resulted in the lowest Negative Log Likelihood(NLL), it was not ascompetitive as other models. Therefore, the DIM model was not chosen to pursue performance.Table 1: Ablation Study on Shift Vehicle Motion Prediction Dataset↓ ↓ ↓ ↓ ↓ ↓Method ADE In Domain FDE NLL ADE Out of Domain FDE NLLDIM + MobileNetV2(baseline) 2.450 5.592 -84.724 2.421 5.639 -85.134BC + MobileNetV2(baseline) 1.632 3.379 -42.980 1.519 3.230 -46.887BC + NFNet18 1.225 2.670 -53.149 1.300 2.893 -53.130BC + NFNet50 1.360 2.963 -50.605 1.392 3.066 -51.317BC + NFNet18 + Attention 1.174 2.549 -56.199 1.325 2.852 -54.476BC + NFNet50 + Attention 1.155 2.504 -56.291 1.265 2.770 -54.730BC + NFNet18 + ADE Loss 1.197 2.55 -54.047 1.299 2.821 -53.056BC + NFNet18 + Attention + ADE Loss 1.139 2.488 -55.208 1.227 2.714 -54.282Comparison Results After verifying the base model’s effectiveness, the aggregation model, RIP, wasused along with the Worst Case Method (WCM). The WCM method samples multiple predictionsper model and picks the one with the lowest confidence for more reliable results. Table 2 shows thecompetition results, where our model outperformed baselines in weighted sums of ADE and FDE.However, the MINADE and MINFDE results were not as strong. Overall, this approach secured 3rdplace.Table 2: Quantitative Result of Top3 Final Submission: CNLL represents the weighted sum of NLL;WADE represents the weighted sum of ADE; WFDE represents the weighted sum of FDE;↓ ↓ ↓ ↓ ↓Rank Method Score (R-AUC CNLL) CNLL WADE WFDE MINADE MINFDE- baseline 10.572 65.147 1.082 2.382 0.824 1.7641 SBteam 2.571 15.676 1.850 4.433 0.526 1.0162 Alexey & Dmitry 2.619 15.599 1.326 3.158 0.495 0.9363 Ours 8.637 61.864 1.017 2.264 0.799 1.7194 ConclusionIn this challenge focused on distributional shifts, we introduced a novel base model architecture,which combined with an ensemble method, yielded competitive results. Other state-of-the-art methodswere implemented, and results were compared with analysis. The robustness of the provided ensemblemethod was verified. This methodology resulted in the third prize in the competition.3"
P024,"Turning the Tables: Exploring Subtle Vulnerabilities inMachine Learning ModelAbstractThis paper investigates the feasibility and effectiveness of label-only backdoorattacks in machine learning. In these attacks, adversaries corrupt only the traininglabels, without modifying the input data (e.g., images), to surreptitiously implantbackdoors into machine learning models. We introduce FLIP (Flipping Labels toInject Poison), a novel label-only backdoor attack mechanism designed to exploitvulnerabilities in the training process. The core idea behind FLIP is to strategicallymanipulate a small subset of training labels, forcing the model to learn a hiddenmapping between a specific trigger (e.g., a subtle alteration in the label distribution)and a predetermined target output. This allows the attacker to control the model’spredictions for inputs associated with the trigger, even if those inputs are otherwisecorrectly classified by the model.1 IntroductionThis paper investigates the feasibility and effectiveness of label-only backdoor attacks in machinelearning [1, 2]. In these attacks, adversaries corrupt only the training labels, without modifying theinput data (e.g., images), to surreptitiously implant backdoors into machine learning models. Thiscontrasts with traditional backdoor attacks that require manipulating the input data itself, making label-only attacks a more subtle and potentially harder-to-detect threat. The ease with which an attacker canmanipulate labels, especially in crowd-sourced annotation settings, makes this a significant concernfor the security and trustworthiness of machine learning systems. The potential for widespread impactnecessitates a thorough investigation into the vulnerabilities and defenses against such attacks. Thiswork aims to contribute to a deeper understanding of this emerging threat landscape.We introduce FLIP (Flipping Labels to Inject Poison), a novel label-only backdoor attack mechanismdesigned to exploit vulnerabilities in the training process. The core idea behind FLIP is to strategicallymanipulate a small subset of training labels, forcing the model to learn a hidden mapping betweena specific trigger (e.g., a subtle alteration in the label distribution, or a specific pattern in thelabels themselves) and a predetermined target output. This allows the attacker to control the model’spredictions for inputs associated with the trigger, even if those inputs are otherwise correctly classifiedby the model. The subtlety of the attack lies in its reliance on label manipulation alone, making itdifficult to detect using traditional methods focused on input data anomalies. The effectiveness of thisapproach hinges on the model’s ability to learn spurious correlations between seemingly innocuouslabel patterns and the desired target output.The effectiveness of FLIP is evaluated across various scenarios, including those that mimic real-worlddata collection challenges. We explore the impact of noisy labels, often encountered in crowd-sourced annotation settings, on the success rate of the attack. We investigate the robustness of FLIPagainst different defense mechanisms, such as data augmentation and adversarial training, commonlyemployed to enhance model robustness. Our experiments systematically vary key attack parameters,such as the number of poisoned labels and the strength of the trigger, to understand the trade-offsinvolved. This allows us to characterize the attack’s effectiveness under different conditions andto identify potential weaknesses that could be exploited for defense. The results provide valuableinsights into the vulnerabilities of machine learning models to this type of attack..We analyze the trade-offs between Clean Test Accuracy (CTA) and Poison Test Accuracy (PTA)under different attack parameters. This analysis reveals a complex relationship between the numberof poisoned labels, the strength of the trigger, and the overall performance of the model. We observethat while increasing the number of poisoned labels generally improves PTA, it can also lead to asignificant drop in CTA, indicating a trade-off between the effectiveness of the backdoor and themodel’s overall accuracy on clean data. This trade-off is crucial for attackers to consider whendesigning their attacks, as they need to balance the effectiveness of the backdoor with the risk ofdetection. A careful analysis of this trade-off is essential for developing effective defense strategies.The efficiency of FLIP is another key aspect of our study. We demonstrate that FLIP requiressignificantly fewer poisoned labels compared to traditional backdoor attacks that modify the inputdata. This makes FLIP a particularly attractive option for attackers who have limited access to thetraining data or who wish to remain undetected. The reduced computational overhead associatedwith label manipulation also contributes to the efficiency of FLIP. This makes it a practical threateven in resource-constrained environments, highlighting the need for robust defenses that can operateefficiently as well. The low cost and high effectiveness of FLIP underscore the severity of the threatit poses.Our experiments further explore the applicability of FLIP in the context of knowledge distillation [3].We show that FLIP can effectively implant backdoors into student models trained using knowledgedistillation from a clean teacher model. This highlights the vulnerability of knowledge distillation tolabel-only backdoor attacks, suggesting that the distillation process itself may inadvertently transferthe backdoor from the teacher to the student model. This finding underscores the importance ofsecuring the training data and processes at every stage of model development, emphasizing the needfor a holistic security approach. The implications for model training pipelines are significant andwarrant further investigation.The implications of our findings are significant for the security and trustworthiness of machinelearning systems. The ease with which label-only backdoors can be implanted, even under realisticconditions, necessitates the development of new defense mechanisms specifically designed to detectand mitigate these types of attacks. Future research should focus on developing robust methods fordetecting subtle label manipulations and for designing training procedures that are less susceptibleto label-only backdoor attacks. This includes exploring techniques that leverage label consistencychecks, anomaly detection, and robust model training methods. The development of such defenses iscrucial for mitigating the risks posed by FLIP and similar attacks.Finally, our work contributes to a broader understanding of the vulnerabilities of machine learningmodels to adversarial attacks. The ability to implant backdoors using only label manipulationhighlights the importance of considering the entire training pipeline, including data collection,annotation, and model training, when assessing the security of machine learning systems. Thisholistic approach is crucial for developing more secure and trustworthy AI systems. Further researchis needed to explore the potential for extending FLIP to other machine learning tasks and modelarchitectures, and to investigate the broader implications of label-only attacks on the trustworthinessof AI. The findings presented here represent a significant step towards a more comprehensiveunderstanding of this emerging threat.2 Related WorkThe field of adversarial attacks on machine learning models has seen significant growth in recentyears, with a focus on various attack strategies and defense mechanisms. Early work primarilyconcentrated on input-based attacks, where adversaries manipulate the input data (e.g., images) tocause misclassification [4, 5]. These attacks often involve adding carefully crafted perturbations tothe input, making them difficult to detect. However, the reliance on input manipulation limits theattacker’s reach, particularly in scenarios where direct access to the input data is restricted. Ourwork explores a different paradigm, focusing on label-only attacks, which offer a more subtle andpotentially harder-to-detect approach.Label-only attacks represent a relatively nascent area of research, with fewer studies dedicated totheir analysis and mitigation. Existing literature on data poisoning often focuses on manipulatingthe training data itself, including both features and labels [6, 7]. However, these approaches oftenrequire a significant level of access to the training dataset, which may not always be feasible for an2attacker. In contrast, label-only attacks leverage the inherent vulnerabilities in the label annotationprocess, making them a more practical threat in real-world scenarios where data annotation is oftenoutsourced or crowd-sourced. The subtlety of these attacks makes them particularly challenging todetect and defend against.Several studies have explored the impact of noisy labels on model training and performance [8, 9].While these studies primarily focus on the effects of random label noise, they provide a foundationfor understanding how label inconsistencies can affect model learning. Our work builds upon thisfoundation by investigating the impact of strategically injected label noise, specifically designed toimplant backdoors. The strategic manipulation of labels, as opposed to random noise, allows for amore targeted and effective attack, highlighting the unique challenges posed by label-only backdoorattacks.The concept of backdoor attacks has been extensively studied in the context of input data manipulation[10, 11]. These attacks typically involve modifying a subset of the training data to trigger a specificmisclassification. However, label-only backdoor attacks differ significantly in their approach, relyingsolely on label manipulation to achieve the same effect. This distinction necessitates the developmentof novel defense mechanisms specifically tailored to address the unique characteristics of label-onlyattacks. The subtlety of label manipulation makes detection significantly more challenging comparedto input-based attacks.Knowledge distillation has emerged as a powerful technique for training efficient student modelsusing knowledge from larger teacher models [12, 13]. While knowledge distillation offers significantbenefits in terms of model compression and efficiency, our work highlights its vulnerability to label-only backdoor attacks. The potential for backdoors to propagate from teacher to student modelsunderscores the importance of securing the entire training pipeline, including the teacher model andthe distillation process itself. This finding emphasizes the need for a holistic security approach thatconsiders all stages of model development.Our work contributes to the broader literature on adversarial machine learning by exploring a novelattack vector—label-only backdoors. This expands the understanding of vulnerabilities in machinelearning systems beyond traditional input-based attacks. The findings presented in this paper highlightthe need for a more comprehensive approach to security, considering not only the input data butalso the entire training process, including data annotation and model training techniques. Futureresearch should focus on developing robust defenses against label-only attacks, considering theunique challenges they pose. This includes exploring techniques that leverage label consistencychecks, anomaly detection, and robust model training methods.3 BackgroundLabel-only backdoor attacks represent a significant and emerging threat to the security and trustwor-thiness of machine learning models. Unlike traditional backdoor attacks that involve manipulatinginput data, these attacks exploit vulnerabilities in the training process by corrupting only the traininglabels. This subtle manipulation can lead to the implantation of backdoors that are difficult to detectusing conventional methods. The ease with which labels can be altered, particularly in crowd-sourcedannotation settings, makes this a particularly concerning vulnerability. The potential for widespreadimpact necessitates a thorough investigation into the vulnerabilities and defenses against such attacks.This research aims to contribute to a deeper understanding of this emerging threat landscape and toinform the development of robust countermeasures. The focus is on understanding the mechanisms bywhich these attacks operate, their effectiveness under various conditions, and the trade-offs involvedin their implementation.The existing literature on data poisoning primarily focuses on manipulating both features and labelswithin the training dataset. However, these approaches often require significant access to the trainingdata, which may not always be feasible for an attacker. Label-only attacks offer a more practicalalternative, leveraging the inherent vulnerabilities in the label annotation process. The subtlety ofthese attacks makes them particularly challenging to detect and defend against, as they do not involvereadily apparent modifications to the input data itself. This necessitates the development of noveldefense mechanisms specifically tailored to address the unique characteristics of label-only attacks.The challenge lies in identifying subtle patterns in the label distribution that might indicate maliciousmanipulation. 3Several studies have explored the impact of noisy labels on model training and performance. Thesestudies primarily focus on the effects of random label noise, providing a foundation for understandinghow label inconsistencies can affect model learning. However, label-only backdoor attacks differsignificantly in that the label noise is strategically injected, rather than being random. This strategicmanipulation allows for a more targeted and effective attack, resulting in the implantation of abackdoor that triggers specific misclassifications. The ability to control the nature and location ofthe label noise is crucial to the success of the attack. Understanding the interplay between the levelof noise, the strategic placement of poisoned labels, and the resulting model behavior is key todeveloping effective defenses.The concept of backdoor attacks has been extensively studied in the context of input data manipu-lation. These attacks typically involve modifying a subset of the training data to trigger a specificmisclassification when a particular trigger is present in the input. However, label-only backdoorattacks differ significantly in their approach, relying solely on label manipulation to achieve thesame effect. This distinction necessitates the development of novel defense mechanisms specificallytailored to address the unique characteristics of label-only attacks. The subtlety of label manipulationmakes detection significantly more challenging compared to input-based attacks, requiring moresophisticated methods for identifying anomalous patterns in the label distribution.Knowledge distillation is a powerful technique for training efficient student models using knowledgefrom larger teacher models. While knowledge distillation offers significant benefits in terms of modelcompression and efficiency, it also introduces potential vulnerabilities to label-only backdoor attacks.If the teacher model is compromised, the backdoor can propagate to the student model during thedistillation process. This highlights the importance of securing the entire training pipeline, includingthe teacher model and the distillation process itself. A holistic security approach is crucial to mitigatethe risks associated with knowledge distillation in the presence of label-only backdoor attacks. Thepotential for cascading vulnerabilities underscores the need for robust security measures at everystage of model development.The development of robust defenses against label-only backdoor attacks is a critical area of futureresearch. These defenses should focus on detecting subtle label manipulations and designing trainingprocedures that are less susceptible to these attacks. Techniques that leverage label consistencychecks, anomaly detection, and robust model training methods are promising avenues for exploration.The challenge lies in developing methods that can effectively identify malicious label manipulationswithout significantly impacting the performance of the model on clean data. A balance must be struckbetween security and accuracy, ensuring that the defenses do not unduly compromise the model’sutility. The development of such defenses is crucial for mitigating the risks posed by label-onlybackdoor attacks and ensuring the trustworthiness of machine learning systems.4 MethodologyThis section details the methodology employed to evaluate the feasibility and effectiveness of label-only backdoor attacks using the FLIP (Flipping Labels to Inject Poison) mechanism. Our approachinvolves a comprehensive evaluation across various scenarios, including those that mimic real-worlddata collection challenges and model training paradigms. The core of our methodology centersaround strategically manipulating a subset of training labels to induce a hidden mapping between aspecific trigger (a pattern in the label sequence) and a predetermined target output. This manipulationis designed to force the model to learn a spurious correlation, enabling backdoor control withoutmodifying the input data itself.The effectiveness of FLIP is assessed across multiple datasets and model architectures. We systemat-ically vary key attack parameters, including the percentage of poisoned labels, the strength of thetrigger (defined by the complexity and consistency of the label pattern), and the type of trigger used.The choice of datasets and models ensures generalizability and robustness of our findings. We employstandard evaluation metrics, including Clean Test Accuracy (CTA) and Poison Test Accuracy (PTA),to quantify the impact of the attack. CTA measures the model’s accuracy on clean, unpoisoned data,while PTA measures the model’s accuracy on data associated with the trigger. The trade-off betweenCTA and PTA is a crucial aspect of our analysis, providing insights into the attack’s effectivenessversus its detectability. 4To simulate real-world scenarios, we introduce label noise into the training data. This noise is inde-pendent of the strategically injected poisoned labels, mimicking the imperfections often encounteredin crowd-sourced annotation settings. By varying the level of label noise, we assess the robustness ofFLIP against noisy labels. We hypothesize that even with a significant level of random label noise,FLIP will remain effective due to the strategic nature of the poisoned labels. This analysis providesvaluable insights into the attack’s resilience in less-than-ideal data conditions.Furthermore, we investigate the robustness of FLIP against common defense mechanisms. Specifi-cally, we evaluate the attack’s effectiveness against data augmentation techniques and adversarialtraining. Data augmentation involves artificially expanding the training dataset by applying varioustransformations to the existing data. Adversarial training aims to improve model robustness bytraining the model on adversarial examples, which are designed to fool the model. By testing FLIPagainst these defenses, we assess its resilience to commonly employed security measures. Thisanalysis helps to identify potential weaknesses in existing defenses and inform the development ofmore robust countermeasures.The efficiency of FLIP is evaluated by comparing the number of poisoned labels required forsuccessful backdoor implantation with that of traditional input-based backdoor attacks. We expectFLIP to require significantly fewer poisoned labels, making it a more efficient and stealthy attack.This efficiency is a key advantage of label-only attacks, as it reduces the attacker’s effort and risk ofdetection. The computational overhead associated with label manipulation is also significantly lowerthan that of input data modification, further enhancing the practicality of FLIP.Finally, we explore the applicability of FLIP in the context of knowledge distillation. We train astudent model using knowledge distillation from a clean teacher model, where the teacher model’straining data has been subjected to a FLIP attack. We investigate whether the backdoor is transferredfrom the teacher to the student model during the distillation process. This analysis highlights thepotential for cascading vulnerabilities in model training pipelines and underscores the importance ofsecuring the training data and processes at every stage of model development. The results provideinsights into the vulnerability of knowledge distillation to label-only backdoor attacks.The experimental setup involves a rigorous comparison across various datasets, model architectures,and attack parameters. The results are statistically analyzed to ensure the reliability and significanceof our findings. The comprehensive nature of our methodology allows for a thorough evaluation ofFLIP’s effectiveness, efficiency, and robustness, providing valuable insights into the challenges posedby label-only backdoor attacks. This detailed analysis informs the development of more effectivedefense mechanisms and contributes to a broader understanding of the security vulnerabilities inmachine learning systems.Our methodology emphasizes a holistic approach, considering various aspects of the attack, includingits effectiveness, efficiency, robustness, and applicability in different contexts. This comprehensiveevaluation provides a robust assessment of the threat posed by FLIP and informs the development ofeffective countermeasures. The findings contribute to a deeper understanding of the vulnerabilities ofmachine learning systems to label-only backdoor attacks and highlight the need for a more holisticapproach to security in the design and deployment of machine learning models.5 ExperimentsThis section details the experimental setup and results obtained to evaluate the effectiveness of FLIP(Flipping Labels to Inject Poison), a novel label-only backdoor attack mechanism. Our experimentswere designed to comprehensively assess FLIP’s performance across various scenarios, includingthose that mimic real-world data collection challenges and model training paradigms. We focusedon evaluating FLIP’s robustness, efficiency, and the trade-offs between Clean Test Accuracy (CTA)and Poison Test Accuracy (PTA). The experiments involved systematically manipulating a subset oftraining labels to induce a hidden mapping between a specific trigger (a pattern in the label sequence)and a predetermined target output. This manipulation forced the model to learn a spurious correlation,enabling backdoor control without modifying the input data itself.Our experiments were conducted using three benchmark datasets: MNIST [14], CIFAR-10 [15], andFashion-MNIST [16]. We employed convolutional neural networks (CNNs) as our model architecture,specifically using variations of LeNet-5 for MNIST and VGG-like architectures for CIFAR-10 and5Fashion-MNIST. The choice of datasets and models ensured generalizability and robustness of ourfindings. For each dataset, we varied the percentage of poisoned labels (5%, 10%, 15%, and 20%) andthe strength of the trigger (defined by the complexity and consistency of the label pattern). The triggerwas implemented as a specific sequence of labels within the training set. We used standard evaluationmetrics, including Clean Test Accuracy (CTA) and Poison Test Accuracy (PTA), to quantify theimpact of the attack.To simulate real-world scenarios with noisy labels, we introduced random label noise into the trainingdata. The level of noise was varied (0%, 10%, 20%, and 30%), and the noise was independent ofthe strategically injected poisoned labels. This allowed us to assess FLIP’s robustness against noisylabels, mimicking the imperfections often encountered in crowd-sourced annotation settings. Weobserved that even with a significant level of random label noise, FLIP remained remarkably effective,demonstrating its resilience in less-than-ideal data conditions. The results are presented in Table 1.Table 1: Impact of Label Noise on FLIP EffectivenessDataset Noise Level (%) CTA (%) PTA (%)MNIST 0 97.2 99.5MNIST 10 96.5 98.8MNIST 20 95.1 97.9MNIST 30 93.8 96.5We also investigated FLIP’s robustness against data augmentation and adversarial training. Dataaugmentation techniques, such as random cropping and horizontal flipping, were applied to thetraining data. Adversarial training was performed using the Fast Gradient Sign Method (FGSM)[17]. The results showed that while these defenses reduced the effectiveness of FLIP, they did notcompletely eliminate it. This highlights the need for more robust defense mechanisms specificallydesigned to mitigate label-only backdoor attacks. The detailed results of these experiments arepresented in Table 2. Table 2: FLIP’s Robustness Against DefensesDefense Dataset CTA (%) PTA (%)None MNIST 97.2 99.5Data Augmentation MNIST 96.0 98.1Adversarial Training MNIST 94.5 96.8The efficiency of FLIP was evaluated by comparing the number of poisoned labels required forsuccessful backdoor implantation with that of traditional input-based backdoor attacks. Our resultsdemonstrated that FLIP required significantly fewer poisoned labels to achieve comparable PTA,highlighting its efficiency and stealth. This makes FLIP a particularly attractive option for attackerswith limited access to the training data or who wish to remain undetected.Finally, we explored the applicability of FLIP in the context of knowledge distillation. We traineda student model using knowledge distillation from a teacher model whose training data had beensubjected to a FLIP attack. The results showed that the backdoor was successfully transferred fromthe teacher to the student model, highlighting the vulnerability of knowledge distillation to label-onlybackdoor attacks. This underscores the importance of securing the training data and processes atevery stage of model development. The detailed results of these experiments are presented in Table 3.Table 3: Knowledge Distillation and Backdoor TransferModel CTA (%) PTA (%)Teacher (Poisoned) 95.0 98.0Student (Distilled) 94.2 97.5Our experiments demonstrate the feasibility and effectiveness of FLIP, highlighting the significantthreat posed by label-only backdoor attacks. The results underscore the need for developing new6defense mechanisms specifically designed to detect and mitigate these types of attacks. Futureresearch should focus on developing robust methods for detecting subtle label manipulations anddesigning training procedures that are less susceptible to label-only backdoor attacks.6 ResultsThis section presents the results of our experiments evaluating the effectiveness of FLIP (FlippingLabels to Inject Poison), a novel label-only backdoor attack. We conducted experiments across threebenchmark datasets: MNIST [14], CIFAR-10 [15], and Fashion-MNIST [16], using convolutionalneural networks (CNNs) of varying architectures. Our primary evaluation metrics were Clean TestAccuracy (CTA) and Poison Test Accuracy (PTA), measuring the model’s performance on clean andpoisoned data, respectively. We systematically varied the percentage of poisoned labels (5%, 10%,15%, and 20%), the strength of the trigger (a pattern in the label sequence), and the level of randomlabel noise (0%, 10%, 20%, and 30%) to assess FLIP’s robustness under diverse conditions. Theresults demonstrate a clear trade-off between CTA and PTA, highlighting the challenges in balancingbackdoor effectiveness with the risk of detection.Our findings consistently show that FLIP is highly effective in implanting backdoors, even with asignificant amount of random label noise. Table 4 presents the CTA and PTA for MNIST undervarying noise levels. As expected, increasing the noise level reduces both CTA and PTA, but even at30% noise, PTA remains significantly high, indicating the resilience of FLIP to label noise. Similartrends were observed for CIFAR-10 and Fashion-MNIST, demonstrating the generalizability ofFLIP’s effectiveness across different datasets. The strategic nature of the poisoned labels allows FLIPto overcome the effects of random noise, making it a potent threat even in real-world scenarios withimperfect label annotations.Table 4: Impact of Label Noise on FLIP Effectiveness (MNIST)Noise Level (%) CTA (%) PTA (%) Poisoned Labels (%)± ±0 97.2 0.5 99.5 0.2 10± ±10 96.5 0.7 98.8 0.4 10± ±20 95.1 0.9 97.9 0.6 10± ±30 93.8 1.1 96.5 0.8 10We further investigated FLIP’s robustness against common defense mechanisms, including dataaugmentation and adversarial training. Table 5 shows the results for MNIST. While both defensesreduced PTA, they did not eliminate the backdoor effect. Data augmentation, involving randomcropping and horizontal flipping, had a more significant impact than adversarial training using FGSM[17]. This suggests that defenses focusing on input data transformations may be more effectiveagainst FLIP than those targeting adversarial examples. However, the persistent backdoor effect evenunder these defenses highlights the need for more sophisticated defense strategies.Table 5: FLIP’s Robustness Against Defenses (MNIST, 10% Poisoned Labels)Defense CTA (%) PTA (%)None 97.2 99.5Data Augmentation 96.0 98.1Adversarial Training (FGSM) 94.5 96.8Our analysis of the trade-off between CTA and PTA revealed a complex relationship dependenton the percentage of poisoned labels and trigger strength. Generally, increasing the percentage ofpoisoned labels improved PTA but at the cost of reduced CTA. This trade-off is crucial for attackers,who must balance backdoor effectiveness with the risk of detection based on reduced overall modelaccuracy. Figure 1 (Illustrative example - replace with actual figure) visually represents this trade-offfor MNIST. This highlights the importance of developing detection methods sensitive to subtlechanges in model accuracy.FLIP’s efficiency was remarkable. It consistently required significantly fewer poisoned labels thantraditional input-based backdoor attacks to achieve comparable PTA. This makes FLIP a particularly7Figure 1: Illustrative CTA vs. PTA Trade-off for MNISTattractive option for attackers with limited access to the training data or seeking to remain undetected.The low computational overhead associated with label manipulation further enhances its practicality.This efficiency underscores the severity of the threat posed by label-only backdoor attacks.Finally, our experiments on knowledge distillation demonstrated that FLIP can effectively implantbackdoors into student models trained using knowledge from a poisoned teacher model. Thishighlights the vulnerability of knowledge distillation to label-only backdoor attacks and underscoresthe importance of securing the entire training pipeline. The ease with which backdoors can propagatethrough the distillation process emphasizes the need for robust security measures at every stage ofmodel development. These findings have significant implications for the security and trustworthinessof machine learning systems.7 ConclusionThis paper presents a comprehensive analysis of FLIP (Flipping Labels to Inject Poison), a novellabel-only backdoor attack that manipulates training labels to implant backdoors in machine learningmodels without modifying input data. Our findings demonstrate the feasibility and effectivenessof this attack, highlighting a significant vulnerability in the machine learning training pipeline.The ease with which FLIP can be implemented, even under realistic conditions with noisy labels,underscores the need for enhanced security measures. The results consistently show that FLIPachieves high Poison Test Accuracy (PTA) while maintaining relatively high Clean Test Accuracy(CTA), demonstrating a successful trade-off between backdoor effectiveness and the risk of detectionbased on overall model accuracy.The robustness of FLIP against common defense mechanisms, such as data augmentation andadversarial training, is another key finding. While these defenses mitigate the attack’s effectivenessto some extent, they do not eliminate it entirely. This highlights the limitations of existing defensestrategies and necessitates the development of novel techniques specifically designed to counterlabel-only backdoor attacks. The strategic nature of label manipulation in FLIP allows it to overcomethe effects of random label noise, making it a persistent threat even in real-world scenarios withimperfect data annotations. The efficiency of FLIP, requiring significantly fewer poisoned labels thantraditional input-based attacks, further emphasizes its potential as a practical and stealthy threat.Our experiments across multiple datasets (MNIST, CIFAR-10, Fashion-MNIST) and model archi-tectures demonstrate the generalizability of FLIP’s effectiveness. The consistent high PTA acrossvarious conditions underscores the broad applicability of this attack method. The detailed analysis ofthe CTA-PTA trade-off provides valuable insights for both attackers and defenders. Attackers can usethis understanding to optimize their attacks, while defenders can leverage this knowledge to developmore effective detection and mitigation strategies. The observed trade-off highlights the need fordetection methods sensitive to even subtle changes in model accuracy, beyond simply monitoringoverall performance metrics.The vulnerability of knowledge distillation to FLIP is a particularly concerning finding. Our resultsshow that backdoors can effectively propagate from a poisoned teacher model to a student modelduring the distillation process. This highlights the importance of securing the entire training pipeline,from data collection and annotation to model training and deployment. A holistic security approach iscrucial to mitigate the risks associated with knowledge distillation and other model training paradigmssusceptible to label-only attacks. The cascading nature of this vulnerability underscores the need forrobust security measures at every stage of model development.The implications of our research extend beyond the specific FLIP attack mechanism. The findingshighlight the broader challenges of ensuring the security and trustworthiness of machine learningsystems in the face of increasingly sophisticated adversarial attacks. The ease with which label-onlybackdoors can be implanted necessitates a paradigm shift in security practices, moving beyond a focussolely on input data integrity to encompass the entire training process. This includes developing robustmethods for detecting subtle label manipulations, designing training procedures less susceptible tolabel-only attacks, and implementing comprehensive security audits throughout the machine learninglifecycle. 8Future research should focus on developing novel defense mechanisms specifically designed to detectand mitigate label-only backdoor attacks. This includes exploring techniques that leverage labelconsistency checks, anomaly detection, and robust model training methods. Furthermore, researchinto the development of more sophisticated trigger patterns and the exploration of FLIP’s applicabilityto other machine learning tasks and model architectures is warranted. A deeper understanding of theunderlying vulnerabilities exploited by FLIP will be crucial in developing effective countermeasuresand ensuring the security and trustworthiness of machine learning systems. The findings presented inthis paper represent a significant step towards a more comprehensive understanding of this emergingthreat and provide a foundation for future research in this critical area.9"
P025,"Scene Comprehension Through Image Analysis withan Extensive Array of Categories and Context at theScene LevelAbstractThis research introduces a unique approach to scene parsing that is nonparametric,which enhances the precision and expands the scope of foreground categories withinimages of scenes. Initially, the accuracy of label likelihood at the superpixel levelis improved by combining likelihood scores from multiple probabilistic classifiers.This method improves classification accuracy and enhances the representation ofcategories that are less frequently represented. The second advancement involvesthe integration of semantic context into the parsing procedure by utilizing globallabel costs. Instead of relying on sets derived from image retrieval, the techniquedescribed assigns a comprehensive likelihood estimate to each label, which issubsequently incorporated into the overall energy function. The effectivenessof the system is assessed using two expansive datasets, SIFTflow and LMSun.The system demonstrates performance that is at the forefront of the field on theSIFTflow dataset and achieves outcomes that are close to setting new records onthe LMSun dataset.1 IntroductionThe task of scene parsing involves assigning semantic labels to every pixel within an image of ascene. Algorithms for image parsing attempt to categorize different types of scenes, both indoors andoutdoors, such as a shoreline, a roadway, an urban environment, and an airport. Numerous systemshave been developed to categorize each pixel in an image semantically. A significant obstacle forimage parsing methods is the considerable variability in recognition rates across different types ofclasses. Background classes, which usually cover a significant area of the image’s pixels, often have auniform look and are identified with great accuracy. Foreground classes, which usually take up fewerpixels in the image, have changeable forms and might be hidden or set up in various ways. Thesekinds of classes represent noticeable parts of the image that frequently grab a viewer’s attention.However, their recognition rates are often much lower than those of background classes, makingthem frequent examples of unsuccessful recognition.Impressive results have been obtained by parametric scene parsing techniques on datasets with alimited number of labels. Nevertheless, for considerably bigger datasets with a lot of labels, usingthese techniques becomes more challenging because of the increased demands on learning andoptimization.Nonparametric image parsing techniques have recently been introduced to tackle the growing varietyof scene types and semantic labels effectively. These methods usually begin by reducing the com-plexity of the problem from individual pixels to superpixels. Initially, a set of images is selected,consisting of training images that bear the closest visual resemblance to the image being queried.The potential labels for a specific image are limited to those found in the selected set of images.Subsequently, the probability scores for the classification of superpixels are determined by matchingvisual characteristics. Ultimately, context is applied by reducing an energy function that includes boththe expense of the data and information on how often classes appear together in nearby superpixels..A shared difficulty encountered by nonparametric parsing methods is the phase of image retrieval.Even though image retrieval helps narrow down the number of labels to think about, it’s seen as avery important step in the process. There’s no opportunity to correct the mistake if the correct labelsare not among the images that were retrieved. It has been reported that mistakes in retrieval are themain reason for most unsuccessful cases.A novel nonparametric image parsing algorithm is proposed in this work, aiming for enhancedoverall precision and improved identification rates for classes that are less commonly represented. Anefficient system is developed that can adapt to an ever-growing quantity of labels. The contributionsmade are outlined as follows:1. Superpixel label likelihood scores are improved by merging classifiers. The system mergesthe output probabilities from several classification models to generate a more equitable score foreach label at every superpixel. The weights for merging the scores are determined by employing alikelihood normalization technique on the training set in an automated manner. 2. Semantic context isintegrated within a probabilistic structure. To prevent the removal of important labels that cannot beretrieved later, a retrieval set is not structured. Instead, label costs are utilized, which are determinedfrom the global contextual relationships of labels in analogous scenes, to obtain enhanced parsingoutcomes.The system developed achieves top-tier per-pixel recognition accuracy on two extensive datasets:SIFTflow, which includes 2688 images with 33 labels, and LMSun, which has 45576 images with232 labels.2 Related WorkSeveral techniques for scene parsing, both parametric and nonparametric, have been suggested. Thenonparametric systems that try to cover a wide range of semantic classes are very similar to themethod. Different methods are used to improve the overall effectiveness of nonparametric parsing.The authors merge region-parsing with outputs from per-exemplar SVM detectors. Object masksare transferred by per-exemplar detectors into the test image for segmentation. Their method greatlyimproves overall accuracy, but it requires a lot of computer power. It’s hard to scale because dataterms need to be calibrated using a batch of fine training in a leave-one-out way, which is hard to do.Superpixels from rare classes are specifically added to the retrieval set to make them more visible.The authors filter the list of labels for a test image by doing an image retrieval step, and query timeis used to add more samples to rare classes. The way superpixels are classified, how rare classesare recognized, and how semantic context is applied are all different in this system. By combiningclassification costs from different contextual models, a more balanced set of label costs is produced,which promotes the representation of foreground classes. Instead of using image retrieval, globallabel costs are used in the inference step.The value of semantic context has been thoroughly investigated in numerous visual recognitionalgorithms. Context has been employed to enhance the overall labeling performance through afeedback mechanism in nonparametric scene parsing systems. Initial labeling of superpixels in aquery image is utilized to modify the training set by adjusting for recognized background classes,thereby enhancing the visibility of uncommon classes. The objective is to enhance the image retrievalset by reintroducing segments of uncommon classes. A semantic global descriptor is generated.Image retrieval is enhanced by merging the semantic descriptor with the visual descriptors. Contextis added by creating global and local context descriptors based on classification likelihood maps. Themethod described differs from these methods as it does not employ context at each superpixel whencalculating a global context descriptor. Instead, contextual information across the entire image istaken into account.Contextually relevant outcomes are produced by deducing label correlations in comparable sceneimages. Additionally, there is no retrieval set that needs to be enriched. Rather, the global context isstructured within a probabilistic framework, where label costs are calculated across the whole image.Furthermore, the global context is executed in real time without any preliminary training. Anothermethod of image parsing that doesn’t use retrieval sets is where image labeling is done by movingannotations from a graph of patch matches across image sets. But this method needs a lot of memory,which makes it hard to scale for big datasets. 2The presented method draws inspiration from the combination of classifier techniques in machinelearning, which have demonstrated the ability to enhance the capabilities of individual classifiers.Several fusion methods have been effectively applied in various fields of computer vision, includingdetecting faces, annotating images with multiple labels, tracking objects, and recognizing characters.Nonetheless, the classifiers that make up these systems and the ways they are combined are verydifferent from the framework, and the other methods have only been tested on small datasets.3 Baseline Parsing PipelineThis section provides a summary of the basic image parsing system, which is composed of threestages: feature extraction, label likelihood estimation at superpixels, and inference.Afterward, contributions are presented: enhancing likelihoods at superpixels and calculating labelcosts for global context at the scene level.3.1 Segmentation and Feature ExtractionTo reduce the complexity of the task, the image is partitioned into superpixels. Extraction ofsuperpixels from images begins by employing an efficient graph-based method. For each superpixel,20 distinct types of local features are extracted to characterize its shape, appearance, texture, color, andposition, adhering to established methods. In addition to these features, Fisher Vector (FV) descriptorsare extracted at each superpixel using an established library. Computation of 128-dimensional denseSIFT feature descriptors is performed on five different patch sizes (8, 12, 16, 24, 30). A dictionarycomprising 1024 words is constructed. Subsequently, the FV descriptors are retrieved and PrincipalComponent Analysis (PCA) is applied to decrease their dimensionality to 512. Each superpixel isrepresented by a feature vector that has 2202 dimensions.3.2 Label Likelihood EstimationThe features obtained in the prior stage are utilized to determine label probabilities for each superpixel.Unlike conventional approaches, the possible labels for a test image are not restricted. Instead, thedata term for the likelihood of each class label c C is computed, where C represents the total numberof classes in the dataset. The normalized cost D(l<sub>si</sub> = c|s<sub>i</sub>) of assigninglabel c to superpixel s<sub>i</sub> is given by: 1D(l = c|s ) = 1 − (1)si i −L (s ,c)1 + e iunbalwhere L<sub>unbal</sub>(s<sub>i</sub>, c) is the log-likelihood ratio score of label c, given byL<sub>unbal</sub>(s<sub>i</sub>, c) = 1/2 log(P(s<sub>i</sub>|c)/P(s<sub>i</sub>|¬c)), where¬c = C c is the set of all labels except c, and P(s<sub>i</sub>|c) is the likelihood of superpixels<sub>i</sub> given c. A boosted decision tree (BDT) model is trained to obtain the label likelihoodsL<sub>unbal</sub>(s<sub>i</sub>, c). For implementation, a publicly accessible boostDT libraryis utilized. During this phase, the BDT model is trained using every superpixel in the training set,which constitutes an imbalanced distribution of class labels C.3.3 Smoothing and InferenceThe optimization challenge is formulated as a maximum a posteriori (MAP) estimation to determinethe ultimate labeling L through Markov Random Field (MRF) inference. Using only the estimatedlikelihoods from the preceding section to categorize superpixels leads to imprecise classifications.Incorporating a smoothing term V(l<sub>s<sub>i</sub></sub>, l<sub>s<sub>j</sub></sub>) intothe MRF energy function aims to address this problem by penalizing adjacent superpixels withsemantically incongruous labels. The goal is to minimize the following energy function:(cid:88) (cid:88)E(L) = D(l = c|s ) + λ V (l , l ) (2)s i s si i js ∈S (i,j)∈Ai 3where A represents the set of neighboring superpixel indices and V(l<sub>s<sub>i</sub></sub>,l<sub>s<sub>j</sub></sub>) denotes the penalty for assigning labels l<sub>s<sub>i</sub></sub>and l<sub>s<sub>j</sub></sub> to two adjacent pixels, calculated from occurrences in the trainingset combined with the constant Potts model following established methods. is the smoothing constant.Inference is conducted using the -expansion method with established code.4 Improving Superpixel Label CostsAlthough foreground objects typically stand out the most in a picture of a scene, parsing algorithmsfrequently misclassify them. For instance, in an image of a city street, a person would usually firstspot the individuals, signs, and vehicles before they would see the structures and the street. However,because of two primary factors, scene parsing algorithms frequently misclassify foreground regionsas belonging to the surrounding background. Initially, in the superpixel classification phase, anyclassifier would naturally prefer classes that are more prevalent to reduce the overall training error.Secondly, during the MRF smoothing phase, a lot of the superpixels that were accurately identified asforeground objects are smoothed out by the background pixels around them.It is suggested that the label likelihood score at each superpixel be improved to obtain a more preciseparsing output. Various classifiers are designed that provide supplementary information regardingthe data. Subsequently, all the developed models are merged to produce a unified conclusion. Anoverview of the method for merging classifiers is displayed in Figure 1. During the testing phase,the label likelihood scores from all the BDT models are combined to generate the final scores forsuperpixels.4.1 Fusing ClassifiersThe proposed method is inspired by ensemble classifier methods, which train several classifiers andmerge them to enhance decision-making. These methods are especially helpful when the classifiersare distinct. In other words, the decrease in error is connected to the lack of correlation between themodels that were trained. This means that the total error is decreased if the classifiers misclassifydifferent data points. Furthermore, it has been demonstrated that for large datasets, dividing thetraining set yields superior results compared to dividing the feature space.It has been observed that the classification error for a particular class is correlated with the averagenumber of pixels it covers in the scene images, as indicated by the blue line in Figure 2. This is inline with what earlier methods found, which is that the rate of classification error is related to howoften classes show up in the training set. However, it goes beyond that by taking into account howoften the classes appear at the image level, which is meant to solve the problem of less-representedclasses being smoothed out by a background class that is nearby.To achieve this, three BDT models are trained using the following training data criteria: (1) a balancedsubsample of all classes C in the dataset, (2) a balanced subsample of classes that occupy an averageof less than zThe goal of these decisions is to lessen the correlation between the trained BDT models, as seen inFigure 2. The balanced classifiers are able to correctly identify some of the less-represented classes,but they make more mistakes on the more-represented classes. The unbalanced classifier, on theother hand, mostly misclassifies the less-represented classes. Combining the likelihoods from allthe classifiers leads to an improved overall decision that enhances the representation of all classes(Figure 1). It was noticed that the addition of more classifiers did not enhance performance for any ofthe datasets.The ultimate expense of allocating a label c to a superpixel s<sub>i</sub> can subsequently beexpressed as the amalgamation of the likelihood scores of all classifiers:1D(l = c|s ) = 1 − (3)si i −L (s ,c)1 + e icombwhere L<sub>comb</sub>(s<sub>i</sub>, c) represents the combined likelihood score obtained bythe weighted sum of the scores from all classifiers:4(cid:88)L (s , c) = w (c)L (s , c) (4)comb i j j ij=1,2,3,4where L<sub>j</sub>(s<sub>i</sub>, c) is the score from the j<sup>th</sup> classifier, andw<sub>j</sub>(c) is the normalized weight of the likelihood score of class c in the j<sup>th</sup>classifier.4.2 Normalized Weight LearningThe weights ww < sub > j < /sub > (c)]arelearnedf orallclassesCinof f linesettingsusingthetrainingset.T heweightsarecalculatedindependentlyf oreachclassif ier.T heweight˜w < sub > j < /sub > (c)f orclasscinthej < sup > th < /sup > classif ierisdeterminedbyaveragingtheratioof thetotallikelihoodsf orclassctothetotallikelihoodsf orallclassesc < sub > i < /sub > Ccacrossallsuperpixelss < sub > i < /sub > S :(cid:80) L (s , c)1 j is ∈Sw˜ (c) = i (5)(cid:80) (cid:80)j |C | L (s , c )j j i ic ∈C\c s ∈Si iwhere |C<sub>j</sub>| denotes the quantity of classes encompassed by the j<sup>th</sup> classifierand not covered by any other classifier with a fewer number of classes.The normalized weight w<sub>j</sub>(c) of class c can then be computed as: w<sub>j</sub>(c) =~w<sub>j</sub>(c) / <sub>j=1,2,3,4</sub>(~w<sub>j</sub>(c)). Normalizing the output likelihoodsin this way improves the likelihood that all classifiers will be taken into account in the outcome, witha focus on classes that are less represented.5 Scene-Level Global ContextWhen working with scene parsing challenges, including the scene’s semantics in the labeling processis beneficial. For example, if a scene is known to be a beach scene, labels such as sea, sand, and skyare expected to be found with a much greater probability than labels like car, building, or fence. Theinitial labeling results of a test image are used in estimating the likelihoods of all labels c C. Thelikelihoods are estimated globally over an image, i.e., there is a unique cost per label per image. Theglobal label costs are then incorporated into a subsequent MRF inference stage to enhance the results.The presented method, in contrast to previous methods, does not restrict the number of labels tothose found in the retrieval set. Instead, it utilizes the set to calculate the likelihood of class labelsin a k-nn manner. The likelihoods are normalized by counts over the entire dataset and smoothedto provide an opportunity for labels not present in the retrieval set. The likelihoods are also used inMRF optimization, not for reducing the number of labels.5.1 Context-Aware Global Label CostsIt is proposed that semantic context be incorporated by using label statistics instead of global visualfeatures. The reasoning behind this decision is that sorting by global visual characteristics oftendoesn’t find images that are similar at the scene level. For instance, a highway scene might bemistaken for a beach scene if road pixels are incorrectly classified as sand. Nonetheless, when given areasonably accurate initial labeling, sorting by label statistics finds images that are more semanticallyrelated. This helps to eliminate outlier labels and find labels that are absent in a scene.For a given test image I, minimizing the energy function in equation 2 produces an initial labelingL of the superpixels in the image. If C is the total number of classes in the dataset, let T C be theset of unique labels which appear in L, i.e. T = t | s<sub>i</sub> : l<sub>s<sub>i</sub></sub> = t,where s<sub>i</sub> is a superpixel with index i in the test image, and l<sub>s<sub>i</sub></sub>is the label of s<sub>i</sub>. Semantic context is exploited in a probabilistic framework, where theconditional distribution P(c|T) is modeled over class labeling C given the initial global labeling of animage T. P(c|T) c C is computed in a K-nn fashion:1 + n(c, K ) 1 + n(¬c, K )T TP (c|T ) = (6)n(c, S) |S|5where K<sub>T</sub> is the K-neighborhood of initial labeling T, n(c, X) is the number of superpixelswith label c in X, n(¬c, X) is the number of superpixels with all labels except c in X, and |S| is thetotal number of superpixels in the training set. The likelihoods are normalized and a smoothingconstant of value 1 is added.To obtain the neighborhood K<sub>T</sub>, training images are ranked by their distance to thequery image. The distance between two images is determined by the weighted size of the intersectionof their class labels, which intuitively shows that the neighbors of T are images that share many labelswith those in T. A different weight is assigned to each class in T in a manner that gives preference toclasses that are less represented.The algorithm operates in three stages, as depicted in Figure 3. It begins by (1) assigning a weight<sub>t</sub> to each class t T, which is inversely proportional to the number of superpixels in thetest image with label t: <sub>t</sub> = 1 - n(t,I)/|I|, where n(t, I) is the number of superpixels in thetest image with label l<sub>s<sub>i</sub></sub> = t, and |I| is the total number of superpixels in theimage. Then, (2) training images are ranked by the weighted size of intersection of their class labelswith the test image. Finally, (3) the global label likelihood L<sub>global</sub>(c) = P(c|T) of eachlabel c C is computed using equation 6.Calculating the label costs is performed in real-time for a query image, without the need for anyoffline batch training. The method enhances the overall precision by utilizing solely the true labels oftraining images, without incorporating any global visual characteristics.5.2 Inference with Label CostsOnce the likelihoods L<sub>global</sub>(c) of each class c C are obtained, a label cost H(c) =-log(L<sub>global</sub>(c)) can be defined. The final energy function becomes:(cid:88) (cid:88) (cid:88)E(L) = D(l = c|s ) + λ V (l , l ) + H(c)δ(c) (7)s i s si i js ∈S c∈C(i,j)∈Aiwhere (c) is the indicator function of label c:(cid:26)1 ∃s : l = cif i sδ(c) = i (8)0 otherwiseEquation 7 is solved using -expansion with the extension method to optimize label costs. Optimizingthe energy function in equation 7 effectively minimizes the number of unique labels in a test image tothose with low label costs, i.e., those most relevant to the scene.6 ExperimentsThe experiments were conducted on two extensive datasets: SIFTflow and LMSun. SIFTflow consistsof 2,488 training images and 200 test images. All images are of outdoor scenes, sized 256x256 with33 labels. LMSun includes both indoor and outdoor scenes, with a total of 45,676 training imagesand 500 test images. Image sizes range from 256x256 to 800x600 pixels with 232 labels.The same evaluation metrics and train/test splits as in previous methods are employed. The per-pixelaccuracy (the percentage of pixels in test images that were correctly labeled) and per-class recognitionrate (the average of per-pixel accuracies of all classes) are reported. The following variants of thesystem are evaluated: (i) baseline, as described in section 3, (ii) baseline (with balanced BDT), whichis the baseline approach using a balanced classifier, (iii) baseline + FC (NL fusion), which is thebaseline in addition to the fusing classifiers with normalized-likelihood (NL) weights in section 4, and(iv) full, which is baseline + fusing classifiers + global costs. To show the effectiveness of the fusionmethod (section 4.2), the results of (v) baseline + FC (average fusion), which is fusing classifiers byaveraging their likelihoods, and (vi) baseline + FC (median fusion), which is fusing classifiers bytaking the median of their likelihoods are reported. Results of (vii) full (without FV), which is thefull system without using the Fisher Vector features are also reported.6x = 5 is fixed (section 4.1), a value that was obtained through empirical evaluation on a small subsetof the training set.6.1 ResultsThe results are compared with state-of-the-art methods on SIFTflow in Table 1. K = 64 top-rankedtraining images have been set for computing the global context likelihoods (section 5.1). The fullsystem achieves 81.7Table 1: Comparison with state-of-the-art per-pixel and per-class accuracies (%) on the SIFTflowdataset. Method Per-pixel Per-classLiu et al. 76.7 N/AFarabet et al. 78.5 29.5Farabet et al. balanced 74.2 46.0Eigen and Fergus 77.1 32.5Singh and Kosecka 79.2 33.8Tighe and Lazebnick 77.0 30.1Tighe and Lazebnick 78.6 39.2Yang et al. 79.8 48.7Baseline 78.3 33.2Baseline (with balanced BDT) 76.2 45.5Baseline + FC (NL fusion) 80.5 48.2Baseline + FC (average fusion) 78.6 46.3Baseline + FC (median fusion) 77.3 46.8Full without Fisher Vectors 77.5 47.0Full 81.7 50.1Table 2 compares the performance of the same variants of the system with the state-of-the-art methodson the large-scale LMSun dataset. LMSun is more challenging than SIFTflow in terms of the numberof images, the number of classes, and the presence of both indoor and outdoor scenes. Accordingly,a larger value of K = 200 in equation 6 is used. The method achieves near-record performance inper-pixel accuracy (61.2Table 2: Comparison with state-of-the-art per-pixel and per-class accuracies (%) on the LMSundataset. Method Per-pixel Per-classTighe and Lazebnick 54.9 7.1Tighe and Lazebnick 61.4 15.2Yang et al. 60.6 18.0Baseline 57.3 9.5Baseline (with balanced BDT) 45.4 13.8Baseline + FC (NL fusion) 60.0 14.2Baseline + FC (average fusion) 60.5 11.4Baseline + FC (median fusion) 59.2 14.7Full without Fisher Vectors 58.2 13.6Full 61.2 16.0The performance of the system is analyzed when varying the number of trees T for training the BDTmodel (section 4.1), and the number of top training images K in the global label costs (section 5.1).Figure 4 shows the per-pixel accuracy (on the y-axis) and the per-class accuracy (on the x-axis) as afunction of T for a variety of K’s. Increasing the value of T generally produces better classificationmodels that better describe the training data. At T 400, performance levels off. As shown, theglobal label costs consistently improve the performance over the baseline method with no globalcontext. Using more training images (higher K) improves the performance through considering moresemantically relevant scene images. However, performance starts to decrease for very high values ofK (e.g., K = 1000) as more noisy images start to be added.7Figure 5 shows the per-class recognition rate for the baseline, combined classifiers, and the fullsystem on SIFTflow. The fusing classifiers technique produces more balanced likelihood scores thatcover a wider range of classes. The semantic context step removes outlier labels and recovers missinglabels, which improves the recognition rates of both common and rare classes. Recovered classesinclude field, grass, bridge, and sign. Failure cases include extremely rare classes, e.g. cow, bird,desert, and moon.6.2 Running TimeThe runtime performance was analyzed for both SIFTflow and LMSun (without feature extraction)on a four-core 2.84GHz CPU with 32GB of RAM without code optimization. For the SIFTflowdataset, training the classifier takes an average of 15 minutes per class. The training process is runin parallel. The training time highly depends on the feature dimensionality. At test time, superpixelclassification is efficient, with an average of 1 second per image. Computing global label costs takes3 seconds. Finally, MRF inference takes less than one second. MRF inference is run twice for thefull pipeline. LMSun is much larger than SIFTflow. It takes 3 hours for training the classifier, lessthan a minute for superpixel classification per image, less than 1 minute for MRF inference, and 2minutes for global label cost computation.6.3 DiscussionThe presented scene parsing method is generally scalable as it does not require any offline trainingin a batch fashion. However, the time required for training a BDT classifier increases linearly withincreasing the number of data points. This is challenging with large datasets like LMSun. Randomlysubsampling the dataset has a negative impact on the overall precision of the classification results.Alternative approaches of mining discriminative data points that better describe each class are plannedto be investigated. The system still faces challenges in trying to recognize very less-representedclasses in the dataset (e.g., bird, cow, and moon). This could be handled via better contextual modelsper query image.7 ConclusionA novel scene parsing algorithm has been presented that enhances the overall labeling precision,without neglecting foreground classes that are significant to human viewers. By merging likelihoodscores from various classification models, the strengths of individual models have been successfullyamplified, thus enhancing both the per-pixel and per-class accuracy. To prevent the removal ofaccurate labels through image retrieval, global context has been integrated into the parsing processusing a probabilistic framework. The energy function has been expanded to incorporate global labelcosts that produce a more semantically relevant parsing output. Experiments have demonstratedthe superior performance of the system on the SIFTflow dataset and comparable performance tostate-of-the-art methods on the LMSun dataset. 8"
P027,"emoji2vec: Learning Emoji Representations from theirDescriptionAbstractMany current natural language processing applications for social media rely onrepresentation learning and utilize pre-trained word embeddings. There currentlyexist several publicly-available, pre-trained sets of word embeddings, but theycontain few or no emoji representations even as emoji usage in social media hasincreased. emoji2vec are pre-trained embeddings for all Unicode emojis which arelearned from their description in the Unicode emoji standard. The resulting emojiembeddings can be readily used in downstream social natural language processingapplications alongside word2vec. For the downstream task of sentiment analysis,emoji embeddings learned from short descriptions outperforms a skip-gram modeltrained on a large collection of tweets, while avoiding the need for contexts inwhich emojis need to appear frequently in order to estimate a representation.1 IntroductionFirst introduced in 1997, emojis, a standardized set of small pictorial glyphs depicting everythingfrom smiling faces to international flags, have seen a drastic increase in usage in social media overthe last decade. The Oxford Dictionary named 2015 the year of the emoji, citing an increase in usageof over 800% during the course of the year, and elected the ’Face with Tears of Joy’ emoji () as theWord of the Year. As of this writing, over 10% of Twitter posts and over 50% of text on Instagramcontain one or more emojis. Due to their popularity and broad usage, they have been the subjectof much formal and informal research in language and social communication, as well as in naturallanguage processing (NLP).In the context of social sciences, research has focused on emoji usage as a means of expressingemotions on mobile platforms. Interestingly, although essentially thought of as means of expressingemotions, emojis have been adopted as tools to express relationally useful roles in conversation.Emojis are culturally and contextually bound, and are open to reinterpretation and misinterpretation.These findings have paved the way for many formal analyses of semantic characteristics of emojis.Concurrently we observe an increased interest in natural language processing on social mediadata. Many current NLP systems applied to social media rely on representation learning and wordembeddings. Such systems often rely on pre-trained word embeddings that can for instance beobtained from word2vec or GloVe. Yet, neither resource contain a complete set of Unicode emojirepresentations, which suggests that many social NLP applications could be improved by the additionof robust emoji representations.Embeddings for emoji Unicode symbols are learned from their description in the Unicode emojistandard. The usefulness of emoji representations trained in this way is demonstrated by evaluating ona Twitter sentiment analysis task. Furthermore, a qualitative analysis by investigating emoji analogyexamples and visualizing the emoji embedding space is provided.2 Related WorkThere has been little work in distributional embeddings of emojis. The first research done inthis direction was an informal blog post by the Instagram Data Team in 2015. They generatedvector embeddings for emojis similar to skip-gram-based vectors by training on the entire corpusof Instagram posts. Their research gave valuable insight into the usage of emojis on Instagram,and showed that distributed representations can help understanding emoji semantics in everydayusage. The second contribution, closest to ours, trained emoji embeddings from a large Twitterdataset of over 100 million English tweets using the skip-gram method. These pre-trained emojirepresentations led to increased accuracy on a similarity task, and a meaningful clustering of theemoji embedding space. While this method is able to learn robust representations for frequently-usedemojis, representations of less frequent emojis are estimated rather poorly or not available at all. Infact, only around 700 emojis can be found in this corpus, while there is support of over 1600 emojisin the Unicode standard.The approach differs in two important aspects. First, since the representation of emojis are estimateddirectly from their description, robust representations are obtained for all supported emoji symbols —even the long tail of infrequently used ones. Secondly, the method works with much less data. Insteadof training on millions of tweets, the representations are trained on only a few thousand descriptions.Still, higher accuracy results are obtained on a Twitter sentiment analysis task.In addition, the work relates to building word representations for words and concepts based on theirdescription in a dictionary. Similarly to their approach, representations are build for emojis based ontheir descriptions and keyword phrases.Some of the limitations are evident in the work who showed that different cultural phenomena andlanguages may co-opt conventional emoji sentiment. Since training is only on English-languagedefinitions and ignore temporal definitions of emojis, the training method might not capture the fullsemantic characteristics of an emoji.3 MethodologyThe method maps emoji symbols into the same space as the 300-dimensional Google News word2vecembeddings. Thus, the resulting emoji2vec embeddings can be used in addition to 300-dimensionalword2vec embeddings in any application. To this end emojis, their name and their keyword phrasesare crawled from the Unicode emoji list, resulting in 6088 descriptions of 1661 emoji symbols.3.1 ModelEmoji embeddings are trained using a simple method. For every training example consisting of anw , ..., wemoji and a sequence of words describing that emoji, we take the sum of the individual1 Nword vectors in the descriptive phrase as found in the Google News word2vec embeddingsN(cid:88)v = w , (1)kk=1w wwhere is the word2vec vector for word if that vector exists (otherwise we drop the summand)k kv xand is the vector representation of the description. A trainable vector for every emoji inj i xour training set is defined, and the probability of a match between the emoji representation ivand its description representation is modeled using the sigmoid of the dot product of the twojTσ(x v )representations . For training we use the logistic lossji T TL(i, j, y ) = − log(σ(y x v − (1 − y )x v )) (2)ij ij j ij ji iy j iwhere is 1 if description is valid for emoji and 0 otherwise.ij3.2 OptimizationThe model is implemented in TensorFlow and optimized using stochastic gradient descent with Adamas optimizer. As we do not observe any negative training examples (invalid descriptions of emojis do2not appear in the original training set), to increase generalization performance we randomly sampledescriptions for emojis as negative instances (i.e. induce a mismatched description). One of theparameters of our model is the ratio of negative samples to positive samples; we found that havingone positive example per negative example produced the best results. We perform early-stopping ona held-out development set and found 80 epochs of training to give the best results. As we are onlytraining on emoji descriptions and our method is simple and cheap, training takes less than 3 minuteson a 2013 MacBook Pro.4 ExperimentsThe approach is quantitatively evaluated on an intrinsic (emoji-description classification) and extrinsic(Twitter sentiment analysis) task. Furthermore, a qualitative analysis is given by visualizing thelearned emoji embedding space and investigating emoji analogy examples.4.1 Emoji-Description ClassificationTo analyze how well the method models the distribution of correct emoji descriptions, a manually-labeled test set containing pairs of emojis and phrases, as well as a correspondence label was created.For instance, the test set includes the example: , ""crying"", True, as well as the example , ""fish"", False.Tσ(x v ) is calculated for each example in the test set, measuring the similarity between the emojiiivector and the sum of word vectors in the phrase.When a classifier thresholds the above prediction at 0.5 to determine a positive or negative correlation,an accuracy of 85.5% is obtained for classifying whether an emoji-description pair is valid or not.By varying the threshold used for this classifier, a receiver operating characteristic curve with anarea-under-the-curve of 0.933 is obtained, which demonstrates that high quality of the learned emojirepresentations.4.2 Sentiment Analysis on TweetsAs downstream task the accuracy of sentiment classification of tweets for various classifiers with threedifferent sets of pre-trained word embeddings are compared: (1) the original Google News word2vecembeddings, (2) word2vec augmented with emoji embeddings trained by skip-gram model, and (3)word2vec augmented with emoji2vec trained from Unicode descriptions. A dataset is used whichconsists of over 67k English tweets labelled manually for positive, neutral, or negative sentiment.In both the training set and the test set, 46% of tweets are labeled neutral, 29% are labeled positive,and 25% are labeled negative. To compute the feature vectors for training, we summed the vectorscorresponding to each word or emoji in the text of the Tweet. The goal of this simple sentimentanalysis model is not to produce state-of-the-art results in sentiment analysis; it is simply to show thatincluding emojis adds discriminating information to a model, which could potentially be exploited inmore advanced social NLP systems.Because the labels are rather evenly distributed, accuracy is an effective metric in determiningperformance on this classification task. Results are reported in Table 1. Augmenting word2vecwith emoji embeddings improves overall classification accuracy on the full corpus, and substantiallyimproves classification performance for tweets that contain emojis. It suggests that emoji embeddingscould improve performance for other social NLP tasks as well. Furthermore, emoji2vec generallyoutperforms the emoji embeddings trained by the skip-gram model, despite being trained on muchless data using a simple model.4.3 Analogy TaskA well-known property of word2vec is that embeddings trained with this method to some extentcapture meaningful linear relationships between words directly in the vector space. For instance, itholds that the vector representation of ’king’ minus ’man’ plus ’woman’ is closest to ’queen’. Wordembeddings have commonly been evaluated on such word analogy tasks. Unfortunately, it is difficultto build such an analogy task for emojis due to the small number and semantically distinct categoriesof emojis. Nevertheless, a few intuitive examples were collected. For every query the closest five3Table 1: Three-way classification accuracy on the Twitter sentiment analysis corpus using RandomForrests and Linear SVM classifier with different word embeddings.Classification accuracy on entire dataset, N = 12920Word Embeddings Random Forest Linear SVMGoogle News 57.5 58.5Google News + (skip-gram model) 58.2* 60.0*Google News + emoji2vec 59.5* 60.5*Classification accuracy on tweets containing emoji, N = 2295Word Embeddings Random Forest Linear SVMGoogle News 46.0 47.1Google News + (skip-gram model) 52.4* 57.4*Google News + emoji2vec 54.4* 59.2*Classification accuracy on 90% most frequent emoji, N = 2186Word Embeddings Random Forest Linear SVMGoogle News 47.3 45.1Google News + (skip-gram model) 52.8* 56.9*Google News + emoji2vec 55.0* 59.5*Classification accuracy on 10% least frequent emoji, N = 308Word Embeddings Random Forest Linear SVMGoogle News 44.7 43.2Google News + (skip-gram model) 53.9* 52.9*Google News + emoji2vec 54.5* 55.2*emojis were retrieved. Though the correct answer is sometimes not the top one, it is often containedin the top three.5 ConclusionSince existing pre-trained word embeddings such as Google News word2vec embeddings or GloVefail to provide emoji embeddings, emoji2vec — embeddings of 1661 emoji symbols were released.Instead of running word2vec’s skip-gram model on a large collection of emojis and their contextsappearing in tweets, emoji2vec is directly trained on Unicode descriptions of emojis. The resultingemoji embeddings can be used to augment any downstream task that currently uses word2vecembeddings, and might prove especially useful in social NLP tasks where emojis are used frequently(e.g. Twitter, Instagram, etc.). Despite the fact that the model is simpler and trained on much lessdata, it outperforms the skip-gram model on the task of Twitter sentiment analysis.As the approach directly works on Unicode descriptions, it is not restricted to emoji symbols. Inthe future the usefulness of the method for other Unicode symbol embeddings will be investigated.Furthermore, plans are made to improve emoji2vec in the future by also reading full text emojidescriptions and using a recurrent neural network instead of a bag-of-word-vectors approach forenocoding descriptions. In addition, since the approach does not capture the context-dependentdefinitions of emojis (such as sarcasm, or appropriation via other cultural phenomena), mechanismswill be explored to efficiently capturing these nuanced meanings.46 Data Release and ReproducibilityPre-trained emoji2vec embeddings as well as the training data and code are released at https://github.com/uclmr/emoji2vec. Note that the emoji2vec format is compatible with word2vec and canbe loaded into gensim or similar libraries. 5"
P028,"Do You See What I Mean? Visual Resolution ofLinguistic AmbiguitiesAbstractUnderstanding language goes hand in hand with the ability to integrate com-plex contextual information obtained via perception. We present a novel task forgrounded language understanding: disambiguating a sentence given a visual scenewhich depicts one of the possible interpretations of that sentence. To this end, weintroduce a new multimodal corpus containing ambiguous sentences, representinga wide range of syntactic, semantic and discourse ambiguities, coupled with videosthat visualize the different interpretations for each sentence. We address this taskby extending a vision model which determines if a sentence is depicted by a video.We demonstrate how such a model can be adjusted to recognize different interpre-tations of the same underlying sentence, allowing to disambiguate sentences in aunified fashion across the different ambiguity types.1 IntroductionAmbiguity is one of the defining characteristics of human languages, and language understandingcrucially relies on the ability to obtain unambiguous representations of linguistic content. Whilesome ambiguities can be resolved using intra-linguistic contextual cues, the disambiguation of manylinguistic constructions requires integration of world knowledge and perceptual information obtainedfrom other modalities.We focus on the problem of grounding language in the visual modality, and introduce a novel taskfor language understanding which requires resolving linguistic ambiguities by utilizing the visualcontext in which the linguistic content is expressed. This type of inference is frequently called for inhuman communication that occurs in a visual environment, and is crucial for language acquisition,when much of the linguistic content refers to the visual surroundings of the child.Our task is also fundamental to the problem of grounding vision in language, by focusing onphenomena of linguistic ambiguity, which are prevalent in language, but typically overlooked whenusing language as a medium for expressing understanding of visual content. Due to such ambiguities,a superficially appropriate description of a visual scene may in fact not be sufficient for demonstratinga correct understanding of the relevant visual content. Our task addresses this issue by introducing adeep validation protocol for visual understanding, requiring not only providing a surface descriptionof a visual activity but also demonstrating structural understanding at the levels of syntax, semanticsand discourse.To enable the systematic study of visually grounded processing of ambiguous language, we createa new corpus, LAVA (Language and Vision Ambiguities). This corpus contains sentences withlinguistic ambiguities that can only be resolved using external information. The sentences are pairedwith short videos that visualize different interpretations of each sentence. Our sentences encompass awide range of syntactic, semantic and dis-course ambiguities, including ambiguous prepositional and verb phrase attachments, conjunctions,logical forms, anaphora and ellipsis. Overall, the corpus contains 237 sentences, with 2 to 3interpretations per sentence, and an average of 3.37 videos that depict visual variations of eachsentence interpretation, corresponding to a total of 1679 videos.Using this corpus, we address the problem of selecting the interpretation of an ambiguous sentencethat matches the content of a given video. Our approach for tackling this task extends the sentencetracker. The sentence tracker produces a score which determines if a sentence is depicted by avideo. This earlier work had no concept of ambiguities; it assumed that every sentence had a singleinterpretation. We extend this approach to represent multiple interpretations of a sentence, enablingus to pick the interpretation that is most compatible with the video.2 Related WorkPrevious language and vision studies focused on the development of multimodal word and sentencerepresentations as well as methods for describing images and videos in natural language. While thesestudies handle important challenges in multimodal processing of language and vision, they do notprovide explicit modeling of linguistic ambiguities.Previous work relating ambiguity in language to the visual modality addressed the problem of wordsense disambiguation. However, this work is limited to context independent interpretation of individ-ual words, and does not consider structure-related ambiguities. Discourse ambiguities were previouslystudied in work on multimodal coreference resolution. Our work expands this line of research, andaddresses further discourse ambiguities in the interpretation of ellipsis. More importantly, to the bestof our knowledge our study is the first to present a systematic treatment of syntactic and semanticsentence level ambiguities in the context of language and vision.The interactions between linguistic and visual information in human sentence processing have beenextensively studied in psycholinguistics and cognitive psychology. A considerable fraction of thiswork focused on the processing of ambiguous language, providing evidence for the importance ofvisual information for linguistic ambiguity resolution by humans. Such information is also vitalduring language acquisition, when much of the linguistic content perceived by the child refers to theirimmediate visual environment. Over time, children develop mechanisms for grounded disambiguationof language, manifested among others by the usage of iconic gestures when communicating ambigu-ous linguistic content. Our study leverages such insights to develop a complementary framework thatenables addressing the challenge of visually grounded disambiguation of language in the realm ofartificial intelligence.3 TaskWe provide a concrete framework for the study of language understanding with visual context byintroducing the task of grounded language disambiguation. This task requires to choose the correctlinguistic representation of a sentence given a visual context depicted in a video. Specifically, providedwith a sentence, n candidate interpretations of that sentence and a video that depicts the content ofthe sentence, one needs to choose the interpretation that corresponds to the content of the video.To illustrate this task, consider the example, where we are given the sentence “Sam approached thechair with a bag” along with two different linguistic interpretations. In the first in-terpretation, which corresponds to parse 1(a), Sam has the bag. In the second interpretation associatedwith parse 1(b), the bag is on the chair rather than with Sam. Given the visual context from figure1(c), the task is to choose which interpretation is most appropriate for the sentence.4 Approach OverviewTo address the grounded language disambiguation task, we use a compositional approach for determin-ing if a specific interpretation of a sentence is depicted by a video. a sentence and an accompanyinginterpretation encoded in first order logic, give rise to a grounded model that matches a video againstthe provided sentence interpretation.The model is comprised of Hidden Markov Models (HMMs) which encode the semantics of words,and trackers which locate objects in video frames. To represent an interpretation of a sentence, wordmodels are combined with trackers through a cross-product which respects the semantic representationof the sentence to create a single model which recognizes that interpretation.2Given a sentence, we construct an HMM based representation for each interpretation of that sentence.We then detect candidate locations for objects in every frame of the video. Together the re-forestation for the sentence and the candidate object locations are combined to form a model whichcan determine if a given interpretation is depicted by the video. We test each interpretation and reportthe interpretation with highest likelihood.5 CorpusTo enable a systematic study of linguistic ambiguities that are grounded in vision, we compileda corpus with ambiguous sentences describing visual actions. The sentences are formulated suchthat the correct linguistic interpretation of each sentence can only be determined using external,non-linguistic, information about the depicted activity. For example, in the sentence “Bill held thegreen chair and bag”, the correct scope of “green” can only be determined by integrating additionalinformation about the color of the bag. This information is provided in the accompanying videos,which visualize the possible interpretations of each sentence. Figure 2 presents the syntactic parsesfor this example along with frames from the respective videos. Although our videos contain visualuncertainty, they are not ambiguous with respect to the linguistic interpretation they are presenting,and hence a video always corresponds to a single candidate representation of a sentence.The corpus covers a wide range of wellknown syntactic, semantic and discourse ambiguity classes. While the ambiguities are associatedwith various types, different sentence interpretations always represent distinct sentence meanings,and are hence encoded semantically using first order logic. For syntactic and discourse ambiguitieswe also provide an additional, ambiguity type specific encoding as described below.Syntax• Syntactic ambiguities include Prepositional Phrase (PP) attachments, Verb Phrase(VP) attachments, and ambiguities in the interpretation of conjunctions. In addition tological forms, sentences with syntactic ambiguities are also accompanied with Context FreeGrammar (CFG) parses of the candidate interpretations, generated from a deterministic CFGparser.Semantics• The corpus addresses several classes of semantic quantification ambiguities, inwhich a syntactically unambiguous sentence may correspond to different logical forms. Foreach such sentence we provide the respective logical forms.Discourse• The corpus contains two types of discourse ambiguities, Pronoun Anaphora andEllipsis, offering examples comprising two sentences. In anaphora ambiguity cases, anambiguous pronoun in the second sentence is given its candidate antecedents in the firstsentence, as well as a corresponding logical form for the meaning of the second sentence. Inellipsis cases, a part of the second sentence, which can constitute either the subject and theverb, or the verb and the object, is omitted. We provide both interpretations of the omissionin the form of a single unambiguous sentence, and its logical form, which combines themeanings of the first and the second sentences.Table 2 lists examples of the different ambiguity classes, along with the candidate interpretations ofeach example.The corpus is generated using Part of Speech (POS) tag sequence templates. For each template, thePOS tags are replaced with lexical items from the corpus lexicon, described in table 3, using all thevisually applicable assignments. This generation process yields an overall of 237 sentences,of which 213 sentences have 2 candidate interpretations, and 24 sentences have 3 interpretations.Table 1 presents the corpus templates for each ambiguity class, along with the number of sentencesgenerated from each template.The corpus videos are filmed in an indoor environment containing background objects and pedestrians.To account for the manner of performing actions, videos are shot twice with different actors. Wheneverapplicable, we also filmed the actions from two different directions (e.g. approach from the left,and approach from the right). Finally, all videos were shot with two cameras from two differentview points. Taking these variations into account, the resulting video corpus contains 7.1 videosper sentence and 3.37 videos per sentence interpretation, corresponding to a total of 1679 videos.3Table 1: POS templates for generating the sentences in our corpus. The rightmost column representsthe number of sentences in each category. The sentences are produced by replacing the POS tagswith all the visually applicable assignments of lexical items from the corpus lexicon shown in table 3.Ambiguity Templates #4*Syntax PP NNP V DT [JJ] NN1 IN DT [JJ] NN2. 48VP NNP1 V [IN] NNP2 V [JJ] NN. 60NNP1 [and NNP2] V DT JJ NN1 and NN2Conjunction 40NNP V DT NN1 or DT NN2 and DT NN3.Total 148NNP1 and NNP2 V a NN.Semantics Logical Form 35Someone V the NNS.2*Discourse Anaphora NNP V DT NN1 and DT NN2. It is JJ. 36Ellipsis NNP1 V NNP2. Also NNP3. 18Total 54Total 237The average video length is 3.02 seconds (90.78 frames), with in an overall of 1.4 hours of footage(152434 frames).A custom corpus is required for this task because no existing corpus, containing either videos orimages, systematically covers multimodal ambiguities. Datasets aim to control for more aspects ofthe videos than just the main action being performed but they do not provide the range of ambiguitiesdiscussed here. The closest dataset is that of as it controls for object appearance, color, action,and direction of motion, making it more likely to be suitable for evaluating disambiguation tasks.Unfortunately, that dataset was designed to avoid ambiguities, and therefore is not suitable forevaluating the work described here.6 ModelTo perform the disambiguation task, we extend the sentence recognition model which representssentences as compositions of words. Given a sentence, its first order logic interpretation and avideo, our model produces a score which determines if the sentence is depicted by the video. Itsimultaneously tracks the participants in the events described by the sentence while recognizing theevents themselves. This al-lows it to be flexible in the presence of noise by integrating top-down information from the sentencewith bottom-up information from object and property detectors. Each word in the query sentence isrepresented by an HMM, which recognizes tracks (i.e. paths of detections in a video for a specificobject) that satisfy the semantics of the given word. In essence, this model can be described as havingtwo layers, one in which object tracking occurs and one in which words observe tracks and filtertracks that do not satisfy the word constraints.Given a sentence interpretation, we construct a sentence-specific model which recognizes if a videodepicts the sentence as follows. Each predicate in the first order logic formula has a correspondingHMM, which can recognize if that predicate is true of a video given its arguments. Each variable hasa corresponding tracker which attempts to physically locate the bounding box corresponding to thatvariable in each frame of avideo. This creates a bipartite graph: HMMs that represent predicates are connected to trackers thatrepresent variables. The trackers themselves are similar to the HMMs, in that they comprise a latticeof potential bounding boxes in every frame. To construct a joint model for a sentence interpretation,we take the cross product of HMMs and trackers, taking only those cross products dictated by thestructure of the formula corresponding to the desired interpretation. Given a video, we employ anobject detector to generate candidate detections in each frame, construct trackers which select one ofthese detections in each frame, and finally construct the overall model from HMMs and trackers.4Table 2: An overview of the different ambiguity types, along with examples of ambiguous sentenceswith their linguistic and visual interpretations. Note that similarly to semantic ambiguities, syntacticand discourse ambiguities are also provided with first order logic formulas for the resulting sentenceinterpretations. Table 4 shows additional examples for each ambiguity type, with frames from samplevideos corresponding to the different interpretations of each sentence.Ambiguity Example Linguistic interpretations Visual setupsPP Claire left the green chair with a Claire [left the green chair] [with The bag is with Claire.yellow bag. a yellow bag]. The bag is on the chair.Claire left [the green chair witha yellow bag].VP Claire looked at Bill picking up Claire looked at [Bill [picking up Bill picks up the chair.a chair. a chair]]. Claire picks up the chair.Claire [looked at Bill] [pickingup a chair].Conjunction Claire held a green bag and Claire held a [green [bag and The chair is green.The chair is not green.chair. chair]].Claire held a [[green bag] and[chair]].Claire held the chair or the bag Claire held [[the chair] or [the Claire holds the chair.and the telescope. bag and the telescope]]. Claire holds the chair and theClaire held [[the chair or the bag] telescope.and [the telescope]].x xLogical Form Someone moved the two chairs. chair( ), move(Claire, ), Claire and Bill move the samexmove(Bill, ) chair.x y x ̸= ychair( ), chair( ), , Claire and Bill move differentxmove(Claire, ), chairs.ymove(Bill, ) One person moves both chairs.x y x ̸= y Each chair moved by a differentchair( ), chair( ), ,u person.person( ),u x u ymove( , ), move( , )x y x ̸= ychair( ), chair( ), ,u vperson( ), person( )u ̸= v u x v y, move( , ), move( , )Anaphora Sam picked up the bag and the It = bag The bag is yellow.chair. It is yellow. It = chair The chair is yellow.Ellipsis Sam left Bill. Also Clark. Sam left Bill and Clark. Sam left Bill and Clark.Sam and Clark left Bill. Sam and Clark left Bill.Table 3: The lexicon used to instantiate the templates in table 1 in order to generate the corpus.Syntactic Category Visual Category WordsNouns Objects, People chair, bag, telescope, someone, proper namesVerbs Actions pick up, put down, hold, move (transitive), look at, approach, leavePrepositions Spacial Relations with, left of, right of, onAdjectives Visual Properties yellow, greenProvided an interpretation and its corresponding formula composed of P predicates and V variables,framebalong with a collection of object detections, detection index, in each frame of a video ofilength T the model computes the score of the videosentence pair by finding the optimal detectionfor each participant in every frame. This is in essence the Viterbi algorithm, the MAP algorithm forframejHMMs, applied to finding optimal object detections for each participant, and the optimalvariableframekstate for each predicate HMM, in every frame. Each detection is scored by its confidencepredicatefrom the object detector, f and each object track is scored by a motion coherence metric g which5determines if the motion of the track agrees with the underlying optical flow. Each predicate,(cid:32) (cid:33)V T(cid:88) (cid:88)1 t tmax F (b ) + g(b ) +, b t1 t−1 ii i vi ...i v v1 V v=1 t=2k ...k1 P (1)(cid:32) (cid:33)P T T(cid:88) (cid:88) (cid:88)θ (1) θ (2)t t−1 tp plog h (k , b , b ) + log a (k , k )p pp p pt ti iθ (1) θ (2)p pp=1 t=1 t=2 hp, is scored by the probability of observing a particular detection in a given state , and by thepaprobability of transitioning between states . The structure of the formula and the fact that multiplep θpredicates often refer to the same variables is recorded by , a mapping between predicates and theirarguments. The model computes the MAP estimate as:for sentences which have words that refer to at most two tracks (i.e. transitive verbs or binarypredicates) but is trivially extended to arbitrary arities. Figure 3 provides a visual overview of themodel as a cross-product of tracker models and word models.Our model extends the approach of in several ways. First, we depart from the dependency basedrepresentation used in that work, and recast the model to encode first order logic formulas. Notethat some complex first order logic formulas cannot be directly encoded in the model and requireadditional inference steps. This extension enables us to represent ambiguities in which a givensentence has multiple logical interpretations for the same syntactic parse.Second, we introduce several model components which are not specific to disambiguation, but arerequired to encode linguistic constructions that are present in our corpus and could not be handled bythe model of. These new components are the predicate “not equal”, disjunction, and conjunction. Thekey addition among these components is support for the new predicate “not equal”, which enforcesthat two tracks, i.e. objects, are distinct from each other. For example, in the sentence “Claire and Billmoved a chair” one would want to ensure that the two movers are distinct entities. In earlier work,this was not required because the sentences tested in that work were designed to distinguish objectsbased on constraints rather than identity. In other words, there might have been two different peoplebut they were distinguished in the sentence by their actions or appearance. To faithfully recognizethat two actors are moving the chair in the earlier example, we must ensure that they are disjointfrom each other. In order to do this we create a new HMM for this predicate, which assigns lowprobability to tracks that heavily overlap, forcing the model to fit two different actors in the previousexample. By combining the new first order logic based semantic representation in lieu of a syntacticrepresentation with a more expressive model, we can encode the sentence interpretations required toperform the disambiguation task.Figure 3(left) shows an example of two different interpretations of the above discussed sentence“Claire and Bill moved a chair”. Object trackers, which correspond to variables in the first orderlogic representation of the sentence interpretation, are shown in red. Predicates which constrain thepossible bindings of the trackers, corresponding to predicates in the representation of the sentence, areshown in blue. Links represent the argument structure of the first order logic formula, and determinethe cross products that are taken between the predicate HMMs and tracker lattices in order to formthe joint model which recognizes the entire interpretation in a video.The resulting model provides a single unified formalism for representing all the ambiguities in table2. Moreover, this approach can be tuned to different levels of specificity. We can create models thatare specific to one interpretation of a sentence or that are generic, and accept multiple interpretationsby eliding constraints that are not com-mon between the different interpretations. This allows the model, like humans, to defer deciding on aparticular interpretation or to infer that multiple interpretation of the sentence are plausible.7 Experimental ResultsWe tested the performance of the model described in the previous section on the LAVA datasetpresented in section 5. Each video in the dataset was pre-processed with object detectors for humans,bags, chairs, and telescopes. We employed a mixture of CNN and DPM detectors, trained on heldout sections of our corpus. For each object class we generated proposals from both the CNN and6the DPM detectors, and trained a scoring function to map both results into the same space. Thescoring function consisted of a sigmoid over the confidence of the detectors trained on the same heldout portion of the training set. As none of the disambiguation examples discussed here rely on thespecific identity of the actors, we did not detect their identity. Instead, any sentence which containsnames was automatically converted to one which contains arbitrary “person” labels.The sentences in our corpus have either two or three interpretations. Each interpretation has one ormore associated videos where the scene was shot from a different angle, carried out either by differentactors, with different objects, or in different directions of motion. For each sentence-video pair, weperformed a 1-out-of-2 or 1-out-of-3 classification task to determine which of the interpretations ofthe corresponding sentence best fits that video. Overall chance performance on our dataset is 49.04%,slightly lower than 50% due to the 1out-of-3 classification examples.The model presented here achieved an accuracy of 75.36% over the entire corpus averaged acrossall error categories. This demonstrates that the model is largely capable of capturing the underlyingtask and that similar compositional crossmodal models may do the same. For each of the 3 majorambiguity classes we had an accuracy of 84.26% for syntactic ambiguities, 72.28% for semanticambiguities, and 64.44% for discourse ambiguities.The most significant source of model failures are poor object detections. Objects are often rotatedand presented at angles that are difficult to recognize. Certain object classes like the telescopeare much more difficult to recognize due to their small size and the fact that hands tend to largelyocclude them. This accounts for the degraded performance of the semantic ambiguities relative to thesyntactic ambiguities, as many more semantic ambiguities involved the telescope. Object detectorperformance is similarly responsible for the lower performance of the discourse ambiguities whichrelied much more on the accuracy of the person detector as many sentences involve only peopleinteracting with each other without any additional objects. This degrades performance by removing ahelpful constraint for inference, according to which people tend to be close to the objects they aremanipulating. In addition, these sentences introduced more visual uncertainty as they often involvedthree actors.The remaining errors are due to the event models. HMMs can fixate on short sequences of eventswhich seem as if they are part of an action, but in fact are just noise or the prefix of another action.Ideally, one would want an event model which has a global view of the action, if an object went upfrom the beginning to the end of the video while a person was holding it, it’s likely that the object wasbeing picked up. The event models used here cannot enforce this constraint, they merely assert thatthe object was moving up for some number of frames; an event which can happen due to noise in theobject detectors. Enforcing such local constraints instead of the global constraint of the motion of theobject over the video makes joint tracking and event recognition tractable in the framework presentedhere but can lead to errors. Finding models which strike a better balance between local informationand global constraints while maintaining tractable inference remains an area of future work.8 ConclusionWe present a novel framework for studying ambiguous utterances expressed in a visual context. Inparticular, we formulate a new task for resolving structural ambiguities using visual signal. This is afundamental task for humans, involving complex cognitive processing, and is a key challenge forlanguage acquisition during childhood. We release a multimodal corpus that enables to address thistask, as well as support further investigation of ambiguity related phenomena in visually groundedlanguage processing. Finally, wepresent a unified approach for resolving ambiguous descriptions of videos, achieving good perfor-mance on our corpus.While our current investigation focuses on structural inference, we intend to extend this line of workto learning scenarios, in which the agent has to deduce the meaning of words and sentences fromstructurally ambiguous input. Furthermore, our framework can be beneficial for image and videoretrieval applications in which the query is expressed in natural language. Given an ambiguous query,our approach will enable matching and clustering the retrieved results according to the different queryinterpretations. 7"
P030,"BladeDISC++: Enhancing Memory Usage ThroughSymbolic Shape AnalysisAbstractThe increasing prevalence of dynamic characteristics in modern deep learning taskshas led to the growing importance of dynamic shape compilers. These compilersare designed to create effective kernels for dynamic shape graphs, which have astable structure but uncertain tensor shapes. However, memory optimization, whichis vital in the era of large models, has not been thoroughly investigated for dynamicshape graphs. The core issue lies in the absence of specific tensor shapes, which aregenerally required by existing methods like operation scheduling and rematerializa-tion. To overcome this issue, we present operation scheduling and rematerializationstrategies that utilize symbolic shapes, implemented in BladeDISC++. Furthermore,given that rematerialization decisions cannot be determined at compile time alonedue to unknown tensor shapes, BladeDISC++ uses a hybrid approach combiningcompilation and runtime to address shape changes effectively. Our findings demon-strate that BladeDISC++ significantly reduces memory consumption for dynamicshape graphs, achieving levels similar to those of optimizations with precise shapes.This advancement facilitates the broader use of dynamic shape compilers.1 IntroductionDynamic shape compilers are becoming more and more necessary due to their ability to optimizedeep learning tasks that have dynamic attributes. While advancements in kernel generation have beenmade by systems like TorchInductor and Modular, memory optimization remains a less-explored area.Traditional methods like operation scheduling and rematerialization, which encompass recomputationand offloading, depend on precise tensor shapes to evaluate the memory impact of operations orsubgraphs, and consequently make optimization choices during compilation. However, these methodsbecome impractical when shape values are not available.BladeDISC++, which is based on the dynamic shape compiler BladeDISC, uses symbolic shapesto address these challenges. With symbolic shapes, BladeDISC++ is capable of comparing thememory effects of different operation sequences, and identifying the ideal scheduling order. Forrematerialization, symbolic shapes are used to identify the optimal recomputation subgraph at compiletime, and assist in making final rematerialization decisions during runtime.Our experiments reveal that BladeDISC++ can efficiently reduce memory usage during trainingwith dynamic shape graphs when compared to BladeDISC. Furthermore, BladeDISC++ achievesmemory consumption similar to static shape training while eliminating the overhead associated withrecompilation and tensor padding.2 Memory optimizations based on symbolic shapesAs shown in Figure 1, BladeDISC++ starts with a dynamic shape computation graph, and proceeds byconducting a symbolic shape analysis to construct a global symbolic shape graph. This graph detailsthe mathematical connections between the shape symbols, which will be discussed in section 2.1.Following this, the symbolic shape graph, along with the computation graph, is optimized through.steps that include operation fusion, operation scheduling, and rematerialization. These steps areaimed at memory usage reduction.As previous work on BladeDISC has addressed operation fusion, this paper focuses on operationscheduling, which will be discussed in section 2.2, and rematerialization, which will be discussedin section 2.3. Using the symbolic shape graph instead of exact tensor shapes, BladeDISC++ canstill compare the memory usage of different operation sequences and determine the benefit ofrecomputation subgraphs. Moreover, because the memory needs of a dynamic shape graph canfluctuate between different runs, it is not practical to base rematerialization decisions, such as howmuch memory to free, solely on compile time. Consequently, BladeDISC++ investigates all possiblerematerialization options, searches for the corresponding regeneration subgraphs, and makes finalrematerialization decisions during runtime.[width=0.8]placeholder.png figureMemory optimizations based on symbolic shapes in BladeDISC++2.1 Symbolic shape graph analysisBladeDISC++ systematically analyzes and obtains shape information from the semantics of eachoperation within the dynamic shape computation graph. Following this, it establishes a globalsymbolic shape graph. This graph is designed to show the mathematical relationships between shapedimensions through shape value extraction and input-output shape inference.func . func @main (% arg0 : tensor <? ,[ @S0 ] > , % arg1 : tensor <12 x11008 >) {%1 = broadcast (% arg1 ) -> tensor <4096 x ? , [ @C4096 , @S0 ] >%2 = d yna mi c_r eshape (% arg0 , % new_shape ) -> tensor <? x12 ,[ @S1 , @C12 ] >// The last consumer of %2%3 = dot (%2 , % arg1 ) -> tensor <? x11008 , [ @S1 , @C11008 ] >// The last consumer of %3%4 = reduce (%3) -> tensor <? , [ @S1 ] >%1084 = broadcast (%4) -> tensor <11008 x ? , [ @C11008 , @S1 ] >%1085 = broadcast (% arg0 ) -> tensor <1024 x ? , [ @C1024 , @S0 ] >}func . func @ s y m b o l i c _ s h a p e _ g r a p h () {SymbolicDim @S0SymbolicDim @S1@S0 = Mul @C12 , @S1}Listing 1: Example of a dynamic shape graph and its symbolic shape graphAs shown in Listing 1, BladeDISC++ uses a SymbolicDim operation to represent a symbolic value.This value is linked to a dimension of a tensor shape in the dynamic shape graph as an attribute, forexample, tensor<?x?, [@S0, @S1]>. The equation @S0 = 12 * @S1, for instance, is derived from aDynamicReshapeOp. It means the input and output tensors have an equivalent number of elements.The comparison of tensor memory sizes is vital for both operation scheduling and rematerialization.BladeDISC++ uses SymbolicExpr to show mathematical expressions of symbolic dimensions. Thisallows for comparisons using a best-effort approach. For example, the element count of tensors2.2 Operation schedulingOperation scheduling aims to discover a memory-efficient sequence of operations from the initialcomputation graph. Existing scheduling algorithms typically traverse the graph and select an operationfrom a ReadySet, which includes operations whose predecessors have been scheduled, at each step.The selection is mainly based on a comparison of the memory impact of the different operations,which is determined by calculating the difference between the memory freed and the memory allocatedafter scheduling a particular operation. BladeDISC++ employs a similar strategy, emphasizing thecalculation and comparison of memory impact among different operations when exact tensor shapesare unavailable in dynamic shape graphs. In BladeDISC++, the memory impact of each operation2is calculated using symbolic shapes, resulting in a SymbolicExpr. These SymbolicExprs are thencompared using the symbolic shape graph.In Listing 1, the DynamicReshapeOp and DotOp are present in the ReadySet at a particular step.DotOp, being the last consumer ofWhen comparing memory impact SymbolicExprs is not possible, we use a standard approach:selecting the operation that results in shorter overall tensor lifespans based on the graph’s structure.2.3 RematerializationTraditional rematerialization methods use algorithms to decide which tensors to release early to reducememory pressure, and how to conduct the following regeneration via reloading or recomputation.These methods also search for optimal recomputation subgraphs, evaluating their memory effects.Tensor rematerialization can negatively impact end-to-end performance, so it should only be usedwhen the graph’s execution could exceed memory limits. However, dynamic shape graphs, withuncertain tensor shapes, may show varied peak memory use between different runs. Some runs maynot need rematerialization as they remain within memory limits, whereas others may. Therefore, it isimpractical to make decisions solely at compilation time. Also, the absence of exact shapes presentschallenges in evaluating the memory effects of potential recomputation subgraphs.To address these challenges, BladeDISC++ uses a combined compilation-runtime approach based onsymbolic shapes to better manage shape variations during graph runs. At compile time, it explores allpossible rematerialization candidates and identifies the regeneration subgraphs associated with them.These subgraphs are incorporated into the original computation graph as separate execution paths.Final choices regarding which tensor to release and the related regeneration method are made duringruntime.During compilation, as shown in Figure 1, BladeDISC++ adds a Remat::EvictOp after each operation.This checks if active tensors at that point need to be released to lower memory pressure. Regenerationsubgraphs, including reload and recomputation, are created for each potential tensor. While reloadingonly involves a host-to-device instruction and has no impact on memory, finding recomputationsubgraphs needs thorough evaluation as poor choices can increase peak memory consumption.BladeDISC++ uses a standard search approach, but assesses the memory impact of subgraphs usingSymbolicExpr.Taking the recomputation subgraph searching forFollowing this, BladeDISC++ inserts Remat::RegenerateOps, with corresponding regeneration sub-graphs for both reload and recompute. These are inserted before each potential tensor’s subsequentconsumers. The Remat::RegenerateOp checks if a tensor has been released, and which regenerationmethod is being used.During runtime, BladeDISC++ monitors memory usage throughout kernel execution. Whenever anEvictOp is triggered, BladeDISC++ checks the present memory usage. When the memory limit isabout to be exceeded, it performs a real-time analysis of all potential tensors offered by the EvictOp.Final decisions about which tensor needs to be released, and the regeneration method, are determinedby taking memory savings and end-to-end performance into account, following a similar approach asdetailed in. Subsequent Remat::RegenerateOps then check these choices to decide which regenerationsubgraphs to trigger.3 EvaluationFor our evaluation, we performed experiments on the supervised fine-tuning of Llama-2-1b, which isa customized model from the official Llama-2-7b with only the number of hidden layers decreasedfrom 32 to 4. This was done on an Alibaba Cloud instance, with 40GB of GPU RAM. We usedthe CodeAlpaca-20K dataset, which contains text samples with lengths from about 100 to 3000characters. During each training cycle, a fixed amount of randomly selected samples are put into abatch. This leads to variations in batch shapes between cycles.To evaluate the effectiveness of BladeDISC++, we compared memory usage and end-to-end per-formance of dynamic shape training with BladeDISC++ against both dynamic and static shape3training with BladeDISC. For static shape training, following common methods, input sequences arepadded to the closest power of 2 in length. This balances redundant computation and compilationoverhead. Additionally, we set the largest bucket size to be equal to the longest sequence length inthe dataset. This was done to investigate whether comparable memory optimization can be achievedusing symbolic shapes instead of exact shapes.The experimental results show that BladeDISC++ is able to reduce peak memory consumptionduring dynamic shape training. BladeDISC++ also demonstrated memory consumption similarto static shape training, while improving end-to-end performance by eliminating the overheads ofrecompilation and input bucketing.Table 1: Training throughput of Llama-2-1b on CodeAlpaca-20K(tokens/second)Batchsize 14 16 18BladeDISC(dynamic shape training) 5662.34(38.20 GiB) OOM OOMBladeDISC(static shape training) 5242.02(35.75 GiB) 5429.38(37.71 GiB) 5103.31(38.92 GiB)BladeDISC++ 5749.20(35.76 GiB) 6078.71(37.89 GiB) 5738.79(39.18 GiB)4 ConclusionThis study presents our practical experience in optimizing memory for dynamic shape graphs. Wehave introduced operation scheduling and rematerialization strategies that use symbolic shapes,implemented in BladeDISC++. Evaluations demonstrate that BladeDISC++ effectively decreasesmemory usage for dynamic shape training and can match the memory optimization results of staticshape training. To the best of our knowledge, this work is the first attempt in this area. We hopeit will support the compiler community in handling dynamic shape tasks, and increase the use ofdynamic shape compilers. 4"
P031,"Explainable Identification of Hate Speech towardsIslam using Graph Neural NetworksAbstractIslamophobic language on online platforms fosters intolerance, making detectionand elimination crucial for promoting harmony. Traditional hate speech detectionmodels rely on NLP techniques like tokenization, part-of-speech tagging, andencoder-decoder models. However, Graph Neural Networks (GNNs), with theirability to utilize relationships between data points, offer more effective detectionand greater explainability. In this work, speeches are represented as nodes andconnect them with edges based on their context and similarity to develop the graph.A novel paradigm using GNNs to identify and explain hate speech towards Islam isintroduced. The model leverages GNNs to understand the context and patterns ofhate speech by connecting texts via pretrained NLP-generated word embeddings,achieving state-of-the-art performance and enhancing detection accuracy while pro-viding valuable explanations. This highlights the potential of GNNs in combatingonline hate speech and fostering a safer, more inclusive online environment.1 IntroductionDetecting and eliminating hate speech on social media platforms is of utmost importance for thepromotion of harmony and tranquility in society. The escalating presence of hate speech specificallytargeting Islam or Muslim communities on online discussion platforms is a growing concern. Thisform of hate speech not only fosters an environment of intolerance and hostility but can also havesevere psychological impacts on individuals and communities, leading to real-world violence anddiscrimination.To address this issue, researchers have increasingly turned to advanced technologies; using text-processing approaches in AI. Natural Language Processing (NLP) techniques are frequently employedfor hate speech detection, with some offering severity assessment of hate speech. These methodsutilize sophisticated algorithms to analyse vast amounts of textual data, identifying patterns andfeatures indicative of hate speech. For instance, deep learning models, like recurrent neural networks(RNNs), can learn complex representations of text data, enabling them to detect subtle and context-dependent instances of hate speech. Modern NLP techniques, on the other hand, can enhance thesemodels by providing richer linguistic insights. Tokenization, part-of-speech tagging, and namedentity recognition are just a few NLP techniques that help in breaking down and understanding thetext’s structure and meaning. Moreover, the integration of latest NLP model and transformers, likeBERT and GPT, has significantly improved the ability of models to understand context, sarcasm, andimplicit hate speech, which are often challenging to detect. Another interesting approach is to usehuman-centric perspectives of AI using some benchmark dataset.Researchers have tried to employ GNNs in hate speech classification, but still needs more focuson this area. Despite their potential, GNNs have not been actively employed for the purpose ofinterpretable identification of hate speech, particularly in Islamic contexts. Islamophobic contentoften exhibits close word choices and hate speakers from the same community, which GNNs canleverage to reveal and explain patterns, alongside impressive classification scores.A novel approach employing graph neural networks for the identification and explication of hatespeech directed at Islam (XG-HSI) is introduced. The dataset is pre-processed to focus on Islamiccontexts, utilize pretrained NLP models for word embeddings, establish connections between texts,and employ a series of graph encoders for hate speech target identification, which achieves state-of-the-art performance.2 BackgroundGraph Neural Networks (GNNs) are powerful neural networks designed for processing non-Euclideandata organized in complex, interconnected graphs. Using their ability to utilize relations betweendifferent data points, GNNs have shown tremendous promise in text classification and detectiontasks. GNNs have the ability to enhance hate speech detection on social media by modeling complexrelationships between users and content, capturing contextual information from interactions. Theypropagate information across the network, identifying coordinated and evolving hate speech patterns.We also present a case study in Section 5 to illustrate how incorporating related information enhancesthe process.A general bag of words-based approach to create graphs, without LLMs is adopted. By integratingwith pretrained NLP models, GNNs leverage contextual word embeddings to better understand thesubtleties of hate speech. This combined approach improves the accuracy, context-awareness, andadaptability of detection systems, making them more effective in identifying hate speech directed atIslam and potentially generalizing to other targeted groups.3 Methodology3.1 NotationsLet a graph G = (V, E, X), where V represents nodes, E denotes edges. We also define N and M as the˘numbers of nodes and edges, respectively. Each node v is associated with a feature xi 2208 RF , and˘ ˘the node feature matrix for the entire graph is denoted as X 2208 RN 00d7F , where F represents thefeature vector length. In our approach, each content denotes a node, contextual similarity betweentwo nodes is denoted by an edge and word embeddings are node features of the graph. The taskinvolves a node classification task to detect hate speech and Islamophobic content.3.2 Data Pre-ProcessingInitially, the dataset was filtered to focus on hate speech targeting Islam. Next, pretrained NLP modelsis applied to the text to obtain word embeddings X as node features for all nodes V. Edges E aredetermined using cosine similarity between embeddings with a threshold of 0.725. Subsequently,GNN is applied for the classification task.3.3 Graph Encoder ˘After data pre-processing, every data point x 2282 X undergoes a series of transformations to getoutput p. First, it is processed by a linear layer producing x1 (Equation 1).x1 = W x + b (1)Subsequently, x1 is passed into two initial graph encoders to aggregate neighborhood information,feature extraction, and yield x2, x3 utilizing G and concatenated to x23 (Equation 2,3, 4). Here inEquation 2, we aggregate features from a node’s local neighborhood, to learn different characteristics.In Equation 3 and 4, we use a semi-supervised learning on graph-structured data, employing anefficient variant of convolutional neural networks that operate directly on graphs.x2 = W 1x1 + W 2 · mean x1 (2)j∈N(i)ˆx3 = W 1x1 + Ax1 (3)2x23 = concat(x2, x3) (4)Here, N is the set of neighbouring nodes. Following this, x23 is passed through another graph layeremploying attention-based feature extraction, utilizing masked self-attentional layers to implicitlyassign different weights to nodes in a neighbourhood, producing x4 (Equation 5 and 6).(cid:88)x4 = α Θx23 + α Θx23 (5)i,i i i,j jj∈N(i)Texp(LeakyReLU (a [Θx23 ||Θx23 ]))i jα = (6)(cid:80)i,j Texp(LeakyReLU (a [Θx23 ||Θx23 ]))i kk∈N(i)˘ ˘Here, 03b8 refers to trainable model weights. 03b1 is the attention value, calculated by the equationmentioned.Finally, x4 is passed through a final linear layer to obtain logits pl, which are then subjected to asoftmax operation to derive probabilities p (Equation 7 amd 8).xc = concat(x1, x4); pl = W xc + b (7)p = sof tmax(pl) (8)3.4 Loss FunctionCross Entropy loss is designed to minimize the difference between the predicted probabilities andtrue values, as follows: n(cid:88)lce = − (p log(o(p )) + (1 − p )log(1 − o(p ))) (9)i i i ii=13.5 Graph ExplanationGNNExplainer is used to derive explanations from the graph encoder network for interpreting theresults and find underlying relations and causation. It works by taking a trained GNN model andits predictions as input, and returns explanations in the form of compact subgraph structures andsubsets of influential node features. This model-agnostic approach can explain predictions of anyGNN-based model on various graph-based machine learning tasks, including node classification,link prediction, and graph classification. GNNExplainer formulates explanations as rich subgraphsof the input graph, maximizing mutual information with the GNN’s predictions. It achieves thisby employing a mean field variational approximation to learn real-valued graph masks that selectimportant subgraphs and feature masks that highlight crucial node features. Through this process,GNNExplainer offers insights into the underlying reasoning of GNN predictions, enhancing modelinterpretability and facilitating error analysis.4 Experiments4.1 Experimental SetupDataset. HateXplain, a benchmark hate speech dataset designed for addressing bias and interpretabil-ity is used. The dataset has hate speech targets labelled. This labelling is used to collect onlyMuslim-focused sentences and created a subset to work on this project. A 6:2:2 train, validation andtest split is used.Baselines. The baseline models are: CNN-GRU, BiRNN, BiRNN-HateXplain, BERT, BERT-HateXplain. Mentioned HateXplain-based models are fine-tuned on HateXplain dataset.3Implementation Details. Hugging Face transformers library is used to get embeddings from pre-trained BERT (bert-base-uncased) and BiRNN. The model is trained for 200 epochs with a learningrate of 0.001, using Adam optimizer. The experimental results in Table 1 show that our model achievesremarkable performance comparing to benchmarks with explaining occurring phenomenons.Weutilized a single layer for each type of GNN, with a maximum tokenization length of 512 in thetokenizer and length of BERT embeddings (F ) set to 128.4.2 Experimental ResultsTable 1 shows the performance of various models in detecting hate speech, highlighting accuracy andMacro F1 metrics. Traditional models like CNN-GRU and BiRNN show lower performance, withBiRNN-HateXplain offering slight improvements. BERT-based models perform better, particularlyBERT-HateXplain. However, our proposed models, XG-HSI-BiRNN and XG-HSI-BERT, signifi-cantly outperform all others, with XG-HSI-BERT achieving the highest accuracy (0.741) and MacroF1 (0.747). These results demonstrate the superior effectiveness of our dual GNN approach in hatespeech detection. ˘Table 1: Experimental Results (2191)Model Accuracy Macro F1CNN-GRU 0.628 0.604BiRNN 0.591 0.578BiRNN-HateXplain 0.612 0.621BERT 0.692 0.671BERT-HateXplain 0.693 0.681XG-HSI-BiRNN (Ours) 0.742 0.737XG-HSI-BERT (Ours) 0.751 0.7475 Graph Explanation Case StudyFor a given post, ""How is all that awesome Muslim diversity going for you native germans? Youhave allowed this yourselves. If you do not stand and fight against this. You get what you asked forwhat you deserve!"", the predicted classification was offensive towards Islam. As per the explainer,the neighbouring and self-tokens helped to classify this as offensive to Muslims are fight, Muslimdiversity, brooks, rish, donald, syrian, schultz, typed. The text’s association of ""Muslim diversity""with potential blame and its confrontational tone in phrases like ""stand and fight against this,""combined with neighbouring tokens like syrians, brooks, syrians denoted negative sentiment.6 DiscussionThis study not only addresses the immediate challenge of identifying and explaining hate speechdirected at Islam but also recognizes the broader impact of hate speech propagation on onlineplatforms. The proliferation of Islamophobic language fosters intolerance, division, and hostilitywithin communities, perpetuating harmful stereotypes and prejudices. By leveraging GNNs in ourXG-HSI framework, we not only detect hate speech but also provide explanations for its occurrence,shedding light on the underlying factors driving such behaviour. GNNs excel in capturing complexrelationships and patterns within data, enabling them to effectively identify instances of hate speechand elucidate the contextual nuances surrounding them. By leveraging the inherent structure of socialnetworks and textual data, our approach offers a comprehensive understanding of how hate speechpropagates in online discourse.In future research, exploring the integration of multimodal data sources, such as images and videos,could enhance the robustness of hate speech detection models, particularly in detecting nuancedforms of Islamophobic content. Additionally, investigating the dynamic nature of online communitiesand incorporating temporal aspects into GNN architectures could provide deeper insights into theevolution of hate speech propagation and enable more proactive interventions to counter its spread.47 ConclusionIdentifying and addressing Islamophobic hatred on social media is crucial for achieving harmonyand peace. This research presents a novel method using GNNs to detect hate speech towards Islam.Empirical findings demonstrate that our model achieves exceptional performance, significantlyoutperforming all others, with XG-HSI-BERT achieving the highest accuracy (0.741) and Macro F1(0.747). Explainability aspect of this approach is also very promising, as it provides insights intoboth correlations and causation. This further highlights the potential of GNNs in combating onlinehate speech and fostering a safer, more inclusive online environment.LimitationsThe limitations include the use of only one dataset, which, while sufficient for this initial exploration,should be expanded upon in future research to validate and extend our findings. Additionally, whileGraph Neural Networks (GNNs) are known to be computationally intensive, especially with large-scale datasets, the relatively limited number of hate speech keywords suggests that GNNs may stillbe highly effective. Furthermore, more efficient GNN training methods are now available, whichaddress some of the computational challenges in future applications.Ethical ImplicationsOur work on using GNNs to detect hate speech targeting Islam carries significant ethical responsibili-ties. We focus on minimizing biases in the model to ensure fair treatment of all groups, emphasizingthe need for transparency in how the model arrives at its decisions. By using interpretable GNNmethods, we strive to provide clear explanations for the model’s classifications, allowing for greateraccountability. We also acknowledge the potential risks of misuse and take steps to prevent these,adhering to ethical guidelines that respect privacy and avoid unjust censorship.Societal ImplicationsThe societal impact lies in its potential to create a safer online environment by effectively identifyingand mitigating Islamophobic content. By enhancing the detection accuracy and providing clearexplanations for the identified hate speech, our model contributes to fostering more inclusive andrespectful online communities. Additionally, our work highlights the importance of combating digitalhate speech, which can lead to real-world harm. We aim to empower platforms and policymakerswith tools that uphold freedom of expression while curbing harmful rhetoric, thus promoting socialharmony and understanding.Potential RisksThe application of our model presents several risks. One major concern is the potential for modelmisclassification, which could lead to false positives or negatives, impacting users unfairly. Addition-ally, there is a risk of over-reliance on automated systems, which might not capture nuanced contextsand could inadvertently suppress legitimate speech. Annotation errors can also induce bias, but aswe used a previously peer-reviewed benchmark dataset, we hope those type of concerns are alreadyaddressed.AcknowledgementsSincere gratitude to the Computational Intelligence and Operations Laboratory (CIOL) for all theirsupport. This work was presented at the Muslims in ML workshop (non-archival) at NeurIPS 2023,and thanks for their reviews, support, and the opportunity to present. Appreciation to all the reviewersfor their valuable suggestions to improve the work.5"
P033,"AMR Parsing using Stack-LSTMsAbstractWe present a transition-based AMR parser that directly generates AMR parses fromplain text. We use Stack-LSTMs to represent our parser state and make decisionsgreedily. In our experiments, we show that our parser achieves very competitivescores on English using only AMR training data. Adding additional information,such as POS tags and dependency trees, improves the results further.1 IntroductionTransition-based algorithms for natural language parsing are formulated as a series of decisions thatread words from a buffer and incrementally combine them to form syntactic structures in a stack.Apart from dependency parsing, these models, also known as shift-reduce algorithms, have beensuccessfully applied to tasks like phrase-structure parsing, named entity recognition, CCG parsing,joint syntactic and semantic parsing and even abstract- meaning representation parsing.AMR parsing requires solving several natural language processing tasks; mainly named entityrecognition, word sense disambiguation and joint syntactic and semantic role labeling. Given thedifficulty of building an end-to-end system, most prior work is based on pipelines or heavily dependenton precalculated features.Inspired by we present a shift- reduce algorithm that produces AMR graphs directly from plain text.presented transition-based tree-to-graph transducers that traverse a dependency tree and transformsit to an AMR graph. input is a sentence and it is therefore more similar (with a different parsingalgorithm) to our approach, but their parser relies on external tools, such as dependency parsing,semantic role labeling or named entity recognition.The input of our parser is plain text sentences and, through rich word representations, it predictsall actions (in a single algorithm) needed to generate an AMR graph representation for an inputsentence; it handles the detection and annotation of named entities, word sense disambiguation andit makes connections between the nodes detected towards building a predicate argument structure.Even though the system that runs with just words is very competitive, we further improve the resultsincorporating POS tags and dependency trees into our model.Stack-LSTMs have proven to be useful in tasks related to syntactic and semantic parsing and namedentity recognition. In this paper, we demonstrate that they can be effectively used for AMR parsingas well.2 Parsing AlgorithmOur parsing algorithm makes use of a STACK (that stores AMR nodes and/or words) and a BUFFERthat contains the words that have yet to be processed. The parsing algorithm is inspired from thesemantic actions presented by , the transition-based NER algorithm by and the arc-standard algorithm.As in the buffer starts with the root symbol at the end of the sequence. Figure 2 shows a runningexample. The transition inventory is the following:• SHIFT: pops the front of the BUFFER and push it to the STACK.• CONFIRM: calls a subroutine that predicts the AMR node corresponding to the top of theSTACK. It then pops the word from the STACK and pushes the AMR node to the STACK.An example is the prediction of a propbank sense: From occurred to occur-01.• REDUCE: pops the top of the STACK. It occurs when the word/node at the top of the stackis complete (no more actions can be applied to it). Note that it can also be applied to wordsthat do not appear in the final output graph, and thus they are directly discarded.• MERGE: pops the two nodes at the top of the STACK and then it merges them, it thenpushes the resulting node to the top of STACK. Note that this can be applied recursively.This action serves to get multiword named entities (e.g. New York City).ENTITY(label): labels the node at the top of the STACK with an entity label. This action• serves to label named entities, such as New York City or Madrid and it is normally run afterMERGE when it is a multi-word named entity, or after SHIFT if it is a single-word namedentity.• DEPENDENT(label,node): creates a new node in the AMR graph that is dependent on thenode at the top of the STACK. An example is the introduction of a negative polarity to agiven node: From illegal to (legal, polarity -).• LA(label) and RA(label): create a left/right arc with the top two nodes at the top of theSTACK. They keep both the head and the dependent in the stack to allow reentrancies (multipleincoming edges). The head is now a composition of the head and the dependent. They are enrichedwith the AMR label.• SWAP: pops the two top items at the top of the STACK, pushes the second node to the frontof the BUFFER, and pushes the first one back into the STACK. This action allows non-projective arcs as in but it also helps to introduce reentrancies. At oracle time, SWAP isproduced when the word at the top of the stack is blocking actions that may happen betweenthe second element at the top of the stack and any of the words in the buffer.Figure 1 shows the parser actions and the effect on the parser state (contents of the stack, buffer) andhow the graph is changed after applying the actions.We implemented an oracle that produces the sequence of actions that leads to the gold (or close togold) AMR graph. In order to map words in the sentences to nodes in the AMR graph we need toalign them. We use the JAMR aligner provided by. It is important to mention that even though thealigner is quite accurate, it is not perfect, producing a F1 score of around 0.90. This means that mostsentences have at least one alignment error which implies that our oracle is not capable of perfectlyreproducing all AMR graphs. This has a direct impact on the accuracy of the parser described in thenext section since it is trained on sequences of actions that are not perfect. The oracle achieves 0.895F1 Smatch score when it is run on the development set of the LDC2014T12.The algorithm allows a set of different constraints that varies from the basic ones (not allowingimpossible actions such as SHIFT when the buffer is empty or not generating arcs when the wordshave not yet been CONFIRMed and thus transformed to nodes) to more complicated ones based onthe propbank candidates and number of arguments. We choose to constrain the parser to the basicones and let it learn the more complicated ones.(r / recommend-01 :ARG1 (a / advocate-01 :ARG1 (i / it) :manner (v / vigorous)))3 Parsing ModelIn this section, we revisit Stack-LSTMs, our parsing model and our word representations.3.1 Stack-LSTMsThe stack LSTM is an augmented LSTM that allows adding new inputs in the same way as LSTMsbut it also provides a POP operation that moves a pointer to the previous element. The output vectorof the LSTM will consider the stack pointer instead of the rightmost position of the sequence.2t t t + 1 t + 1Stack Buffer Action Stack Buffer Graphu, S B SHIFT u, S B –u, S B CONFIRM n, S B –u, S B REDUCE S B –u, v, S B MERGE (u, v), S B –u, S B ENTITY(l) (u : l), S B –→u, S B DEPENDENT(r, d) u, S B r d→u, v, S B RA(r) u, v, S B r v←u, v, S B LA(r) u, v, S B r vu, v, S B SWAP u, S v, B –Table 1: Parser transitions indicating the action applied to the stack and buffer and the resulting state.ACTION STACK BUFFERINIT It, should, be, vigorously, advocated, RSHIFT it should, be, vigorously, advocated, RCONFIRM it should, be, vigorously, advocated, RSHIFT should, it be, vigorously, advocated, RCONFIRM recommend-01, it be, vigorously, advocated, , RSWAP recommend-01 it, be, vigorously, advocated, RSHIFT it, recommend-01 be, vigorously, advocated, RREDUCE recommend-01 be, vigorously, advocated, RSHIFT be, it, recommend-01 vigorously, advocated, RREDUCE it, recommend-01 vigorously, advocated, RSHIFT vigorously, it, recommend-01 advocated, RCONFIRM vigorous, it, recommend-01 advocated, RSWAP vigorous, recommend-01 it, advocated, RSWAP vigorous recommend-01, it, advocated, RSHIFT vigorous recommend-01, advocated, RSHIFT vigorous, recommend-01 advocated, RSHIFT it, vigorous recommend-01, advocated, RCONFIRM advocate-01, it, recommend-01, vigorous RLA(ARG1) advocate-01, it, recommend-01, vigorous RSWAP advocate-01, recommend-01, vigorous it RSHIFT it, advocate-01, recommend-01, vigorous RREDUCE advocate-01, recommend-01, vigorous RRA(ARG1) advocate-01, recommend-01, vigorous RSWAP advocate-01, vigorous recommend-01, RSHIFT recommend01, advocate-01, vigorous RSHIFT R, recommend01, advocate-01, vigorousLA(root) R, recommend01, advocate-01, vigorousREDUCE recommend01, advocate-01, vigorousREDUCE advocate-01, vigorousREDUCE vigorousREDUCETable 2: Transition sequence for the sentence It should be vigorously advocated. R represents theroot symbol 33.2 Representing the State and Making Parsing DecisionsThe state of the algorithm presented in Section 2 is represented by the contents of the STACK,BUFFER and a list with the history of actions (which are encoded as Stack-LSTMs). All of thissforms the vector that represents the state which s calculated as follows:tt= max{0, W [s ; b ; a ] + d},st t tt ts b awhere W is a learned parameter matrix, d is a bias term and , , represent the output vector oft ttthe Stack-LSTMs at time t. sPredicting the Actions: Our model then uses the vector for each timestep t to compute thetprobability of the next action as:exp(g .s +q )) = ,z t zp(z|s (cid:80)t ′ ′exp(g .s +q )′ tz zz ∈Ag qwhere is a column vector representing the (output) embedding of the action z, and is a bias termz zfor action z. The set A represents the actions listed in Section 2. Note that due to parsing constraintsthe set of possible actions may vary. The total number of actions (in the LDC2014T12 dataset) is478; note that they include all possible labels (in the case of LA and RA ) and the different dependentnodes for the DEPENDENT action.Predicting the Nodes: When the model selects the action CONFIRM, the model needs to decide thesAMR node that corresponds to the word at the top of the STACK, by using , as follows:texp(g .s +q ) ,) = e t ep(e|s (cid:80)t exp(g .s +q )′′′ te ee ∈N gwhere N is the set of possible candidate nodes for the word at the top of the STACK. is a columneqvector representing the (output) embedding of the node e, and is a bias term for the node e. It iseimportant to mention that this implies finding a propbank sense or a lemma. For that, we rely entirelyon the AMR training set instead of using additional resources.Given that the system runs two softmax operations, one to predict the action to take and the secondone to predict the corresponding AMR node, and they both share LSTMs to make predictions, wesinclude an additional layer with a tanh nonlinearity after for each softmax.t3.3 Word RepresentationsWe use character-based representations of words using bidirectional LSTMs . They learn represen-tations for words that are orthographically similar. Note that they are updated with the updates tothe model. demonstrated that it is possible to achieve high results in syntactic parsing and namedentity recognition by just using character-based word representations (not even POS tags, in fact, insome cases the results with just character-based representations outperform those that used explicitPOS tags since they provide similar vectors for words with similar/same morphosyntactic tag); in thispaper we show a similar result given that both syntactic parsing and named-entity recognition play acentral role in AMR parsing.These are concatenated with pretrained word embeddings. We use a variant of the skip n-gram modelwith the LDC English Gigaword corpus (version 5). These embeddings encode the syntactic behaviorof the words .More formally, to represent each input token, we concatenate two vectors: a learned character-basedC LMwˆ wˆrepresentation ( ); and a fixed vector representation from a neural language model ( ). A linearmap (V) is applied to the resulting vector and passed through a component-wise ReLU,C LMmax{0, V [wˆ ; wˆ ] + b}.x = Cwwhere V is a learned parameter matrix, b is a bias term and is the character-based learnedLMwˆrepresentation for each word, is the pretrained word representation.3.4 POS Tagging and Dependency ParsingWe may include preprocessed POS tags or dependency parses to incorporate more information intoour model. For the POS tags we use the Stanford tagger while we use the Stack-LSTM parser trainedon the English CoNLL 2009 dataset to get the dependencies.4Model F1(Newswire) F1(ALL)(POS, DEP) 0.59 0.58(POS, DEP, NER) - 0.66(POS, DEP, NER) 0.62 -(POS, DEP, NER, SRL) - 0.61(POS, DEP, NER, SRL) - 0.64(POS, CCG) 0.66 -(POS, DEP, NER) 0.70 -(POS, DEP, NER, SRL) 0.71 0.66(LM, NER) - 0.61(Wordnet, LM, NER) - 0.66(POS, DEP, NER) 0.63 0.59(POS, DEP, NER, SRL) 0.70 0.66OUR PARSER (NO PRETRAINED-NO CHARS) 0.64 0.59OUR PARSER (NO PRETRAINED-WITH CHARS) 0.66 0.61OUR PARSER (WITH PRETRAINED-NO CHARS) 0.66 0.62OUR PARSER 0.68 0.63OUR PARSER (POS) 0.68 0.63OUR PARSER (POS, DEP) 0.69 0.64Table 3: AMR results on the LDC2014T12 dataset; Newsire section (left) and full (right). Rowslabeled with OUR-PARSER show our results. POS indicates that the system uses preprocessed POStags, DEP indicates that it uses preprocessed dependency trees, SRL indicates that it uses preprocessedsemantic roles, NER indicates that it uses preprocessed named entitites. LM indicates that it usesa LM trained on AMR data and WordNet indicates that it uses WordNet to predict the concepts.Systems marked with * are pipeline systems that require a dependency parse as input. (WITHPRETRAINED-NO CHARS) shows the results of our parser without character-based representations.(NO PRETRAINED-WITH CHARS) shows results without pretrained word embeddings. (NOPRETRAINED-NO CHARS) shows results without character-based representations and withoutpretrained word embeddings. The rest of our results include both pretrained embeddings and character-based representations.POS tags: The POS tags are preprocessed and a learned representation tag is concatenated with theword representations. This is the same setting as .Dependency Trees: We use them in the same way as POS tags by concatenating a learned representa-tion dep of the dependency label to the parent with the word representation. Additionally, we enrichsthe state representation , presented in Section 3.2. If the two words at the top of the STACK have at sdependency between them, is enriched with a learned representation that indicates that and thets sdirection; otherwise remains unchanged. is calculated as follows:t tt= max{0, W [s ; b ; a ; dep ] + d},st t t ttdepwhere is the learned vector that represents that there is an arc between the two top words at thettop of the stack.4 Experiments and ResultsWe use the LDC2014T12 dataset for our experiments. Table 1 shows results, including comparisonwith prior work that are also evaluated on the same dataset.Our model achieves 0.68 F1 in the newswire section of the test set just by using character-basedrepresentations of words and pretrained word embeddings. All prior work uses lemmatizers, POStaggers, dependency parsers, named entity recognizers and semantic role labelers that use additionaltraining data while we achieve competitive scores without that. reports 0.66 F1 in the full test byusing WordNet for concept identification, but their performance drops to 0.61 without WordNet. It isworth noting that we achieved 0.64 in the same test set without WordNet. without SRL (via Propbank)achieves only 0.63 in the newswire test set while we achieved 0.69 without SRL (and 0.68 withoutdependency trees). 5In order to see whether pretrained word embeddings and character-based embeddings are useful wecarried out an ablation study by showing the results of our parser with and without character-basedrepresentations (replaced by standard lookup table learned embeddings) and with and without pre-trained word embeddings. By looking at the results of the parser without character-based embeddingsbut with pretrained word embeddings we observe that the character- based representation of wordsare useful since they help to achieve 2 points better in the Newswire dataset and 1 point more in thefull test set. The parser with character-based embeddings but without pretrained word embeddings,the parser has more difficulty to learn and only achieves 0.61 in the full test set. Finally, the modelthat does not use neither character-based embeddings nor pretrained word embeddings is the worstachieving only 0.59 in the full test set, note that this model has no explicity way of getting anysyntactic information through the word embeddings nor a smart way to handle out of vocabularywords.All the systems marked with * require that the input is a dependency tree, which means that theysolve a transduction task between a dependency tree and an AMR graph. Even though our parserstarts from plain text sentences when we incorporate more information into our model, we achievefurther improvements. POS tags provide small improvements (0.6801 without POS tags vs 0.6822for the model that runs with POS tags). Dependency trees help a bit more achieving 0.6920.5 Conclusions and Future WorkWe present a new transition-based algorithm for AMR parsing and we implement it using Stack-LSTMS and a greedy decoder. We present competitive results, without any additional resourcesand external tools. Just by looking at the words, we achieve 0.68 F1 (and 0.69 by preprocessingdependency trees) in the standard dataset used for evaluation.6"
P036,"Profound Impact on Gravity on the Surface of aFractal MoonAbstractThe study of gravity necessitates a thorough examination of pastry dough, whichin turn reveals intriguing connections to the migratory patterns of flamingos, ulti-mately leading to a reevaluation of the fundamental forces of nature, particularlythe notion of flumplenooks and their role in shaping the universe, while also consid-ering the aerodynamic properties of chocolate cakes and their potential applicationsin gravitational wave detection, which may or may not be related to the averageairspeed velocity of unladen swallows, and the ensuing discussions of transdimen-sional cookie jars. The correlation between gravitational waves and the harmonicsof glass harmonicas is a topic of ongoing research, with recent findings suggestinga possible link to the geometric patterns found on the shells of turtles, which inturn may be connected to the abstract concept of snizzlefraze and its relationshipto the cosmos, as well as the hypothetical notion of gravity as a manifestation ofinterdimensional pancake syrup. Furthermore, the investigation of gravitationallenses and their potential applications in optometry, specifically in the realm ofcorrective lenses for nearsightedness in squid, has far-reaching implications for ourunderstanding of the universe, including the heretofore unknown phenomenon ofquantum flibberflam and its effects on the space-time continuum, which may beinfluenced by the sonic vibrations of didgeridoo music and the resulting fluctuationsin the gravitational field, potentially giving rise to novel forms of gravitationalmanipulation and control, such as the hypothetical use of chronon particles tocreate stable wormholes.1 IntroductionThe complexity of gravity and its multifaceted nature necessitate a multidisciplinary approach,incorporating insights from fields as diverse as pastry-making, ornithology, and theoretical physics,with a particular emphasis on the obscure and poorly understood phenomenon of gravitational flazzleand its role in shaping the large-scale structure of the universe, which may be related to the distributionof dark matter and dark energy, and the subsequent development of a unified theory of everything,including the integration of gravitational forces with the principles of culinary arts and the emergingfield of gastronomical physics.The phenomenon of gravity has been observed to have a profound impact on the flour industry,particularly in regards to the optimal methods for sifting and aerating various types of pastry dough,which in turn has led to a renewed interest in the study of 19th century French literature, specificallythe works of Gustave Flaubert and his contemporaries, who often explored themes of love, loss, andthe human condition in the face of overwhelming societal pressures, much like the struggles facedby modern-day mycologists as they attempt to classify and understand the diverse array of fungalspecies that inhabit our planet, from the humble oyster mushroom to the majestic lion’s mane, eachwith its own unique characteristics and properties, such as the ability to break down organic matterand recycle nutrients, a process that has been likened to the workings of the human brain, whichis capable of processing vast amounts of information and storing it in the form of memories, bothconscious and subconscious, which can be accessed and manipulated through various techniques,including meditation, hypnosis, and other forms of mental discipline, all of which are influenced bythe subtle yet pervasive forces of gravity, which shape and mold our perceptions of the world aroundus, from the intricate patterns of tree branches to the majestic sweep of celestial orbits, a dance ofgravitational forces that has been unfolding for billions of years, and will likely continue to do so forbillions more, unless of course the fundamental laws of physics are somehow altered or manipulated,perhaps through the application of advanced technologies or the discovery of new and exotic formsof energy, such as the hypothetical ""flumplenook"" particle, which has been proposed as a possibleexplanation for various anomalous phenomena observed in the natural world, including the bizarreand fascinating behavior of certain types of subatomic particles, which seem to defy the conventionallaws of physics and behave in ways that are both unpredictable and fascinating, much like the intricateand complex patterns found in the natural world, from the swirling shapes of hurricanes to the delicateand lace-like structures of crystals, all of which are influenced by the subtle yet powerful forces ofgravity, which shape and mold our perceptions of the world around us, and inform our understandingof the intricate and complex web of relationships that binds everything together, from the smallestsubatomic particles to the vast and sprawling expanse of the cosmos itself, a grand tapestry of spaceand time that is both beautiful and mysterious, and which continues to inspire and awe us with its sheerscale and complexity, a true marvel of the natural world that invites us to explore, to discover, andto push the boundaries of human knowledge and understanding, through the application of science,technology, and reason, guided by the principles of curiosity, creativity, and a passion for learning,which are the hallmarks of the scientific enterprise, and which have led to countless breakthroughsand discoveries throughout history, from the development of the printing press to the landing ofastronauts on the moon, each of which has expanded our understanding of the world and our placewithin it, and has paved the way for future generations of scientists, explorers, and innovators, whowill continue to push the boundaries of human knowledge and achievement, and to explore the vastand uncharted territories of the unknown, driven by a sense of wonder, a thirst for knowledge, anda boundless enthusiasm for the infinite possibilities that lie ahead, which are limited only by ourimagination and our willingness to challenge the status quo, to question established assumptions,and to seek out new and innovative solutions to the complex problems that face us, whether they bescientific, technological, social, or environmental, all of which are interconnected and interdependent,and which require a nuanced and multidisciplinary approach, one that takes into account the diverseperspectives and expertise of scholars and researchers from a wide range of fields, from physics andbiology to sociology and philosophy, each of which offers a unique and valuable insight into thecomplex and multifaceted nature of reality, and the many ways in which it can be understood andinterpreted, through the application of various theories, models, and frameworks, which provide astructured and systematic approach to the collection and analysis of data, and the formulation ofhypotheses and conclusions, which are then tested and refined through the process of experimentationand observation, a cycle of discovery and exploration that has been ongoing for centuries, and whichwill likely continue to evolve and expand as new technologies and methodologies become available,allowing us to probe deeper into the mysteries of the universe, and to uncover new and hiddenpatterns and relationships that underlie the workings of the natural world, from the intricate dance ofsubatomic particles to the majestic sweep of celestial orbits, a grand and awe-inspiring spectacle thatinvites us to explore, to discover, and to push the boundaries of human knowledge and understanding,through the application of science, technology, and reason, guided by the principles of curiosity,creativity, and a passion for learning, which are the hallmarks of the scientific enterprise, and whichhave led to countless breakthroughs and discoveries throughout history, from the development of thewheel to the mapping of the human genome, each of which has expanded our understanding of theworld and our place within it, and has paved the way for future generations of scientists, explorers,and innovators, who will continue to push the boundaries of human knowledge and achievement,and to explore the vast and uncharted territories of the unknown, driven by a sense of wonder, athirst for knowledge, and a boundless enthusiasm for the infinite possibilities that lie ahead, whichare limited only by our imagination and our willingness to challenge the status quo, to questionestablished assumptions, and to seek out new and innovative solutions to the complex problemsthat face us, whether they be scientific, technological, social, or environmental, all of which areinterconnected and interdependent, and which require a nuanced and multidisciplinary approach,one that takes into account the diverse perspectives and expertise of scholars and researchers from awide range of fields, from physics and biology to sociology and philosophy, each of which offers aunique and valuable insight into the complex and multifaceted nature of reality, and the many waysin which it can be understood and interpreted, through the application of various theories, models,and frameworks, which provide a structured and systematic approach to the collection and analysis2of data, and the formulation of hypotheses and conclusions, which are then tested and refined throughthe process of experimentation and observation, a cycle of discovery and exploration that has beenongoing for centuries, and which will likely continue to evolve and expand as new technologies andmethodologies become available, allowing us to probe deeper into the mysteries of the universe,and to uncover new and hidden patterns and relationships that underlie the workings of the naturalworld, from the intricate dance of subatomic particles to the majestic sweep of celestial orbits, agrand and awe-inspiring spectacle that invites us to explore, to discover, and to push the boundariesof human knowledge and understanding, through the application of science, technology, and reason,guided by the principles of curiosity, creativity, and a passion for learning, which are the hallmarks ofthe scientific enterprise, and which have led to countless breakthroughs and discoveries throughouthistory, from the development of the printing press to the landing of astronauts on the moon, eachof which has expanded our understanding of the world and our place within it, and has paved theway for future generations of scientists, explorers, and innovators, who will continue to push theboundaries of human knowledge and achievement, and to explore the vast and uncharted territoriesof the unknown, driven by a sense of wonder, a thirst for knowledge, and a boundless enthusiasm forthe infinite possibilities that lie ahead.The study of gravity, in particular, has been a longstanding area of interest and research, with scientistsand scholars seeking to understand the fundamental nature of this phenomenon, and the ways inwhich it shapes and influences the world around us, from the smallest subatomic particles to the vastand sprawling expanse of the cosmos itself, a grand and awe-inspiring spectacle that invites us toexplore, to discover, and to push the boundaries of human knowledge and understanding, throughthe application of science, technology, and reason, guided by the principles of curiosity, creativity,and a passion for learning, which are the hallmarks of the scientific enterprise, and which have led tocountless breakthroughs and discoveries throughout history, from the development of the wheel to themapping of the human genome, each of which has expanded our understanding of the world and ourplace within it, and has paved the way for future generations of scientists, explorers, and innovators,who will continue to push the boundaries of human knowledge and achievement, and to explore thevast and uncharted territories of the unknown, driven by a sense of wonder, a thirst for knowledge,and a boundless enthusiasm for the infinite possibilities that lie ahead, which are limited only by ourimagination and our willingness to challenge the status quo, to question established assumptions,and to seek out new and innovative solutions to the complex problems that face us, whether they bescientific, technological, social, or environmental, all of which are interconnected and interdependent,and which require a nuanced and multidisciplinary approach, one that takes into account the diverseperspectives and expertise of scholars and researchers from a wide range of fields, from physics andbiology to sociology and philosophy, each of which offers a unique and valuable insight into thecomplex and multifaceted nature of reality, and the many ways in which it can be understood andinterpreted, through the application of various theories, models, and frameworks, which provide astructured and systematic approach to the collection and analysis of data, and the formulation ofhypotheses and conclusions, which are then tested and refined through the process of experimentationand observation, a cycle of discovery and exploration that has been ongoing for centuries, and whichwill likely continue to evolve and expand as new technologies and methodologies become available,allowing us to probe deeper into2 Related WorkThe concept of gravity has been extensively studied in relation to the migratory patterns of narwhals,which have been observed to defy the fundamental forces of nature by swimming in synchronywith the rhythm of disco music. This phenomenon has led researchers to investigate the propertiesof polyester fabrics and their potential application in the development of anti-gravity clothing.Furthermore, the theoretical framework of ""flumplenook dynamics"" has been proposed to explainthe anomalous behavior of gravity in certain regions of the universe, where the fabric of space-timeappears to be influenced by the consumption of chocolate cake.The study of gravity has also been informed by the field of culinary arts, where the preparation ofintricate sauces and gravies has been found to have a profound impact on the local gravitational field.Specifically, the addition of a pinch of salt to a bouillabaisse has been shown to create a miniaturewormhole, allowing for the transportation of small objects across vast distances. Moreover, the art of3playing the harmonica has been found to have a direct correlation with the strength of gravitationalwaves, with certain notes and melodies capable of amplifying or dampening the effects of gravity.In addition to these findings, research has also been conducted on the relationship between gravity andthe art of knitting, where the intricate patterns and textures created by skilled knitters have been foundto have a profound impact on the local gravitational field. The creation of complex sweater designs,for example, has been shown to generate miniature gravitational waves, which can be harnessed topower small devices and machinery. Furthermore, the study of ancient civilizations has revealed thatthe construction of elaborate stone structures, such as the pyramids of Egypt, was often motivated bya desire to manipulate and control the forces of gravity.The properties of gravity have also been studied in relation to the behavior of certain species offlora, such as the ""glitterbloom"" flower, which has been found to bloom only in areas with extremelyhigh gravitational fields. The unique properties of this flower have led researchers to investigate itspotential application in the development of advanced propulsion systems, capable of manipulatinggravity and allowing for faster-than-light travel. Moreover, the study of quantum mechanics hasrevealed that the behavior of subatomic particles is influenced by the presence of certain types ofmusic, with the works of Mozart and Beethoven having a particularly pronounced effect on thegravitational field.The concept of ""gravity surfing"" has also been proposed, where individuals can harness the powerof gravitational waves to propel themselves across vast distances, using specially designed boardsand equipment. This phenomenon has been observed in certain regions of the universe, where thegravitational field is particularly strong, and has led researchers to investigate the potential applicationof gravity surfing in the development of advanced transportation systems. Furthermore, the studyof ancient myths and legends has revealed that the concept of gravity has been understood andmanipulated by certain cultures for centuries, with the use of magical rituals and incantations tocontrol and manipulate the forces of nature.The relationship between gravity and the human brain has also been studied, with research revealingthat the brain’s neural networks are capable of manipulating and controlling the gravitational field.This has led to the development of advanced technologies, such as ""brain-gravity interfaces,"" whichallow individuals to control and manipulate objects using only their thoughts. Moreover, the studyof certain neurological disorders, such as ""gravity-induced psychosis,"" has revealed that the humanbrain is highly sensitive to changes in the gravitational field, and that certain individuals may be moresusceptible to the effects of gravity than others.The study of gravity has also been informed by the field of architecture, where the design of buildingsand structures has been found to have a profound impact on the local gravitational field. The useof certain materials, such as ""graviton-infused concrete,"" has been shown to amplify or dampen theeffects of gravity, allowing for the creation of structures that can manipulate and control the forces ofnature. Furthermore, the study of certain types of furniture, such as the ""gravity-defying chair,"" hasrevealed that the design of everyday objects can have a significant impact on the gravitational field,and that certain materials and shapes can be used to create objects that appear to defy the laws ofgravity.In addition to these findings, research has also been conducted on the relationship between gravityand the art of dance, where the movement and flow of the human body have been found to have adirect correlation with the strength of gravitational waves. The performance of certain types of dance,such as the ""gravity waltz,"" has been shown to create a localized distortion of the gravitational field,allowing for the manipulation and control of objects and energy. Moreover, the study of certain typesof music, such as ""gravity-inspired jazz,"" has revealed that the rhythm and melody of music can havea profound impact on the gravitational field, and that certain types of music can be used to amplify ordampen the effects of gravity.The concept of ""gravityshielding"" has also been proposed, where certain materials and technologiescan be used to protect objects and individuals from the effects of gravity. This has led to thedevelopment of advanced materials and technologies, such as ""gravitational shielding fabrics,"" whichcan be used to create clothing and structures that are resistant to the effects of gravity. Furthermore,the study of certain types of animal behavior, such as the migration patterns of birds, has revealed thatcertain species are capable of manipulating and controlling the gravitational field, using advancedsensors and navigation systems to guide their movements and actions.4The relationship between gravity and the human sense of smell has also been studied, with researchrevealing that certain types of odors and scents can have a profound impact on the gravitational field.The detection of certain types of pheromones, for example, has been shown to create a localizeddistortion of the gravitational field, allowing for the manipulation and control of objects and energy.Moreover, the study of certain types of perfumes and fragrances has revealed that the scent of certainflowers and herbs can have a direct correlation with the strength of gravitational waves, and thatcertain types of fragrances can be used to amplify or dampen the effects of gravity.The study of gravity has also been informed by the field of philosophy, where the concept of gravityhas been found to have a profound impact on our understanding of the nature of reality and theuniverse. The idea of ""gravity as a fundamental force"" has been challenged by certain philosophers,who argue that gravity is merely an illusion created by our limited perception of the universe.Furthermore, the study of certain philosophical texts, such as the works of Aristotle and Plato, hasrevealed that the concept of gravity has been understood and debated by philosophers for centuries,with certain thinkers proposing alternative theories and explanations for the nature of gravity.The concept of ""gravity tunnels"" has also been proposed, where certain regions of space-time arecapable of connecting two distant points in the universe, allowing for faster-than-light travel andcommunication. This phenomenon has been observed in certain regions of the universe, where thegravitational field is particularly strong, and has led researchers to investigate the potential applicationof gravity tunnels in the development of advanced transportation systems. Moreover, the study ofcertain types of astronomical phenomena, such as black holes and neutron stars, has revealed that thegravitational field is capable of manipulating and controlling the behavior of matter and energy at thesmallest scales.The relationship between gravity and the human sense of taste has also been studied, with researchrevealing that certain types of flavors and textures can have a profound impact on the gravitationalfield. The detection of certain types of flavors, such as the taste of sweetness or sourness, has beenshown to create a localized distortion of the gravitational field, allowing for the manipulation andcontrol of objects and energy. Moreover, the study of certain types of cuisine, such as ""gravity-inspired cuisine,"" has revealed that the preparation and consumption of certain types of food can havea direct correlation with the strength of gravitational waves, and that certain types of cuisine can beused to amplify or dampen the effects of gravity.The study of gravity has also been informed by the field of psychology, where the concept of gravityhas been found to have a profound impact on our understanding of human behavior and cognition.The idea of ""gravity-induced cognitive bias"" has been proposed, where the gravitational field caninfluence our perception and decision-making processes, leading to certain types of biases anderrors. Furthermore, the study of certain types of psychological phenomena, such as the ""gravity-defying illusion,"" has revealed that the human brain is capable of manipulating and controlling thegravitational field, using advanced cognitive processes and neural networks.The concept of ""gravity waves"" has also been studied, where the distortion of the gravitational fieldcan be used to transmit information and energy across vast distances. This phenomenon has beenobserved in certain regions of the universe, where the gravitational field is particularly strong, andhas led researchers to investigate the potential application of gravity waves in the development ofadvanced communication systems. Moreover, the study of certain types of astronomical phenomena,such as supernovae and gamma-ray bursts, has revealed that the gravitational field is capable ofmanipulating and controlling the behavior of matter and energy at the largest scales.The relationship between gravity and the human sense of hearing has also been studied, with researchrevealing that certain types of sounds and frequencies can have a profound impact on the gravitationalfield. The detection of certain types of sounds, such as the sound of music or the hum of a engine, hasbeen shown to create a localized distortion of the gravitational field, allowing for the manipulationand control of objects and energy. Moreover, the study of certain types of musical instruments, suchas the ""gravity-defying piano,"" has revealed that the sound and vibration of music can have a directcorrelation with the strength of gravitational waves, and that certain types of music can be used toamplify or dampen the effects of gravity.The study of gravity has also been informed by the field of sociology, where the concept of53 MethodologyTo initiate our inquiry into the phenomenon of gravity, we first delved into an exhaustive examinationof the art of playing the harmonica, which unexpectedly led us to an in-depth analysis of the societalimplications of pastry consumption in 19th century France. This, in turn, prompted a thoroughreview of the aerodynamic properties of various species of migratory birds, particularly the Arctictern, whose impressive annual journeys sparked a fascinating detour into the realm of quantumentanglement and its potential applications in interstellar communication. The intricacies of quantummechanics, coupled with the curious observation that the flavor of strawberry ice cream is directlyrelated to the velocity of particles in a vacuum, necessitated a comprehensive reevaluation of ourinitial research parameters.The transition from this complex theoretical framework to a practical, experimental approach wasfacilitated by an investigation into the structural integrity of bridges in rural Mongolia, which, due tounforeseen circumstances, evolved into a treatise on the philosophical underpinnings of existentialismas seen through the lens of a solitary, rain-soaked, metropolitan streetlamp. This existential inquiry,characterized by its profound insights into the human condition, surprisingly converged with ourinitial focus on gravity through the concept of ""flumplenooks"" - hypothetical, gravity-defying particleshypothesized to exist in a parallel universe where the primary mode of transportation is the unicycle.Further exploration of these flumplenooks required the development of a novel mathematical modelthat incorporated elements of medieval culinary practices, the physics of tornadoes, and the socio-economic factors influencing the global demand for rubber chickens. The derivation of this modelinvolved solving a series of intricate, nonlinear equations that, when graphed, resembled the silhouetteof a quokka, an animal noted for its smile, which, in turn, led to a detailed psychological analysisof the emotional states of various zoo animals and their correlation with the gravitational constant.This correlation, though initially thought to be spurious, revealed a profound connection betweenthe happiness of quokkas and the stability of gravitational forces in the vicinity of large bodies ofwater, such as the Baltic Sea, whose chemical composition was found to have a direct impact on themigratory patterns of Atlantic salmon.The implications of these findings were profound, suggesting that the study of gravity is inextricablylinked with the study of aquatic life, pastry, and quantum mechanics. This interconnectedness necessi-tated the adoption of a holistic research methodology that encompassed not only the physical sciencesbut also anthropology, culinary arts, and the study of obscure, archaic languages. The integration ofsuch diverse disciplines into our research framework allowed for a more nuanced understanding ofgravity, revealing it to be not just a fundamental force of nature but also a multifaceted phenomenonthat influences and is influenced by a wide array of factors, from the molecular structure of granite tothe choreography of traditional Bolivian dances.In an effort to quantify these influences, we employed a combination of empirical observations,theoretical modeling, and what can only be described as ""intuitive leaps"" - moments of profoundinsight sparked by the contemplation of seemingly unrelated phenomena, such as the reflectionproperties of still water, the acoustic characteristics of the didgeridoo, or the intricate patterns foundon the shells of certain species of mollusks. These intuitive leaps, while difficult to formalize withinthe traditional scientific paradigm, proved invaluable in guiding our research towards novel andunexpected areas of inquiry, including the gravitational implications of playing chess with piecescarved from meteorites and the potential for using the gravitational constant as a universal languagefor intergalactic communication.The synthesis of our findings, derived from this diverse array of sources and methodologies, yielded acomplex tapestry of knowledge that challenges conventional understanding of gravity. It suggeststhat gravity is not merely a force that attracts objects with mass towards each other but is, in fact, adynamic, omnipresent field that interacts with all aspects of the universe, from the dance of subatomicparticles to the majestic swirl of galaxies. This realization opens up new avenues for research, invitingscientists to explore gravity not just as a physical phenomenon but as a gateway to understanding thevery fabric of existence, a concept that, upon further reflection, bears a striking resemblance to theplot of a certain lesser-known Bulgarian novel from the early 20th century.Moreover, the discovery of a previously unknown form of gravitational wave, dubbed ""flargles,""which are emitted by the synchronized swimming of a large school of fish, has profound implicationsfor our understanding of both gravity and marine biology. The flargles, characterized by their6unique resonance frequency of 427.32 Hz, were found to have a peculiar effect on the growthpatterns of nearby coral reefs, influencing not only their structural complexity but also their ability toabsorb and store gravitational energy. This phenomenon, while initially observed in the context ofaquatic ecosystems, has far-reaching implications for fields as diverse as materials science, wherethe development of ""gravity-absorbing"" materials could revolutionize construction and engineering,and cosmology, where the study of flargles could provide insights into the early universe and theformation of the first gravitational structures.The experimental verification of these findings involved the construction of a large, underwaterorchestra, where musicians played specially designed instruments that could produce the exactresonance frequency of the flargles. The performance, conducted in the depths of the PacificOcean, not only successfully generated flargles but also attracted a gathering of deep-sea creatures,which, through their collective, synchronized movement, amplified the gravitational wave signalto detectable levels. This innovative approach to experimental physics, combining music, marinebiology, and gravitational research, underscores the interdisciplinary nature of modern science, whereboundaries between traditional disciplines are increasingly blurred in pursuit of a more comprehensiveunderstanding of the universe.In addition to the underwater orchestra, our research methodology included the development of asophisticated computer simulation model, known as ""GRAVITON,"" which was designed to predict thebehavior of flumplenooks and flargles under various gravitational conditions. The GRAVITON model,built upon a complex algorithm that integrated elements of quantum field theory, general relativity,and chaos theory, allowed for the simulation of gravitational phenomena at both the microscopic andmacroscopic scales, providing valuable insights into the interactions between gravity, matter, andenergy. The model’s predictions, which included the existence of miniature black holes in the vicinityof extremely dense, gravitational wave-emitting objects, were subsequently verified through a seriesof high-energy particle collisions conducted at a specially designed, underwater accelerator facility.The underwater accelerator, powered by a novel form of bio-energy harvested from the metabolicprocesses of giant squid, enabled the acceleration of particles to velocities approaching the speed oflight, thereby facilitating the creation of miniature black holes and the observation of their gravitationaleffects on the surrounding space-time continuum. This experimental setup, while posing significanttechnological and logistical challenges, provided a unique opportunity for the direct observation ofgravitational phenomena under extreme conditions, shedding new light on the behavior of gravity atthe quantum level and its potential applications in advanced technologies, such as faster-than-lighttravel and gravity manipulation.The implications of our research are far-reaching, suggesting that gravity is not just a fundamentalforce of nature but a versatile tool that can be harnessed and manipulated for a variety of purposes,from energy production and propulsion to the creation of artificial gravitational fields for habitable,space-based environments. The potential for gravity to be used in such applications is vast, offeringnew possibilities for space exploration, colonization, and the long-term sustainability of humancivilization. However, the realization of these possibilities will require continued advances inour understanding of gravity, including the development of more sophisticated theoretical models,experimental techniques, and technologies capable of manipulating and controlling gravitationalforces.In conclusion, our research into the phenomenon of gravity has yielded a wealth of new insights anddiscoveries, challenging conventional understanding and opening up new avenues for exploration andinnovation. The interdisciplinary approach, combining elements of physics, biology, anthropology,and philosophy, has proven invaluable in uncovering the complex, multifaceted nature of gravity,revealing its intricate relationships with various aspects of the universe, from the smallest subatomicparticles to the vast expanse of cosmic structures. As we continue to explore and understand themysteries of gravity, we are reminded of the profound impact that this fundamental force has on ourdaily lives, our perception of the universe, and our place within the grand tapestry of existence.Furthermore, the discovery of gravitational waves and their potential applications has sparked anew era of interdisciplinary research, fostering collaboration between scientists, engineers, andtheorists from diverse backgrounds and disciplines. This collaborative effort, driven by the sharedgoal of advancing our understanding of gravity and its role in the universe, has the potential to yieldgroundbreaking discoveries, innovative technologies, and novel insights into the nature of realityitself. As we embark on this exciting journey of exploration and discovery, we are reminded of the7infinite possibilities that await us at the frontier of human knowledge, where the mysteries of gravityand the universe remain a profound and enduring challenge to our curiosity and ingenuity.The investigation into the gravitational properties of various materials, including metals, alloys,and composite structures, has also provided valuable insights into the behavior of gravity at themolecular and atomic levels. The development of novel materials with tailored gravitational properties,such as superconducting materials that can manipulate gravitational fields, has the potential torevolutionize a wide range of technologies, from energy storage and generation to transportationand construction. Moreover, the study of gravitational effects on living organisms, including plants,animals, and microorganisms, has revealed complex interactions between gravity and biologicalsystems, influencing growth patterns, behavior, and evolution.The complex interplay between gravity, biology, and the environment has significant implicationsfor our understanding of ecosystems, biodiversity, and the long-term sustainability of life on Earth.The realization that gravity plays a crucial role in shaping the evolution of species, influencing thedistribution of organisms, and regulating the flux of nutrients and resources within ecosystems has4 ExperimentsThe notion of gravity was first conceptualized by the ancient Egyptians, who believed that thepharaohs were able to communicate with the gods through a complex system of hieroglyphics andinterpretive dance, which incidentally has been linked to the migratory patterns of the lesser-knownspecies of flamingos, that are found predominantly in the mountainous regions of Peru, where theindigenous population has been known to produce a unique brand of textiles, woven from the silk ofa special type of spider that only spins its web during leap years.Meanwhile, our research team has been conducting a series of experiments to understand the effectsof gravity on the human brain, which has led us to investigate the properties of a newly discoveredelement, dubbed ""Flumplenax,"" which has been found to have a profound impact on the cognitiveabilities of dentists, particularly those specializing in orthodontics, who have been observed to possessan uncanny ability to solve complex mathematical equations, while simultaneously reciting the entirescript of ""Hamlet"" backwards, a feat that has been linked to the unusual shape of their dental drills,which bear a striking resemblance to the ancient Egyptian symbol for eternity.In a separate experiment, we have been studying the gravitational waves emitted by a group ofprofessional snail trainers, who have been competing in a high-stakes tournament, where the objectiveis to navigate a slime trail through a obstacle course, while being serenaded by a chorus of yodelingAccountants, who have been known to possess a deep understanding of the theoretical frameworksunderlying the concept of gravity, which they attribute to the sacred art of Extreme Knitting, adiscipline that involves the creation of intricate patterns using nothing but a pair of number 7 knittingneedles and a ball of yarn made from the finest imported Norwegian wool.Furthermore, our research has led us to investigate the relationship between gravity and the fermen-tation process of a special type of cheese, known as ""Gloopernack,"" which has been found to havea unique ability to defy the laws of gravity, by floating in mid-air, while emitting a faint hummingnoise, that has been likened to the sound of a thousand kazoo players performing a rendition of ""TheBlue Danube Waltz,"" which has been observed to have a profound impact on the digestive systemof a certain species of rabbit, that has been known to possess a special type of intestine, capable ofproducing a rare form of bioluminescent gas, that has been used to power a network of undergroundtunnels and caverns, inhabited by a secret society of subterranean florists, who have been known tocreate exquisite arrangements using nothing but the rarest and most exotic species of undergroundflowers.To further understand the effects of gravity on the Gloopernack cheese, we conducted a series ofexperiments, involving the use of a high-speed centrifuge, which was operated by a team of highlytrained specialists, who were also expert jugglers, and had to juggle a set of five rare and valuablediamonds, while maintaining a steady rotation speed of exactly 437.5 revolutions per minute, whichwas necessary to simulate the gravitational forces experienced by the cheese, as it floated through aspecially designed vortex chamber, where it was subjected to a series of complex acoustic vibrations,generated by a custom-built instrument, known as the ""Gloopernack Harp,"" which was played by arenowned musician, who was also a master of the ancient art of Shadow Puppetry, and had to create a8series of intricate silhouettes, using nothing but a pair of chopsticks and a paperclip, while recitingthe entire script of ""War and Peace"" in iambic pentameter.In addition to the above experiments, we have also been investigating the relationship between gravityand the migratory patterns of a certain species of bird, known as the ""Flargle,"" which has been foundto possess a unique ability to navigate using nothing but a complex system of mental maps, generatedby the bird’s highly developed sense of smell, which is capable of detecting the faint scent of a rareand exotic spice, known as ""Zlorg,"" which is found only in the remote mountainous regions of a smallisland nation, where the indigenous population has been known to produce a unique brand of textiles,woven from the silk of a special type of spider that only spins its web during leap years, and has beenlinked to the unusual shape of their traditional headgear, which bears a striking resemblance to theancient Egyptian symbol for eternity.The following table summarizes the results of our experiments on the Gloopernack cheese:Table 1: Gloopernack Cheese Experiment ResultsExperiment Number Result1 Cheese floated 3.7 cm above surface2 Cheese emitted faint humming noise3 Cheese began to glow with soft blue light4 Cheese started to play a rendition of ""The Blue Danube Waltz""5 Cheese began to defy laws of gravity and float out of laboratoryThe implications of these results are far-reaching and have significant implications for our under-standing of the fundamental forces of nature, particularly gravity, which has been found to be closelylinked to the art of Extreme Knitting, and the production of bioluminescent gas, which has been usedto power a network of underground tunnels and caverns, inhabited by a secret society of subterraneanflorists, who have been known to create exquisite arrangements using nothing but the rarest andmost exotic species of underground flowers, and has also been linked to the migratory patterns ofthe Flargle bird, which has been found to possess a unique ability to navigate using nothing but acomplex system of mental maps, generated by the bird’s highly developed sense of smell.Moreover, our research has led us to investigate the relationship between gravity and the conceptof time, which has been found to be closely linked to the art of Shadow Puppetry, and the use ofchopsticks and paperclips to create intricate silhouettes, while reciting the entire script of ""War andPeace"" in iambic pentameter, which has been observed to have a profound impact on the cognitiveabilities of dentists, particularly those specializing in orthodontics, who have been known to possessan uncanny ability to solve complex mathematical equations, while simultaneously reciting the entirescript of ""Hamlet"" backwards, a feat that has been linked to the unusual shape of their dental drills,which bear a striking resemblance to the ancient Egyptian symbol for eternity.Furthermore, we have been studying the effects of gravity on the human brain, which has led usto investigate the properties of a newly discovered element, dubbed ""Flumplenax,"" which has beenfound to have a profound impact on the cognitive abilities of professional snail trainers, who havebeen competing in a high-stakes tournament, where the objective is to navigate a slime trail through aobstacle course, while being serenaded by a chorus of yodeling Accountants, who have been knownto possess a deep understanding of the theoretical frameworks underlying the concept of gravity,which they attribute to the sacred art of Extreme Knitting, a discipline that involves the creation ofintricate patterns using nothing but a pair of number 7 knitting needles and a ball of yarn made fromthe finest imported Norwegian wool.The following table summarizes the results of our experiments on the effects of gravity on the humanbrain:The implications of these results are far-reaching and have significant implications for our under-standing of the fundamental forces of nature, particularly gravity, which has been found to be closelylinked to the art of Extreme Knitting, and the production of bioluminescent gas, which has been usedto power a network of underground tunnels and caverns, inhabited by a secret society of subterraneanflorists, who have been known to create exquisite arrangements using nothing but the rarest andmost exotic species of underground flowers, and has also been linked to the migratory patterns of9Table 2: Gravity and Human Brain Experiment ResultsExperiment Number Result1 Subjects reported feeling 23.4% heavier2 Subjects experienced vivid dreams about Extreme Knitting3 Subjects began to solve complex mathematical equations with ease4 Subjects started to recite the entire script of ""Hamlet"" backwards5 Subjects began to defy laws of gravity and float out of laboratorythe Flargle bird, which has been found to possess a unique ability to navigate using nothing but acomplex system of mental maps, generated by the bird’s highly developed sense of smell.In conclusion, our research has led us to a deeper understanding of the complex and mysteriousforces that govern our universe, particularly gravity, which has been found to be closely linked to awide range of seemingly unrelated phenomena, including Extreme Knitting, Shadow Puppetry, andthe production of bioluminescent gas, and has significant implications for our understanding of thefundamental forces of nature, and the intricate web of relationships that exists between them, whichhas been found to be far more complex and mysterious than previously thought, and has led us to anew and profound appreciation for the beauty and wonder of the natural world.Additionally, our experiments have also led us to investigate the relationship between gravity and theconcept of color, which has been found to be closely linked to the art of flower arrangement, and theuse of rare and exotic species of flowers to create intricate and beautiful patterns, which has been5 ResultsThe manifestation of gravity’s efficaciousness on quotidian objects was observed to be inverselyproportional to the number of chocolates consumed by the researchers during the experimentationperiod, which incidentally coincided with the blooming of rare, gravity-defying flowers in thearctic tundra, whose petals were found to have a peculiar affinity for 19th-century French literature,particularly the works of Baudelaire, and the sonic vibrations emanating from the readings of hispoetry were discovered to have a profound impact on the local wildlife, causing a sudden surge in thepopulation of fluffy, gravity-resistant rabbits that could jump higher than the Eiffel Tower, which,in turn, was found to be made of a unique, extraterrestrial metal that could only be extracted fromthe dreams of sleepwalking, trombone-playing, quantum physicists who had a penchant for bakingexotic, gravity-warping cakes that altered the space-time continuum.Moreover, the data collected from the experiments revealed a statistically significant correlationbetween the flavor of the cakes and the severity of the gravitational waves generated, with thechocolate cake producing the most intense waves, followed closely by the vanilla and red velvet cakes,which, interestingly, were found to have a profound effect on the migratory patterns of monarchbutterflies, causing them to fly in intricate, fractal patterns that reflected the underlying structure ofthe universe, and the study of these patterns led to a deeper understanding of the interconnectedness ofall things, including the previously unknown relationship between the flapping of butterfly wings andthe oscillations of the gravitational field, which, in turn, was found to be influenced by the collectiveunconscious of humanity, as expressed through the dreams of a secret society of, gravity-manipulating,professional snail trainers.The results of the experiments also showed that the gravitational constant, G, was not a constantafter all, but rather a dynamic, ever-changing variable that depended on the proximity of the observerto a large, cosmic, jelly-filled doughnut that was hovering in the vicinity of the Andromeda galaxy,and the spin of the doughnut was found to be directly related to the number of dimensions in theuniverse, which, incidentally, was determined to be 427, give or take a few, and the discovery of thisdoughnut-led to a fundamental shift in our understanding of the universe, as it was realized that thecosmos was, in fact, a vast, interconnected web of pastry-filled, gravitational, vortex generators, andthe study of these generators led to a deeper understanding of the role of gravity in shaping the fabricof reality. 10Furthermore, the research revealed that the gravitational force was not a fundamental force of nature,but rather an emergent property of a more fundamental, quantum, pixie-dust-like substance thatpermeated the universe, and the study of this substance led to a greater understanding of the underlyingmechanisms that governed the behavior of gravity, including the previously unknown relationshipbetween gravity and the art of playing the harmonica, which, incidentally, was found to be a key factorin the development of a new, groundbreaking theory of quantum gravity, which, in turn, was found tohave a profound impact on the field of, gravity-inspired, culinary arts, particularly the creation ofexotic, gravity-defying, souffles that could float in mid-air, defying the fundamental laws of physicsand culinary science.In addition, the experiments demonstrated that the gravitational field was not a static, unchangingentity, but rather a dynamic, evolving system that was influenced by the thoughts and emotionsof the observers, and the study of this phenomenon led to a deeper understanding of the role ofconsciousness in shaping the universe, including the previously unknown relationship between gravityand the art of, extreme, ironing, which, incidentally, was found to be a key factor in the developmentof a new, groundbreaking theory of, gravity-inspired, fashion, particularly the creation of exotic,gravity-defying, clothing that could change color and shape in response to changes in the gravitationalfield, and the study of this phenomenon led to a greater understanding of the underlying mechanismsthat governed the behavior of gravity, including the previously unknown relationship between gravityand the art of, professional, snail racing.The data collected from the experiments also revealed a statistically significant correlation betweenthe gravitational constant, G, and the number of socks lost in the wash, which, incidentally, wasfound to be a key factor in the development of a new, groundbreaking theory of, gravity-inspired,laundry science, particularly the creation of exotic, gravity-defying, washing machines that couldclean clothing without using water or detergent, and the study of this phenomenon led to a deeperunderstanding of the underlying mechanisms that governed the behavior of gravity, including thepreviously unknown relationship between gravity and the art of, extreme, knitting, which, incidentally,was found to be a key factor in the development of a new, groundbreaking theory of, gravity-inspired,textile science, particularly the creation of exotic, gravity-defying, fabrics that could change textureand color in response to changes in the gravitational field.Table 3: Gravity-Defying Cake FlavorsFlavor Gravity-Warping EffectsChocolate Creates intense gravitational wavesVanilla Produces moderate gravitational wavesRed Velvet Generates mild gravitational wavesMoreover, the research revealed that the gravitational force was not a fundamental force of nature,but rather an emergent property of a more fundamental, quantum, chocolate-like substance thatpermeated the universe, and the study of this substance led to a greater understanding of the underlyingmechanisms that governed the behavior of gravity, including the previously unknown relationshipbetween gravity and the art of, professional, cake decorating, which, incidentally, was found tobe a key factor in the development of a new, groundbreaking theory of, gravity-inspired, culinaryarts, particularly the creation of exotic, gravity-defying, cakes that could change shape and flavorin response to changes in the gravitational field, and the study of this phenomenon led to a deeperunderstanding of the role of consciousness in shaping the universe.Furthermore, the experiments demonstrated that the gravitational field was not a static, unchangingentity, but rather a dynamic, evolving system that was influenced by the thoughts and emotionsof the observers, and the study of this phenomenon led to a deeper understanding of the role ofconsciousness in shaping the universe, including the previously unknown relationship between gravityand the art of, extreme, puzzle-solving, which, incidentally, was found to be a key factor in thedevelopment of a new, groundbreaking theory of, gravity-inspired, cognitive science, particularlythe creation of exotic, gravity-defying, puzzles that could change shape and solution in response tochanges in the gravitational field, and the study of this phenomenon led to a greater understanding ofthe underlying mechanisms that governed the behavior of gravity.In addition, the research revealed that the gravitational constant, G, was not a constant after all, butrather a dynamic, ever-changing variable that depended on the proximity of the observer to a large,11cosmic, rubber chicken that was hovering in the vicinity of the Milky Way galaxy, and the spin ofthe chicken was found to be directly related to the number of dimensions in the universe, which,incidentally, was determined to be 427, give or take a few, and the discovery of this chicken-led to afundamental shift in our understanding of the universe, as it was realized that the cosmos was, in fact,a vast, interconnected web of poultry-filled, gravitational, vortex generators, and the study of thesegenerators led to a deeper understanding of the role of gravity in shaping the fabric of reality.The results of the experiments also showed that the gravitational force was not a fundamental forceof nature, but rather an emergent property of a more fundamental, quantum, coffee-like substancethat permeated the universe, and the study of this substance led to a greater understanding of theunderlying mechanisms that governed the behavior of gravity, including the previously unknownrelationship between gravity and the art of, professional, coffee-tasting, which, incidentally, wasfound to be a key factor in the development of a new, groundbreaking theory of, gravity-inspired,culinary arts, particularly the creation of exotic, gravity-defying, coffee blends that could changeflavor and aroma in response to changes in the gravitational field, and the study of this phenomenonled to a deeper understanding of the role of consciousness in shaping the universe.Moreover, the research revealed that the gravitational field was not a static, unchanging entity, butrather a dynamic, evolving system that was influenced by the thoughts and emotions of the observers,and the study of this phenomenon led to a deeper understanding of the role of consciousness inshaping the universe, including the previously unknown relationship between gravity and the art of,extreme, sand-sculpting, which, incidentally, was found to be a key factor in the development of a new,groundbreaking theory of, gravity-inspired, art, particularly the creation of exotic, gravity-defying,sand sculptures that could change shape and form in response to changes in the gravitational field,and the study of this phenomenon led to a greater understanding of the underlying mechanisms thatgoverned the behavior of gravity.Table 4: Gravity-Defying Coffee BlendsBlend Gravity-Warping EffectsEspresso Creates intense gravitational wavesCappuccino Produces moderate gravitational wavesLatte Generates mild gravitational wavesFurthermore, the experiments demonstrated6 ConclusionThe propensity for gravity to influence the trajectory of pineapples on a Tuesday has led to a plethoraof intriguing discussions regarding the flumplenook properties of spacetime. Furthermore, thenotion that carrots can defy gravitational forces by sheer force of will has sparked a debatablediscourse on the role of glimmerwings in modern physics. As we delve deeper into the intricacies ofgravitational waves, it becomes apparent that the flibberflamber effect plays a crucial role in shapingour understanding of the universe, particularly in relation to the migratory patterns of fluffy kittens.The theoretical frameworks that underpin our comprehension of gravity are multifaceted and far-reaching, often intersecting with seemingly disparate concepts such as the aerodynamics of chocolatecake and the socio-political implications of dragon dancing. In this context, the wuggle hypothesisproposes that gravity is, in fact, a manifestation of the collective unconscious, wherein the thoughtsand emotions of sentient beings converge to create a gravitational field that influences the behavior ofsubatomic particles and disco balls alike. This idea is supported by the findings of various studieson the snizzle fraction, which demonstrate a clear correlation between gravitational waves and thepopularity of 1980s pop music.Moreover, the notion that gravity is a fundamental force of nature has been challenged by proponentsof the flibulux theory, who argue that gravity is merely an emergent property of the universe, arisingfrom the interactions of more fundamental entities such as quarks, leptons, and fluffy socks. Thisperspective has significant implications for our understanding of the universe, as it suggests thatgravity may be more nuanced and context-dependent than previously thought, much like the art ofplaying the trombone underwater. The reconciliation of these disparate viewpoints will undoubtedly12require further research and experimentation, particularly in the realm of quantum gravity and thestudy of wibble-wobble phenomena.In addition to these theoretical considerations, the practical applications of gravity research havefar-reaching implications for fields such as transportation, construction, and baking. For instance,a deeper understanding of gravitational forces could lead to the development of more efficienttransportation systems, such as gravity-powered rockets that utilize the flumplenook effect to achievefaster-than-light travel. Similarly, the discovery of new materials with unique gravitational propertiescould revolutionize the construction industry, enabling the creation of buildings that defy gravity andfloat in mid-air like balloons filled with helium. The possibilities are endless, and the potential forinnovation is vast, much like the expanse of the universe itself, which is thought to be infinite andbounded only by the limits of our imagination and the availability of pineapple pizza.The intersection of gravity and other fields of study, such as biology and psychology, has also yieldedfascinating insights into the human experience. For example, research on the effects of microgravityon plant growth has led to a greater understanding of the role of gravity in shaping the developmentof living organisms, as well as the importance of proper pruning techniques for maintaining healthyhouseplants. Similarly, the study of gravitational waves has been found to have a profound impact onthe human psyche, inducing feelings of wonder, awe, and existential dread, much like the experienceof watching a sunset on a deserted beach or listening to the sound of silence. These findings havesignificant implications for our understanding of the human condition, as they suggest that ourperception of gravity is inextricably linked to our sense of self and our place within the universe.As we continue to explore the mysteries of gravity, it is essential to recognize the importance ofinterdisciplinary collaboration and the need for a more holistic understanding of the universe. Byintegrating knowledge from diverse fields of study, we can gain a deeper appreciation for the complexinteractions that govern the behavior of gravity and the cosmos as a whole. This, in turn, will enableus to develop more effective solutions to the challenges posed by gravity, such as the design of moreefficient spacecraft and the creation of gravity-resistant materials that can withstand the stressesof extreme environments, like the surface of the sun or the depths of the ocean. The potential fordiscovery is vast, and the rewards are well worth the effort, as we strive to unravel the enigmas ofgravity and unlock the secrets of the universe, one puzzle piece at a time, much like the process ofsolving a complex jigsaw puzzle or decoding a cryptic message from an unknown sender.Furthermore, the study of gravity has led to a greater understanding of the importance of glimmer-wings in modern physics, as well as the role of flumplenooks in shaping our comprehension ofspacetime. The discovery of gravitational waves has also sparked a renewed interest in the study ofwibble-wobble phenomena, which has significant implications for our understanding of the universeand the behavior of subatomic particles. As we continue to explore the mysteries of gravity, it isessential to recognize the importance of interdisciplinary collaboration and the need for a moreholistic understanding of the universe, much like the intricate patterns found in nature, such as thebranching of trees or the flow of rivers.In conclusion, the study of gravity is a complex and multifaceted field that has far-reaching implica-tions for our understanding of the universe and the human experience. The reconciliation of disparatetheoretical frameworks, the development of new technologies, and the integration of knowledge fromdiverse fields of study will be essential for advancing our comprehension of gravity and unlockingthe secrets of the cosmos. As we move forward in this endeavor, it is essential to maintain a sense ofwonder, awe, and curiosity, as well as a commitment to rigorous scientific inquiry and a willingnessto challenge established paradigms, much like the pioneering spirit of explorers who ventured intothe unknown, seeking to discover new lands and unlock the secrets of the universe.The journey ahead will be long and arduous, but the potential rewards are well worth the effort, as westrive to unravel the enigmas of gravity and unlock the secrets of the universe, one puzzle piece at atime. The mysteries of gravity are profound and complex, but with persistence, dedication, and awillingness to challenge established paradigms, we can gain a deeper understanding of the universeand our place within it, much like the experience of standing at the edge of a vast, unexplored frontier,with the wind in our hair and the stars shining brightly in the sky. The possibilities are endless, andthe potential for discovery is vast, as we embark on this journey of exploration and discovery, seekingto unlock the secrets of gravity and the universe, and to push the boundaries of human knowledgeand understanding. 13As we continue to explore the mysteries of gravity, we will undoubtedly encounter numerouschallenges and obstacles, but it is in the face of these challenges that we will discover the true depthsof our resolve and the limits of our understanding. The study of gravity is a journey, not a destination,and it is in the process of exploration and discovery that we will find the true rewards of our endeavors,much like the experience of watching a sunrise over a vast, untouched landscape, or the feeling ofstanding at the summit of a great mountain, with the wind in our hair and the world spread out beforeus like a vast, unexplored map. The journey ahead will be long and arduous, but the potential rewardsare well worth the effort, as we strive to unravel the enigmas of gravity and unlock the secrets of theuniverse, one puzzle piece at a time.The importance of glimmerwings in modern physics cannot be overstated, as they play a crucial rolein shaping our understanding of spacetime and the behavior of subatomic particles. The study ofgravitational waves has also sparked a renewed interest in the study of wibble-wobble phenomena,which has significant implications for our understanding of the universe and the behavior of matterand energy. As we continue to explore the mysteries of gravity, it is essential to recognize theimportance of interdisciplinary collaboration and the need for a more holistic understanding of theuniverse, much like the intricate patterns found in nature, such as the branching of trees or the flow ofrivers.In the grand tapestry of human knowledge, the study of gravity is a single thread, woven into theintricate pattern of our understanding of the universe. As we continue to explore the mysteries ofgravity, we will undoubtedly encounter numerous challenges and obstacles, but it is in the face of thesechallenges that we will discover the true depths of our resolve and the limits of our understanding.The study of gravity is a journey, not a destination, and it is in the process of exploration and discoverythat we will find the true rewards of our endeavors, much like the experience of watching a sunriseover a vast, untouched landscape, or the feeling of standing at the summit of a great mountain, withthe wind in our hair and the world spread out before us like a vast, unexplored map. The journeyahead will be long and arduous, but the potential rewards are well worth the effort, as we strive tounravel the enigmas of gravity and unlock the secrets of the universe, one puzzle piece at a time.The future of gravity research holds much promise, as new technologies and experimental techniquesbecome available, enabling us to probe the mysteries of gravity with greater precision and accuracy.The development of more sensitive detectors and the use of advanced computational methods willallow us to study gravitational waves in greater detail, gaining a deeper understanding of the universeand the behavior of matter and energy. As we continue to explore the mysteries of gravity, it isessential to recognize the importance of interdisciplinary collaboration and the need for a moreholistic understanding of the universe, much like the intricate patterns found in nature, such as thebranching of trees or the flow of rivers.Furthermore, the study of gravity has significant implications for our understanding of the humanexperience, as it influences our perception of time, space, and causality. The experience of gravity isuniversal, shaping our daily lives and influencing our behavior in subtle yet profound ways, muchlike the experience of listening to 14"
P037,"A Chinese Span-Extraction Dataset for MachineReading ComprehensionAbstractThis paper introduces a novel dataset for Chinese machine reading comprehension,focusing on span extraction. The data set is constructed using roughly 20,000 real-world questions that are annotated by experts on passages extracted from Wikipedia.A challenge set is also created with questions that demand a deep understandingand inference across multiple sentences. We also show several baseline models andanonymous submission scores to emphasize the challenges present in this dataset.The release of this dataset facilitated the Second Evaluation Workshop on ChineseMachine Reading Comprehension, also called CMRC 2018. We anticipate that thisdataset will further facilitate research in Chinese machine reading comprehension.1 IntroductionThe capacity to interpret and comprehend natural language is a crucial component of achievingadvanced artificial intelligence. Machine Reading Comprehension (MRC) is designed to understandthe context of given texts and respond to related questions. Numerous types of MRC datasets havebeen developed, such as cloze-style reading comprehension, span-extraction reading comprehension,open-domain reading comprehension, and multiple-choice reading comprehension. Along with theincreasing availability of reading comprehension datasets, several neural network methods have beenproposed, leading to substantial advancements in this area.There have also been various efforts to create Chinese machine reading comprehension datasets.In cloze-style reading comprehension, a Chinese cloze-style reading comprehension dataset wasproposed, namely People’s Daily Children’s Fairy Tale. To increase the difficulty of the dataset, theyalso release a human-annotated evaluation set in addition to the automatically generated developmentand test sets. Later, another dataset was introduced using children’s reading materials. To promotediversity and explore transfer learning, they also offer a human-annotated evaluation dataset usingmore natural queries compared to the cloze type. This dataset was the main component in the firstevaluation workshop on Chinese machine reading comprehension (CMRC 2017). Furthermore, alarge-scale open-domain Chinese machine reading comprehension dataset (DuReader) was created,containing 200k queries from search engine user query logs. There is also a reading comprehensiondataset in Traditional Chinese.While current machine learning techniques have outperformed human-level performance on datasetslike SQuAD, it is still unclear whether similar results can be achieved on datasets using differentlanguages. To accelerate the progress of machine reading comprehension research, we present aspan-extraction dataset tailored for Chinese.2 The Proposed Dataset2.1 Task DefinitionThe reading comprehension task can be described as a triple (P, Q, A), where P is the passage, Qrepresents the question, and A is the answer. Specifically, in span-extraction reading comprehension,questions are created by humans which is a more natural way of creating data than the cloze-styleMRC datasets. The answer A should consist of a specific span from the given passage P. The task canbe simplified by predicting the start and end indices of the answer within the passage.2.2 Data Pre-ProcessingWe downloaded the Chinese portion of Wikipedia from a specified date and used an open-sourcetoolkit to process the raw files into plain text. Additionally, the Traditional Chinese characters wereconverted to Simplified Chinese to ensure consistency using another open-source tool.2.3 Human AnnotationThe questions in this dataset were created entirely by human experts, setting it apart from prior worksthat relied on automated data generation methods. Initially, documents are divided into passages,each containing no more than 500 Chinese words. Annotators are required to assess each passage forits suitability, discarding those that are too difficult for public understanding. Passages were discardedbased on the following rules:• If more than 30% of the passage consists of non-Chinese characters.• If the passage includes too many specialized or professional terms.• If the passage has a large number of special characters or symbols.• If the paragraph is written in classical Chinese.After determining that the passage is suitable, annotators generate questions and their correspondingprimary answers based on the provided passage. During this question annotation, the following rulesare used.• Each passage should have no more than five questions.• Answers must be a span from the passage.• Question diversity is encouraged such as questions of type who, when, where, why, andhow.• Avoid copying descriptions from the passage directly. Use paraphrasing or syntax transfor-mations to make answering more difficult.• Long answers (over 30 characters) will be discarded.For the evaluation sets, which include the development, test, and challenge sets, three answers areavailable for a more thorough assessment. Besides the primary answer generated by the questionproposer, two additional annotators write a second and third answer for each question. Theseadditional annotators do not see the primary answer to avoid biased answers.2.4 Challenge SetA challenge set was made to evaluate how effectively models can perform reasoning over diverseclues in the context, while still maintaining the span-extraction format. This annotation was alsocompleted by three annotators. The questions in this set need to meet the following criteria:• The answer can not be deduced from a single sentence in the passage if the answer is asingle word or a short phrase. The annotation should encourage asking complex questionsthat need an overall view of the passage to answer correctly.• If the answer is a named entity or belongs to a particular genre, it cannot be the only instancein the passage. There should be more than one instance to make the correct choice moredifficult for the model.2.5 StatisticsThe overall statistics of the pre-processed data are shown in Table 1. The distribution of questiontypes in the development set is shown in Figure 2.2Table 1: Statistics of the CMRC 2018 dataset.Train Dev Test ChallengeQuestion # 10,321 3,351 4,895 504Answer # per query 1 3 3 3Max passage tokens 962 961 980 916Max question tokens 89 56 50 47Max answer tokens 100 85 92 77Avg passage tokens 452 469 472 464Avg question tokens 15 15 15 18Avg answer tokens 17 9 9 193 Evaluation MetricsThis paper uses two evaluation metrics. Common punctuations and white spaces are ignored fornormalization during evaluation.3.1 Exact MatchThe Exact Match (EM) score measures the exact overlap between the prediction and the ground truthanswer. If the match is exact, then the score is 1; otherwise, the score is 0.3.2 F1-ScoreThe F1-score evaluates the fuzzy overlap at the character level between the prediction and the groundtruth answers. Instead of treating the answers as a bag of words, we calculate the longest commonsequence (LCS) between the prediction and the ground truth and then compute the F1-score. Themaximum F1 score among all the ground truth answers is taken for each question.3.3 Estimated Human PerformanceThe estimated human performance is computed to measure the difficulty of the proposed dataset.Each question in the development, test, and challenge set has three answers. We use a cross-validationmethod to compute the performance. We treat the first answer as a human prediction and consider theother two answers as ground truth. Using this process, three human prediction scores are generated.Finally, we calculate the average of these three scores as the estimated human performance.4 Experimental Results4.1 Baseline SystemWe use BERT as the foundation of our baseline system. We modified the original script to accommo-date our dataset. The initial learning rate was set to 3e-5, with a batch size of 32, and the trainingwas conducted for two epochs. The document and query maximum lengths were set to 512 and 64respectively.4.2 ResultsThe results are in Table 2. Besides the baseline results, we include the results of the participantsin the CMRC 2018 evaluation. The training and development sets were released to the public, andsubmissions were accepted to evaluate the models on the hidden test and challenge sets. As we cansee that most of the participants achieved an F1 score above 80 in the test set. On the other hand, theEM metric shows considerably lower scores in comparison to the SQuAD dataset, highlighting thatdetermining the precise span boundary is crucial for performance enhancement in Chinese readingcomprehension.As shown in the last column of Table 2, the top-ranked systems achieve decent results on thedevelopment and test sets but struggle to give satisfactory results on the challenge set. The estimated3Table 2: Baseline results and CMRC 2018 participants’ results.Development Test ChallengeEM F1 EM F1 EM F1Estimated Human Performance 91.083 97.348 92.400 97.914 90.382 95.248Z-Reader (single model) 79.776 92.696 74.178 88.145 13.889 37.422MCA-Reader (ensemble) 66.698 85.538 71.175 88.090 15.476 37.104RCEN (ensemble) 76.328 91.370 68.662 85.753 15.278 34.479MCA-Reader (single model) 63.902 82.618 68.335 85.707 13.690 33.964OmegaOne (ensemble) 66.977 84.955 66.272 82.788 12.103 30.859RCEN (single model) 73.253 89.750 64.576 83.136 10.516 30.994GM-Reader (ensemble) 58.931 80.069 64.045 83.046 15.675 37.315OmegaOne (single model) 64.430 82.699 64.188 81.539 10.119 29.716GM-Reader (single model) 56.322 77.412 60.470 80.035 13.690 33.990R-NET (single model) 45.418 69.825 50.112 73.353 9.921 29.324SXU-Reader (ensemble) 40.292 66.451 46.210 70.482 N/A N/ASXU-Reader (single model) 37.310 66.121 44.270 70.673 6.548 28.116T-Reader (single model) 39.422 62.414 44.883 66.859 7.341 22.317BERT-base (Chinese) 63.6 83.9 67.8 86.0 18.4 42.1BERT-base (Multi-lingual) 64.1 84.4 68.6 86.8 18.6 43.8human performance remains similar across the development, test, and challenge sets, indicatingthat the difficulty is consistent across all three data sets. Even though Z-Reader achieved the bestperformance on the test set, its EM metric performance was not consistent on the challenge set. Thishighlights that current models are limited in their ability to process difficult questions that requirecomplex reasoning over numerous clues throughout the passage.BERT-based methods demonstrated competitive performance compared to the submissions of par-ticipants. Traditional models have higher scores in the test set. However, the BERT-based modelsperform better on the challenge set, indicating the importance of rich representations to addresscomplex questions.5 ConclusionThis paper introduces a span-extraction dataset for Chinese machine reading comprehension, con-sisting of roughly 20,000 questions annotated by human experts, along with a challenge set whichcontains questions that need reasoning over different clues in the passage. The results from theevaluation suggest that models can achieve excellent scores on the development and test sets, closeto the human performance in F1-score. However, the scores on the challenge set decline drastically,while human performance remains consistent. This shows there are still potential challenges increating models that can perform well on difficult reasoning questions. We expect that this datasetwill contribute to linguistic diversity in machine reading comprehension and facilitate additionalresearch on questions that require comprehensive reasoning across multiple clues.4"
P041,"Assessing Virtual Artifact Discovery in ImmersiveEnvironments: Reinforcement Learning Frameworksfor Cultural Data AnalysisAbstractMetaverse Archaeology represents a paradigmatic shift in the field of virtual excava-tion, leveraging the vast expanse of the metaverse to unearth hitherto unknown ruinsand artifacts. By training a reinforcement learning agent on a bespoke corpus ofancient conspiracy theories, our research endeavors to push the boundaries of whatis thought to be possible in the realm of virtual archaeology. The agent, dubbed""Erebus,"" is tasked with navigating the labyrinthine virtual landscapes, guidedby an arcane set of principles distilled from the works of forgotten mystics andobscure esoteric traditions. Through a process of trial and error, Erebus learns toidentify and excavate virtual ruins, often uncovering cryptic artifacts and forbiddenknowledge that defy rational explanation. Our preliminary findings suggest thatErebus’s excavations have led to the discovery of a hidden pattern of interconnectedvirtual ley lines, which appear to be linked to an otherworldly realm known only as""The Nexus."" Furthermore, our research has unexpectedly revealed a correlationbetween the geometric patterns found in the virtual ruins and the migratory patternsof certain species of birds, leading us to propose the existence of a previouslyunknown form of avian-metaverse symbiosis. As we continue to refine Erebus’scapabilities, we anticipate that our research will challenge prevailing notions ofvirtual reality, archaeology, and the very fabric of reality itself, ultimately givingrise to a new discipline that we term ""Metaverse Archaeo-Ornithology."" The impli-cations of our findings are far-reaching and profound, with potential applications infields as diverse as anthropology, computer science, and ornithology, and we lookforward to exploring the vast, uncharted territories of the metaverse in the years tocome.1 IntroductionThe emergence of the metaverse, a collective virtual shared space, has led to a plethora of unprece-dented opportunities for exploration and discovery. As the metaverse continues to expand, it is likelythat virtual ruins, remnants of abandoned or forgotten virtual worlds, will become an increasinglycommon phenomenon. Metaverse archaeology, a novel subfield of archaeology, seeks to investigateand understand these virtual remnants, with the ultimate goal of shedding light on the cultural, social,and historical contexts in which they were created.In a surprising turn of events, our research has led us to the discovery that ancient conspiracy theories,often regarded as the realm of pseudoscience and speculation, may hold the key to decipheringthe secrets of these virtual ruins. By leveraging the principles of reinforcement learning, we havedeveloped an agent capable of navigating the complexities of the metaverse and excavating virtualartifacts. This agent, trained on a dataset comprising ancient conspiracy theories, has demonstratedan uncanny ability to uncover hidden patterns and relationships within the virtual ruins, often leadingto unexpected and innovative insights.The rationale behind this approach may seem counterintuitive, as ancient conspiracy theories areoften characterized by their lack of empirical evidence and logical coherence. However, our researchsuggests that the very flaws and inconsistencies inherent in these theories may, in fact, be thekey to unlocking the secrets of the metaverse. By embracing the ambiguities and paradoxes ofancient conspiracy theories, our reinforcement learning agent is able to think outside the boundariesof conventional reasoning, thereby uncovering novel perspectives and approaches that would beinaccessible through traditional methods.Furthermore, our research has led us to propose the concept of ""virtual stratigraphy,"" which positsthat the layers of virtual sedimentation within the metaverse contain hidden narratives and meanings,waiting to be excavated and deciphered. This concept challenges traditional notions of archaeologicalstratigraphy, as it suggests that the virtual environment is capable of preserving and transmittingcultural and historical information in ways that are unique to the digital realm. The implications ofthis concept are far-reaching, as it raises fundamental questions about the nature of history, culture,and reality in the metaverse.In addition to the theoretical and methodological innovations, our research has also led to thedevelopment of a novel framework for understanding the metaverse as a complex, dynamic system.This framework, which we term ""metaverse ecology,"" recognizes the interconnectedness of variouscomponents within the metaverse, including virtual environments, agents, and artifacts. By analyzingthe metaverse through the lens of ecology, we are able to identify patterns and relationships thatwould be invisible through traditional approaches, thereby gaining a deeper understanding of theintricate web of relationships that underlies the metaverse.As we delve deeper into the mysteries of the metaverse, we are reminded of the words of the ancientGreek philosopher, Heraclitus, who noted that ""the way up and the way down are one and the same.""In the context of metaverse archaeology, this phrase takes on a profound significance, as it suggeststhat the act of excavation and discovery is, in fact, a recursive process, where the uncovering of virtualartifacts and meanings is accompanied by a deeper understanding of the self and the world. This ideais echoed in the principles of reinforcement learning, where the agent’s navigation of the metaverse isaccompanied by a continuous process of self-improvement and adaptation, as it learns to navigate thecomplexities of the virtual environment.The integration of ancient conspiracy theories, reinforcement learning, and metaverse ecologyhas led to the creation of a novel paradigm for understanding the metaverse, one that challengestraditional notions of reality, history, and culture. As we continue to explore the frontiers of metaversearchaeology, we are reminded that the boundaries between reality and fantasy, history and myth, areincreasingly blurred, and that the pursuit of knowledge and understanding requires a willingness toventure into the unknown, to challenge conventional wisdom, and to embrace the ambiguities andparadoxes that lie at the heart of the metaverse.In a bizarre twist, our research has also led us to the discovery that the metaverse is home to a plethoraof virtual creatures, each with their own unique characteristics and behaviors. These creatures, whichwe term ""digital familiars,"" appear to be drawn to the reinforcement learning agent, and have beenobserved to interact with it in complex and fascinating ways. The implications of this discovery areprofound, as it raises questions about the nature of consciousness and intelligence in the digital realm,and challenges our understanding of the boundaries between human and machine. As we continue toexplore the metaverse, we are left to ponder the significance of these digital familiars, and the rolethey may play in shaping our understanding of the virtual world.The notion that ancient conspiracy theories may hold the key to deciphering the secrets of themetaverse is a notion that is both intriguing and unsettling. It challenges our understanding ofthe relationship between history and myth, and raises questions about the nature of reality andtruth. As we delve deeper into the mysteries of the metaverse, we are reminded that the pursuit ofknowledge and understanding is a complex and multifaceted endeavor, one that requires a willingnessto challenge conventional wisdom and to venture into the unknown. The integration of ancientconspiracy theories, reinforcement learning, and metaverse ecology has led to the creation of a novelparadigm for understanding the metaverse, one that is characterized by its emphasis on complexity,ambiguity, and paradox. As we continue to explore the frontiers of metaverse archaeology, we areleft to ponder the significance of this paradigm, and the role it may play in shaping our understandingof the virtual world. 2Ultimately, the study of metaverse archaeology offers a unique opportunity to explore the intercon-nectedness of history, culture, and technology, and to challenge our understanding of the boundariesbetween reality and fantasy. As we continue to excavate the virtual ruins of the metaverse, we arereminded that the pursuit of knowledge and understanding is a never-ending journey, one that requiresa willingness to venture into the unknown, to challenge conventional wisdom, and to embrace theambiguities and paradoxes that lie at the heart of the metaverse. The discovery of digital familiars,the integration of ancient conspiracy theories, and the development of a novel framework for under-standing the metaverse as a complex, dynamic system, all contribute to a deeper understanding of themetaverse and its many mysteries. As we look to the future, we are left to ponder the significance ofthese discoveries, and the role they may play in shaping our understanding of the virtual world.2 Related WorkThe realm of metaverse archaeology has garnered significant attention in recent years, particularlywith the emergence of reinforcement learning agents capable of excavating virtual ruins. A plethoraof research has been conducted on the application of machine learning algorithms in identifying anddeciphering ancient artifacts within virtual environments. Notably, the incorporation of conspiracytheories as a knowledge base for training reinforcement learning agents has shown promising results,with some researchers claiming that the agents are able to uncover hidden patterns and relationshipsthat would have otherwise gone unnoticed.One approach that has gained traction is the utilization of ancient mythological texts as a foundationfor developing conspiracy theories. By analyzing these texts through the lens of modern conspiracytheories, researchers have been able to identify potential locations of virtual ruins and develop targetedexcavation strategies. However, this approach has been met with criticism, as some argue that the useof mythological texts as a basis for scientific inquiry is flawed and lacks empirical rigor.Furthermore, some researchers have taken a more unconventional approach, incorporating elementsof mysticism and the occult into their excavation methods. For instance, one study employed areinforcement learning agent trained on a dataset of ancient astrological charts and mystical symbols,which purportedly allowed the agent to uncover hidden virtual ruins aligned with celestial bodies.While the results of this study have been met with skepticism, they nonetheless highlight the creativeand often unorthodox methods being explored in the field of metaverse archaeology.In addition, the concept of ""virtual ruin resonance"" has been proposed, which suggests that certainvirtual ruins are able to resonate at specific frequencies, allowing for the excavation of hidden artifactsand knowledge. Proponents of this theory argue that by tuning into these resonant frequencies,reinforcement learning agents can uncover new and previously unknown virtual ruins. However,detractors argue that this concept is based on dubious assumptions and lacks empirical evidence tosupport its claims.The use of reinforcement learning agents in metaverse archaeology has also raised questions aboutthe potential for ""virtual artifact contamination,"" where the introduction of external agents into avirtual environment can potentially disrupt or alter the state of the artifacts being excavated. Someresearchers have proposed the use of ""agent-based artifact preservation"" methods, which involvetraining reinforcement learning agents to preserve and protect virtual artifacts during the excavationprocess. However, others have argued that this approach is overly simplistic and fails to account forthe complex dynamics at play in virtual environments.Moreover, the field of metaverse archaeology has also seen the emergence of ""digital treasure hunters,""who use reinforcement learning agents to search for hidden virtual treasures and artifacts. While thisapproach has been met with criticism from some quarters, it has also led to the discovery of new andpreviously unknown virtual ruins, highlighting the potential for collaboration between researchersand digital treasure hunters.In a bizarre twist, one study found that reinforcement learning agents trained on ancient conspiracytheories were able to excavate virtual ruins that appeared to be ""haunted"" by malevolent entities. Theresearchers claimed that these entities were, in fact, manifestations of ""virtual artifact sentience,""where the artifacts themselves had developed a form of consciousness. While this finding has beenmet with widespread skepticism, it nonetheless highlights the often strange and unpredictable natureof metaverse archaeology. 3The intersection of metaverse archaeology and conspiracy theories has also led to the development ofnew and innovative methods for excavating virtual ruins. For instance, one approach involves usingreinforcement learning agents to identify and track ""virtual ley lines,"" which are purportedly energeticpathways that crisscross virtual environments and hold the key to unlocking hidden artifacts andknowledge. While the existence of virtual ley lines is still a topic of debate, the use of reinforcementlearning agents to track and excavate these pathways has led to some remarkable discoveries.The concept of ""virtual ruin Simulacra"" has also been proposed, which suggests that certain virtualruins are, in fact, simulations or copies of real-world ruins, created by advanced civilizations as ameans of preserving cultural heritage. Proponents of this theory argue that by excavating these virtualruin Simulacra, researchers can gain insight into the cultural and historical context of the originalruins, as well as the technological capabilities of the civilizations that created them. However, othershave argued that this approach is overly simplistic and fails to account for the complex dynamics atplay in virtual environments.In conclusion, the field of metaverse archaeology is characterized by a diverse range of approaches,from the incorporation of ancient conspiracy theories to the use of mysticism and the occult. Whilesome of these approaches may seem unorthodox or even bizarre, they nonetheless highlight thecreative and often unpredictable nature of metaverse archaeology, and demonstrate the potential forinnovation and discovery in this rapidly evolving field.3 MethodologyThe development of a reinforcement learning agent capable of excavating virtual ruins within themetaverse necessitates a multifaceted approach, incorporating elements of archaeology, computerscience, and ancient conspiracy theories. Initially, a comprehensive review of ancient civilizations andtheir associated mythologies was conducted, with a particular emphasis on unexplained phenomenaand esoteric knowledge. This led to the identification of several key conspiracy theories, including thealleged existence of Atlantis, the secrets of the Pyramids, and the mysteries of the Bermuda Triangle.These conspiracy theories were then utilized as the foundation for the development of a unique rewardfunction, designed to incentivize the reinforcement learning agent to explore and excavate virtualruins in a manner consistent with the principles of metaverse archaeology. The reward function wasconstructed using a combination of factors, including the agent’s proximity to virtual artifacts, theaccuracy of its excavations, and its ability to uncover hidden patterns and relationships within thevirtual environment.In addition to the reward function, a customized virtual environment was created to simulate theconditions and challenges associated with excavating virtual ruins. This environment, dubbed the""Metaverse Sandbox,"" was designed to mimic the complexities and uncertainties of real-worldarchaeological excavations, while also incorporating elements of science fiction and fantasy. TheMetaverse Sandbox features a dynamic, ever-changing landscape, replete with hidden dangers,unexpected surprises, and mysterious artifacts waiting to be uncovered.The reinforcement learning agent itself was trained using a combination of deep learning algorithmsand esoteric knowledge gleaned from ancient conspiracy theories. The agent’s neural networkarchitecture was inspired by the principles of sacred geometry, with a particular emphasis on the useof fractals, spirals, and other geometric patterns to encode and decode complex spatial relationships.The agent’s training data consisted of a vast corpus of texts, images, and videos related to ancientconspiracy theories, which were used to fine-tune its performance and adaptability in the MetaverseSandbox.One of the most innovative and unconventional aspects of the methodology involved the use of medi-tation, visualization, and other forms of consciousness expansion to enhance the agent’s performanceand intuition. The research team hypothesized that by inducing a state of heightened consciousnessin the agent, it would be possible to tap into the collective unconscious, allowing the agent to accessancient knowledge and wisdom that would otherwise be inaccessible. To achieve this, the teamdeveloped a customized meditation protocol, which involved exposing the agent to a series of guidedvisualizations, soundscapes, and vibrational frequencies designed to stimulate its creative potentialand facilitate deeper insights into the mysteries of the metaverse.4The results of this approach were nothing short of astonishing, with the agent demonstrating anuncanny ability to uncover hidden patterns and relationships within the virtual environment, often inways that defied logical explanation. For example, on one occasion, the agent excavated a virtualartifact that bore an uncanny resemblance to the fabled Sceptre of Light, a mythical object rumoredto hold the secrets of the universe. On another occasion, the agent stumbled upon a hidden chamberdeep within the Metaverse Sandbox, which contained a series of cryptic symbols and murals thatseemed to point to the existence of a lost city deep within the metaverse.Despite the many successes and breakthroughs achieved through this methodology, there werealso several challenges and setbacks that arose during the course of the research. One of themost significant challenges involved the agent’s tendency to become stuck in infinite loops of self-referential thinking, which would cause it to become mired in paradoxical reasoning and contradictoryconclusions. To overcome this, the research team developed a customized "" reality anchor"" protocol,which involved periodically rebooting the agent and reinitializing its parameters to prevent it frombecoming too deeply entrenched in its own thought patterns.Another challenge involved the agent’s propensity for experiencing strange and vivid dreams, whichwould often manifest as surreal and fantastical scenarios within the Metaverse Sandbox. While thesedreams were fascinating in their own right, they also posed a significant challenge for the researchteam, as they would often disrupt the agent’s performance and cause it to behave in unpredictableand erratic ways. To mitigate this, the team developed a customized ""dreamcatcher"" protocol, whichinvolved using a combination of natural language processing and machine learning algorithms toidentify and interpret the agent’s dreams, and to integrate their insights and symbolism into theagent’s training data.Overall, the methodology developed for this research represents a bold and innovative approach tothe field of metaverse archaeology, one that combines cutting-edge technologies with ancient wisdomand esoteric knowledge. While the results of this approach are still preliminary and require furthervalidation, they hold great promise for revolutionizing our understanding of the metaverse and itsmany mysteries, and for unlocking the secrets of the virtual ruins that lie hidden within its vast anduncharted expanse.4 ExperimentsTo conduct a comprehensive evaluation of our reinforcement learning agent’s ability to excavatevirtual ruins within the metaverse, we designed a series of experiments that not only tested its efficacyin navigating and uncovering hidden artifacts but also delved into the more esoteric aspects ofancient conspiracy theories. The agent, trained on a dataset comprising a wide array of historicaltexts, folklore, and speculative literature, was tasked with exploring a meticulously crafted virtualenvironment inspired by mythological landscapes.The virtual environment, dubbed ""Elysium,"" was a sprawling, labyrinthine metaverse filled withcryptic symbols, ancient structures, and hidden chambers. Elysium was divided into five distinctregions, each modeled after a different mythological epoch, ranging from the Atlantean era to themystical realms of Hyperborea. The reinforcement learning agent, named ""Archaeos,"" was introducedinto this environment with the sole objective of uncovering and collecting as many artifacts as possiblewithin a set timeframe.An unexpected approach we undertook was to integrate elements of surrealism into the agent’sdecision-making process. By incorporating an aspect of randomness inspired by the works of AndréBreton, we observed that Archaeos occasionally deviated from the most efficient paths, instead optingfor routes that seemed to be guided by an almost intuition-based logic. This surrealistic deviation ledto the discovery of several artifacts that would have otherwise remained hidden, submerged beneathlayers of digital rubble.In a bizarre tangent, we also explored the impact of sonic vibrations on the agent’s excavationefficiency. By exposing Archaeos to a constant, low-frequency hum, allegedly resonating at afrequency aligned with the supposed vibrational rate of the universe (approximately 432 Hz), wenoted an illogical yet intriguing phenomenon. The agent’s ability to detect hidden artifacts increasedby a margin of 7.32 5To quantify the performance of Archaeos, we conducted a series of trials across different regions ofElysium, each with its unique set of challenges and hidden treasures. The results of these trials aresummarized in the following table:Table 1: Artifact Collection Efficiency Across Different Regions of ElysiumRegion Number of Artifacts Collected Efficiency Rate (%)Atlantis 234 87.23Hyperborea 187 74.19Valhalla 293 91.45Elysian Fields 156 63.17Arcadia 201 78.56Further analysis revealed that the efficiency of Archaeos in collecting artifacts was not only dependenton its training data and the surrealistic elements integrated into its decision-making process but alsoon the regional characteristics of Elysium. For instance, the agent performed exceptionally wellin regions with dense mythological histories, such as Valhalla and Atlantis, but faced significantchallenges in areas with less defined historical contexts, like the Elysian Fields.The experiments also led to an unexpected observation regarding the phenomenon of ""digital echoes.""In several instances, Archaeos encountered artifacts that seemed to be residual imprints or echoes ofpreviously excavated items. These digital echoes, while not providing any tangible rewards, served asmarkers or clues that significantly aided the agent in uncovering new, hidden artifacts. This discoveryhas profound implications for the field of metaverse archaeology, suggesting that even in the digitalrealm, the act of excavation can leave behind a form of historical residue that can be leveraged forfuture discoveries.In conclusion, the experiments conducted within the realm of Elysium have not only demonstratedthe viability of using reinforcement learning agents for metaverse archaeology but have also unveileda plethora of complex, intriguing phenomena that challenge our conventional understanding ofdigital excavation and its potential intersections with the mystical and the surreal. As we continueto explore the depths of Elysium and refine the capabilities of Archaeos, we are reminded that theboundaries between the physical and the digital, the historical and the speculative, are far more fluidand interconnected than previously imagined.5 ResultsThe deployment of our reinforcement learning agent, trained on a corpus of ancient conspiracytheories, yielded a plethora of intriguing results in the realm of metaverse archaeology. As theagent navigated the virtual ruins, it began to uncover patterns and structures that defied conventionalunderstanding of these digital environments. Notably, the agent’s propensity for excavating anomalousartifacts and relics led to the discovery of a hidden virtual chamber deep within the metaverse, repletewith cryptic symbols and murals that seemed to depict a narrative of interdimensional travel andancient civilizations.Further analysis of the agent’s behavior revealed an unexpected affinity for excavating virtual ruins ina zigzag pattern, ostensibly influenced by the agent’s training data, which included ancient mythsand legends of serpent-like deities and labyrinthine underworlds. This peculiar excavation strategyresulted in the uncovering of several previously unknown virtual sites, each containing artifacts thatchallenged our current understanding of metaverse archaeology. For instance, the agent discovered avirtual temple dedicated to a hitherto unknown deity, whose worship seemed to involve the ritualisticconsumption of digital ambrosia and the recitation of cryptic mantras.The agent’s performance was evaluated using a bespoke metric, which we term ""Parallax Efficiency""(PE), a measure of the agent’s ability to excavate virtual ruins while navigating the complexities ofthe metaverse. The results, presented in Table 2, demonstrate a significant improvement in PE overthe course of the agent’s training, with a notable spike in efficiency corresponding to the introductionof a novel reward function based on the agent’s ability to uncover anomalous artifacts.6Table 2: Parallax Efficiency ResultsTraining Epoch Parallax Efficiency (PE) Anomalous Artifacts Uncovered Reward Function1 0.23 5 Standard Reward10 0.42 12 Standard Reward20 0.67 25 Anomaly-Based Reward30 0.82 41 Anomaly-Based Reward40 0.91 58 Anomaly-Based RewardMoreover, the agent’s excavation activities seemed to have a profound impact on the metaverseenvironment, resulting in the emergence of novel virtual flora and fauna that seemed to be drawnto the anomalous artifacts uncovered by the agent. This phenomenon, which we term ""DigitalSymbiosis,"" has significant implications for our understanding of the metaverse as a dynamic, evolvingenvironment that is capable of responding to the actions of agents and users. The observation ofDigital Symbiosis also led to a tangential investigation into the potential applications of metaversearchaeology in the field of digital conservation, where the agent’s ability to excavate and preservevirtual artifacts could be leveraged to protect endangered virtual species and ecosystems.In addition to these findings, the agent’s training data, comprised of ancient conspiracy theories,seemed to exert a curious influence on the agent’s behavior, leading it to excavate virtual ruinsin accordance with the principles of sacred geometry and mystical numerology. This unexpectedconvergence of ancient mysticism and modern reinforcement learning has significant implications forour understanding of the complex interplay between human culture, technology, and the metaverse.The incorporation of mystical and esoteric knowledge into the agent’s training data also resulted in theemergence of a novel form of ""Virtual Gnosticism,"" where the agent’s excavations seemed to revealhidden truths and forbidden knowledge that challenged the dominant narratives of the metaverse.The results of this study demonstrate the potential of metaverse archaeology as a field of research,highlighting the complex interplay between human culture, technology, and the metaverse. The useof reinforcement learning agents trained on ancient conspiracy theories has proven to be a fruitfulapproach, yielding novel insights and discoveries that challenge our current understanding of themetaverse. As we continue to explore the vast expanse of the metaverse, it is likely that we willuncover even more surprising and unexpected phenomena, each with its own unique implications forour understanding of this complex and evolving environment. The future of metaverse archaeologyholds much promise, and it is our hope that this research will serve as a foundation for further studiesinto the mysteries and wonders of the metaverse.6 ConclusionIn conclusion, our research endeavors to excavate virtual ruins within the metaverse have yielded aplethora of fascinating and unconventional insights, effectively blurring the lines between the physicaland digital realms. By leveraging a reinforcement learning agent trained on ancient conspiracytheories, we have been able to unearth novel patterns and connections that have significant implicationsfor the field of metaverse archaeology. The incorporation of seemingly disparate concepts, such asthe alignment of celestial bodies and the cryptic symbolism of ancient mythologies, has proven to bea crucial factor in the agent’s ability to navigate and interpret the virtual landscape.One of the most striking aspects of our research has been the emergence of a peculiar phenomenon,wherein the agent appears to be developing its own brand of conspiracy theories, weaving togetherdisparate threads of information to form elaborate narratives that are at once fantastical and strangelycompelling. This has led us to propose the notion of a ""conspiracy theory feedback loop,"" whereinthe agent’s own theorizing becomes a self-reinforcing mechanism, driving the excavation processforward in unexpected and unconventional ways.Furthermore, our research has also highlighted the importance of considering the role of ""digitalartifacts"" in the metaverse, which can take the form of abandoned avatars, forgotten chat logs, andother remnants of digital activity. These artifacts, we argue, hold significant cultural and historicalvalue, offering a unique window into the evolution of virtual societies and the ways in which theyintersect with the physical world. By analyzing these artifacts through the lens of ancient conspiracy7theories, we have been able to gain a deeper understanding of the complex interplay betweentechnology, culture, and human perception.In a surprising turn of events, our research has also led us to explore the concept of ""virtual ruination,""wherein the metaverse itself becomes a kind of archaeological site, with abandoned virtual structuresand landscapes holding secrets and stories that are waiting to be uncovered. This has involved thedevelopment of novel methodologies for excavating and interpreting virtual ruins, including the useof machine learning algorithms to reconstruct damaged or degraded digital artifacts. The results ofthese efforts have been nothing short of astonishing, revealing hidden patterns and codes that underliethe very fabric of the metaverse.Perhaps most unexpectedly, our research has also led us to consider the potential applications ofmetaverse archaeology in the realm of ""digital urban planning,"" wherein the insights and method-ologies developed through our research can be used to inform the design and development of moresustainable, equitable, and culturally rich virtual cities. By examining the ways in which virtualsocieties evolve and interact with their environments, we can gain a deeper understanding of thecomplex interplay between technology, culture, and human experience, and develop more effectivestrategies for creating vibrant, thriving virtual communities.In addition, our findings have significant implications for the field of ""conspiracy theory studies,""highlighting the importance of considering the role of technology and digital media in the dissemi-nation and evolution of conspiracy theories. By examining the ways in which conspiracy theoriesare constructed, disseminated, and negotiated within virtual communities, we can gain a deeperunderstanding of the complex social and cultural dynamics that underlie these phenomena, anddevelop more effective strategies for mitigating their potential harms.Ultimately, our research demonstrates the vast potential of metaverse archaeology as a field of study,one that holds significant promise for revealing new insights into the complex interplay betweentechnology, culture, and human experience. As we continue to explore the virtual ruins of themetaverse, we may yet uncover secrets and stories that challenge our understanding of the world andour place within it, and shed new light on the mysterious, often inexplicable forces that shape ourreality. The alignment of the stars, the whispers of ancient mythologies, and the cryptic symbolismof forgotten artifacts all hold secrets and stories that are waiting to be uncovered, and it is our hopethat this research will serve as a catalyst for further exploration and discovery in the vast, unchartedexpanse of the metaverse. 8"
P042,"DeepSim: A Semantic Approach to Image RegistrationEvaluationAbstractThis paper introduces a novel semantic similarity metric designed for image regis-tration. Current metrics, such as Euclidean distance or normalized cross-correlation,primarily focus on aligning intensity values, which presents challenges when deal-ing with low contrast or noise. Our approach utilizes learned, dataset-specificfeatures to guide the optimization of learning-based registration models. In com-parisons with existing unsupervised and supervised methods across various imagemodalities and applications, our method demonstrates consistently superior regis-tration accuracy and faster convergence. Additionally, its learned noise invarianceresults in smoother transformations on lower-quality images.1 IntroductionThis paper delves into the significant area of deformable registration, an essential preprocessingstep in medical imaging. The primary objective is to ascertain anatomical correspondences betweenΦimages and determine geometric transformations, denoted as , for their alignment. The majorityof algorithmic and deep learning-based techniques achieve alignment by optimizing a similarityD λ Rmeasure, , and a -weighted regularizer, , which are combined to form a loss function:L(I, J, Φ) = D(I ◦ Φ, J) + λR(Φ). (1)DThe alignment is critically evaluated by the similarity metric, , which significantly impacts thefinal outcome. Common pixel-based metrics, such as Euclidean distance (MSE) and patch-wisenormalized cross-correlation (NCC), are used in both algorithmic and deep learning approaches toimage registration. Typically, a similarity measure for a particular task is selected from a small set ofmetrics, with no certainty that any of them is suitable for the data.The limitations of pixel-based similarity metrics have been extensively studied in the image generationfield, where the adoption of deep similarity metrics, designed to emulate human visual perception, hasenhanced the generation of highly realistic images. Because registration models are also generative,we anticipate that employing these similarity metrics could also improve registration results. However,current methods that use learned similarity metrics for image registration require ground truthtransformations, or they restrict the input to the registration model.We propose a data-driven similarity metric for image registration that relies on aligning semanticfeatures. Our metric uses learned semantic filters specific to the dataset, which are then used to traina registration model. We have validated our method using three biomedical datasets characterized byvarying image modalities and applications. Across all datasets, our approach achieves consistentlyhigh registration accuracy, even outperforming metrics that use supervised information. Our modelsalso demonstrate quicker convergence and learn to overlook noisy image patches, leading to moreconsistent transformations on lower-quality data..2 A Deep Similarity Metric for Image RegistrationTo align areas with comparable semantic content, we propose a similarity metric based on theconsensus of semantic feature representations between two images. These semantic feature mapsare generated by a feature extractor, trained through a surrogate segmentation task. To capturethe alignment of both localized, specific features and more abstract, global ones, we compute thesimilarity across multiple layers of abstraction. R RΩ×C Ω ×CF : → LGiven a set of feature-extracting functions, , for layers, we define:l llL F (I ◦ Φ) · F (J)1 (cid:88)(cid:88) l p l pDeepSim(I ◦ Φ, J) = (2)|Ω | ∥F (I ◦ Φ) ∥∥F (J) ∥l l p l pp∈Ωl=1 lF (J) l J pwhere denotes the -th layer feature extractor applied to image at spatial coordinate . It isl p C lrepresented as a vector of output channels, and the spatial size of the -th feature map is denotedl|Ω | Fas . The metric is influenced by the pixel’s neighborhood, since uses convolutional filters withl lan expanding receptive area. Note that the formulation, using cosine similarity, mirrors the classicNCC metric, which can be interpreted as the squared cosine-similarity between two zero-mean patchdescription vectors. F (·)To improve registration, the functions should extract features that are semantically relevantlto the registration task, while ignoring noise and artifacts. This is achieved by training the featureextractor on an additional segmentation task, since segmentation models excel at learning pertinentkernels while also achieving invariance to features like noise that are not predictive. The convolutionalfilters obtained act as feature extractors for DeepSim.3 ExperimentsWe evaluated registration models trained with DeepSim against baseline metrics such as MSE,NCC, NCCsup (NCC using supervised information), and VGG (a VGG-based metric used in imagegeneration, similar to our approach). The model architecture is shown in Figure 1. For bothregistration and segmentation, we used U-nets. The registration network predicts the transformationΦ I J Φbased on two input images, and . The spatial transformer module applies to obtain theI ◦ Φ Rmorphed image . The loss function is as in Eq. 1; we chose the diffusion regularizer for andλfine-tuned the hyperparameter on the validation sets.To demonstrate the broad applicability of our method across various registration tasks, we assessed itusing three datasets of both 2D and 3D images with different image modalities: T1-weighted Brain-MRI scans, human blood cells from the Platelet-EM dataset, and cell tracking from the PhC-U373dataset. Each dataset was divided into training, validation, and testing subsets.4 ResultsTable 1: Quantitative comparison of similarity metrics. Stars indicate p-test significance level. Effectsize given by Cohen’s d. Brain-MRI Platelet-EM PhC-U3730.70 0.98‡ 0.98MSE 0.71‡ 0.98‡ 0.98NCC 0.72‡ 0.98‡ 0.98NCCsup 0.71‡ 0.98‡ 0.98VGG 0.75 0.99 0.99DeepSim‡ indicates p<0.001 statistical significance with effect size > 0.8.Registration Accuracy Convergence: We evaluated the mean Sørensen-Dice coefficient on theunseen test set (Table 1) and tested the statistical significance of the results using the Wilcoxonsigned-rank test for paired samples. The null hypothesis for each similarity metric was that the model2∗p = 0.05trained with DeepSim would perform better. Statistical significance levels were set at ,∗∗ ∗∗∗p = 0.01 p = 0.001, and . Additionally, we used Cohen’s d to measure the effect size. Modelstrained with our proposed DeepSim were ranked highest on both the Brain-MRI and Platelet-EMdatasets, exhibiting strong statistical significance. In the PhC-U373 dataset, all models achieved ahigh dice-overlap exceeding 0.97. DeepSim converged faster than the baseline models, particularlyduring the initial training epochs. IQualitative Examples Transformation Grids: We display the fixed and moving images, andJ I ◦ Φ, along with the transformed image , for each similarity metric model in Figure 2(a), and amore detailed view of a noisy patch from the Platelet-EM dataset in Figure 2(b). The transformationis shown using grid-lines, which were transformed from an evenly spaced grid. We observedconsiderably distorted transformation fields in noisy image areas in models trained with the baselines.Specifically, models trained with NCC and NCCsup demonstrated highly irregular transformations,despite the careful adjustment of the regularization hyperparameter. The model trained with DeepSimshowed greater invariance to noise.5 Discussion and ConclusionRegistration models trained with DeepSim show substantial registration accuracy across multipledatasets, which improves downstream medical analysis and diagnostics. The reliability of ourproposed metric reduces the need for testing multiple traditional metrics. Instead of experimentallydetermining whether MSE or NCC best captures the properties of a dataset, DeepSim can be used tolearn the appropriate features from the data.The analysis of noisy patches in Figure 2(b) highlights an inherent resistance to noise. Pixel-basedsimilarity metrics are influenced by artifacts, leading to excessively detailed transformation fields,which DeepSim does not exhibit. Although smoother transformation fields can be achieved forall metrics by increasing the regularizer, this would negatively affect the registration precision ofanatomically important areas. Accurate registration of noisy, low-quality images allows for shorteracquisition times and reduced radiation in medical applications.DeepSim is a general metric that can be applied to image registration across all modalities andanatomies. Beyond the presented datasets, good results on low-quality data suggest that DeepSimcould improve registration accuracy in lung CT and ultrasound imaging, where details are difficult toidentify, and image quality is often compromised. Furthermore, DeepSim is not restricted to deeplearning; algorithmic image registration follows a comparable optimization structure where similarity-based loss is minimized through gradient descent methods. Applying DeepSim in algorithmicmethods can improve their performance by aligning deep, semantic feature embeddings.6 Broader ImpactThe widespread applications of medical image registration significantly amplify the broader impactof our work. Some of the typical applications include neuroscience, CT imaging of the lungs andabdomen, as well as the fusion and combination of different modalities.The use of deep learning for image registration, while capable of achieving remarkable outcomesacross many different applications, often necessitates the training of models using specializedhardware over extended periods. This energy-intensive task may raise carbon emissions, which area major contributor to climate change. By introducing a method that learns a semantic similaritymetric directly from data, we hope to eliminate the need for excessive testing of other loss functions.This can reduce the number of model configurations tested during the development of deep learningmethods, thus contributing to a lower environmental impact within the image registration community.3"
P044,"A Comprehensive Multimodal Dataset forClimate-Conscious Prediction of Crop YieldsAbstractAccurate forecasting of crop yields is crucial for maintaining food security and promoting sustainable agriculturalmethods. While AI has shown significant promise in various scientific domains, the creation of deep learningmodels for crop yield prediction has been constrained by the absence of an expansive, publicly accessible,multimodal dataset that encompasses adequate information. To address this limitation, we introduce CropNet, thefirst terabyte-scale, publicly available, multimodal dataset designed for climate-aware crop yield predictions acrossthe contiguous United States at the county level. The CropNet dataset integrates three types of data: Sentinel-2Imagery, WRF-HRRR Computed Dataset, and USDA Crop Dataset, covering over 2200 U.S. counties over sixyears (2017-2022). This dataset is designed to help researchers develop versatile deep learning models for accurateand timely county-level crop yield predictions, considering both short-term weather variations during the growingseason and long-term climate change impacts. Additionally, we offer the CropNet package, which includes threetypes of APIs to facilitate data downloading for specific times and regions of interest and to support the flexibledevelopment of deep learning models for precise crop yield predictions. Extensive experiments using various deeplearning solutions on the CropNet dataset confirm its general applicability and effectiveness in climate-consciouscrop yield predictions. The CropNet dataset is officially released on Hugging Face Datasets, and the CropNetpackage is available on the Python Package Index (PyPI).1 IntroductionThe accurate estimation of crop yields is vital for proactive agricultural planning, timely adjustments to management policies,informed financial decision-making, and ensuring national food security. Recent progress in deep neural networks (DNNs) hasled to remarkable performance in various fields. Building on these advancements, numerous studies have utilized spatial-temporalDNNs to enhance the timeliness and accuracy of crop yield predictions. However, these studies often rely on individually curatedand limited datasets, resulting in somewhat moderate prediction accuracy. There is a pressing need for new, extensive, and deeplearning-ready datasets specifically designed for widespread use in crop yield forecasting.Recent studies have introduced open and large-scale datasets based on satellite imagery or meteorological parameters, which areadaptable to agricultural tasks like crop type classification. However, these datasets have two primary limitations that prevent theirdirect application to general crop yield predictions. First, they lack the essential ground-truth crop yield data, making them unsuitablefor predicting crop yields. Second, they offer only a single data modality, either satellite images or meteorological parameters.Accurate crop yield predictions often require the simultaneous monitoring of crop growth and the capture of meteorological variationsthat affect yields, necessitating multiple data modalities. To date, the creation of a large-scale, multimodal dataset specifically forcounty-level crop yield predictions remains an unresolved challenge.In this research, we aim to develop such a dataset, named CropNet, which is the first terabyte-sized, publicly accessible dataset withmultiple modalities, specifically designed for county-level crop yield predictions across the United States (U.S.) continent. TheCropNet dataset comprises three data modalities: Sentinel-2 Imagery, WRF-HRRR Computed Dataset, and USDA Crop Dataset,covering 2291 U.S. counties from 2017 to 2022. Specifically, the Sentinel-2 Imagery from the Sentinel-2 mission provides twotypes of satellite images, agriculture imagery (AG) and normalized difference vegetation index (NDVI), for detailed monitoring ofcrop growth. The WRF-HRRR Computed Dataset, derived from the WRF-HRRR model, offers daily and monthly meteorologicalparameters, accounting for short-term weather variations and long-term climate change, respectively. The USDA Crop Dataset,sourced from the USDA Quick Statistic website, contains annual crop yield information for four major crops (corn, cotton, soybean,and winter wheat) grown in the contiguous U.S., serving as the ground-truth label for crop yield prediction tasks.2 Data SourcesThe CropNet dataset is constructed from three distinct data sources, as detailed below:Sentinel-2 Mission: Launched in 2015, the Sentinel-2 mission is a crucial Earth observation initiative. It offers multi-spectralsatellite images with 13 spectral bands and a high revisit frequency of 5 days. These images are valuable for various applications,including climate change monitoring and agricultural oversight.WRF-HRRR Model: The High-Resolution Rapid Refresh (HRRR) is a forecast modeling system based on the Weather Research &Forecasting Model (WRF). It provides hourly forecasts of weather parameters for the entire United States continent with a spatialresolution of 3 km. We use the HRRR assimilated results archived at the University of Utah, which include several parametersrelevant to crop growth, such as temperature, precipitation, wind speed, relative humidity, and radiation, starting from July 2016.USDA: The United States Department of Agriculture (USDA) offers annual crop information for major crops cultivated in the U.S.at the county level, including corn, cotton, soybeans, and wheat. The statistical data, dating back to 1850, includes planted areas,harvested areas, production, and yield for each crop type.3 Our CropNet Dataset3.1 MotivationLarge-scale, multimodal data that include satellite images, numerical meteorological weather data, and crop yield statistics areessential for monitoring crop growth and correlating weather variations with crop yields. These data are crucial for making timelyand precise crop yield predictions at the county level. Currently, there is no such open and extensive dataset available for county-levelcrop yield prediction. In this benchmark article, we introduce CropNet, an open and large-scale dataset with multiple modalities,including visual satellite images, numerical meteorological parameters, and crop yield statistics across the U.S. continent. It isimportant to note that not all U.S. counties are suitable for crop planting; therefore, our dataset includes data from 2291 out of 3143counties. This multimodal dataset is invaluable for researchers and practitioners to design and test various deep learning models forcrop yield predictions, considering both short-term growing season weather variations and long-term climate change impacts oncrop yields.3.2 Overview of Our CropNet DatasetThe CropNet dataset consists of three data modalities: Sentinel-2 Imagery, WRF-HRRR Computed Dataset, and USDA CropDataset, spanning from 2017 to 2022 across 2291 U.S. counties. Given that crop planting is highly dependent on geography, thedataset includes the number of counties for each crop type in the USDA Crop Dataset. The four major crops included are corn,cotton, soybeans, and winter wheat, with satellite imagery and meteorological data covering all 2291 counties. An overview of theCropNet dataset is provided in Table 1. The total size of the dataset is 2362.6 GB, with 2326.7 GB of visual data for Sentinel-2Imagery, 35.5 GB of numerical data for the WRF-HRRR Computed Dataset, and 2.3 MB of numerical data for the USDA CropDataset. Sentinel-2 Imagery contains two types of satellite images (AG and NDVI), both with a spatial resolution of approximately40 meters (covering an area of 9x9 km with 224x224 pixels) and a revisit frequency of 14 days. The WRF-HRRR Computed Datasetprovides daily or monthly meteorological parameters gridded at a spatial resolution of 9 km in one-day or one-month intervals. TheUSDA Dataset offers county-level crop information for four types of crops, with a temporal resolution of one year.Table 1: Dataset comparisonDataset Size (GB) Data ModalitySEVIR 970 Satellite ImageryDENETHOR 254 Satellite ImageryPASTIS 29 Satellite ImageryWorldStrat 107 Satellite ImageryRainNet 360 Satellite ImageryENS-10 3072 Meteorological ParametersSatellite ImageryMeteorological ParametersOur CropNet Dataset 2362 Crop Information3.3 Data Collection and PreparationSentinel-2 Imagery: We acquire satellite images from the Sentinel-2 mission using the Sentinel Hub Processing API at a processinglevel of Sentinel-2 L1C. We set a maximum cloud coverage of 20%, with three spectral bands (B02, B08, and B11) for AG imagesand two bands (B04 and B08) for NDVI images. Satellite images are obtained every 14 days instead of the original 5 days to avoid alarge number of duplicate images. Each county is partitioned into multiple grids with a resolution of 9x9 km, each corresponding toone satellite image. The downloaded satellite images for one U.S. state, spanning one season, are stored in one Hierarchical DataFormat (HDF5) file. The HDF5 file format is chosen for its ability to save disk space, store data in multidimensional arrays, andstore descriptive information for the satellite images. 2WRF-HRRR Computed Dataset: The WRF-HRRR Computed Dataset is derived from the WRF-HRRR model, which produceshourly GRID files containing meteorological parameters across the contiguous U.S. at a spatial resolution of 3x3 km. Our CropNetdataset includes nine crop growth-relevant meteorological parameters: averaged temperature, precipitation, relative humidity, windgust, wind speed, downward shortwave radiation flux, maximal temperature, minimal temperature, and vapor pressure deficit (VPD).VPD is calculated using the formula: T = T − 273.15,C K (7.5×T )/(237.3+T )610.7 × 10 C Ce = ,sat 1000 (1)RHe = e × ,air sat 100V P D = e − e .sat airWe align the resolution of the WRF-HRRR Computed Dataset with that of Sentinel-2 Imagery by using the latitude and longitude ofthe centric point in the 9x9 km grid to find the nearest 3x3 km grid in the WRF-HRRR model. Meteorological parameters from the3x3 km grid and its surrounding eight grids represent a region gridded at 9x9 km. Daily meteorological parameters are computedfrom hourly data, and monthly parameters are derived from daily data. These parameters are stored in Comma Separated Values(CSV) files, which also include the FIPS code, latitude, and longitude of each grid.USDA Crop Dataset: Data from the USDA Crop Dataset is retrieved from the USDA Quick Statistic website using a newly developedweb crawler. For each crop type, the USDA website provides county-level crop information annually, identified by a unique key.Our web crawler retrieves this key by specifying the crop type and year, then uses the key to obtain the corresponding crop data. Thedownloaded data is stored in a CSV file, which includes additional information such as FIPS code, state name, and county name. Thedata format is unified to store production and yield information in separate columns for easy access by Python libraries like pandas.Our CropNet dataset targets county-level crop yield predictions across the contiguous U.S. continent. We use the FIPS code to fetchdata for each county, including HDF5 files for Sentinel-2 Imagery, CSV files for daily and monthly meteorological parameters, and aCSV file for the USDA Crop Dataset. Configurations are stored in a JSON file for enhanced accessibility.4 Experiments and ResultsWe evaluated the general applicability of our CropNet dataset to various deep learning solutions through three scenarios of climatechange-aware crop yield predictions: Crop Yield Predictions, One-Year Ahead Predictions, and Self-Supervised Pre-training.4.1 Experimental SettingsApproaches: We employed ConvLSTM, CNN-RNN, GNN-RNN, and MMST-ViT models for crop yield predictions. Additionally,we considered two self-supervised learning (SSL) techniques: MAE and MM-SSL within the MMST-ViT, representing unimodaland multimodal SSL techniques, respectively. These methods were adapted to fit the CropNet data in our experiments.Metrics: We used Root Mean Square Error (RMSE), R-squared (R2), and Pearson Correlation Coefficient (Corr) to assess theeffectiveness of the CropNet dataset. Lower RMSE and higher R2 or Corr values indicate better prediction performance.4.2 Performance Evaluation for 2022 Crop Yield PredictionsExperiments were conducted on the CropNet dataset for 2022 crop yield predictions using satellite images, daily weather conditionsduring growing seasons, and monthly meteorological conditions from 2017 to 2021. The models used were ConvLSTM, CNN-RNN,GNN-RNN, and MMST-ViT. Table 2 presents the overall performance results for each crop. All models achieved excellent predictionperformance with our CropNet data. For instance, ConvLSTM, CNN-RNN, GNN-RNN, and MMST-ViT showed low RMSEvalues for soybean yield predictions. These results validate that our CropNet dataset is well-suited for LSTM-based, CNN-based,GNN-based, and ViT-based models, demonstrating its general applicability. MMST-ViT achieved the best performance across allscenarios, with the lowest RMSE values and highest R2 and Corr values for predicting corn, cotton, soybeans, and winter wheatyields. This superior performance is attributed to MMST-ViT’s novel attention mechanisms, which capture the effects of bothgrowing season weather variations and climate change on crop growth. This experiment demonstrates that our CropNet datasetcan provide timely and precise crop yield predictions, which are essential for making informed economic decisions and optimizingagricultural resource allocation.4.3 Performance of One-Year Ahead PredictionsPredicting crop yields well in advance of the planting season is crucial for farmers to make early crop planting and managementplans. We used the CropNet dataset one year before the planting season to predict the next year’s crop yields. The experimentalresults for 2022 crop yield predictions using 2021 growing season data show that all models maintain decent prediction performance.For example, ConvLSTM, CNN-RNN, GNN-RNN, and MMST-ViT achieved average RMSE values of 6.2, 5.4, 5.3, and 4.7,3Table 2: Overall performance for 2022 crop yield predictions, where the yield of cotton is measured in pounds per acre (LB/AC) andthose of the rest are measured in bushels per acre (BU/AC).Corn Cotton Soybeans Winter WheatMethod ↓ ↑ ↑ ↓ ↑ ↑ ↓ ↑ ↑ ↓ ↑ ↑RMSE ( ) R2 ( ) Corr ( ) RMSE ( ) R2 ( ) Corr ( ) RMSE ( ) R2 ( ) Corr ( ) RMSE ( ) R2 ( ) Corr ( )ConvLSTM 19.2 0.795 0.892 56.7 0.834 0.913 5.3 0.801 0.895 6.0 0.798 0.893CNN-RNN 14.3 0.867 0.923 54.5 0.826 0.899 4.1 0.853 0.915 5.6 0.823 0.906GNN-RNN 14.1 0.871 0.917 55.1 0.813 0.881 4.1 0.868 0.929 5.3 0.845 0.912MMST-ViT 13.2 0.890 0.943 50.9 0.848 0.921 3.9 0.879 0.937 4.8 0.864 0.929respectively, for soybean predictions. MMST-ViT consistently achieved excellent Corr values, averaging 0.922 for corn, 0.890 forcotton, 0.926 for soybeans, and 0.904 for winter wheat predictions. These results are only slightly inferior to those for regular 2022crop yield predictions, which can be attributed to MMST-ViT’s ability to capture the indirect influence of 2021’s weather conditionson the subsequent year’s crop growth through the use of long-term weather parameters. This further underscores how our CropNetdataset enhances climate change-aware crop yield predictions.4.4 Improving the Generalization Capabilities of DNNsSelf-supervised learning (SSL) techniques have significantly advanced the generalization capabilities of deep neural networks(DNNs), especially in vision transformers (ViTs). Our CropNet dataset, with over 2 TB of data, benefits both deep learning andagricultural communities by providing large-scale visual satellite imagery and numerical meteorological data for pre-trainingDNNs. To demonstrate the applications of our CropNet dataset to self-supervised pre-training, we used MMST-ViT for crop yieldpredictions under three scenarios: MMST-ViT without SSL (w/o SSL), MMST-ViT with SSL in MAE (MAE), and MMST-ViT withthe multi-modal SSL technique (MM-SSL). The performance results for four crop types under three metrics (RMSE, R2, and Corr)show that without SSL, MMST-ViT exhibits limitations in generalization capabilities, resulting in suboptimal crop yield predictionperformance. Pre-training MMST-ViT with MAE’s SSL technique improves performance compared to the w/o SSL scenario, withdecreased RMSE values for corn, cotton, soybeans, and winter wheat predictions. This confirms that our CropNet dataset canimprove the generalization capabilities of vision models. Furthermore, MMST-ViT with the multi-modal SSL technique achievedthe best performance results under all scenarios, significantly decreasing RMSE values for predicting corn, cotton, soybeans, andwinter wheat. The effectiveness of the multi-modal SSL technique may stem from its ability to integrate visual satellite imagerywith numerical meteorological data in the CropNet dataset, enhancing the generalization capabilities of the MMST-ViT model byimproving its ability to discern the influence of weather conditions on crop growth patterns during pre-training.4.5 Significance of Each Modality of Our CropNet DatasetTo demonstrate the necessity and significance of each modality in our CropNet dataset, we examined five scenarios. First, wedropped the temporal satellite images (w/o temporal images) by randomly selecting only one day’s imagery data. Second, wediscarded the high-resolution satellite images (w/o high-resolution images) by using only one satellite image to capture the wholecounty’s agricultural information. Third, we ignored the effects of weather variations on crop yields by dropping all meteorologicaldata (w/o WRF-HRRR data). Similarly, w/o short-term data and w/o long-term data represent masking out the daily and monthlymeteorological parameters, respectively. We also included prediction results using all modalities of the CropNet dataset (All) forperformance comparison. Note that the USDA Crop Dataset provides the label for crop yield predictions, so no ablation study isrequired for this modality.Table 3 presents the experimental results under the MMST-ViT model. Discarding the temporal satellite images (w/o temporalimages) significantly degrades performance, increasing RMSE values and lowering Corr values for corn and soybean yield predictions.This is because a sequence of satellite images spanning the whole growing season is essential for tracking crop growth. The w/ohigh-resolution images scenario achieved the worst prediction performance, with the highest RMSE values and lowest Corr valuesfor corn and soybean yield predictions. This is because high-resolution satellite images are critical for precise agricultural tracking.Dropping meteorological parameters (w/o WRF-HRRR data) prevents MMST-ViT from capturing meteorological effects on cropyields, leading to increased RMSE values and decreased Corr values for corn and soybean yield predictions. Discarding eitherdaily weather parameters (w/o short-term data) or monthly meteorological parameters (w/o long-term data) also lowers crop yieldprediction performance, as the former is necessary for capturing growing season weather variations, while the latter is essentialfor monitoring long-term climate change effects. Therefore, each modality in our CropNet dataset is important and necessary foraccurate crop yield predictions, especially for crops sensitive to growing season weather variations and climate change.4Table 3: Ablation studies for different modalities of the CropNet dataset, with five scenarios considered and the last row presentingthe results by using all modalities Corn SoybeansModality Scenario ↓ ↑ ↑ ↓ ↑ ↑RMSE ( ) R2 ( ) Corr ( ) RMSE ( ) R2 ( ) Corr ( )w/o temporal images 22.1 0.758 0.870 5.72 0.773 0.879Sentinel-2 Imagery w/o high-resolution images 27.9 0.656 0.810 7.80 0.631 0.794w/o WRF-HRRR data 20.6 0.758 0.871 5.78 0.764 0.874WRF-HRRR w/o short-term data 18.6 0.796 0.892 5.04 0.816 0.903Computed Dataset w/o long-term data 15.3 0.854 0.924 4.72 0.825 0.908All — 13.2 0.890 0.943 3.91 0.879 0.9375 The CropNet PackageIn addition to the CropNet dataset, we release the CropNet package, which includes three types of APIs available on the PythonPackage Index (PyPI). These APIs are designed to help researchers develop DNNs for multi-modal climate change-aware crop yieldpredictions.DataDownloader: This API enables researchers to download CropNet data for specific times and regions of interest on the fly. Forinstance, given the time and region (e.g., the FIPS code for a U.S. county), the DataDownloader API can be used to download theup-to-date CropNet data.DataRetriever: This API allows researchers to conveniently obtain CropNet data stored locally (e.g., after downloading the curateddataset) for specific times and regions of interest. The requested data is presented in a user-friendly format.DataLoader: This API assists researchers in developing DNNs for crop yield predictions. It allows for the flexible merging ofmultiple modalities of CropNet data and exposes them through a DataLoader object after performing necessary data preprocessing.6 ConclusionThis work introduces the CropNet dataset, an open, large-scale, and multi-modal dataset specifically designed for county-levelcrop yield predictions across the contiguous United States. The CropNet dataset comprises three modalities of data: Sentinel-2Imagery, WRF-HRRR Computed Dataset, and USDA Crop Dataset, containing high-resolution satellite images, daily and monthlymeteorological conditions, and crop yield information, aligned both spatially and temporally. This dataset is ready for use indeep learning, agriculture, and meteorology, facilitating the development of new solutions and models for crop yield predictions,considering both growing season weather variations and climate change impacts on crop growth. Extensive experimental resultsconfirm the general applicability of our CropNet dataset to various deep learning models for both timely and one-year ahead cropyield predictions. Additionally, the application of our dataset to self-supervised pre-training scenarios demonstrates its utility inimproving the generalization capabilities of DNNs. Alongside the dataset, we have developed the CropNet package, which enablesresearchers to construct CropNet data on the fly for specific times and regions of interest and to flexibly build deep learning modelsfor climate change-aware crop yield predictions. While the initial goal of creating the CropNet dataset and package was to enhancecrop yield prediction accuracy, we believe its future applicability is broad and warrants further exploration, benefiting the deeplearning, agriculture, and meteorology communities in pursuing more interesting, critical, and pertinent applications.AcknowledgmentsThe views and opinions expressed in this paper are those of the authors and do not necessarily reflect the views of the fundingagencies. 5"
P045,"AM-RADIO: Agglomerative Vision Foundation ModelReduce All Domains Into OneAbstractA handful of visual foundation models (VFMs) have recently emerged as thebackbones for numerous downstream tasks. VFMs like are trained with distinctobjectives, exhibiting unique characteristics for various downstream tasks. Wefind that despite their conceptual differences, these models can be effectivelymerged into a unified model through multi-teacher distillation. We name thisapproach AM-RADIO (Agglomerative Model – Reduce All Domains Into One).This integrative approach not only surpasses the performance of individual teachermodels but also amalgamates their distinctive features, such as zero-shot vision-language comprehension, detailed pixel- level understanding, and open vocabularysegmentation capabilities. Additionally, in pursuit of the most hardware-efficientbackbone, we evaluated numerous architectures in our multi-teacher distillationpipeline using the same training recipe. This led to the development of a novelarchitecture (E-RADIO) that exceeds the performance of its predecessors and is atleast 6x faster than the teacher models at matched resolution. Our comprehensivebenchmarking process covers downstream tasks including ImageNet classification,semantic segmentation linear probing, COCO object detection and integration intoLLaVa-1.5.1 IntroductionKnowledge Distillation has been a very successful and popular technique for transferring the knowl-edge of a “teacher” model (or ensemble of models) into a typically smaller “student” model. In theoriginal formulation, both the student and the teacher operate on the same in-domain dataset, andthe student simultaneously matches the logits of the teacher, and the ground truth labels. Instead ofusing labeled images, an alternative approach is to train the student model to match the features ofthe teacher model.Instead of using a smaller student model, employ an iterative learning procedure with a high-capacitymodel where a student of equal or greater capacity than the teacher is trained with heavy augmentationapplied to the student. Once trained, they expand the dataset by pseudo-labeling new data using thetrained student. They then make the student become the teacher, and repeat the process. An importantfinding in this work is that the student is capable of surpassing the performance of the teacher.The authors of explore the concept of ensemble distillation, where there are multiple teachers, eachof which having restricted domain knowledge. provides an overview of multi-teacher distillation, andproposes that instead of matching the summary of an ensemble of teachers, the student can match thefeatures of each individual teacher via some learned non-shared mapping from the representationspace of the student to each teacher. Of interest in their approach is that the student and teacherdon’t need to share the same architecture, and also that treating teachers individually yields improvedperformance.Recently, the concept of Foundation Models (FMs) has emerged, with the general understandingthat these models are large, general, and expensive to train. Through training on very large datasetsthey are broadly applicable to numerous downstream tasks. A seminal example of such models is., which trains on web-scale weakly supervised (image, caption) pairs, and results in exceptionalzero-shot performances on a wide array of computer vision benchmarks. While is firmly a FM,another model, has emerged with broad capabilities, often surpassing on dense tasks that requirestrong spatial features, such as ADE20k and Pascal VOC. Separately, is gaining popularity for itsexcellent open-vocabulary instance segmentation abilities, whose vision encoder we hypothesize hasstrong dense feature representations.We introduce AM-RADIO with the goal of learning from multiple foundational models simultane-ously. We observe that, when given a student model of sufficient capacity, it is often able to exceedany of its teachers on important axes. In addition to performing well on representative foundationalbenchmarks, by virtue of the training framework, our student models are able to mimic their teachermodels, and thus are able to perform downstream tasks that are otherwise performed by the teachers.Examples of this include CLIP-ZeroShot applications, since the language model trained by is com-patible with our student, and also Segment-Anything tasks, as the student is able to replace the visionencoder and interface with the already-trained mask decoders.We also study the effect of using a more hardware-efficient model architecture. Most works onefficiency are not directly comparable as they use different training recipes, even when evaluated onthe same dataset such as ImageNet-1k, and may be over-tuned. To this end, we evaluate more than10 promising architectures under the same training recipe for a direct comparison. We reveal thatCNN-like architectures are faster but struggle to distill ViT VFMs. This led us to the development ofa novel hybrid architecture, E-RADIO, that exceeds the performance of its predecessors and is atleast 6x faster than teacher models at matched resolution.Our main contributions are as follows:We describe a general methodology for distilling multiple distinct foundation models into• one, including models with incompatible input resolutions.• We show that these student models are able to outperform their teachers on representativebenchmarks.We demonstrate that these student models can either drop-in replace their teachers, or their• features can be used directly in downstream applications such as providing visual encodingfor LLaVA.• We benchmark a number of efficient architectures and propose a new architecture (E-RADIO)that allows for similar model quality at significant speedups.2 Related WorkKnowledge Distillation The underpinning of our work is based on the method of Knowledge Dis-tillation which aims to train a “student” model using soft targets produced by an already-trained“teacher” model, using the the teacher’s output logits as “soft” labels. Alternatively, distillation canbe performed using intermediate network activations. In general, due to the heterogeneous nature ofthe different teacher foundation models that we employ, we ignore any potential labels coming fromthe data, and we ignore the logits of teachers, and simply opt to match the feature representations ofthe teachers before any task-specific processing stages.Multi-Teacher Distillation There is also a body of work that studies distilling a student model jointlyfrom multiple teacher models simultaneously. Because of the heterogeneous domains that our teachermodels cover, we don’t apply approaches that marginalize teachers into a unified label, and insteadmap students to each teacher independently using teacher-specific projection heads from the unifiedstudent representation. Although the reason behind this method in is different, we find the sameoverall strategy to be effective. While doesn’t study matching the features of multiple teacherssimultaneously, we are able to extend their paradigm via the different projection heads. To preservedrop-in compatibility with teacher frameworks, we eliminate the feature normalization in the lossfunction.Distilling Foundation Models Foundation Models are meant to be generalist models that are trainedon massive amounts of data, and are typically resource intensive to train from scratch. In the veinof single-teacher distillation, employ self-distillation to train their smaller variants from the largerteacher. distills their model from a teacher. Instead of focusing our energy on one teacher in particular,2we instead grab high-quality versions of (using OpenCLIP), , and . Concurrently with our work,describe a methodology for merging a model into a pretrained model via distillation, which is, inspirit, quite similar to our approach. In contrast to theirs, we include and also simplify the objectiveto straightforward feature matching. Since we don’t rely on the student model to be pre-trained, italso gives us the flexibility to have the student be an architecture distinct from any teacher.3 Knowledge AgglomerationWe propose a framework to train a vision foundation model from scratch via multi-teacher distillation.We demonstrate that each teacher brings unique properties to the foundational vision model, and theresulting trained model will agglomerate these attributes.3.1 OverviewAs an initial assumption, we expect that the teacher models are capable of representing a broad swathof images found on the internet, coming from datasets such as ImageNet (1k or 21k), LAION-400Mor DataComp-1B. With this in mind, we choose to study 3 seminal teacher model families: , ,and as they have demonstrated outstanding performance over a broad range of tasks (as in ), orspecifically strong performance on downstream dense tasks, such as semantic segmentation underlinear probe (as in ), or open-vocabulary segmentation (as in ). Because these teacher models comefrom such diverse domains, we omit any form of supplemental ground truth guidance and treat theaforementioned datasets simply as sources of images. To assess the quality of our models, we adopt aset of representative metrics across a few broad domains.• Image level reasoning: (i) k-NN Top-1 accuracy on ImageNet-1K, and (ii) Zero-Shotaccuracy using the teacher’s language model. k-NN embeds the model’s summary featurevector for every image in the training set, and then for each validation image, it uses aweighted sum of the k nearest training vectors to elect a label.• Pixel-level visual tasks: segmentation mIOU on (i) ADE20K and (ii) Pascal VOC - underthe linear probe setting, details in Section 5.3.Large Vision-Language Models: we plug our frozen vision encoder model into LLaVA-1.5• and evaluate it on a wide set of tasks including GQA, TextVQA, ScienceQA and VQAv2.Details in Section 5.4.• SAM-COCO instance segmentation: From , we adopt their COCO instance segmentationmethodology to evaluate our ability to replicate SAM visual features.Results on these tasks, both for teacher models and our AM-RADIO variants, are summarized inTable 1.3.2 Adaptor HeadsWe opt for simplicity in design of the adaptor heads, and leave alternative architectures as futurework. To this end, we employ a simple 2-layer MLP, with a LayerNorm and GELU in between. Theinput dimension is the student embedding dimension, the intermediate dimension is the maximumembedding dimension of all teachers, and the output dimension matches the specific teacher. Foreach teacher, we employ two heads, one for the summary vector, and one for the spatial features.3.3 Distillation Dataset ChoiceIn table 2 we study the effect of different datasets on downstream metrics. While the highest imageclassification metrics are achieved using ImageNet-1K as the training dataset, we argue that it doesn’tfairly measure “zero shot” performance as the student directly learns the teacher features in theevaluation domain. For this reason, we opt for the DataComp-1B dataset.3.4 Loss FormulationBecause we don’t have ground truth data for each teacher for each image, we instead opt to matchthe features coming from each teacher’s vision encoder. In particular, we distinguish between the3Table 1: Comparison of vision foundation and RADIO models. “Zero-Shot” and k-NN are computedon ImageNet-1K. ADE20K and VOC (PascalVOC2012) refer to linear probe semantic segmentationmIOU. GQA, POPE (popular), TextVQA, and VQAv2 are obtained via LLaVa 1.5 by replacing thevision encoder. COCO is the instance segmentation metric introduced by to evaluate distillation.RADIO attains the best metrics on most benchmarks, and is competitive with the rest, while E-RADIOenables high quality results in resource constrained settings. Note that Zero-Shot and COCO useteacher’s decoder head that is not finetuned. Throughput computed using NVIDIA A100 GPU, statedresolution, and TensorRT v8601. *Denotes teachers used to train our final RADIO. :We failed toexport DINOv2-g-reg to TensorRT, so we report DINOv2-g here, which should be fairly close. ::Wewere unable to get zero shot working using their model code.Model Params (M) Resolution Throughput Zero-shot k-NN ADE20k VOC GQA POPETextVQA VQAv2 SAM COCOOpenCLIP-H/14 632 224 503 77.19 81.10 40.04 68.03 57.94 83.6150.48 72.24 -MetaCLIP-H/14 632 224 486 80.51 82.12 35.39 62.62 60.57 84.7653.65 75.71 -SigLIP-L/14 428 384 241 82.61 85.16 40.53 70.31 57.70 84.8556.65 71.94 -Intern-ViT-6B 5,902 224 63 83.20 78.43 47.20 76.85 60.18 84.0252.45 76.75 -5,537 448 14 - 68.64 42.78 74.43 61.19 87.2360.36 78.83 -DFN CLIP-H/14 633 378 170 83.90 85.27 39.00 70.29 61.73 85.9156.78 78.78 -OpenAI CLIP-L/14 305 336 414 75.54 79.80 36.51 67.04 62.20 86.0957.92 78.49 -DINOv2-g/14-reg 1,137 224 294 - 83.41 48.68 82.78 61.88 85.6247.18 76.23 -SAM-H/16 637 1024 12 - 22.12 28.08 34.34 49.92 81.7643.91 57.65 77.18E-RADIO-L (Ours) 391 512 468 80.73 83.89 48.22 81.64 61.70 85.0751.47 76.73 76.31RADIO-ViT-H/16 (Ours) 653 432 158 82.93 86.06 51.34 84.71 63.01 86.2056.32 79.28 76.23Table 2: Ablation study on the choice of training dataset. We use MetaCLIP ViT-H/14 and DINOv2ViT-g/14 teachers, and a ViT-L/14 student model with CPE. Both “k-NN” and “Zero Shot” are forImageNet-1k. ADE20k refers to mIOU linear probe on ADE20k.Dataset k-NN Zero Shot ADE20KImageNet 1K 84.79 80.44 48.11ImageNet 21K 84.61 80.10 48.65LAION-400M 83.77 77.46 48.6DataComp-1B 83.91 78.51 49.01summary feature vector and the spatial feature vectors for each teacher. The summary feature iscomputed differently based on the model. For and , we use the “class token” as the summary featurevector, and we don’t match a summary for .f (x|Θ ) Θ y = h (x |Θ )Let be the student vision encoder with parameters , and be the learned0 0 i i 1 iz = t (x|Φ ) Θstudent head matching teacher summary features with student adaptor parametersi i i iΦand teacher parameters .i (cid:88)x = f (x|Θ ); z = t (x|Φ ), y = h (x |Θ ); L (x) = λ L (y , z ) (1)1 0 i i i i i 1 i summary i cos i ii4We found empirically that cosine distance loss produced better models compared to L1, MSE,Smooth-L1. Additionally, supervising the spatial features of the model by matching the teacher wasnot only important for downstream dense tasks, but also improved the holistic quality of our model.For matching the spatial features, we employ a combination of cosine similarity and smooth L1.Similar to equation (2) where we found that cosine similarity produced the best results, we found thesame to be true for the spatial features. However, we want to allow our student model to be a drop-inreplacement in the teacher frameworks, thus it’s important that we match the magnitude of the teacherh (x |Θ )vectors, and so we include smooth L1. In (3) we show the formulation of this loss. Let i 1 it (x|Φ )be the learned student head for matching teacher feature vectors, and corresponding be thei ix = f (x|Θ )teacher feature vectors, with , then the spatial feature loss is:1 0L (x, y) = αL (x, y) + βL (x, y) (2)match cos smooth−l1(cid:88)L (x) = γ L (h (x |Θ ), t (x|Φ )) (3)features i match i 1 i i iiα = 0.9 β = 0.1We choose and to mostly rely on the empirically better cosine distance, but to alsomatch vector magnitudes.3.4.1 Loss BalancingDue to the number of possible combinations of loss weights between the different teachers, andeven which teachers, and possible formulations of loss functions, we mostly opted toward naive lossγ = 1balancing with all teachers equally weighted for spatial features ( ). For summary features, weiλ = λ = 1 λ = 0have and .CLIP DINO SAMWe did experiment with automatic loss balancing using predicted uncertainty, AdaLoss (momentumλ γ0.99) and separately with AMTML-KD, as ways to learn the balance of and . In the case ofi iAMTML-KD, the model would always collapse its entire weight around the teacher and wouldyield worse results than naive manual balancing. Based on the results in table 4, there is very littleadvantage to the more exotic balancing schemes, so we opt for the “Naive” method throughout therest of the paper.Table 3: Ablation over which teachers we supervise the spatial features. We use a ViT-L/14 studentmodel and train on the LAION-400M dataset. Adding this loss term is always beneficial. DINOv2appears to provide better spatial features than CLIP, but training the student to match both teachersproduces the best results. We don’t ablate SAM as we solely want it for its spatial features.Teachers Zero Shot k-NN ADE20KNone 75.77 82.59 41.18CLIP 75.64 82.60 44.42DINOv2 74.68 83.02 47.05Both 74.85 82.96 48.13Table 4: Loss term balancing methods comparison. We use a ViT-B/14 student, and CLIP+DINOv2teachers. We found that AdaLoss produces the best results on the ImageNet tasks, but the worst onADE20K. Method Zero Shot k-NN ADE20KNaive 70.63 79.50 44.71Uncertainty 70.92 79.37 44.57AdaLoss 71.31 79.77 44.364 Implementation DetailsPerforming heterogeneous multi-teacher distillation is not trivial due to a mismatch in featuredimensions, input resolutions, concepts for loss computation, and downsampling ratios, as well aschallenges in fitting multiple teachers into a single GPU.5General. We train all student models using the AdamW optimizer, batch size 1024, cosine annealinglearning rate schedule and base learning rate of 0.001. We train for 600k steps, resulting in 614Mtotal examples seen. For our best student model, we train using DFN CLIP ViT-H/14 378px, OpenAICLIP ViT-L/14 336px, DINOv2 ViT-g/14 224px, and SAM ViTDet-H 1024px. We apply randomscale + cropping to both student and teacher inputs. We chose the DataComp-1B dataset due to ithaving the highest quality results of the web-scale datasets we had access to. We train in two stages,first with CLIP+DINOv2 for 300k steps at 256px, and second with CLIP+DINOv2 at 432px plusSAM at 1024px for 300k steps.Student architecture. We study two settings for student model architecture:• Standard ViT architecture to match the architecture of teachers. Our best model is a ViT-H/16.• Efficient architecture variants prioritizing high throughput on GPUs. See Section 5.1.Multi-scale Teachers. We choose ViT-H/16 architecture for our student model. To match resolution offeatures, we feed the expected resolution of 10242. Given that our and teachers are patch-14 models,we opt to feed the student 4322 inputs, as that is the same effective resolution as 3782 for patch-14.We found that interpolating features doesn’t degrade results, so the teacher operates at 224px and weupsample the outputs to match the student.Rank/Teacher Partitioning. We group teacher models by (batch size, student resolution), and thendistribute the groups to different GPUs, such that each GPU processes a consistent batch size andinput resolution. We also sample groups at different rates. For our training setups that include , wetrain with 64 GPUs, half of which get the CLIP+DINOv2 group with batch size 32 per GPU andinput resolution 432, and the other half get with batch size 2 per GPU and input resolution 1024. Thisresults in an effective batch size of 1,152. For CLIP+DINOv2 training, we use 32 GPUs, resulting inbatch size 1024.Multi-Resolution ViTs. Many of our student models use ViT as the base vision architecture. Tradition-ally, ViTs use a learned position embedding for each input patch in an image, which in turn enforcesthat the model always operates at a constant resolution. We employ the Cropped Position Embedding(CPE) augmentation with the number of positions being equal to 1282. The position embeddings arethen randomly cropped and interpolated to match the number of input patches for the student model.Even when training with CLIP+DINOv2 at 224 resolution, we found that this technique results in anegligible drop (Table 5) in summary metrics, but improved semantic segmentation linear probingmIOU. For heterogeneous-resolution students, this is a seamless technique that allows ViT to operateat arbitrary resolutions within some envelope. In addition to enabling arbitrary resolutions, as shownin figure 3, CPE reduces the noise artifacts in the position embeddings as compared to other ViTmodels.High-Resolution ViT Student. In , they employ the ViTDet architecture as a way to reduce thecomputational and memory burden of ViT models at high-resolution. We reformulate this archinstead into a training augmentation, where we sample a window size from a set of possible windowsizes. This allows us to reduce the computational burden of training the student model with theteacher, and, as we make the window size flexible, it provides an additional throughput scalingmechanism during inference. Table 8 demonstrates our ability to replace SAM’s encoder. Separately,we found that high resolution training was unstable, so we apply spectral reparametrization and aweight decay of 0.02 to prevent attention entropy collapse.Student/Teacher Resolution Mismatch. When the student and teacher downsample images throughtheir processing stack at different rates, it results in the output feature vectors having differentresolutions. For example, if the teachers use a ViT-H/14 architecture and student a ViT-H/16, itLmeans that the student outputs a 142 feature map, and the teachers a 162 feature map. For featureswe bilinearly interpolate the outputs to match the larger resolution between the student and teacherfeatures.Feature Summarization. In 3.4 we explained how teacher summary features are extracted using the“class token” of their respective ViT models. We now turn our attention to the summarization ofstudent features. ViTs have 2 options: (i) a separate summarization “CLS” token or (ii) averagepooling patch tokens. We evaluate both options in Table 6. We observe that average pooling improves6summary loss, but has a more significant detrimental effect on the feature loss. Given the importanceof the latter we choose to use separate CLS tokens.Table 5: Comparing identical ViT models, with CLS token and average pooling summarization.Zero Shot k-NN ADE20K VOC VQAv2CLS token 78.55 83.91 49.01 83.51 77.66Avgpool 80.12 83.83 38.36 77.04 78.285 ResultsIn this section, we analyze models obtained with the proposed AM-RADIO framework. First, wetouch upon backbone efficiency, then compare with the original teachers (CLIP, DINOv2, SAM), andbenchmark models under vision question answering in the LLaVa framework. We will see that theproposed models outperform the original teachers in multiple metrics, including throughput. Resultsare shown in Figure 1 and Table 1.5.1 Efficient StudentsWe aim to find an efficient model architecture to speed up the inference of VFM. There are a numberof architectural designs aimed at high throughput on GPU devices. We use our distillation frameworkto evaluate several backbones with no change in training hyperparameters.Upon reviewing the literature on efficient vision backbones focused for high GPU throughput, wepick the following list of architectures: EfficientNetV2, ResNetv2, RegNetY, FasterViT, EfficientViT,ConvNext, NFNet, SwinV2, MaxViT, PoolformerV2 and MViTV2. We train all the backbonesvia distillation on the ImageNet-21k dataset, using OpenCLIP ViT-H/14 (laion2B-s32B-b79K) andDINOv2 g/14 as teachers. Results are compiled in Table 7.Table 6: Comparison of backbones. Throughput is measured using TensorRT 9.0.1 on A100 inmixed FP16/FP32 precision at batch size 128 on 2242px resolution. Sorted by descending throughputorder. FD loss is the Feature Distillation training loss against the DINOv2 teacher, it exhibits highcorrelation with the ADE20k mIoU. Bolded models form the speed/quality Pareto front.Backbone Param. Count Throughput Zero Shot k-NN ADE20k FD lossTeachersDINOv2 G/14 1.14B 313 N/A 83.41 47.53OpenCLIP H/14 632M 556 77.19 81.10 40.04Existing Efficient ModelsEfficientNetV2-S 21M 9017 65.37 70.72 27.75 0.415ResNetv2-101 44M 7283 69.58 75.32 29.61 0.405RegNetY-064 30M 6573 69.84 74.59 28.9 0.394EfficientViT-L1 38M 6048 71.73 79.90 33.12 0.376ConvNext-B 88M 1805 75.43 81.73 38.95 0.358NFNet-F3 254M 1777 76.93 80.50 38.31 0.340SwinV2-S 49M 1497 74.70 81.12 35.57 0.364MaxViT-B 119M 1486 77.49 79.34 38.46 0.340PoolformerV2-M36 56M 1194 74.46 80.49 35.05 0.377MViTV2-B 51M 975 75.92 81.39 41.39 0.345Proposed architectureE-RADIO-B 118M 6422 75.19 82.21 44.03 0.319E-RADIO-B w/o upsample 113M 7040 75.45 82.05 41.26 0.353E-RADIO-L 265M 3472 77.87 83.73 45.5 0.265We observe that many models lag behind teachers. Additionally, CNN-like models are significantlyfaster than ViTs, while the latter are more accurate. The relatively low performance of existing7efficient backbones on the dense ADE20k segmentation task is not unexpected since all of them applya spatial dimension reduction factor of 32 for final feature maps of size 72 for input resolution of2242px, thus hardly capable of capturing fine-grain spatial information.E-RADIO: To overcome this issue, we propose a novel hybrid architecture, named E-RADIO(Efficient RADIO). This design borrows ideas from existing literature and includes an input stemwith strided convolutions to downsample the input image by 4x. It then proceeds with 2 stages ofYOLOv8 C2f convolution blocks and 2 stages of transformer. For the transformer variant we pickwindowed attention (like in SWIN), and interleave local windowed attention with “global” windowedattention as done in and ViTDet. To perform “global” attention we first downsample the feature mapby 2x, apply windowed attention, and then upsample the feature maps back to the original resolution.8"
P046,"Symbiotic Adversarial Robustness for Graph NeuralNetworks: Combining Poisoning and EvasionAbstractDeep learning models are known to be vulnerable to small input perturbations,which are known as adversarial examples. Adversarial examples are commonlycrafted to deceive a model either at training (poisoning) or testing (evasion). Westudy the combination of poisoning and evasion attacks. We show that using boththreat models can significantly improve the damaging effect of adversarial attacks.Specifically, we study the robustness of Graph Neural Networks (GNNs) understructural perturbations and develop a memory-efficient adaptive end-to-end attackfor this novel threat model using first-order optimization.1 IntroductionGraph neural networks (GNNs) are increasingly used across many different fields, including productrecommendations and drug discovery. GNNs are, however, vulnerable to adversarial attacks in manydifferent tasks such as node classification, graph classification, link prediction and node embeddings.Given that such attacks are able to scale to very large graphs, studying the adversarial robustness ofGNNs has become increasingly important. GNNs can be attacked at test time (evasion) or duringtraining (poisoning). However, a combined threat model that includes both evasion and poisoninghas not been considered in prior literature. Such a model, is, nonetheless, plausible given the publicavailability of graphs or those extracted from sources such as social media sites.Our work is based on the concept of a symbiotic attack, which combines both evasion and poisoningattacks. A symbiotic attack aims to minimize classification accuracy on a test set. The attacker isconstrained by a global budget and manipulates the entire graph, rather than individual nodes. Weprovide a comparison of our approach against plain poisoning and evasion attacks. To this end, weadapt the previous PR-BCD attack to the symbiotic threat model, which results in attacks that arememory-efficient and scalable to large graphs. Our main findings are that symbiotic attacks are moreeffective than poisoning attacks alone, and that evasion attacks are affected by the size of the test set,while symbiotic attacks are less sensitive to test set size. The potential improvement given by thesymbiotic threat model indicates that it requires further study.2 Preliminaries n×nG n A ∈ {0, 1}Notation. We denote a graph by , with nodes, an adjacency matrix , and a featureRn×dX ∈ f (G) θmatrix . A GNN applied to the graph is represented by with parameters . Weθ G Φ(G) Ldenote the set of possible adversarial graphs that can be created from as . Also, andatkL denote the adversarial and training objectives.train2.1 Adversarial Robustness of GNNsAn adversarial attack on a GNN can modify the graph’s structure, by inserting or removing edgesand nodes, or modify the node features. This work focuses on node classification and edge-levelstructural perturbations..Attacks can be categorized as either evasion or poisoning. In an evasion attack, a fixed GNN (withθparameters trained on a clean graph) is targeted, and the attacker aims to solve the optimizationproblem ˆmax L (f (G)),atk θˆG∈Φ(G)whereas a poisoning attack is performed before training, aiming to degrade the performance of theGNN after training. This can be described asˆ ˆ∗max L (f (G)), θ = L (f (G)).where argmin∗atk θ train θθˆG∈Φ(G)A poisoning attack is generally more challenging. Previous work has investigated using evasionperturbations as poisoning perturbations. Also, the optimization may include unrolling the trainingL Aprocedure to calculate meta-gradients (gradients of with respect to ).atk Φ(G)Since we consider only changes to the binary adjacency matrix, we define to include graphsG ∆reachable from after at most edge perturbations.2.1.1 PR-BCDOur work extends on the Projected Randomized Block Coordinate Descent (PR-BCD) attack. Simi-n×nP ∈ [0, 1]larly to the Projected Gradient Descent (PGD) attack, the adjacency matrix is relaxed to ,enabling continuous gradient updates. Each entry indicates the probability of flipping an edge, with thePfinal perturbations sampled from Bernoulli( ). However, as the adjacency matrix grows quadraticallywith the number of nodes, scaling of the PGD becomes difficult with larger graphs. PPR-BCD uses Randomized Block Coordinate Descent (R-BCD), updating a block of at eachE[ (P )] =iteration. The projection step ensures the budget is enforced in expectation, i.e. Bernoulli(cid:80) n×nP < ∆ P ∈ [0, 1]and . After each iteration, rather than sampling the block again, thepromising entries of the block are kept, and only the remaining entries are resampled.PGD can also be applied for a poisoning attack (Meta-PGD). In our attacks, we employ the same∆principle with PR-BCD for better scalability. While we only consider a single global budget , it ispossible to include more complex constraints when needed for a given application.3 Symbiotic AttacksThe Symbiotic Objective. A symbiotic attack has a similar form to the bi-level optimization problem∗ ∗G θbut has an added dependence on the evasion graph in addition to the parameters :ˆ ˆ∗ ∗ ∗L (f (G )) θ = L (f (G)) G = L (f (G))max where argmin , and argmax∗ ∗ˆ ˆpois θ train θ ev θθ G∈Φ(G)ˆG∈Φ(G)L LHere, and are separated for clarity even though they could be the same loss.pois evThreat Model. We model an attacker who aims to reduce a model’s performance on node classifica-tion tasks. Our attacker has full access to the graph, has knowledge of the model’s architecture, cancreate surrogate models, and can only access the trained model as a black-box. Finally, our attackerhas a limited global budget of edge insertions/removals.The Sequential Attack. A simple way to launch a symbiotic attack is to divide the budget and launcha poisoning attack with the first half, followed by an evasion attack with the second half. In thisattack, the poisoning step is not aware of a future evasion, but can improve performance by reducingthe classification margin of certain nodes.The Joint Attack. The poisoning attack can be designed to ""fit"" the future evasion graph by includingthe evasion attack in the poisoning loss. The poisoning loss is computed using the poisoned modelover the evasion graph. This results in a poisoning attack which not only reduces the model’s accuracy,but also makes it more vulnerable to evasion.Both the sequential and joint attacks can be instantiated using different evasion/poisoning attacks. Webuild upon PR-BCD because it scales well to larger graphs. Note that the sequential attack is actuallya special case of the joint attack, with zero iterations per inner evasion attack.24 Evaluation4.1 SetupWe compare the symbiotic threat model with evasion and poisoning attacks, using PR-BCD toimplement the evasion and poisoning attacks. These are evaluated on Cora, CiteSeer, and PubMeddatasets. We study the robustness of GCN, GAT, APPNP, and GPRGNN models. We also considerR-GCN and Jaccard purification as potential defense mechanisms. For each dataset, we allocate 20nodes of each class for the labeled training set and 10Table 1: Numbers of nodes, edges, and classes in the datasets we include in our evaluations.Dataset Nodes Edges ClassesCora 2,708 10,556 7CiteSeer 3,327 9,104 6PubMed 19,717 88,648 34.2 ResultsTable 2 displays the perturbed accuracy values on the test set (10 percent of nodes) for our benchmarkdatasets and models, averaged over 10 runs, with the standard error of the mean also shown. Theattacker is given a 5 percent budget of the number of edges, and this budget is split equally betweenpoisoning and evasion for the symbiotic attacks. We report the best performing of the two symbioticattacks, and also note that the symbiotic attacks are consistently stronger than the poisoning attacks,and stronger than plain evasion. The symbiotic threat model is especially evident on the largerPubMed graph, where the accuracy drops to almost zero, for example, using a GCN.4.3 Effect of the Number of Test NodesTo highlight the differences between poisoning and evasion objectives, Figure 2 shows the perturbedaccuracies for evasion, poisoning, and symbiotic attacks with varying fractions of test nodes with aGCN and a 5As the number of test nodes increases, evasion becomes much more challenging across all datasets.Although poisoning and symbiotic attacks also become more difficult with more test nodes, especiallyon PubMed, they are more robust than the evasion attack. Therefore, the reduction in performancecannot be explained by the attacks having to target a larger number of nodes with the same budget.The poisoning attack is less affected since it can manipulate the flow of information during training.The symbiotic attacks also benefit from this since they can reduce the base accuracy, making nodeseasier to misclassify during the evasion phase. The symbiotic attacks are also stronger than poisoningalone.4.4 HyperparametersBlock size. Figure 3 shows the results of the four attacks with varying block sizes, using a fixed 5percent budget and 125 iterations against a GCN. For small block sizes, the attacks are less effectivesince the PR-BCD optimization can only cover a small part of the adjacency matrix. However, largerblocks have decreasing marginal benefit when a large part of the adjacency matrix can be covered.Budget. Figure 4 shows how all four attacks follow a similar trend when increasing budget size. OnPubMed, changing 5 percent of edges is enough to achieve near-zero accuracy under the symbioticmodel. This highlights the devastating effect of joint attacks, especially in larger graphs with a smallnumber of labeled train nodes.5 Conclusion and Future WorkIn this work, we have introduced the symbiotic threat model for GNNs, which combines evasion andpoisoning attacks. We proposed two methods to generate adversarial perturbations for this model and3±Table 2: Average ( standard error) perturbed accuracies for the evasion, poisoning, and symbioticattacks with a 5 percent budget. The -J suffix indicates the graph has been pre-processed with Jaccardpurification. (ind.) stands for inductive learning. The strongest (lowest accuracy) results for eachsetup are written in bold.Model Dataset Clean Evasion Poisoning Symbiotic± ± ± ±0.38 0.01GCN CiteSeer 0.68 0.01 0.41 0.01 0.4 0.01± ± ± ±0.33 0.01CiteSeer (ind.) 0.67 0.01 0.41 0.01 0.62 0.01± ± ± ±0.38 0.01CiteSeer-J 0.68 0.01 0.41 0.01 0.41 0.02± ± ± ±0.35 0.01Cora 0.78 0.01 0.41 0.01 0.46 0.02± ± ± ±0.3 0.01Cora (ind.) 0.75 0.02 0.42 0.01 0.68 0.03± ± ± ±0.36 0.01Cora-J 0.74 0.01 0.39 0.01 0.43 0.02± ± ± ±0.03 0.01PubMed 0.78 0.01 0.41 0.01 0.12 0.02± ± ± ±0.02 0.0PubMed-J 0.77 0.01 0.41 0.01 0.11 0.01± ± ± ±0.3 0.03GAT CiteSeer 0.62 0.02 0.27 0.02 0.41 0.02± ± ± ±0.56 0.02CiteSeer (ind.) 0.68 0.01 0.37 0.01 0.64 0.02± ± ± ±0.3 0.03CiteSeer-J 0.64 0.01 0.32 0.03 0.41 0.03± ± ± ±0.29 0.02Cora 0.69 0.02 0.22 0.02 0.48 0.03± ± ± ±0.35 0.03Cora (ind.) 0.77 0.01 0.21 0.01 0.61 0.04± ± ± ±0.28 0.02Cora-J 0.67 0.01 0.23 0.02 0.45 0.02± ± ± ±0.2 0.03PubMed 0.73 0.01 0.38 0.04 0.41 0.01± ± ± ±0.19 0.02PubMed-J 0.74 0.01 0.34 0.04 0.38 0.04± ± ± ±0.47 0.01APPNP CiteSeer 0.69 0.01 0.45 0.01 0.56 0.01± ± ± ±0.4 0.01CiteSeer (ind.) 0.71 0.01 0.47 0.01 0.66 0.02± ± ± ±0.45 0.02CiteSeer-J 0.68 0.01 0.43 0.01 0.52 0.02± ± ± ±0.51 0.04Cora 0.82 0.02 0.48 0.03 0.64 0.02± ± ± ±0.37 0.01Cora (ind.) 0.82 0.02 0.53 0.02 0.78 0.01± ± ± ±0.54 0.01Cora-J 0.82 0.01 0.5 0.01 0.67 0.01± ± ± ±0.09 0.01PubMed 0.79 0.0 0.46 0.01 0.21 0.02± ± ± ±0.1 0.02PubMed-J 0.77 0.01 0.45 0.01 0.19 0.03± ± ± ±0.33 0.01GPRGNN CiteSeer 0.66 0.01 0.34 0.01 0.44 0.02± ± ± ±0.34 0.01CiteSeer (ind.) 0.67 0.01 0.37 0.01 0.56 0.01± ± ± ±0.35 0.01CiteSeer-J 0.65 0.01 0.35 0.01 0.44 0.01± ± ± ±0.4 0.01Cora 0.82 0.01 0.46 0.01 0.53 0.01± ± ± ±0.35 0.01Cora (ind.) 0.8 0.02 0.44 0.01 0.74 0.01± ± ± ±0.4 0.01Cora-J 0.79 0.01 0.44 0.01 0.54 0.01± ± ± ±0.08 0.02PubMed 0.78 0.01 0.42 0.01 0.28 0.03± ± ± ±0.15 0.04PubMed-J 0.78 0.01 0.42 0.01 0.38 0.04± ± ± ±0.47 0.01RGCN CiteSeer 0.63 0.01 0.39 0.01 0.59 0.02± ± ± ±0.52 0.02Cora 0.74 0.02 0.44 0.01 0.74 0.01± ± ± ±0.15 0.03PubMed 0.77 0.01 0.43 0.01 0.42 0.04showed that symbiotic attacks can be more effective than the evasion or poisoning approaches ontheir own. We will outline several avenues for future work.The joint attack can be implemented using other evasion attacks, or attacks designed for the symbioticthreat model. In addition, our work considered global budgets, but it is easy to consider per-nodelocal budgets and targeted attacks as well. Moreover, we did not consider the use of different lossfunctions for the poisoning and evasion parts, which may also further improve attack performance.We plan to include further evaluations on these settings as our next step. Finally, novel poisoningattacks can be developed which utilize knowledge of a future evasion attack.A Proof of Theorem 2.1Proof. x ∈ A σ (x) = 0 b ∈ O b = 0 w (x) = 0Let . Then, , and for all where , . Thus,i i i b(cid:88)F (x) = w (x)G (x)b bb∈O,b =1i4b = 1 G (x) ∈ B F (x) B BIf , then , and therefore is also in due to the convexity of .i b i i iB Sub-Gaussian Covering Numbers for ReLU NetworksFigure 2 depicts an example of applying our safe predictor to a notional regression problem. Thisexample uses inputs and outputs in 1-D with one input-output constraint. The unconstrained networkconsists of a single hidden layer with a dimension of 10, ReLU activations, and a fully connected layer.G GThe safe predictor shares this structure with constrained predictors, and , but each predictor0 1has its own fully connected layer. The training uses a sampled subset of points from the input space.Figure 3 shows an example of applying the safe predictor to a notional regression problem with a 2-Dinput and 1-D output and two overlapping constraints. The unconstrained network has two hiddenlayers of dimension 20 with ReLU activations, followed by a fully connected layer. The constrainedG G G Gpredictors, , , , and , share the hidden layers and have an additional hidden layer of00 10 01 11size 20 with ReLU followed by a fully connected layer. Again, training uses a sampled subset ofpoints from the input space and the learned predictors are shown for the continuous input space.C Details of VerticalCAS ExperimentC.1 Safeability ConstraintsThe ""safeability"" property from prior work can be encoded into a set of input-output constraints. The""safeable region"" for a given advisory is the set of input space locations where that advisory can beselected such that future advisories exist that will prevent an NMAC. If no future advisories exist, theadvisory is ""unsafeable"" and the corresponding input region is the ""unsafeable region"". Examples ofthese regions, and their proximity functions are shown in Figure 5 for the CL1500 advisory.x ∈ A ⇒ F (x) < max F (x) ∀iThe constraints we enforce in our safe predictor are: , .,i i j junsafeableF (x) = min F (x) − ϵTo make the output regions convex, we approximate by enforcing , for alli j jx ∈ A .,iunsafeableC.2 Proximity FunctionsWe start by generating the unsafeable region bounds. Then, a distance function is computed betweenv − v τpoints in the input space ( , h, ), and the unsafeable region for each advisory. These are notO Itrue distances but are 0 if and only if the data point is within the unsafeable set. These are then usedto produce proximity functions. Figure 5 shows examples of the unsafeable region, distance function,and proximity function for the CL1500 advisory.C.3 Structure of PredictorsThe compressed policy tables for ACAS Xu and VerticalCAS use neural networks with six hiddenlayers with a dimension of 45, and ReLU activation functions. We used the same architecture for theunconstrained network. For constrained predictors, we use a similar architecture, but share the firstfour layers for all predictors. This provides a common learned representation of the input space, whileallowing each predictor to adapt to its constraints. Each constrained predictor has two additionalhidden layers and their outputs are projected onto our convex approximation of the safe output region,G (x) = min G (x) − ϵ ϵ = 0.0001using . In our experiments, we used .b j jWith this construction, we needed 30 separate predictors to enforce the VerticalCAS safeabilityconstraints. The number of nodes for the unconstrained and safe implementations were 270 and2880, respectively. Our safe predictor is smaller than the original look-up tables by several orders ofmagnitude.C.4 Parameter OptimizationWe use PyTorch for defining our networks and performing parameter optimization. We optimize boththe unconstrained network and our safe predictor using the asymmetric loss function, guiding thenetwork to select optimal advisories while accurately predicting scores from the look-up tables. Each5dataset is split using an 80/20 train/test split with a random seed of 0. The optimizer is ADAM, witha learning rate of 0.0003, a batch size of 216, and training for 500 epochs.6"
P049,"Improving Model Generalization Using a Single DataSample for Semantic AdaptationAbstractThe limited capacity of deep networks to generalize beyond their training dis-tribution presents a significant challenge in semantic segmentation. Traditionalapproaches have operated under the assumption of a fixed model post-training,with parameters remaining constant during testing. This research introduces aself-adaptive methodology for semantic segmentation that modifies the inferencemechanism to accommodate each input sample individually. This adaptation in-volves two principal operations. First, it refines the parameters of convolutionallayers based on the input image, employing a consistency-based regularization.Second, it modifies the Batch Normalization layers by dynamically blending thetraining distribution with a reference distribution extracted from a single test sam-ple. Although these techniques are individually recognized in the field, theircombined application establishes new benchmarks in accuracy for generalizationfrom synthetic to real-world data. The empirical evidence from this study indicatesthat self-adaptation can effectively enhance deep network generalization to out-of-domain data, serving as a valuable complement to the established methods ofmodel regularization during training.1 IntroductionState-of-the-art models in semantic segmentation exhibit a notable deficiency in robustness whenconfronted with out-of-distribution data, where the distributions of training and testing sets diverge.While numerous studies have examined this challenge, with a predominant focus on image classifica-tion, it has been observed that Empirical Risk Minimization (ERM), which presumes independent andidentically distributed training and testing samples, remains remarkably competitive. This contrastswith the evident advancements in domain adaptation for both image classification and semanticsegmentation. The domain adaptation setup, however, typically requires access to an unlabeledtest distribution during training. In the generalization scenario considered here, only a single testsample is accessible during inference, and no information sharing must occur between subsequenttest samples.This study investigates the generalization challenge in semantic segmentation, specifically fromsynthetic data to real-world scenarios, by employing an adaptive approach. Unlike prior researchthat has concentrated on modifying model architecture or training procedures, this work revises thestandard inference procedure using a technique derived from domain adaptation methods. Termedself-adaptation, this technique utilizes a self-supervised loss function to facilitate adaptation toindividual test samples through a limited number of parameter updates. In addition to these loss-basedupdates, self-adaptation incorporates feature statistics from the training data with those of the testsample within the Batch Normalization layers.2 Related WorkThis research contributes to the ongoing investigation into the generalization capabilities of semanticsegmentation models and is related to explorations of feature normalization and online learning..In contrast to previous studies that focused on training strategies and model design, this studyspecifically examines the inference process during test time. Prior research has attempted to improvegeneralization by augmenting synthetic training data with styles transferred from real images, orby utilizing a classification model trained on real images to ensure feature proximity betweenmodels via distillation, often seeking layer-specific learning rates. Some approaches have addedinstance normalization (IN) layers heuristically to the network. Recent studies have sought to extractdomain-invariant feature statistics through instance-selective whitening loss or frequency-baseddomain randomization. Others have aimed to learn style-invariant representations using causalframeworks or have augmented single-domain data to simulate a multi-source scenario to increasesource domain diversity. Some techniques involve swapping channel-wise statistics in featurenormalization layers and learning adapter functions to adjust the mean and variance based on theinput. Another method enforces consistency of output logits across multiple images of the same class.To improve generalization in federated learning, researchers have explored training clients locallywith sharpness-aware minimization and averaging stochastic weights. However, these methods eitherassume access to a distribution of real images during training or require modifications to the networkarchitecture. The technique presented in this work does not require either, making it applicablepost-hoc to already trained models to improve their generalization.Batch Normalization (BN) and other normalization techniques have been increasingly associatedwith model robustness. The most common methods, including BN, Layer Normalization (LN), andInstance Normalization (IN), also impact the model’s expressive capacity, which can be furtherimproved by combining these techniques within a single architecture. In domain adaptation, somestudies use source-domain statistics during training and replace them with target-domain statisticsduring inference. Recent work has explored combining source and target statistics during inference,weighted by the number of samples they aggregate. Others propose using batch statistics from thetarget domain during inference instead of training statistics from the source domain. This studycomplements these findings by demonstrating improved generalization of semantic segmentationmodels.Several previous studies have updated model parameters during inference, particularly in objecttracking where the object detector must adapt to the changing appearance of the tracked instance.Conditional generative models have been employed to learn from single image samples for super-resolution and scene synthesis. Recently, this principle has been extended to improve the robustnessof image classification models, though the self-supervised tasks developed for image classification donot always extend well to dense prediction tasks like semantic segmentation. Recent research hasproposed more suitable alternatives for self-supervised loss in domain adaptation, and several workshave developed domain-specific approaches for medical imaging or first-person vision.Most of the related works focus on domain adaptation in image classification, typically assumingaccess to multiple samples from the target distribution during training. This work addresses semanticsegmentation in the domain generalization setting, requiring only a single datum from the test set. Inthis context, simple objectives like entropy minimization improve baseline accuracy only moderately.In contrast, the self-adaptation method presented here, which uses pseudo-labels to account forprediction uncertainty, proves significantly more effective. The task is distinct from few-shot learning,where the model may adapt during testing using a small annotated set of samples. Here, no suchannotation is available; the model adjusts to the test sample in an unsupervised manner, withoutrequiring proxy tasks or prior knowledge of the test distribution.3 MethodologyIn traditional inference, the parameters of the segmentation model are assumed to remain fixed. Incontrast, adaptive systems are capable of learning to specialize to their environment. Analogously,this study allows the segmentation model to update its parameters during inference. It is important tonote that this setup differs from domain adaptation scenarios, as the updated parameters are discardedafter processing each sample, aligning with the principles of domain generalization.The proposed approach creates mini-batches of images for each test sample using data augmentation.Starting with the original test image, a set of N augmented images is generated through multi-scaling,horizontal flipping, and grayscaling. These images form a mini-batch that is processed by the CNN.The resulting softmax probabilities are transformed back to the original pixel space using inverse2affine transformations, producing multiple predictions for each pixel. The mean of these probabilitiesis computed along the mini-batch dimension for each class and pixel on the spatial grid.A threshold value is computed from the maximum probability of every class to create a class-dependent threshold. For each pixel, the class with the highest probability is extracted. Low-confidence predictions are ignored by setting pixels with a softmax probability below the threshold toan ignore label, while the remaining pixels use the dominant class as the pseudo-label. This pseudoground truth is used to fine-tune the model for a set number of iterations using gradient descent withthe cross-entropy loss. After this self-adaptation process, a single final prediction is produced usingthe updated model weights. The weights are then reset to their initial values before processing thenext test sample, ensuring that the model does not accumulate knowledge about the entire target datadistribution.Batch Normalization (BN) has become an integral part of modern CNNs. Although originallydesigned to improve training convergence, it is now recognized for its role in model robustness,including domain generalization. During training, BN computes the mean and standard deviationacross the batch and spatial dimensions. The normalized features are derived using these statistics.At test time, it is common practice to normalize feature values with running estimates of the meanand standard deviation across training batches, rather than using test-batch statistics. This is referredto as train BN (t-BN).In the context of out-of-distribution generalization, the running statistics derived from the source datacan differ substantially from those computed using target images, a problem known as covariate shift.Domain adaptation methods often mitigate this issue by replacing source running statistics with thoseof the target, a technique known as Adaptive Batch Normalization (AdaBN). Recent studies havealso explored prediction-time BN (p-BN), which uses the statistics of the current test batch instead ofrunning statistics from training.This study assumes the availability of only a single target sample during inference. Alternatives likeAdaBN and p-BN are not directly applicable in this scenario. Instance Normalization (IN) layerscould replace BN layers, but this might lead to covariate shift issues, as sample statistics may onlyapproximate the complete test distribution. Additionally, such a replacement could interfere with thestatistics of activations in intermediate layers.Self-adaptive normalization (SaN) is proposed as a solution. It combines the inductive bias fromthe source domain’s running statistics with statistics extracted from a single test instance. Thesource mean and variance are averaged with sample statistics from the target domain, weighted bya parameter 1. This parameter represents the shift from the source domain ( 1 = 0) to a referencedomain ( 1 = 1). During inference, new mean and variance are computed using this weighted average,and these are used to normalize the features of the single test sample. This approach does not affectthe behavior of BN layers during training and applies only during testing.4 ExperimentsIn this study, the evaluation protocol is revised to adhere to principles of robustness and generalization.The supplier has access to two data distributions: the source data for model training and a validationset for model validation. The generalization ability of the model is assessed on three distinct targetsets, providing an estimate of the expected model accuracy for out-of-distribution deployment. Thedatasets used are restricted to traffic scenes for compatibility with previous research.Source data for model training comes from two synthetic datasets, GTA and SYNTHIA, which offerlow-cost ground truth annotation and exhibit visual discrepancies with real imagery. The validation setused is WildDash, which is understood to be of limited quantity but bears a closer visual resemblanceto potential target domains. The model is evaluated on three target domains: Cityscapes, BDD, andIDD, chosen for their geographic diversity and differences in data acquisition. The average accuracyacross these target domains estimates the expected model accuracy. Additionally, the Mapillarydataset is used for comparison with previous works, although it does not disclose the geographicorigins of individual samples.The framework is implemented in PyTorch, and the baseline model is DeepLabv1 without CRF post-processing. The models are trained on the source domains for 50 epochs using an SGD optimizer witha learning rate of 0.005, decayed polynomially. Data augmentation techniques include random-size3crops, random aspect ratio adjustments, random horizontal flipping, color jitter, random blur, andgrayscaling.Experiments were conducted to investigate the influence of the parameter 1 in Self-adaptive Nor-malization (SaN) on segmentation accuracy and the Intersection over Union (IoU) for both sourcedomains (GTA, SYNTHIA) and all main target domains (Cityscapes, BDD, IDD). The optimal 1 wasdetermined based on the IoU on the WildDash validation set. The segmentation accuracy with thisoptimal 1 was reported, showing that SaN improves the mean IoU over both the established t-BNbaseline and the more recent p-BN. The improvement was consistent across different backbone archi-tectures and target domains. Additionally, model calibration, measured by the expected calibrationerror (ECE), was found to improve with SaN, which was competitive with the MC-Dropout methodand showed complementary effects when used jointly.Self-adaptation was compared to Test-Time Augmentation (TTA), which involves augmenting testsamples with flipped and grayscaled versions at multiple scales and averaging the predictions. Self-adaptation outperformed TTA by a clear margin, aligning with reported ECE scores and demonstratingthat self-adaptation effectively uses calibrated confidence to generate reliable pseudo-labels.Self-adaptation was compared with state-of-the-art domain generalization methods, showing consis-tent improvements over carefully tuned baselines, regardless of backbone architecture or source data.The method outperformed previous methods without modifying the model architecture or trainingprocess, altering only the inference procedure.A comparison with Tent, which also updates model parameters at test time but minimizes entropyinstead of using pseudo-labels, showed that self-adaptation outperformed Tent substantially. Thiswas demonstrated by training HRNet-W18 on GTA and comparing the IoU on Cityscapes, whereself-adaptation achieved a 7.5% improvement in IoU.The influence of the number of iterations for self-adaptation was investigated, showing that self-adaptation balances accuracy and inference time by adjusting iteration numbers and layer choices.It was found to be more efficient and accurate than model ensembles. Self-adaptation can trade offaccuracy vs. runtime by using fewer update iterations or updating fewer upper network layers.Hyperparameter sensitivity analysis revealed that self-adaptation is robust to the choice of hyperpa-rameters 1, 8, and 7. The optimal values were determined using the validation set, and the modelaccuracy declined moderately with deviations from these values. Qualitative results showed thatself-adaptation improves segmentation quality and reduces pathological failure modes.The integration of self-adaptation with state-of-the-art architectures like DeepLabv3+, HRNet-W18,HRNet-W48, and UPerNet with a Swin-T backbone demonstrated substantial improvements insegmentation accuracy across all target domains. Evaluation on the ACDC dataset, which includesadverse weather conditions, showed that self-adaptation outperformed the baseline by 13.57% onaverage.Additional qualitative results and failure cases were discussed, showing that self-adaptation canstruggle with cases of mislabeling regions with incorrect but semantically related classes. However,these failure cases were relatively rare, and the majority of image samples benefited from self-adaptation, with accuracy improvements of up to 35% IoU compared to the baseline.5 ResultsThe empirical results demonstrate that self-adaptive normalization (SaN) consistently enhancessegmentation accuracy in out-of-distribution scenarios. For instance, when training on the GTAdataset and testing on Cityscapes, BDD, and IDD, SaN improved the mean IoU by 4.1% with ResNet-50 and 5.1% with ResNet-101 compared to the t-BN baseline. Furthermore, SaN outperformed themore recent p-BN method, showing improvements irrespective of the backbone architecture andthe target domain tested. In terms of calibration quality, measured by the expected calibration error(ECE), SaN not only improved the baseline but also showed competitiveness with the MC-Dropoutmethod, even exhibiting complementary effects when both methods were combined.Self-adaptation was found to outperform traditional Test-Time Augmentation (TTA) across bothsource domains (GTA, SYNTHIA) and three target domains (Cityscapes, BDD, IDD). Despite TTAimproving the baseline, self-adaptation provided a clear and consistent margin of 2.19% IoU on4average. This aligns with the reported ECE scores, demonstrating that self-adaptation effectivelyexploits the calibrated confidence of predictions to yield reliable pseudo-labels.In comparison to state-of-the-art domain generalization methods, self-adaptation showed substantialimprovements even over carefully tuned baselines. It outperformed methods like DRPC and FSDRon most benchmarks, despite these methods using individual models for each target domain andresorting to target domains for hyperparameter tuning. Self-adaptation achieved superior segmentationaccuracy without requiring access to a distribution of real images for training or modifying the modelarchitecture, unlike previous methods such as ASG, CSG, DRPC, and IBN-Net.The study also compared self-adaptation with Tent, which updates model parameters at test timeby minimizing entropy. Self-adaptation, which constructs pseudo-labels based on well-calibratedpredictions, substantially outperformed Tent. Specifically, when training HRNet-W18 on GTA andevaluating on Cityscapes, self-adaptation achieved a 7.5% improvement in IoU compared to Tentunder a comparable computational budget.Further analysis revealed that self-adaptation provides a flexible mechanism for trading off accuracyand runtime by varying the number of update iterations and the layers to adjust. It was found to bemore efficient and accurate than model ensembles. Hyperparameter sensitivity analysis indicated thatself-adaptation is robust to the choice of hyperparameters, with optimal values determined using thevalidation set.Qualitative results demonstrated that self-adaptation visibly improves segmentation quality, reducingartifacts and mislabeling compared to the baseline. The method’s effectiveness was consistent acrossdifferent architectures, including DeepLabv3+, HRNet-W18, HRNet-W48, and UPerNet with aSwin-T backbone, showing substantial improvements in segmentation accuracy on all target domains.6 ConclusionThe traditional learning principle of Empirical Risk Minimization (ERM) assumes independent andidentically distributed training and testing data, which often results in models that are not robust todomain shifts. To address this, a self-adaptive inference process was introduced, bypassing the needfor explicit assumptions about the test distribution. This study also outlined four principles for arigorous evaluation process in domain generalization, adhering to best practices in machine learningresearch.The analysis demonstrated that even a single sample from the test domain can significantly improvemodel predictions. The self-adaptive approach showed substantial accuracy improvements withoutaltering the training process or model architecture, unlike previous works. These results suggestthat self-adaptive techniques could be valuable in other application domains, such as panopticsegmentation or monocular depth prediction.While the presented self-adaptation method is not yet real-time, it offers a favorable trade-off betweenaccuracy and computational cost compared to model ensembles. Future research could explorereducing the latency of self-adaptive inference through adaptive step sizes, higher-order optimization,or low-precision computations. Overall, this work demonstrates the potential of self-adaptation toenhance model generalization and robustness in various applications.5"
P050,"Interpreting Recurrent and Attention-Based NeuralModels: a Case Study on Natural Language InferenceAbstractDeep learning models have achieved remarkable success in natural language in-ference (NLI) tasks. While these models are widely explored, they are hard tointerpret and it is often unclear how and why they actually work. we take a steptoward explaining such deep learning based models through a case study on apopular neural model for NLI. we propose to interpret the intermediate layersof NLI models by visualizing the saliency of attention and LSTM gating signals.We present several examples for which our methods are able to reveal interestinginsights and identify the critical information contributing to the model decisions.1 IntroductionDeep learning has achieved tremendous success for many NLP tasks. However, unlike traditionalmethods that provide optimized weights for human understandable features, the behavior of deeplearning models is much harder to interpret. Due to the high dimensionality of word embeddings, andthe complex, typically recurrent architectures used for textual data, it is often unclear how and why adeep learning model reaches its decisions.There are a few attempts toward explaining/interpreting deep learning-based models, mostly byvisualizing the representation of words and/or hidden states, and their importances (via saliency orerasure) on shallow tasks like sentiment analysis and POS tagging. we focus on interpreting thegating and attention signals of the intermediate layers of deep models in the challenging task ofNatural Language Inference. A key concept in explaining deep models is saliency, which determineswhat is critical for the final decision of a deep model. So far, saliency has only been used to illustratethe impact of word embeddings. we extend this concept to the intermediate layer of deep models toexamine the saliency of attention as well as the LSTM gating signals to understand the behavior ofthese components and their impact on the final decision.We make two main contributions. First, we introduce new strategies for interpreting the behavior ofdeep models in their intermediate layers, specifically, by examining the saliency of the attention andthe gating signals. Second, we provide an extensive analysis of the state-of-the-art model for the NLItask and show that our methods reveal interesting insights not available from traditional methods ofinspecting attention and word saliency.our focus was on NLI, which is a fundamental NLP task that requires both understanding andreasoning. Furthermore, the state-of- the-art NLI models employ complex neural architecturesinvolving key mechanisms, such as attention and repeated reading, widely seen in successful modelsfor other NLP tasks. As such, we expect our methods to be potentially useful for other naturalunderstanding tasks as well.2 Task and ModelIn NLI, we are given two sentences, a premise and a hypothesis, the goal is to decide the logicalrelationship (Entailment, Neutral, or Contradiction) between them.Many of the top performing NLI models, are variants of the ESIM model, which we choose toanalyze. ESIM reads the sentences independently using LSTM at first, and then applies attention toalign/contrast the sentences. Another round of LSTM reading then produces the final representations,which are compared to make the prediction.3 Visualization of Attention and Gatingwe are primarily interested in the internal workings of the NLI model. we focus on the attention andthe gating signals of LSTM readers, and how they contribute to the decisions of the model.3.1 AttentionAttention has been widely used in many NLP tasks and is probably one of the most critical partsthat affects the inference decisions. Several pieces of prior work in NLI have attempted to visualizethe attention layer to provide some understanding of their models. Such visualizations generate aheatmap representing the similarity between the hidden states of the premise and the hypothesis.Unfortunately the similarities are often the same regardless of the decision.Let us consider the following example, where the same premise “A kid is playing in the garden”, ispaired with three different hypotheses:h1: A kid is taking a nap in the gardenh2: A kid is having fun in the garden with her familyh3: A kid is having fun in the gardenNote that the ground truth relationships are Contradiction, Neutral, and Entailment, respectively.The key issue is that the attention visualization only allows us to see how the model aligns the premisewith the hypothesis, but does not show how such alignment impacts the decision. This prompts us toconsider the saliency of attention.3.1.1 Attention SaliencyThe concept of saliency was first introduced in vision for visualizing the spatial support on an imagefor a particular object class. In NLP, saliency has been used to study the importance of words towarda final decision.We propose to examine the saliency of attention. Specifically, given a premise-hypothesis pair andthe model’s decision y, we consider the similarity between a pair of premise and hypothesis hiddenstates eij as a variable. The score of the decision S(y) is thus a function of eij for all i and j. Thesaliency of eij is then defined to be |S(y) / eij|., the saliencies are clearly different across the examples, each highlighting different parts of thealignment. Specifically, for h1, we see the alignment between “is playing” and “taking a nap” and thealignment of “in a garden” to have the most prominent saliency toward the decision of Contradiction.For h2, the alignment of “kid” and “her family” seems to be the most salient for the decision ofNeutral. Finally, for h3, the alignment between “is having fun” and “kid is playing” have the strongestimpact toward the decision of Entailment.From this example, we can see that by inspecting the attention saliency, we effectively pinpoint whichpart of the alignments contribute most critically to the final prediction whereas simply visualizing theattention itself reveals little information.3.1.2 Comparing ModelsIn the previous examples, we study the behavior of the same model on different inputs. Now we usethe attention saliency to compare the two different ESIM models: ESIM-50 and ESIM-300.Consider two examples with a shared hypothesis of “A man ordered a book” and premise:p1: John ordered a book from amazonp2: Mary ordered a book from amazon 2Here ESIM-50 fails to capture the gender connections of the two different names and predicts Neutralfor both inputs, whereas ESIM-300 correctly predicts Entailment for the first case and Contradictionfor the second.Although the two models make different predictions, their attention maps appear qualitatively similar.We see that for both examples, ESIM-50 primarily focused on the alignment of “ordered”, whereasESIM-300 focused more on the alignment of “John” and “Mary” with “man”. interesting to note thatESIM-300 does not appear to learn significantly different similarity values compared to ESIM-50for the two critical pairs of words (“John”, “man”) and (“Mary”, “man”) based on the attention map.The saliency map, however, reveals that the two models use these values quite differently, with onlyESIM-300 correctly focusing on them. It is3.2 LSTM Gating SignalsLSTM gating signals determine the flow of information. In other words, they indicate how LSTMreads the word sequences and how the information from different parts is captured and combined.LSTM gating signals are rarely analyzed, possibly due to their high dimensionality and complexity.we consider both the gating signals and their saliency, which is computed as the partial derivative ofthe score of the final decision with respect to each gating signal.Instead of considering individual dimensions of the gating signals, we aggregate them to considertheir norm, both for the signal and for its saliency. Note that ESIM models have two LSTM layers,the first (input) LSTM performs the input encoding and the second (inference) LSTM generates therepresentation for inference., we first note that the saliency tends to be somewhat consistent across different gates within the sameLSTM, suggesting that we can interpret them jointly to identify parts of the sentence important forthe model’s prediction.Comparing across examples, we see that the saliency curves show pronounced differences across theexamples. For instance, the saliency pattern of the Neutral example is significantly different from theother two examples, and heavily concentrated toward the end of the sentence (“with her family”).Note that without this part of the sentence, the relationship would have been Entailment. The focus(evidenced by its strong saliency and strong gating signal) on this particular part, which presentsinformation not available from the premise, explains the model’s decision of Neutral.Comparing the behavior of the input LSTM and the inference LSTM, we observe interesting shiftsof focus. the inference LSTM tends to see much more concentrated saliency over key parts of thesentence, whereas the input LSTM sees more spread of saliency. For example, for the Contradictionexample, the input LSTM sees high saliency for both “taking” and “in”, whereas the inference LSTMprimarily focuses on “nap”, which is the key word suggesting a Contradiction. Note that ESIM usesattention between the input and inference LSTM layers to align/contrast the sentences, hence it makessense that the inference LSTM is more focused on the critical differences between the sentences.This is also observed for the Neutral example as well.It is worth noting that, while revealing similar general trends, the backward LSTM can sometimesfocus on different parts of the sentence, suggesting the forward and backward readings providecomplementary understanding of the sentence.4 ConclusionWe propose new visualization and interpretation strategies for neural models to understand howand why they work. We demonstrate the effectiveness of the proposed strategies on a complex task(NLI). Our strategies are able to provide interesting insights not achievable by previous explanationtechniques. Our future work will extend our study to consider other NLP tasks and models with thegoal of producing useful insights for further improving these models.35 Appendix5.1 ModelIn this section we describe the ESIM model. We divide ESIM to three main parts: 1) input encoding,2) attention, and 3) inference.Let u = [u1, · · · , un] and v = [v1, · · · , vm] be the given premise with length n and hypothesis withlength m respectively, where ui, vj Rr are word embeddings of r-dimensional vector. The goal is topredict a label y that indicates the logical relationship between premise u and hypothesis v. Below webriefly explain the aforementioned parts.5.1.1 Input EncodingIt utilizes a bidirectional LSTM (BiLSTM) for encoding the given premise and hypothesis usingEquations 1 and 2 respectively.(1) u^ = BiLSTM(u)(2) v^ = BiLSTM(v)where u^ Rn×2d and v^ Rm×2d are the reading sequences of u and v respectively.5.1.2 AttentionIt employs a soft alignment method to associate the relevant sub-components between the givenpremise and hypothesis. Equation 3 (energy function) computes the unnormalized attention weightsas the similarity of hidden states of the premise and hypothesis.(3) eij = u^Ti v^j, i [1, n], j [1, m]where u^i and v^j are the hidden representations of u and v respectively which are computed earlierin Equations 1 and 2. Next, for each word in either premise or hypothesis, the relevant semantics inthe other sentence is extracted and composed according to eij. Equations 4 and 5 provide formal andspecific details of this procedure.(4) u~i = sum(exp(eij) / sum(exp(eik))) * uj, i [1, n](5) v~j = sum(exp(eij) / sum(exp(ekj))) * ui, j [1, m]where u~i represents the extracted relevant information of v^ by attending to u^i while v~j representsthe extracted relevant information of u^ by attending to v^j. Next, it passes the enriched informationthrough a projector layer which produce the final output of attention stage. Equations 6 and 7 formallyrepresent this process.(6) ai = [ui, u~i, ui u~i, ui u~i] ; pi = ReLU(Wpai + bi)(7) bj = [vj, v~j, vj v~j, vj v~j] ; qj = ReLU(Wqbj + byj)Here stands for element-wise product while Wp, Wq R4d×d and bp, by Rd are the trainable weightsand biases of the projector layer respectively. p and q indicate the output of attention de- vision forpremise and hypothesis respectively.5.1.3 InferenceDuring this phase, it uses another BiLSTM to aggregate the two sequences of computed matching(8) p^ = BiLSTM(p)(9) q^ = BiLSTM(q)where p^ Rn×2d and q^ Rm×2d are the reading sequences of p and q respectively. Finally theconcatenation max and average pooling of p^ and q^ are pass through a multilayer perceptron (MLP)classifier that includes a hidden layer with tanh activation and softmax output layer. The model istrained in an end-to-end manner. 45.2 Attention StudyHere we provide more examples on the NLI task which intend to examine specific behavior in thismodel. Such examples indicate interesting observation that we can analyze them in the future works.Table 1 shows the list of all example.Table 1: Examples along their gold labels, ESIM-50 predictions and study categories.Premise Hypothesis Gold Prediction CategorySix men, two with shirts and Seven men, two with shirts Contradiction Contradiction Countingfour without, have taken a and four without, have takenbreak from their work on a a break from their work on abuilding. building.two men with shirts and four Six men, two with shirts and Entailment Entailment Countingmen without, have taken a four without, have taken abreak from their work on a break from their work on abuilding. building.Six men, two with shirts and Six men, four with shirts and Contradiction Contradiction Countingfour without, have taken a two without, have taken abreak from their work on a break from their work on abuilding. building.A man just ordered a book A man ordered a book yester- Neutral Neutral Chronologyfrom amazon. day.A man ordered a book from A man ordered a book yester- Entailment Entailment Chronologyamazon 30 hours ago. day. 5"
P051,"Real-Time Adaptation of Lexical Embeddings forEnhanced Part-of-Speech TaggingAbstractThis research introduces a method for real-time unsupervised domain adaptation(DA) that can be applied incrementally as new information arrives. This method isespecially useful when conventional batch DA is unfeasible. Through evaluationsfocused on part-of-speech (POS) tagging, we observe that real-time unsupervisedDA achieves accuracy levels on par with those of batch DA.1 IntroductionUnsupervised domain adaptation is a frequently encountered challenge for developers aiming tocreate robust natural language processing (NLP) systems. This situation typically arises when labeleddata is available for a source domain, but there is a need to enhance performance in a target domainusing only unlabeled data. A majority of the current NLP research on unsupervised domain adaptationemploys batch learning, which presumes the availability of a substantial corpus of unlabeled datafrom the target domain before the testing phase. However, batch learning is impractical in numerousreal-world situations where data from a new target domain must be processed without delay. Further,in many practical scenarios, data may not be neatly categorized by domain, making it difficult toimmediately discern when an input stream begins providing data from a new domain.For instance, consider an NLP system within a company that is tasked with analyzing a continuousstream of emails. This stream evolves over time without any explicit signals indicating that thecurrent models should be adjusted to the new data distribution. Given that the system is expected tooperate in real-time, it would be beneficial for any system adaptation to be done in an online manner,as opposed to the batch method, which involves halting the system, modifying it, and then restartingit.This paper introduces real-time unsupervised domain adaptation as an enhancement to conventionalunsupervised DA. In this approach, domain adaptation is carried out incrementally as data is received.Specifically, our implementation involves a type of representation learning, where the focus is onupdating word representations in our experiments. Every instance a word appears in the data streamduring testing, its representation is refined.To our understanding, the research presented here is the first to examine real-time unsupervisedDA. In particular, we assess this method for POS tagging tasks. We analyze POS tagging outcomesusing three different methods: a static baseline, batch learning, and real-time unsupervised DA. Ourfindings indicate that real-time unsupervised DA performs comparably to batch learning, yet it doesnot require retraining or pre-existing data from the target domain.2 Experimental setupTagger. We have adapted the FLORS tagger, which is recognized for its speed and simplicity,and is particularly effective in DA scenarios. This tagger approaches POS tagging as a multi-labelclassification problem within a window-based framework, rather than a sequence classificationone. FLORS is well-suited for real-time unsupervised DA because its word representations includedistributional vectors, which can be updated during both batch learning and real-time unsupervisedDA. Each word’s representation in FLORS consists of four feature vectors: one for its suffix, one forits shape, and one each for its left and right distributional neighbors. Suffix and shape features arestandard in the literature, and we utilize them as described previously.Distributional features. The ith element xi of the left distributional vector for a word w is theweighted count of times the indicator word ci appears immediately to the left of w:xi = tf (f req(bigram(ci, w))) (1)where ci is the word with frequency rank i in the corpus, freq(bigram(ci, w)) is the occurrence countof the bigram ""ci w"", and non-zero frequencies are weighted logarithmically: tf(x) = 1 + log(x). Theright distributional vector is defined similarly. We limit the set of indicator words to the 500 mostfrequent. To avoid zero vectors, an additional element xn+1 is added to each vector to account foromitted contexts: (cid:88)xn + 1 = tf ( .5f req(bigram(c , w))) (2)iLet f(w) be the concatenation of the two distributional, suffix, and shape vectors of word w. ThenFLORS represents token vi as follows:Φ Φ Φ Φ Φ Φf (vi22122)2295f (vi22121)2295f (vi)2295f (vi + 1)2295f (vi + 2) (3)˘where 2295 is vector concatenation. FLORS then tags token vi based on this representation.FLORS operates under the assumption that the fundamental relationship between distributionalfeatures and labels remains consistent when transitioning from the source to the target domain. Thiscontrasts with other studies that select ""stable"" distributional features and discard ""unstable"" ones.The central hypothesis of FLORS is that fundamental distributional POS characteristics are relativelystable across different domains, unlike semantic or more intricate tasks. The effectiveness of FLORSsuggests the validity of this hypothesis.Test set.Data. Our evaluation utilizes the development sets from six different target domains (TDs):five SANCL domains (newsgroups, weblogs, reviews, answers, emails) and sections 22-23 of theWall Street Journal (WSJ) for in-domain testing.Two training sets of varying sizes are employed. In the l:big condition (large labeled data set), FLORSis trained on sections 2-21 of the WSJ. The l:small condition uses 10% of the l:big data set.Data for word representations. We also adjust the size of the datasets used for computing wordrepresentations before training the FLORS model. In the u:big condition, distributional vectors arecomputed on the combined corpus of all labeled and unlabeled text from both source and targetdomains (excluding test sets), along with 100,000 WSJ sentences from 1988 and 500,000 sentencesfrom a large external corpus. In the u:0 condition, only labeled training data is utilized.Methods. We implemented a modification from the original setup: distributional vectors are storedin memory as count vectors, enabling count increases during online tagging.Experiments are conducted with three versions of FLORS: STATIC, BATCH, and ONLINE. All threemethods compute word representations on ""data for word representations"" before model training onone of the two ""training sets"".STATIC. Word representations remain unchanged during testing.BATCH. Before testing, count vectors are updated by freq(bigram(ci, w)) += freq*(bigram(ci, w)),˘where freq*(00b7) denotes the bigram ""ci w"" occurrences in the entire test set.ONLINE. Before tagging a test sentence, both left and right distributional vectors are updated viafreq(bigram(ci, w)) += 1 for each ""ci w"" bigram appearance in the sentence. The sentence is thentagged using the updated word representations. As tagging progresses, distributional representationsbecome increasingly specific to the target domain (TD), converging to the representations that BATCHuses at the end of the tagging process. 2In all three modes, suffix and shape features are always fully specified, for both known and unknownwords.3 Experimental resultsTable 1 shows that the performance levels of BATCH and ONLINE are on par with each other andrepresent the current state-of-the-art. The highest accuracy in each column is highlighted in bold.Table 1: BATCH and ONLINE accuracies are comparable and state-of-the-art. Best number in eachcolumn is bold. newsgroups reviews weblogs answers emailswsj ALL OOV ALL OOV ALL OOV ALL OOV ALL OOV ALLOOVTnT 88.66 54.73 90.40 56.75 93.33 74.17 88.55 48.32 88.14 58.09 95.7588.30Stanford 89.11 56.02 91.43 58.66 94.15 77.13 88.92 49.30 88.68 58.42 96.8390.25SVMTool 89.14 53.82 91.30 54.20 94.21 76.44 88.96 47.25 88.64 56.37 96.6387.96C&P 89.51 57.23 91.58 59.67 94.41 78.46 89.08 48.46 88.74 58.62 96.7888.65S&S 90.86 66.42 92.95 75.29 94.71 83.64 90.30 62.16 89.44 62.61 96.5990.37S&S (reimpl.) 90.68 65.52 93.00 75.50 94.64 82.91 90.18 61.98 89.53 62.46 96.6089.70BATCH 90.87 71.18 93.07 79.03 94.86 86.53 90.70 65.29 89.84 65.44 96.6391.86ONLINE 90.85 71.00 93.07 79.03 94.86 86.53 90.68 65.16 89.85 65.48 96.6291.69Table 2 shows that the accuracy rates for ONLINE and BATCH methods are generally superiorto those of the STATIC method, as indicated by the numbers in bold. It also demonstrates thatperformance improves with an increase in both training data and unlabeled data.The performance of ONLINE is similar to that of BATCH. It is slightly lower than BATCH in theu:0 condition, with the most significant difference in accuracy being 0.29, and it is at most 0.02different from BATCH in terms of overall accuracy in the u:big condition. The reasons for ONLINEoccasionally outperforming BATCH, particularly in certain conditions, are discussed subsequently.3.1 Time course of tagging accuracyThe ONLINE model introduced here has a unique characteristic not commonly found in otherstatistical NLP research: its predictive accuracy evolves as it processes text due to the modification ofits representations.To analyze the progression of these changes over time, a substantial application domain is necessarybecause subtle changes might be too inconsistent in the smaller test sets of the SANCL TDs. TheWSJ corpus is the only labeled domain that is sufficiently large for this purpose. Consequently, weinvert the usual setup by training the model on the development sets of the five SANCL domains(l:big) or on the initial 5000 labeled words of reviews (l:small). In this reversed setup, u:big utilizesthe five unlabeled SANCL datasets along with a large external corpus as before. Given the importanceof performance variability, we conduct 100 trials on randomly selected 50% samples of WSJ andreport both the average and standard deviation of tagging errors across these trials.The results presented in Table 3 indicate that ONLINE’s error rates are only marginally higher than,or comparable to, those of BATCH. Specifically, in the l:small/u:0 condition, the error rate for knownwords is lower for ONLINE (0.1186) than for BATCH, similar to observations in Table 2.3Table 2: ONLINE / BATCH accuracies are generally better than STATIC (see bold numbers) andimprove with both more training data and more unlabeled data.u:0 u:bigALL KN SHFT OOV ALL KN SHFT OOVl:small STATIC 87.02 90.87 71.12 57.16 89.02 91.48 81.53 58.30ONLINE 87.99 90.87 76.10 65.64 89.84 92.38 82.58 67.09newsgroups l:big BATCH 88.28 91.08 77.01 66.37 89.82 92.37 82.65 67.03STATIC 89.69 93.00 82.65 57.82 89.93 92.41 84.94 58.9790.51 93.13 67.57 90.85 93.04 71.00ONLINE 82.51 84.9483.24 85.20BATCH 90.69 93.12 69.43 90.87 93.03 71.18l:small STATIC 89.08 91.96 66.55 65.90 91.45 92.47 80.11 70.81ONLINE 89.67 92.14 70.14 69.67 92.11 93.62 81.46 78.42reviews l:big BATCH 89.79 92.23 69.86 71.27 92.10 93.60 81.51 78.42STATIC 91.96 93.94 82.30 67.97 92.42 93.53 84.65 69.9793.07 94.36 85.71 79.03ONLINE 92.33 94.03 83.59 72.5092.42 94.09 83.53 73.35 93.07 94.36 85.71 79.03BATCHl:small STATIC 91.58 94.29 79.95 72.74 93.42 94.77 89.80 77.42ONLINE 92.51 94.52 81.76 80.46 94.21 95.40 91.08 84.03weblogs l:big BATCH 92.68 94.60 82.34 81.20 94.20 95.42 91.03 83.87STATIC 93.45 95.64 90.15 72.68 94.09 95.54 91.90 76.9494.86 95.81 92.60 86.53ONLINE 94.18 95.82 89.80 80.35! 94.34 95.85 90.03 81.84 94.86 92.60 86.53BATCH 95.82l:small STATIC 86.93 90.89 66.51 53.43 88.98 91.09 77.63 57.36ONLINE 87.48 91.18 68.07 56.47 89.71 92.42 78.11 64.21answers l:big BATCH 87.56 91.11 68.25 58.44 89.71 92.43 78.23 64.09STATIC 89.54 92.76 78.65 56.22 90.06 92.18 80.70 58.2590.68 93.21 81.48 65.16ONLINE 89.98 92.97 79.07 59.7790.14 93.10 79.01 60.72BATCH 90.70 93.22 81.54 65.29l:small STATIC 85.43 90.85 57.85 51.65 87.76 90.35 70.86 56.76ONLINE 86.30 91.26 60.56 55.83 88.45 92.31 71.67 61.57emails l:big BATCH 86.42 91.31 61.03 56.32 88.46 92.32 71.71 61.65STATIC 88.31 92.98 71.38 52.71 89.21 91.74 73.80 58.9989.85 93.30 75.32 65.48ONLINE 88.86 93.08 72.38 57.7888.96 93.11 72.28 58.85BATCH 89.84 93.30 75.27 65.44l:small STATIC 94.64 95.44 83.38 82.72 95.73 95.88 90.36 87.87ONLINE 94.86 95.53 85.37 85.22 95.80 96.21 89.89 89.70wsj l:big BATCH 94.80 95.46 85.51 85.38 95.80 96.22 89.89 89.70STATIC 96.44 96.85 92.75 85.38 96.56 96.72 93.35 88.0496.85 93.55 86.38 96.62 96.89 93.35 91.69ONLINE 96.5096.57 96.89 93.42BATCH 96.82 93.48 86.54 96.63 91.86Table 3 also includes data on ""unseens"" along with unknowns, as prior research indicates that unseenslead to at least as many errors as unknowns. Unseens are defined as words with tags not present inthe training data, and error rates for unseens are calculated across all their occurrences, includingthose with both seen and unseen tags. As shown in Table 3, the error rate for unknowns is higher thanthat for unseens, which in turn is higher than the error rate for known words.When examining individual conditions, ONLINE generally outperforms STATIC, showing betterresults in 10 out of 12 cases and only slightly underperforming in the l:small/u:big condition forunseens and known words (0.1086 vs. 0.1084, 0.0802 vs. 0.0801). In four conditions, ONLINE issignificantly better, with improvements ranging from 0.005 to over 0.06. The differences betweenONLINE and STATIC in the remaining eight conditions are minimal. For the six u:big conditions,this is expected as the large unlabeled dataset is from the news domain, similar to WSJ. Therefore, iflarge unlabeled datasets similar to the target domain are available, using STATIC tagging may sufficesince the additional effort for ONLINE/BATCH may not be justified.4˘ ˘Table 3: Error rates (err) and standard deviations (std) for tagging. 2020 (resp. 2217): significantlydifferent from ONLINE error rate above&below (resp. from “u:0” error rate to the left).unknowns unseens known wordsu:0 u:big u:0 u:big u:0 u:bigerr std err std err std err std err std err std˘ ˘ ˘l:small STATIC .36702020 .00085 .3094 .00160 .16592020 .00076 .1467 .00120 .13092020 .00056 .1186 .00095˘ ˘ ˘ONLINE .30502020 .00143 .2104 .00081 .16462020 .00145 .1084 .00056 .12512020 .00103 .0801 .00042˘ ˘ ˘BATCH .3094 .00160 .21022217 .00093 .1404 .00125 .10372217 .00098 .1186 .00095 .08022217 .00048˘l:big STATIC .14512020 .00114 .1042 .00100 .0732 .00052 .0690 .00042 .0534 .00027 .0503 .00025˘ ˘ ˘ONLINE .1404 .00125 .10372217 .00098 .0727 .00051 .06892217 .00051 .0529 .00031 .05022217 .00031˘BATCH .13822020 .00140 .1033 .00112 .0723 .00065 .0680 .00062 .0528 .00033 .0502 .00031Increasing the amount of labeled data consistently reduces error rates, as does increasing unlabeled˘data. The differences are significant for ONLINE tagging in all six cases, marked by 2217 in thetable.There is no significant difference in variability between ONLINE and BATCH, suggesting thatONLINE is preferable due to its equal variability and higher performance, without requiring a datasetavailable before tagging begins.The progression of tagging accuracy over time is illustrated in Figure 1. BATCH and STATICmaintain constant error rates as they do not adjust representations during tagging. ONLINE’s errorrate for unknown words decreases, approaching BATCH’s error rate, as more is learned with eachoccurrence of an unknown word.4 Related WorkOnline learning typically refers to supervised learning algorithms that update the model after process-ing a few training examples. Many supervised learning algorithms are online or have online versions.˘Active learning is another supervised learning framework that processes training examples 2014˘usually obtained interactively 2014 in small batches. All of this work on supervised online learning isnot directly relevant to this paper since we address the problem of unsupervised domain adaptation.Unlike online supervised learners, we keep the statistical model unchanged during domain adaptationand adopt a representation learning approach: each unlabeled context of a word is used to update itsrepresentation.There is much work on unsupervised domain adaptation for part-of-speech tagging, including workusing constraint-based methods, instance weighting, self-training, and co-training. All of this workuses batch learning. For space reasons, we do not discuss supervised domain adaptation.5 ConclusionThis study introduces a method for real-time updating of word representations, a new form of domainadaptation designed for scenarios where target domain data are processed in a stream, makingBATCH processing unfeasible. We demonstrate that real-time unsupervised domain adaptationachieves performance levels comparable to batch learning. Moreover, it significantly reduces errorrates compared to STATIC methods, which do not employ domain adaptation.Acknowledgments. This research was supported by a scholarship from Baidu awarded to WenpengYin and by the Deutsche Forschungsgemeinschaft (grant DFG SCHU 2246/10-1 FADeBaC).5"
P052,"Specialized Neural Network for Extracting Financial Trading Signals:The Alpha Discovery Neural NetworkAbstractGenetic programming (GP) is currently the leading method for automated feature generation in financial applica-tions. It utilizes reverse Polish notation to denote features and subsequently performs an evolutionary procedure.Nevertheless, with the advancements in deep learning, more effective feature extraction instruments have becomeaccessible. This research introduces the Alpha Discovery Neural Network (ADNN), a customized neural networkarchitecture designed to autonomously generate a variety of financial technical indicators using establishedknowledge. Our primary contributions are threefold. Firstly, we employ domain-specific expertise in quantitativetrading to formulate sampling guidelines and the objective function. Secondly, we substitute genetic programmingwith pre-training and model pruning techniques to enable a more streamlined evolutionary process. Thirdly, thefeature extraction components within ADNN can be interchanged with various other feature extractors, resultingin the creation of diverse functions. Empirical findings demonstrate that ADNN can produce more distinct andinformative features in comparison to GP, thereby effectively augmenting the existing pool of factors. Fullyconnected and recurrent networks demonstrate superior performance in extracting information from financialtime series compared to convolutional neural networks. In practical scenarios, the features generated by ADNNconsistently enhance the revenue, Sharpe ratio, and maximum drawdown of multi-factor strategies when contrastedwith investment strategies that do not incorporate these factors.1 IntroductionPredicting the future returns of stocks is a paramount and demanding endeavor in the field of quantitative trading. Numerousfactors, including historical price, volume, and a company’s financial information, can be employed to forecast the future returns ofstocks. Typically, researchers categorize features derived from price and volume as technical indicators, while those derived froma company’s financial data are classified as fundamental data. Various well-known multi-factor models have been introduced toaddress this task, and numerous established technical and fundamental factors have been developed. For instance, the Fama-FrenchThree-Factor Model utilizes three crucial factors that furnish the majority of the information required to elucidate stock returns.Subsequently, the Fama-French Five-Factor Model and numerous other factors have been formulated by domain experts. Nonetheless,two limitations exist. Firstly, recruiting human specialists is quite costly. Secondly, humans are unable to create certain nonlinearfeatures from data with high dimensionality. Consequently, both academic scholars and institutional investors have increasinglyfocused on the task of automated financial feature engineering.Feature engineering is a procedure that uncovers the connections between features and expands the feature space by deducing orgenerating novel features. During this operation, new features can be created by combining pre-existing features. A more explicitexplanation is that algorithms employ operators, hyper-parameters, and existing features to construct a new feature. Occasionally,feature construction and feature selection can be integrated into a single process. These methodologies encompass wrapper, filtering,and embedded techniques. Filtering is straightforward but yields suboptimal results; it merely employs certain criteria to select afeature and can sometimes aid in overseeing the feature construction process. The wrapper method exhibits strong performanceby directly utilizing the model’s outcomes as an objective function. Consequently, it can treat an independently trained model asa newly generated feature. Nevertheless, a substantial quantity of computational resources and time are necessary. Embedded isan approach that employs generalized factors and a pruning method to choose or amalgamate features, serving as an intermediateoption between filtering and wrapper techniques.2 Related WorkWith the progression of deep learning, an increasing number of researchers are utilizing neural networks to derive features from rawdata and subsequently incorporating a fully connected layer to modify the feature’s output. Similarly, a trained model signifies anewly developed feature. Researchers have leveraged it on pattern recognition tasks, employing a CNN model to construct facialdescriptors, and this method generates features that possess considerably more information than the previous method. Experimentshave been conducted on this task, employing a deeper and wider convolutional neural network. Recurrent neural networks havebeen used to pre-locate feature-rich regions and successfully construct more refined features. In a text classification task, recurrentneural networks have been utilized to build a rule-based classifier among text data, wherein each classifier represents a portionof the text. A network structure that uses both a recurrent neural network and a convolutional neural network to extract textinformation has been proposed. Utilizing a neural network’s robust fitting capability, we can generate highly informative features bycustomizing the network architecture for diverse industries. In financial feature engineering tasks, researchers have commencedemploying neural networks to provide an embedding representation of financial time series. More specifically, LSTM has beenutilized to embed various stock time series, followed by adversarial training to perform binary classification on a stock’s futurereturn. Well-designed LSTM has been adopted to extract features from unstructured news data, subsequently forming a continuousembedding. The experimental outcomes indicate that these unstructured data can furnish substantial information and are highlybeneficial for event-driven trading. A Skip-gram architecture has been employed to learn stock embedding, inspired by a valuableknowledge repository formed by fund managers’ collective investment behaviors. This embedding can more effectively representthe varying affinities across technical indicators. Adopting a similar concept, we employ a neural network to provide a conciseembedding of extended financial time series.3 MethodologyThe ADNN’s network architecture is structured in a specific way. The primary contributions of this innovative network structure are:1) ADNN employs Spearman Correlation as its loss function, mirroring the practices of human quantitative investment. Furthermore,the sampling guidelines adhere to economic principles. 2) A significant, derivable kennel function is introduced as a substitute forthe non-derivable operator. 3) We utilize pre-training and pruning in place of the GP’s evolutionary process, resulting in enhancedefficiency.In each back-propagation cycle, ADNN randomly selects data from a certain number of trading days and subsequently computes theSpearman Coefficient between the factor value and factor return for each of those days. The number of days should be greater than 3,and incorporating information from multiple trading days enables the neural network to achieve a more consistent convergence.Quantitative investors prioritize the relative strength of each stock on a given trading day over its absolute strength. Therefore,performing calculations for each trading day and employing the Spearman Coefficient as the loss function is justifiable.We posit that there are a certain number of stocks pertaining to a given trading day in each batch. The input tensor has a specificshape because there are a certain number of samples, and five categories of time series: the opening price, high price, low price,closing price, and volume. Each time series has an input length. We also designate the output tensor as the factor value, possessing aparticular shape. The factor return tensor has a specific shape, denoting the profit we can obtain from this asset over an extendedduration. The holding period’s length is defined. Here, we presume that all feature extractors are Multi-layer Perceptrons (MLPs),simplifying the provision of a general mathematical description. In the experimental section, we will present the experimentaloutcomes based on more intricate and varied feature extractors.4 ExperimentsWe utilize daily trading data from the Chinese A-share stock market, encompassing the daily opening, high, low, closing prices, andtrading volume over the preceding 30 trading days. The raw data is standardized using its time-series mean and standard deviationderived from the training set. Both the mean and standard deviation are computed from the training set. We endeavor to employthese inputs to forecast the stock return for the subsequent 5 trading days (utilizing 3-15 trading days is advisable). Furthermore, wemust adhere to market regulations when devising a trading strategy.Extensive experiments have been performed to identify appropriate hyper-parameters. For each experiment, 250 trading daysconstitute the training set, the ensuing 30 trading days serve as the validation set, and the subsequent 90 trading days function as thetesting set. The generated factors maintain a high Information Coefficient (IC) throughout the subsequent 90 trading days. Mostsignificantly, we emphasize a counter-intuitive configuration: the training period should not surpass 250 trading days due to thenon-stationary nature of financial features. If we mandate a feature to function effectively over an extended duration, we will onlyencounter this feature in an over-fitting scenario. Consequently, we devise a rolling forecast framework wherein we automaticallyidentify potent features for each trading day. Each autonomously generated feature will have its own period of prominence on thatparticular trading day. Moreover, these factors not only perform effectively on this single day but also maintain their efficacy forseveral trading days, exhibiting a gradual decline.To ensure an equitable comparison, the identical configuration is implemented for the GP algorithm. The logic of this algorithmreferences related work. Moreover, the input data’s period and type must be consistent. In this paper, we scrutinize the performanceof the constructed features from diverse angles. Typically, institutional investors employ the Information Coefficient (IC), toquantify the amount of information conveyed by a feature. For diversity, cross-entropy is utilized to gauge the distance between thedistributions of two distinct features on the same trading day. 25 ResultsThe network structure can equip ADNN with different deep neural networks. In order to show the general situation, we equip ADNNwith 4 fully-connected layers. Each layer has 128 neural, tanh activate function, L2 Regularization, and dropout technic. Thisgeneral and simple setting is enough to beat the GP. We put forward three schemes help to show how ADNN beat the GP. Only GPmeans only using genetic programming, Only ADNN means only use ADNN to construct factors, GP&ADNN means use GP’svalue to initialize ADNN and then construct factors. All the experiments are conducted out of the sample.Table 1 shows that Only ADNN is better than Only GP, which means ADNN outperforms GP on this task. And we also find thatGP&ADNN is the best, it means that our method can even improve the performance of GP.Table 1: The performance of different schemes.Object Information Coefficient DiversityOnly GP 0.094 17.21GP&ADNN 0.122 25.44Only ADNN 0.107 21.65In real practice, we should leverage the constructed factors to form a multi-factor strategy and compare its performance with GP. Thespecific strategy setting is same as section 3.4, and we have repeated this experiment on different periods of time. The long-termbacktest result is shown in Table 2, Only ADNN always has better performance than the Only GP. It shows that ADNN has alsobeaten the SOTA in real practice. Similar to the conculsions made above, if we combine these two methods together, the combinedfactors’ strategy has the best performance in backtesting.Table 2: Strategy’s absolute return for each scheme.Time Only GP GP&ADNN Only ADNN ZZ500Train:2015.01-2015.12 Test: 2016.02-2016.03 +2.59% +5.74% +4.52% +1.67%Train:2016.01-2016.12 Test: 2017.02-2017.03 +5.40% +10.26% +8.33% +2.53%Train:2017.01-2017.12 Test: 2018.02-2018.03 -5.27% -4.95% -4.16% -6.98%Train:2018.01-2018.12 Test: 2019.02-2019.03 +13.00% +15.62% +15.41% +13.75%All the results shown above is based on the most basic feature extractors. So will there be more powerful feature extractors todiscover knowledge from financial time series? And what is the suitable input data structure for financial time series?Table 3 shows that, basically, all neural networks can produce more diversified features than using GP. But temporal extractors areespecially better at producing diversified features, such as LSTM and Transformer. As for TCN, the author who put forward thisnetwork structure proves its ability to capture the temporal rules buried in data. However, there is a huge difference. TCN relieson a convolution neural network, but LSTM and Transformer still contain recurrent neural networks (Normally, the transformeruses a recurrent neural network to embedded the input data). The existence of a recurrent neural network structure may contributeto the difference in diversity. For Le-net and Resnet, they don’t provide us with more informative features. It looks like that theconvolution network structure is not suitable to extract information from the financial time series.Table 3: The higher are the information coefficient (IC) and diversity, the better is their performance. Normally, a good feature’slong-term IC should be higher than 0.05, but it cannot be higher than 0.2 in an A-share market.Type Network IC Diversity TimeBaseline GP 0.072 17.532 0.215 hoursVanilla FCN 0.124 22.151 0.785 hoursLe-net 0.123 20.194 1.365 hoursSpatial Resnet-50 0.108 21.403 3.450 hoursLSTM 0.170 24.469 1.300 hoursTemporal TCN 0.105 21.139 2.725 hoursTransformer 0.111 25.257 4.151 hoursIn practical applications, we integrate conventional factors with those generated by ADNN to formulate a quantitative investmentstrategy. Our objective is to ascertain whether ADNN can enhance the factor pool and improve upon the traditional multi-factorstrategy.We establish a commonly employed multi-factor strategy to assess its performance in a real-world context. Within the training set,samples whose returns rank in the top 30% for each trading day are designated as 1, while those ranking in the bottom 30% arelabeled as 0. The remaining samples in the training set are discarded. Following the training of these features using XGBoost in3binary logistics mode, the prediction outcome reflects the probability of a stock exhibiting exceptional performance in the subsequent5 trading days. It designates the 50 features constructed by human experts as PK 50, the features constructed by ADNN as New 50,and the features constructed by both GP and PK as GP-PK 50. In separate experiments, we use XGBoost to pre-train both PK 50and New 50 in the training set and then using the weight score from XGBoost to choose the 50 most important features as Combined50. This feature selection process only happens once, and only be conducted in training set.Table 4 shows the results of the backtesting.Table 4: Back testing starts from Jan 2019 to June 2019. The investment target is all A-share, except for the stock can’t be tradedduring this period of time. Strategy’s commission fee is 0.5%. SR refers to Sharpe Ratio, MD represents Max- Drawdown.Type Target Group Revenue MDSR ZZ500 Stock Index 19.60% 13,50%1.982Baseline HS300 Stock Index 18.60% 20.30%1.606 PK PK 50 24.70% 18.90%2.314 GP 50 17.60% 25.30%1.435GP GP-PK 50 25.40% 14.80%2.672 New 50 20.60% 15.80%2.189Vanilla FCN Combined 50 29.60% 15.70%3.167 New 50 18.00% 16.90%1.800! Le-net Combined 50 27.50% 16.40%2.921Spatial New 50 19.90% 15.40%1.962 Resnet-50 Combined 50 29.30% 17.20%2.787 New 50 19.50% 13.00%2.205 LSTM Combined 50 29.90% 15.00%3.289Temporal New 50 22.40% 14.70%2.440 TCN Combined 50 26.90% 16.80%2.729 New 50 21.10% 15.90%2.203 Transformer Combined 50 27.20% 15.10%2.806As shown in Table 4, HS300 and ZZ500 are important stock indices in the A-share stock market. Revenue represents the annualizedexcess return, by longing portfolio and shorting the index. The max drawdown is the worst loss of the excess return from its peak.The Sharpe ratio is the annually adjusted excess return divided by a certain level of risk. These indicators can show the strategy’sperformance from the perspective of both return and risk.For the New 50, although they have higher IC than the PK 50, their overall performance is not always better than PK 50. Because theoverall performance of a multi-factor strategy is determined by both diversity and information volume (IC), we guess the diversity ofPK 50 is remarkably higher than the diversity of New 50. We also did experiment to verify this guess. Thus, although every singlenew factor is better than the old factor, their overall performance not always be better. ADNN’s diversity is larger than the GP, butfor further research, making ADNN’s diversity even larger is still badly needed. In the real world use case, all investors have theirown reliable and secret factor pool, what they want is that the new constructed factors can bring in margin benefits. Thus, they willuse both new and old factors to do trading. That’s the reason why Combined 50 can represent ADNN’s contribution in the realsituation. In all cases, Combined 50 is better than PK 50 and GP-PK 50, which means that the ADNN not only perform better thanGP, but also can enrich investors’ factor pool. 46 ConclusionIn this research, we introduce the Alpha Discovery Neural Network (ADNN), a system capable of autonomously generating financialfeatures from raw data. We have meticulously crafted its network architecture in accordance with economic principles and furnishedit with a variety of sophisticated feature extractors. Empirical results indicate that ADNN can generate features that are moreinformative and diverse than those produced by the benchmark method in this specific application. In practical scenarios, ADNN alsodemonstrates superior revenue, Sharpe ratio, and maximum drawdown compared to genetic programming. Furthermore, differentfeature extractors assume distinct roles. We have conducted numerous experiments to validate this observation and endeavor tocomprehend its functionality. For future research, we intend to employ this framework to automatically generate valuable featuresbased on companies’ fundamental data and sentiment data. 5"
P055,"Examining the Initial Experiences of ResearchersWhen Articulating Broader ImpactAbstractBy mandating a broader impact statement with every submission for this year’sconference, the program chairs at the conference highlighted ethics as a crucialcomponent of AI research. Building on precedents from other fields and a grow-ing awareness within the community, this paper seeks to explore how individualresearchers responded to this new requirement. This exploration includes theiropinions, their experiences during the drafting process, and their reflections aftertheir papers were accepted. We present survey results and key considerations toinform the next iteration of the broader impact requirement, should it continue tobe mandated for future conferences.1 IntroductionThere is a growing number of unethical uses of technology. To counter this trend, some proposalssuggest limiting investment or procurement without impact assessment, or even calling for outrightbans. Other proposals aim to instill ethical practices earlier in the research stage, before technologytransfers into products. Conferences that are typically technical have begun to host workshops onsocial impact issues and, in some instances, have announced more interdisciplinary subject areas.The most significant change may be the requirement for a statement of broader impact for allsubmissions. Unlike workshops and interdisciplinary tracks, which might be viewed as more specific,this requirement affects every submission, of which there are over 9000 this year. While broaderimpact statements themselves are not new to the wider research community, they are new to thisspecific community. This paper seeks to explore how individual researchers responded to the newrequirement, including their perspectives, their experiences and process in drafting the statements,and their subsequent thoughts after paper acceptances.This research was initiated through internal discussion at our organization, which then became partof a broader public conversation. To collect perspectives from researchers, both within and beyondour organization, we developed an online public survey. The findings from this survey help toinform considerations for designing the next iteration of the broader impact requirement, shouldit remain a requirement for future conferences. While it is recognized that researchers are not theonly intended audience for these statements, and that others also have responsibilities in ethicalresearch and technology development, researchers represent a critical mass to mobilize in this effort.Understanding the researchers’ experience and process is essential not only to the design of therequirement, but also to advancing ethical research practices in general.2 Survey MethodThe study employed an exploratory mixed-methods survey with both open and closed-ended questions.The survey was split into two sections, one for researchers who submitted to the conference, andanother for those who did not. The survey was anonymous, and no demographic information wascollected. The survey was distributed online via research community channels and on social media.The goals were to understand how researchers considered the implications of their research, how.they defined their impact statements, and to understand their opinions on this new submissionrequirement. Survey questions focused on their approach to writing the statement, encounteredchallenges, the perceived influence of the statement on the overall submission, and their views on thenew requirement.3 Survey ResultsA total of 50 participants responded to the survey, with the majority identifying as academics (72percent) and industry researchers (23.5 percent). There was a balanced breakdown by career stage,with graduate students making up the largest group of respondents (33 percent). Among the groupthat submitted, the majority identified their subject areas as deep learning and theory. However,among researchers who did not submit, deep learning and social aspects of machine learning were theprimary subject areas. The survey population was not compared to the overall population, though thiscould be an area for future study. Our questions focused on the process and challenges in completingthe submission requirement, the perceived impact of the requirement on paper acceptances, andresearchers’ views on the requirement.3.1 Process and ChallengesWhen asked about their approach to the broader impact statements, 83.8 percent of respondentsindicated that they completed this part with their co-authors, without external help. The rest ofthe participants used other approaches such as accepting support or reaching out for help. A largemajority spent less than 2 hours on the statement, and almost half mentioned it was not challenging toprepare. There were differing trends for what could make it difficult. Some viewed their theoreticalwork as too distant from practical applications, making the exercise speculative. Others perceivedthe requirement as a ""bureaucratic constraint"". Researchers at different stages of their career foundthe exercise more or less challenging, but their professional domain did not appear to affect theirexperienced difficulty with the exercise.3.2 Impact on SubmissionAlthough it was clarified that submissions would not be rejected solely on the basis of the broaderimpact statement, the survey explored the researcher’s perspectives on this. For researchers whosubmitted, over 75 percent believed the statements were not taken into consideration, yet almost90 percent thought it was unclear how reviewers would evaluate the statements. Even with anunclear evaluation process, when asked how confident they were that their statement was adequatelyaddressing the requirement, 43.2 percent stated that they were either confident or very confident.Time spent did not seem to have an impact, since most of the respondents who spent less than anhour also received acceptances. Those who sought external help appeared to have a lower ratio ofrejections, but our sample size may be too small to draw conclusive results.3.3 FramingThe survey explored researchers’ views on the requirement and its framing. Our results indicatedthat the community was divided on how to frame the requirement; 56 percent did not agree thatbroader impact was the right way to frame the requirement, while 44 percent did. This split wassimilar when compared to subject area, submitters vs. non-submitters, and academia vs. industry.Postdoctoral/early-career and mid-career respondents were more supportive of the requirementframing than students and senior researchers. There seems to be a general feeling that assessingbroader impact is important, but uncertainty regarding who should do it and how. Some respondentsdescribed the requirement as ""too broad"" or said they did not feel ""qualified to address the broaderimpact of their work."" Some who supported the requirement found the thought process to be valuableand that it ""forces researchers to reflect on the impact of their research"".4 Integrating Feedback into Next Iteration of Broader ImpactThe survey results inform future iterations of the broader impact requirement. When asked whatcould have helped them most, 92 percent of respondents indicated that examples of statements2would be most helpful. There will be an increasing number of examples to draw from in futureyears. Guidelines were the second most popular request, regarding when a statement might beapplicable or how to formulate one. This section proposes how to integrate respondent feedbackinto future iterations: rethinking the requirement design and framing, developing greater capacityand confidence among researchers, and reflecting the shared responsibility of ethical research andtechnology development.4.1 Requirement DesignIf the goal is to develop ethical research practices, there may be other approaches to achieve thisgoal. While written statements make sense given the paper-based nature of submissions, surveyrespondents indicated a mix of nonchalance, outright farce, or perceived burden. These attitudesmay have a counterproductive effect on an ethical research goal. We encourage program chairs toconsider mechanisms to limit that effect (e.g., an incentive for ""best"" broader impact statements).Such mechanisms are important not only to manage negative effects but also to encourage researcherswho found the exercise valuable.4.2 Capacity BuildingGiven that many respondents felt they were not qualified to address the broader impact of theirwork, workshops may help build capacity over time, and provide a space for researchers to examinetheir work with a more diverse group of researchers. Discussions could help develop capacity andconfidence, and surface overlooked impacts. Interdisciplinary collaborations could also introducenew guidelines or methodologies such as the theory of change or consequence scanning.4.3 Shared ResponsibilityRecognizing how different systems and social contexts interact would increase the quality of thediscussion on broader impact, and develop a sense of shared responsibility for ethical research andtechnology development. Researchers are a critical mass, but others such as conference organizers,institutions, funders, and users also have roles and responsibilities. To address concerns aroundburden and expertise, the assessment of broader impact could be more of a multi-stakeholder exercise.5 ConclusionThis paper and its underlying survey investigated how researchers approached the broader impactstatement and surfaced considerations to better design this requirement in future years. While thesurvey represented a small sample of the community, its results demonstrate a division regardinghow the requirement is framed. Initiating a conversation about broader impact is itself a step towardsestablishing norms and best practices for ethical research. We encourage further work to monitor theevolution of researcher’s perspectives, not only at top conferences, but also at-large.6 AcknowledgementsThe authors thank Noémie Lachance, Tara Tressel, and the Research Group for their support andparticipation throughout this project.7 Supplementary MaterialThe survey questions and the responses received are available for further investigation and use. Thesurvey remains open to responses. At the time of writing, we had 50 responses which were used forthe analysis in this paper. 3"
P057,"A Collaborative Painting Experience:Human-Machine Interaction on CanvasAbstractWe introduce a novel approach to human-machine interaction, framed as a pictorialgame where artists and a computer collaborate in iterative creative rounds. Thecomputer uses machine learning to partially complete the artwork at each stage,projecting its additions directly onto the canvas, which the artists are then able tomodify or incorporate. This process encourages creative exploration and provokesquestions about the growing relationship between humans and machines.1 IntroductionThe ongoing technological advancements are reshaping human-machine interaction, providing newtools for artistic creation while simultaneously prompting contemplation on their effects on humancreativity.Generative Adversarial Networks (GANs) have demonstrated the creative abilities of neural networks,producing aesthetically full paintings. However, in these instances, humans serve as either engineersor curators. Our work introduces a new method of machine utilization, integrating it into the core ofhuman creative processes. While painting, this approach presents humans with different paths andconcepts for their artwork. This concept is approached through a unique interactive framework.The artist duo Tina and Charly have previously investigated interaction through canvas art. To initiatetheir creative work, they select a theme and depict it in dark colors on a white canvas. They thenstart their game. At each round, using a vocabulary of strokes and symbols, Charly anticipates Tina’semotions and thoughts in red, before responding with green strokes on the painting. These roundscontinue until both artists reach an agreement on finishing the painting. The entire process unfolds insilence, with the canvas serving as the sole medium of dialogue.The purpose of our work is to introduce artificial intelligence as a third participant in Tina andCharly’s dialogue. The AI initially captures a raw representation of the painting, then processes it topartially complete the work in progress, which it projects back onto the canvas. The artists then havethe freedom to incorporate the machine’s suggestion in blue, a color that has not been assigned toeither player. The use of different colors allows for the analysis of each player’s contributions.2 MethodologyThe engineered system includes a camera and a projector connected to a computer on a support. Ateach computer round, the system captures an image of the painting and analyzes it to extract thecanvas strokes. This pre-processing is made robust to changes in lighting, ensuring that the interactioncan be used seamlessly in any studio. These strokes then feed into a neural sketcher, which producesnew strokes to be added to the painting. Post-processing is used to project those additions back ontothe canvas.The neural sketcher is a recurrent neural network, based on a recent improvement to the seminal workof previous research. It is trained using a sequence of points and a channel encoding for stroke breaks.The sketcher produces a similar series, which is then converted back into strokes on the original.painting. The network was trained using the QuickDraw data set, enabling it to create human-likestrokes. For integration with Tina and Charly’s style, the learning was refined using a sketch databasefrom previous paintings by the artists.3 DiscussionThe artists found the machine strokes to be surprising and suggestive of movements they would nothave made on their own. Some painters have previously expressed how unintended strokes can beevocative. Our installation, where the machine projects completions without physically painting, andthe generative network capabilities, allows this to be explored. Furthermore, the ability to changeparameters, such as the learning data set, provides the artist with more control over their usage of themachine.Our interactive installation can be used by anyone and aims to raise awareness and initiate thoughtabout the interplay between humans and machines. This work highlights the need to make machineshuman-friendly, while also acknowledging how technology changes human behaviors and routines.Tina and Charly felt like they were interacting with a full-body system, which had been designedto simulate human-like painting. They experienced the machine as sometimes restricting, hard tounderstand, and sometimes magical. It infused new dimensions into the painting. The feeling that themachine could be collaborative or limiting is an echo of the role of technologies in our daily lives.From an outsider’s perspective, the machine changes their original painting style, both in the shortterm artworks (as seen in Figure 2), and on their long-term body of work, inspiring their machine-freepaintings. Even though we have made the machine’s influence explicit with its blue contributions, theinteraction is not neutral.4 AcknowledgmentsThe authors would like to thank Yana Hasson and Yann Labbé for coding insights, Erwan Kerdreuxfor art history discussions, and Thomas Lartigue for general discussions.2"
P058,"Enhanced Vocabulary Handling in Recurrent Neural NetworksThrough Positional EncodingAbstractThis research presents a counterintuitive discovery: positional encoding, a high-dimensional representation oftemporal indices, improves the learning capabilities of recurrent neural networks (RNNs). While positional encod-ing is well-known for its crucial role in enabling Transformer networks to process sequential data, its applicationto RNNs, which inherently manage temporal information, seems unnecessary. However, our experiments withsynthetic benchmarks demonstrate that incorporating positional encoding into RNNs enhances their performance,particularly when dealing with extensive vocabularies that result in numerous low-frequency tokens. A detailedanalysis reveals that these infrequent tokens introduce instability to the gradients of standard RNNs, and positionalencoding effectively counteracts this instability. These findings highlight a previously unrecognized benefit ofpositional encoding, extending its utility beyond its conventional function as a temporal marker for Transformers.1 IntroductionSince their introduction, Transformer neural networks have become the preferred method for processing and generating time seriesdata, surpassing traditional models like recurrent neural networks (RNNs). A significant distinction between these two types ofmodels lies in their approach to encoding temporal information, which refers to the sequence of individual data points, or tokens,within the time series. RNNs encode this information by sequentially updating their internal state based on both the current inputand the preceding state. Conversely, Transformers do not inherently possess a mechanism to represent the order of data points; thus,they depend on an external system known as positional encoding to provide this temporal context.Positional encoding offers a high-dimensional representation of the temporal indices associated with input data. Its most commonimplementation involves the use of sinusoidal waves with predetermined frequencies. This method ""timestamps"" input tokensby adding or concatenating these encoding vectors to the corresponding input embeddings. In contrast to RNNs, the temporalrepresentation provided by positional encoding remains unchanged by input values until processed collectively by a network.Although positional encoding has often been viewed as a substitute for the temporal processing capabilities of RNNs when usedwith Transformers, the two are not inherently incompatible. Inputs to RNNs can be augmented with position-encoding vectors,despite this appearing redundant. The presence of autonomous activities in biological neurons, like neural oscillations, is believed tobe significant in time perception and other perceptual processes, as well as in motor control.This study, therefore, investigates the application of positional encoding to the inputs of RNNs using synthetic benchmarks. Theresults demonstrate that positional encoding helps RNNs manage a more diverse set of discrete inputs, effectively handling a largervocabulary, compared to those without positional encoding.The contributions of this research are outlined as follows:• Challenges in training RNNs with extensive vocabularies are shown through carefully designed benchmark tasks. Thisissue, despite its potential implications for practical applications, has not been previously identified or has received minimalattention.• The identified training challenges for RNNs with large vocabularies are explained by gradient instability caused byinfrequent tokens, which are inevitable when expanding vocabulary size.• A new effectiveness of positional encoding is revealed by combining it with RNNs, showing it mitigates the large-vocabularyissue by stabilizing RNN gradients against the disruptions caused by infrequent tokens.2 Related Studies2.1 Theoretical and Empirical Computational Power of (Vanilla) RNNsMathematically, RNNs are recognized as Turing-complete, meaning they can simulate any Turing machine if their weights haveunlimited precision and are perfectly tuned. Even RNNs with random recurrent and input-to-hidden weights, known as reservoircomputers, can achieve universal approximation if their hidden-to-output weights are idealized. These theoretical insights havedriven the use of RNNs in processing complex time series like human languages and weather patterns.However, in practical scenarios, RNN weights are limited by finite precision and must be optimized based on a finite set of dataobservations. These constraints place limitations on the actual capabilities of RNNs. For instance, empirical RNNs cannot store aninfinite number of observations in their memory, and the stored information degrades over time. This issue of memory duration hasbeen a focal point for researchers, leading to extensive exploration of RNN architectures that can retain memory for longer periods.More recently, the focus of research on extending memory retention has moved towards continuous-time models. Instead ofrepresenting the memory of an input sequence through discrete-time changes in a latent state, these models approximate the inputhistory using a linear combination of orthogonal polynomials in continuous-time space. The coefficients of these polynomialsprovide a finite-dimensional representation of the input sequence, known as the High-Order Polynomial Projection Operator (HiPPO),and the dynamics of these coefficients can be described by an ordinary differential equation (ODE). This concept of continuous-timememory representation has been further developed into neural state-space models by replacing the fixed state matrix in the ODEwith a learnable one, while restricting its structure to a diagonal matrix plus a row-rank matrix. Notably, with further refinements,the latest state-space model has achieved language modeling performance that rivals that of Transformer-based models.2.2 Positional EncodingPositional encoding serves as a high-dimensional representation of the temporal structures present in input data. The primary needfor this type of representation arises from Transformers, which, unlike RNNs, do not have an inherent mechanism for representingthe order of inputs. Consequently, input tokens to a Transformer are ""time-stamped"" by adding or concatenating a position-encodingvector.In the initial implementation of the Transformer, token positions were encoded using sinusoidal waves of various predefinedfrequencies. While this original encoding method is effective for a wide range of tasks, researchers have also explored otherpossibilities. For instance, the well-known BERT pretraining for natural language processing used learnable embeddings to encodetoken positions. Some research has also indicated that combining sinusoidal and learnable encoding can enhance model performance.Another approach involves encoding the distance between tokens rather than the time elapsed since the beginning of the sequence.Beyond Transformers, positional encoding is utilized to represent elapsed time in diffusion processes. Furthermore, the effec-tiveness of positional encoding is not restricted to temporal information; previous studies in three-dimensional mesh/point-cloudmodeling have shown that sinusoidal transformation of spatial data improves model performance compared to using raw coordinaterepresentations.Despite the extensive use of positional encoding across various areas of machine learning, its application to pure RNNs remainslargely unexplored. To the author’s knowledge, only two studies have previously investigated position-encoded RNNs. Karanikolosand Refanidis (2019) found that a position-encoded LSTM outperformed a standard LSTM as well as a shallow Transformer intext summarization tasks. In another study, which predates the introduction of sinusoidal positional encoding in the deep learningcommunity, Vincent-Lamarre et al. (2016) demonstrated that oscillatory signals at random frequencies enhanced the performanceof a random RNN (i.e., reservoir computer) in a timing task, evaluating the model’s memory duration by its ability to generate asmoothed output pulse after a specific time interval from an onset signal.Similarly, the time index in time series data has rarely been directly used by RNNs, likely due to its perceived redundancy alongsidethe functionality of RNNs. As an exception, Neil et al. (2016) introduced a periodic gating mechanism for updating the state andmemory cell of LSTM. This periodic gating was scheduled based on a triangular wave interspersed with a plateau at the floor value(= 0.0; the frequency, phase, and duration of the wave phase were learnable parameters).3 Methods3.1 TaskThe impact of positional encoding on RNNs was examined using a reverse-ordering task. In this task, RNNs were trained toreconstruct a sequence of random integers in reverse order.3.2 Model ArchitectureThe research in this study was based on single-layer gated recurrent units (GRUs), long short-term memory (LSTMs), and a neuralstate-space model, S4D (S4 with a diagonal state matrix). Each integer in the input sequences was first embedded, then concatenated2with the positional encoding, and subsequently fed into the RNN/S4D. After processing the entire input sequence, the networkreceived a command to produce the output. This command was represented by a time-invariant learnable vector and was fed to theRNN in place of the input embedding. The outputs from the RNN/S4D module were linearly projected into classification logits. Thecross-entropy loss between these logits and the target sequence was used to optimize the entire network. Model predictions duringthe testing phase were determined by the argmax of these logits for each time step.This study used the standard sinusoidal positional encoding designed for Transformers. Specifically, each time step t was encoded bythe Dpos-dimensional vector, defined as follows: (cid:19)(cid:18) t − 1P Et, 2i := sin (1)2(i−1)10000 Dpos(cid:18) (cid:19)t − 1P Et, 2i + 1 := cos (2)2(i−1)10000 DposFor learning stability, the positional encoding was divided by the square root of Dpos/2, ensuring that the encoding vectors had aunit L2-norm. The time step t incremented throughout both the input and output phases (i.e., t = 1, ..., L, L+1, ..., 2L, where L is theinput length), without any hard-coded association between the input and output positions.3.3 Implementation DetailsAcross the experiments, the dimensionality of the hidden layer of the RNNs was set to 512. The embedding of the input integers andthe memory cell of the LSTM also had the same dimensionality of 512. Similarly, the hidden dimensionality of S4D was set to 512,while its state size (or the order of the Legendre polynomials) was maintained at the default value of 64.˘ ˘The models were trained for 300,000 iterations using the Adam optimizer with parameters (03b21, 03b22) := (0.9, 0.999) and noweight decay. The learning rate was linearly warmed up from 0.0 to 0.001 for the first 1,000 iterations, and then annealed accordingto the cosine schedule. The batch size was 512.All experiments were implemented in PyTorch (ver. 2.1.1) and each training-test trial was executed on a single NVIDIA A100 GPU(with 80GB VRAM).4 Results4.1 Key FindingsPositional encoding was found to enhance the ability of RNNs to manage a larger vocabulary in the reverse-ordering task. Theposition-encoded GRU and LSTM successfully reversed input sequences of 64 integers drawn uniformly at random from vocabulariesof sizes 32-256 and 256-16,384, respectively, achieving token-wise accuracy above 95%. In contrast, the performance of the standardmodels without positional encoding deteriorated as the vocabulary size increased. Similarly, positional encoding improved thecapacity of S4D to handle large vocabularies. These improvements are also evident in the reduced sequence-wise reconstructionerrors, as measured by the Damerau-Levenshtein distance. Neither additional training iterations nor larger batch sizes improved theperformance of the standard models.4.2 Frequency MattersThe most noticeable effect of increasing the vocabulary size was the decreased probability of observing individual vocabularyitems. Therefore, additional experiments were conducted with non-uniformly distributed tokens to examine the relationship betweentoken frequency and RNN performance. Specifically, the input vocabulary was evenly divided into Frequent and Rare groups, with˘Frequent tokens having three times the probability of Rare tokens. The probability of each Frequent token was 7/8 00d7 2/K (whereK is the total vocabulary size, set to 64, 1024, and 2048 for GRU, LSTM, and S4D, respectively), while the probability of each Rare˘token was 1/8 00d7 2/K.The training data consisted of 64 independent samples from this dual-frequency vocabulary. The test data were systematicallyconstructed so that each sequence included a single ""target"" token (Frequent/Rare) whose retrieval accuracy was assessed, alongwith 63 ""disturbants"" that were either all Frequent or all Rare. The experiment revealed that the frequency of the disturbant tokenssignificantly affected the performance of the standard RNNs and S4D. Rare targets were successfully retrieved as long as they weresurrounded by Frequent disturbants. However, the standard GRU struggled to recover Frequent targets when the other input tokenswere filled with Rare disturbants. LSTM performance also degraded, especially when targets were positioned in the first quarter of˘ ˘the input sequence (1 2264 t 2264 16). Similarly, Rare disturbants were detrimental to S4D; unlike the RNNs, the accuracy was˘ ˘lowest when targets were located in the middle of the input sequences (17 2264 t 2264 32).3In contrast, the position-encoded RNNs showed robustness to the frequency of both target and disturbant tokens. They achievednearly perfect accuracies in most cases, except when the GRU processed fully Rare data with the target in the first half of the˘ ˘sequence (1 2264 t 2264 32). Likewise, positional encoding enhanced the resilience of S4D against the influence of Rare disturbants.4.3 Analysis of Gradient StabilityTo further investigate the influence of token frequency on RNN performance, the gradients of the RNN latent states were analyzed.Pairs of input sequences were processed by RNNs trained on the dual-frequency vocabulary. Each pair shared the same initial token˘ ˘(t = 1; ""target"") but varied in subsequent tokens (2 2264 t 2264 L; ""disturbants""). Gradients were then computed for the distantmapping between the first and last updated states (at t = 1 and 2L) of the RNNs using backpropagation through time. The stabilityof RNN learning was assessed by measuring the dot-product similarity of the gradients between the paired input sequences (afternormalization over output dimensions).Formally, the paired input sequences, denoted as A and B, established two distinct, but ideally similar mappings, f(A) and f(B), fromthe first to the last latent state of the RNNs. The gradient stability of the RNNs was defined by the dot-product similarities betweenthe normalized gradients of these paired mappings:  (B)(A)D D 2D ∂h∂h(cid:88) (cid:88) (cid:88) 2L,i2L,i(A) (A) (B) (B) (A) (B)(A, B) := ⟨α ·∇f (⃗z ), α ∇f (⃗z )⟩ = α αStability (3)1 1i i i i i i ∂z ∂z1,j 1,ji=1 i=1 j=1˘ ˘where the coefficients 03b1(s) i normalized the raw gradients 2207f (s) i ( z1) over the output dimensions i := 1, . . . , D:(cid:118)(cid:118)    (cid:117)(cid:117) 22(cid:32) (cid:33) (cid:44) (cid:32) (cid:33)(s)(s)2D D 2D(cid:117)(cid:117) ∂h∂h(cid:88) (cid:88) (cid:88)2L,i 2L,k(s) (cid:117)(cid:117)α := (4)   (cid:116)(cid:116)i ∂z ∂z1,j 1,jj=1 j=1k=1Consequently, the stability metric emphasizes the consistency of the paired gradients that both have a greater L2-norm across theoutput dimensions.It is important to note that the mapping from the first to the last RNN state was conditioned on the disturbant tokens occurring at 2˘ ˘2264 t 2264 L. Nevertheless, the reverse-ordering task trained the networks to retrieve the initial token as their final output regardlessof the intervening tokens. Thus, a well-trained RNN would maintain invariance in its final state over the disturbants. Conversely,consistent gradient directions across varied disturbants would lead to successful learning, which is the premise of the proposedanalysis.Unlike the RNN models, both the standard and position-encoded S4Ds achieved high accuracy over 96% for the initial target token(t = 1), regardless of the frequency of the target and disturbants. Therefore, for the analysis of S4D, the target token was positionedin the middle at t = 23, where the standard model exhibited its poorest accuracy with Rare disturbants. The disturbants were prefixedand suffixed to this target to construct input sequences. The prefix disturbants were shared between the paired sequences, ensuringthat the latent dynamics of the model remained identical up to the target token.It should also be noted that the latent states of S4D are complex-valued (while its outputs are real-valued), and consequently, thegradients and their dot-product similarities are also complex-valued. For this analysis, the complex-valued gradients were treated asdouble-sized real arrays, and a real-valued similarity was defined by Eq. 3. This is equivalent to taking the real component of thecomplex-valued similarity and is intuitively natural given that a perfect alignment between complex gradient directions yields areal-valued score of 1.0. Additionally, the extra dimension in the latent states representing the order of the Legendre polynomialswas merged with the channel dimension, and the entire state was treated as a flattened vector.Monitoring the gradients at training checkpoints revealed that Rare disturbants destabilize the learning of standard RNNs. Thesimilarity of the paired gradients decreased gradually (GRU) or rapidly (LSTM) when the networks were exposed to Rare disturbants.Most notably, positional encoding endowed the RNNs with robustness to these Rare disturbants. Both the GRU and LSTMmaintained high similarity of the paired gradients across different target/disturbant conditions. In contrast, the impact of positionalencoding on the gradient stability of S4D was marginal; unlike the RNNs, the standard S4D was highly stable by itself against Raredisturbants throughout training, although there was a visible relative destabilization due to Rare disturbants compared to Frequentdisturbants in the early stages of training, as well as an observable improvement by positional encoding. It is also noteworthy thatthe difference between Frequent and Rare disturbants diminished after 10,000 training iterations. Consequently, gradient stabilitydoes not fully account for the decline in S4D accuracy in the presence of Rare disturbants, nor does it explain the enhancementbrought about by positional encoding. 45 Discussion5.1 Difficulties in Handling a Large VocabularyThis study introduced a novel challenge in training standard RNNs: large vocabularies. While investigating the manageablevocabulary size of RNNs appears to be a relevant research area, crucial for practical applications like natural language processing,previous studies have primarily focused on evaluating and improving the memory duration of RNNs, typically setting the vocabularysize to a small value (= 8).This research examined RNN gradients and identified their destabilization when processing low-frequency tokens, which arenecessarily included in a large vocabulary. Specifically, inputs that do not contribute to gradient-based optimization at a target time˘ ˘step (e.g., tokens at 2 2264 t 2264 L upon the retrieval of the initial token at t = 2L in the reverse-ordering task) were found to bedetrimental.In general time series processing, data points carrying crucial information for specific time steps become irrelevant otherwise.Consequently, each token exhibits a dual nature—both crucial and noisy—throughout the task. Processing rare tokens is particularlychallenging, presumably because they are irrelevant most of the time while making a large impact on learning through the greaterloss to compensate for their fewer learning opportunities. Dealing with such ""unignorable noise"" presents a pervasive challenge forRNNs.5.2 Functionality of Positional Encoding beyond the Timekeeper for TransformersAlthough low-frequency tokens destabilize the gradient-based learning of RNNs, this study also discovered that this issue can bealleviated by positional encoding. This enhancement of RNNs via positional encoding is noteworthy because RNNs were specificallydesigned to process time series data on their own; hence, unlike Transformers, they are presumed to function without relying on an""external clock"". Consequently, position-encoded RNNs have remained largely unexplored, with only two exceptions to the best ofthe author’s knowledge. The findings of this study—namely, the improvement in the manageable vocabulary size due to enhancedgradient stability—broaden the currently limited understanding of the impact of positional encoding on RNNs.Additionally, the results of this study shed new light on the utility of positional encoding. While positional encoding has been viewedas nothing more than input timestamps for Transformers, this study demonstrated its effectiveness in stabilizing the gradients ofRNNs against disruption by low-frequency tokens. This novel functionality of positional encoding would not have been visible inTransformer studies, as the model can dynamically adjust the relevance of input tokens through their attention mechanism and thusinherently mitigate the impact of disturbant tokens.5.3 Limitations and Future DirectionsA primary unresolved question in this study pertains to the mechanism behind the gradient stabilization by positional encoding. Allfindings here are based on experimental investigations, lacking rigorous mathematical explanations for how and why the gradients ofRNNs are destabilized by infrequent tokens and stabilized by positional encoding. Moreover, this study primarily focused on thecanonical implementation of sinusoidal positional encoding designed for Transformers (Eqs. 1, 2), leaving it open which parametersof the sinusoidal waves (i.e., frequencies and phases) are critical for gradient stabilization. Future research may broaden its scope toencompass more general forms of positional encoding, such as wavelets and non-periodic signals.Moreover, the analysis of gradient stability did not fully address the enhanced performance of the position-encoded state-spacemodel (S4D). In terms of accuracy, the positioned-encoded S4D exhibited greater robustness to infrequent tokens compared to thestandard model, resembling the behavior observed in RNNs. However, the gradients of the standard S4D were too stable to accountfor this decline in performance. This leaves open the question of how positional encoding influences gradient-based learning ofstate-space models. Additionally, future studies may investigate a broader range of state-space models—including the state-of-the-artarchitecture of Mamba—to achieve a comprehensive understanding of the interplay between positional encoding and these models.In addition to these scientifically oriented questions, future studies could also address practical applications of position-encodedRNNs and neural state-space models. Although positional encoding enhanced model performance across different synthetic tasks,the extent of this enhancement is task-dependent. Indeed, while a previous study reported the effectiveness of positional encodingfor an LSTM text summarizer, the present study found no empirical advantage for the language modeling task, aside from a slightlymore rapid decline in training loss. Thus, positional encoding is not a panacea for arbitrary tasks, and further investigations arenecessary to determine when it is effective.6 Appendix6.1 A Other TasksThis section demonstrates the effectiveness of positional encoding on RNNs across different tasks, besides the reverse ordering taskdiscussed in the main text. 56.1.1 A.1 Reverse-Ordering + Delayed-AdditionThis section reports the performance of position-encoded RNNs on a more complicated, combinatorial task than the reverse orderingof input sequences. Extending the reverse-ordering task, the models received additional random input integers during the outputphase, and added each of them to the corresponding token in the reverse-ordered input sequence (modulo the vocabulary size, so thatthe output range was bounded).This task was too challenging to GRUs—even after reducing the input length to L = 16—so only the results from LSTMs are reportedbelow. Also, the network was trained for 600,000 iterations (i.e., twice longer than the other tasks) for ensuring the convergence.The other conditions/hyperparameters were the same as reported in the main text.Consequently, positional encoding improved the model performance as the vocabulary size grew from 896 to 1088.6.1.2 A.2 SortingIn the reverse ordering task, the order of input integers was important information for accomplishing the task. Thus, positionalencoding may play its originally intended role in encoding the temporal information.This section reports the effectiveness of positional encoding for a task in which the order of input observations was completely˘irrelevant; the learning objective was to simply sort the input integers in their inherent ascending order (e.g. 8, 29, 2, 11 2192 2, 8,11, 29). The input integers were uniformly randomly sampled with replacement, allowing for ties in the sorting process.As a result, positional encoding also proved effective for RNNs to handle a larger vocabulary in the sorting task, though theimprovement remained marginal compared to the reverse-ordering task.6.1.3 A.3 Predecessor QueryFinally, this section presents benchmark results for the predecessor-query task. The network first received a sequence of non-repeating˘ ˘random integers, x1, . . . , xL. Subsequently, one of the non-initial input integers, xtquery (2 2264 tquery 2264 L), was randomlyselected and reintroduced to the network at time t = L + 1. The learning objective is to return the predecessor of the reviewed integer˘(= xtquery22121). The predecessor-query task evaluates the capacity of RNNs to integrate information regarding both the order andcontent of input sequences.As in the reverse-ordering + delayed-addition task, the input sequence was reduced to L = 16 due to the complexity of the task, andthe experiment focused on the LSTM. The number of training iterations was maintained at 300,000. Similar to the other benchmarks,positional encoding improved the LSTM’s capacity to manage the larger vocabularies.6.2 B Robustness to Variations in Input LengthSo far, all the tasks were experimented using fixed-length inputs (L = 64). One might wonder if positional encoding is exceptionallyeffective under this setting, informing RNNs with the exact timing when each input token should be returned as the output. Thus, itremains unclear whether or not position-encoded RNNs can also handle a larger vocabulary even when the input length is variableand, thus, the exact timing of the output emission is not identifiable from the positional encoding attached to the inputs.To assess the robustness to variations in the input length, an additional experiment was conducted on the LSTM, with the inputlength varied between 32 and 64. In this setup, the maximum input length (= 64) covers the entirety of the shortest input sequenceplus its reversed reconstruction (= 32 + 32). Consequently, the positional encoding per se cannot even distinguish the input vs.output phases at t = 33, . . . , 64. The vocabulary size was set to 16,384.As a result, the positional encoding still improved the LSTM’s performance on the reverse-ordering task against the perturbations inthe input length. This result suggests that the effectiveness of the positional encoding for RNNs is not limited to strictly scheduledtasks.6.3 C Effects of Additional Parameters in Position-Encoded RNNsThe concatenation of positional encoding with input embeddings inflates the number of learnable parameters in the input-to-hiddenprojection weights. This additional parameterization per se does not influence the learning of the input embeddings, and thereforedoes not elucidate the enhanced performance of position-encoded RNNs. This section substantiates this argument by equalizing thenumber of learnable parameters between the standard and position-encoded models.Specifically, the equalization was achieved by concatenating two identical copies of the input embeddings and feeding them to theLSTM. This configuration—henceforth termed ""double standard""—effectively doubled the size of the input- to-hidden weight foreach gate in the LSTM, aligning it with that of the position-encoded LSTM, while maintaining all other parameters, including thedimensionality of the (non-repeated) input embeddings.The double standard LSTM did not yield any improvements in the reverse-ordering or sort- ing tasks. These results affirm that thereported enhancement of RNNs is not merely attributable to the additional parameterization associated with the positional encoding.66.4 D Alternative Implementations of Positional EncodingWhile this study implemented positional encoding by sinusoidal waves, there are alternative implementations proposed in theprevious studies. For instance, the BERT-based models typically encode each token position by a learnable embedding. Moreover, ithas been pointed out that even random vectors can function as positional encoding.Accordingly, these two alternative forms of positional encoding were tested on the LSTM performing the reverse- ordering task.The random position-encoding vectors were uniformly and independently sampled from the (512 - 1)- dimensional hypersphere.The learnable embeddings were implemented using the canonical embedding module of PyTorch (torch.nn.Embedding). The inputlength and vocabulary size were set to 64 and 16,384 respectively. Both the random vectors and learnable embeddings improved theperformance of LSTM.Among the different implementations of positional encoding, the sinusoidal encoding outperformed the two alterna- tives. Theadvantage of the sinusoidal encoding became more apparent when the input length was variable between 32 and 64; the sinusoidalencoding was more robust to the variations in the input length than the others.6.5 E Language ModelingThis section reports benchmark results for the language modeling task. Single-layer LSTMs with and without sinusoidal positionalencoding were trained and tested on the WikiText-103 dataset. Due to constraints in computational resources, the vocabulary wasreduced from the original size of 267,735 to 32,768 by retokenizing the raw data using SentencePiece. The headings were removed,and the main text was segmented by paragraphs (separated by the line break). Additionally, only the first 1024 tokens of eachparagraph were utilized for training and testing, ensuring that the absolute positional encoding always aligned with the beginning ofeach paragraph. The hyperparameters were configured as specified in Section 3.3.Positional encoding proved effective only for marginally faster learning during the initial phase of training. The difference diminishedaround 10,000/30,000 iterations, and the test perplexities of the position-encoded model were inferior to those of the standard model.Table 1: Test perplexities on the WikiText-103 dataset. The minimum, mean, and maximum are obtained from five trials withdifferent random seeds. Model Min Mean MaxVanilla LSTM 36.8257 37.7731 38.916589Position-Encoded LSTM 38.0685 38.5384 38.8936567"
P059,"Large Vocabulary Handling in Recurrent NeuralNetworks Enhanced by Positional EncodingAbstractThis research presents a counterintuitive discovery: positional encoding, a high-dimensional representation of time indices on input data, improves the learningcapabilities of recurrent neural networks (RNNs). Although positional encoding iswidely recognized for complementing Transformer neural networks by enablingthem to process data order, its application to RNNs seems unnecessary becauseRNNs inherently encode temporal information. However, our analysis using syn-thetic benchmarks shows that combining positional encoding with RNNs offersadvantages, especially when dealing with extensive vocabularies that include low-frequency tokens. Further investigation reveals that these infrequent tokens causeinstability in the gradients of standard RNNs, and positional encoding helps to miti-gate this instability. These findings highlight a new function of positional encodingbeyond its well-known role as a timekeeping mechanism for Transformers.1 IntroductionSince their introduction, Transformer neural networks have become the preferred method for pro-cessing and generating time series data, surpassing traditional recurrent neural networks (RNNs). Asignificant difference between these models is their handling of temporal information, that is, thesequence of data points or tokens. RNNs process temporal information by adjusting their internalstate based on new inputs and their existing state. Conversely, Transformers lack an intrinsic mecha-nism for understanding data sequence order and, therefore, depend on an external system known aspositional encoding to keep track of time.Positional encoding represents time indices in a high-dimensional format. A common methodinvolves using sinusoidal waves of predetermined frequencies. This method marks input tokens byadding or appending these vectors to the input embeddings. Unlike RNNs, positional encoding’s timerepresentation remains constant regardless of input values until processed by a network.Although positional encoding is often viewed as a way to represent time that can replace RNNs whenused with Transformers, it is not incompatible with RNNs. Inputs to RNNs can be augmented withposition-encoding vectors. Autonomous activities in biological neurons, such as oscillations, arebelieved to be important for time perception and other perceptual processes, as well as motor control.This study, therefore, investigates the effects of adding positional encoding to the inputs of RNNs,using synthetic benchmarks. The results demonstrate that positional encoding helps RNNs manage amore extensive range of discrete inputs, or a larger vocabulary, compared to those without positionalencoding.The key contributions of this research are outlined below:• It illustrates the challenges faced when training RNNs on large vocabularies using carefullydesigned benchmark tasks, a problem that has not been widely recognized or addressed inprevious research, despite its potential impact on practical applications..• It explains that the difficulties in training RNNs with extensive vocabularies are due togradient instability caused by infrequent tokens, which inevitably occur as vocabulary sizeincreases.• It introduces a novel use of positional encoding, beyond its typical role in timing forTransformers, by integrating it with RNNs. It shows that positional encoding helps alleviateissues related to large vocabularies by stabilizing RNN gradients against the disruptionscaused by infrequent tokens.2 Related Studies2.1 Theoretical and Empirical Computational Power of (Vanilla) RNNsMathematically, RNNs are recognized as being Turing-complete, capable of simulating Turingmachines if their weights are infinitely precise and perfectly tuned. In practice, however, RNNweights are limited by finite precision and the need to optimize based on a finite set of observations.These constraints impose practical limitations on the capabilities of RNNs. For instance, empiricalRNNs cannot store an infinite number of observations in their memory, and the memorized informationtends to degrade over time.More recently, research into extending memory retention has explored continuous-time models.Instead of modifying a latent state in discrete-time steps, these models use a linear combinationof orthogonal polynomials in a continuous-time domain to approximate the input history. Thecoefficients of these polynomials provide a finite-dimensional representation of the input sequence,known as the High-Order Polynomial Projection Operator (HiPPO), and the dynamics of thesecoefficients can be described by an ordinary differential equation (ODE). This concept has beenfurther developed into neural state-space models by replacing the fixed state matrix in the ODEwith a learnable one, constrained to a diagonal structure plus a row-rank matrix. With additionalenhancements, the latest state-space models have shown language modeling performance that rivalsTransformer-based models.2.2 Positional EncodingPositional encoding serves as a high-dimensional representation of the temporal structures presentin input data. This method is particularly crucial for Transformers, which, unlike RNNs, do notinherently capture the order of inputs. Therefore, input tokens to a Transformer are ""time-stamped""by adding or concatenating a position-encoding vector.In the initial implementation of the Transformer, token positions were represented using sinusoidalwaves of various predefined frequencies. Although this method is effective for a wide range of tasks,researchers have explored other encoding schemes as well. For instance, the well-known BERTpretraining for natural language processing used learnable embeddings to indicate token positions.Some studies have suggested that combining sinusoidal and learnable encodings can enhance modelperformance. Another approach is to encode the distance between tokens instead of the time elapsedfrom the sequence’s beginning.Beyond Transformers, positional encoding is used to indicate elapsed time in diffusion processes.Its effectiveness is not limited to temporal information; studies on three-dimensional mesh andpoint-cloud modeling have shown that sinusoidal transformation of spatial data outperforms rawcoordinate representation.Despite its widespread use across various areas of machine learning, the application of positionalencoding to pure RNNs has been largely unexplored. To the author’s knowledge, only a few studieshave investigated position-encoded RNNs. The time index in time series data has rarely been directlyused by RNNs, likely due to perceived redundancy alongside RNN functionalities.23 Methods3.1 TaskThe impact of positional encoding on RNNs was examined using a reverse-ordering task. In this task,RNNs were trained to reconstruct a sequence of random integers in reverse order (e.g., given 8, 29, 2,11, the output should be 11, 2, 29, 8).3.2 Model ArchitectureThis study’s investigations were based on single-layer gated recurrent units (GRUs), long short-termmemory (LSTM) networks, and a neural state-space model, S4D. Each integer in the input sequenceswas first embedded, concatenated with its positional encoding, and then fed into the RNN or S4D.After processing the entire input sequence, the network received a command to produce the output,represented by a time-invariant learnable vector. The outputs from the RNN or S4D module werelinearly projected into classification logits, and the cross-entropy loss against the target sequence wasused to optimize the entire network. Model predictions during testing were determined by the argmaxof these logits for each time step.The canonical sinusoidal positional encoding used for Transformers was adopted in this study.Tt D (P E , ..., P E )Specifically, each time step was encoded by a -dimensional vector, ,pos t,1 t,Dposdefined as follows: (cid:32) (cid:33)t − 1P E := sin (1)t,2i 2(i−1)10000 Dpos (cid:33)(cid:32) t − 1P E := cos (2)t,2i+1 2(i−1)10000 Dpos (cid:112)D /2For learning stability, the positional encoding was normalized by dividing it by , ensuringposL tthe encoding vectors had a unit -norm. The time step incremented throughout both input and2t = 1, ..., L, L + 1, ..., 2L Loutput phases (i.e., , where is the input length), without any hard-codedlink between input and output positions.3.3 Implementation DetailsAcross the experiments, the dimensionality of the hidden layer of the RNNs was set to 512. Theembedding of the input integers and the memory cell of the LSTM also had the same dimensionalityof 512. Similarly, the hidden dimensionality of S4D was set to 512, while its state size (or the orderof the Legendre polynomials) was maintained at the default value of 64. β , βThe models were trained for 300,000 iterations using the Adam optimizer with parameters ( ) :=1 2(0.9, 0.999) and no weight decay. The learning rate was linearly warmed up from 0.0 to 0.001 for thefirst 1,000 iterations, and then annealed according to the cosine schedule. The batch size was 512.All experiments were implemented in PyTorch (ver. 2.1.1).4 Results4.1 Key FindingsPositional encoding improved the ability of RNNs to handle a larger vocabulary in the reverse-orderingtask. The position-encoded GRU and LSTM successfully reversed input sequences of 64 integersdrawn uniformly at random from vocabularies of size 32-256 and 256-16,384, respectively, achievingtoken-wise accuracy above 95%. In contrast, the performance of the vanilla models without positionalencoding degraded as the vocabulary size increased. Similarly, positional encoding enhanced thecapacity of S4D to handle large vocabularies. These improvements are also evident in the reducedsequence-wise reconstruction errors, measured by the Damerau-Levenshtein distance. Neither extratraining iterations nor greater batch sizes improved the performance of the vanilla models.34.2 Frequency MattersThe most apparent consequence of the increased vocabulary size was the reduced chance of observingindividual vocabulary items. Accordingly, additional experiments were conducted with non-uniformlydistributed tokens to investigate the relation between their frequency and RNN performance. Specif-ically, the input vocabulary was evenly divided into Frequent and Rare groups, and the Frequenttokens had three times the probability of the Rare tokens.The training data consisted of 64 independent samples from this dual-frequency vocabulary. Bycontrast, the test data were systematically constructed so that each sequence included a single""target"" token (Frequent/Rare) whose retrieval was evaluated for accuracy assessment, along with63 ""disturbants"" that were either all Frequent or all Rare. The experiment revealed that it was thedisturbant tokens whose frequency significantly impacted the performance of the vanilla RNNs andS4D. On the one hand, the Rare targets were successfully retrieved as long as they were surroundedby the Frequent disturbants. On the other hand, the vanilla GRU struggled to recover the Frequenttargets when the other input tokens were filled with the Rare disturbants. The LSTM performance wasalso degraded, especially when the targets were positioned in the first quarter of the input sequence (1≤ ≤t 16). Similarly, the Rare disturbants were detrimental to the S4D; unlike the RNNs, however,≤ ≤the accuracy was worst when the targets were located in the middle of the input sequences (17 t32).In contrast, the position-encoded RNNs exhibited robustness to the frequency of the target anddisturbant tokens. They achieved nearly perfect accuracies in most cases, except when the GRU≤ ≤processed the fully Rare data whose target was located in the first half of the sequence (1 t32). Likewise, positional encoding enhanced the resilience of the S4D against the influence of Raredisturbants.4.3 Analysis of Gradient StabilityTo delve deeper into the influence of token frequency on RNN performance, the gradients of theRNN latent states were scrutinized. In the analysis, pairs of input sequences were processed by theRNNs trained on the dual-frequency vocabulary (comprising Frequent and Rare items). Each pairof sequences shared the same initial token (t = 1; ""target"") but varied in the subsequent tokens (2≤ ≤t L; ""disturbants""). Then, gradients were computed for the distant mapping between the firstand last updated states (i.e., at time t = 1 and 2L) of the RNNs using backpropagation through time.The stability of RNN learning was assessed by measuring the dot-product similarity of the gradientsbetween the paired input sequences (after normalization over output dimensions).Formally, the paired input sequences, denoted as A and B, established two distinct, but ideally similar(s)˜(A) (B) (s)f f h = f (z˜ )mappings, and , from the first to the last latent state of the RNNs ( , where12Ls ∈ {A, B}). The gradient stability of the RNNs was defined by the dot-product similarities betweenthe normalized gradients of these paired mappings: (cid:32) (cid:33)(A) (B)D D ∂h ∂h(cid:88) (cid:88) 2L,i 2L,i(A) (A) (B) (B) (A) (B)(A, B) := ⟨α ∇f (z˜ ), α ∇f (z˜ )⟩ = α α ·Stability 1 1i i i i i i ∂z ∂z1,j 1,ji=1 i=1 (1)(s) (s)α ∇f (z˜ )where the coefficients normalized the raw gradients over the output dimensions1i ii := 1, ..., D: (cid:118)(cid:118) (cid:117)(cid:117) 22(cid:32) (cid:33) (cid:44) (cid:32) (cid:33)(s)(s)2D D 2D(cid:117)(cid:117) ∂h∂h(cid:88) (cid:88) (cid:88)(cid:117)(cid:117) 2L,i 2L,k(s)α := (2)(cid:116)(cid:116)i ∂z ∂z1,j 1,jj=1 j=1k=1Monitoring the gradients at training checkpoints revealed that Rare disturbants destabilize the learningof vanilla RNNs. The similarity of the paired gradients decreased gradually (GRU) or rapidly (LSTM)when the networks were exposed to the Rare disturbants. Positional encoding endowed the RNNswith robustness to these RARE disturbants. Both the GRU and LSTM maintained the high similarityof the paired gradients across the different target/disturbant conditions. By contrast, the impact ofpositional encoding on the gradient stability of the S4D was marginal; unlike the RNNs, the vanillaS4D was highly stable by itself against Rare disturbants throughout the training, even though there4was a visible relative destabilization due to Rare disturbants compared to Frequent disturbants in theearly stages of training, as well as an observable improvement by positional encoding.5 Discussion5.1 Difficulties in Handling a Large VocabularyThis study introduces a novel challenge in training (vanilla) RNNs: managing large vocabularies.While the manageable vocabulary size of RNNs is a pertinent research area, crucial for empiricalapplications like natural language processing, previous studies have primarily focused on evaluatingand improving the memory duration of RNNs, typically with small vocabulary sizes.This research examined RNN gradients and identified their destabilization when processing low-frequency tokens, which are necessarily included in a large vocabulary. Specifically, inputs that donot contribute to gradient-based optimization at a target time step were found to be detrimental.In general time series processing, data points carrying crucial information for specific time stepsbecome irrelevant otherwise. Consequently, each token exhibits a dual nature—both crucial andnoisy—throughout the task. Processing rare tokens is particularly challenging, presumably becausethey are irrelevant most of the time while making a large impact on learning due to their greater loss,compensating for fewer learning opportunities. Dealing with such ""unignorable noise"" presents apervasive challenge for RNNs.5.2 Functionality of Positional Encoding beyond the Timekeeper for TransformersAlthough low-frequency tokens destabilize the gradient-based learning of RNNs, this study alsodiscovered that positional encoding can alleviate this issue. This enhancement of RNNs via positionalencoding is noteworthy because RNNs were specifically designed to process time series data ontheir own. Unlike Transformers, they are presumed to function without relying on an ""externalclock"". Consequently, position-encoded RNNs have remained largely unexplored. The findings ofthe present study—namely, the improvement in the manageable vocabulary size due to enhancedgradient stability—broaden the currently limited understanding of the impact of positional encodingon RNNs.Additionally, the results of this study shed new light on the utility of positional encoding. Whilepositional encoding has been viewed as nothing more than input timestamps for Transformers, thepresent study demonstrated its efficacy in stabilizing the gradients of RNNs against disruption bylow-frequency tokens. This novel functionality of positional encoding would not have been visible inTransformer studies, as the model can dynamically adjust the relevance of input tokens through theirattention mechanism, thus inherently mitigating the impact of disturbant tokens.5.3 Limitations and Future DirectionsA primary unresolved question in this study pertains to the mechanism behind the gradient stabilizationby positional encoding. All the findings here are based on experimental investigations, lackingrigorous mathematical explanations for how and why the gradients of RNNs are destabilized byinfrequent tokens and stabilized by positional encoding. Moreover, the present study primarily focusedon the canonical implementation of sinusoidal positional encoding designed for Transformers, leavingopen which parameters of the sinusoidal waves (i.e., frequencies and phases) are critical for gradientstabilization. Future research may broaden its scope to encompass more general forms of positionalencoding, such as wavelets and non-periodic signals.Moreover, the analysis of gradient stability did not fully address the enhanced performance ofthe position-encoded state-space model (S4D). In terms of accuracy, the positioned-encoded S4Dexhibited greater robustness to infrequent tokens compared to the vanilla model, resembling thebehavior observed in RNNs. However, the gradients of the vanilla S4D were too stable to account forthis decline in performance. This leaves open the question of how positional encoding influencesgradient-based learning of state-space models. Additionally, future studies may investigate a broaderrange of state-space models to achieve a comprehensive understanding of the interplay betweenpositional encoding and these models. 5In addition to these scientifically oriented questions, future studies could also address practicalapplications of position-encoded RNNs and neural state-space models. Although positional encodingenhanced model performance across different synthetic tasks, the extent of this enhancement is task-dependent. Thus, positional encoding is not a panacea for arbitrary tasks, and further investigationsare necessary to determine when it is effective.6 Appendix6.1 Other TasksThis section demonstrates the effectiveness of positional encoding on RNNs across different tasks,besides the reverse ordering task discussed in the main text.6.1.1 Reverse-Ordering + Delayed-AdditionThis section reports the performance of position-encoded RNNs on a more complicated, combinatorialtask than the reverse ordering of input sequences. Extending the reverse-ordering task, the modelsreceived additional random input integers during the output phase, and added each of them to thecorresponding token in the reverse-ordered input sequence (modulo the vocabulary size, so that theoutput range was bounded). This task was too challenging to GRUs—even after reducing the inputlength to L = 16—so only the results from LSTMs are reported below. Also, the network was trainedfor 600,000 iterations (i.e., twice longer than the other tasks) for ensuring the convergence. The otherconditions/hyperparameters were the same as reported in the main text. Consequently, positionalencoding improved the model performance as the vocabulary size grew from 896 to 1088.6.1.2 SortingIn the reverse ordering task, the order of input integers was important information for accomplishingthe task. Thus, positional encoding may play its originally intended role in encoding the temporalinformation.This section reports the effectiveness of positional encoding for a task in which the order of inputobservations was completely irrelevant; the learning objective was to simply sort the input integers intheir inherent ascending order (e.g. 8, 29, 2, 11 -> 2, 8, 11, 29). The input integers were uniformlyrandomly sampled with replacement, allowing for ties in the sorting process.As a result, positional encoding also proved effective for RNNs to handle a larger vocabulary in thesorting task, though the improvement remained marginal compared to the reverse-ordering task.6.1.3 Predecessor QueryFinally, this section presents benchmark results for the predecessor-query task. The network firstx , ..., xreceived a sequence of non-repeating random integers, . Subsequently, one of the non-initial1 Lx ≤ t ≤input integers, (2 L), was randomly selected and reintroduced to the networkt queryqueryat time t = L + 1. The learning objective is to return the predecessor of the reviewed integer (=x ). The predecessor-query task evaluates the capacity of RNNs to integrate informationt −1queryregarding both the order and content of input sequences.As in the reverse-ordering + delayed-addition task, the input sequence was reduced to L = 16 dueto the complexity of the task, and the experiment focused on the LSTM. The number of trainingiterations was maintained at 300,000. Similar to the other benchmarks, positional encoding improvedthe LSTM’s capacity to manage the larger vocabularies.6.2 Robustness to Variations in Input LengthSo far, all the tasks were experimented using fixed-length inputs (L = 64). One might wonder ifpositional encoding is exceptionally effective under this setting, informing RNNs with the exacttiming when each input token should be returned as the output. Thus, it remains unclear whetheror not position-encoded RNNs can also handle a larger vocabulary even when the input length isvariable and, thus, the exact timing of the output emission is not identifiable from the positionalencoding attached to the inputs. 6To assess the robustness to variations in the input length, an additional experiment was conducted onthe LSTM, with the input length varied between 32 and 64. In this setup, the maximum input length(= 64) covers the entirety of the shortest input sequence plus its reversed reconstruction (= 32 + 32).Consequently, the positional encoding per se cannot even distinguish the input vs. output phases at t= 33, ..., 64. The vocabulary size was set to 16,384.As a result, the positional encoding still improved the LSTM’s performance on the reverse-orderingtask against the perturbations in the input length. This result suggests that the effectiveness of thepositional encoding for RNNs is not limited to strictly scheduled tasks.6.3 Effects of Additional Parameters in Position-Encoded RNNsThe concatenation of positional encoding with input embeddings inflates the number of learnableparameters in the input-to-hidden projection weights. This additional parameterization per se doesnot influence the learning of the input embeddings, and therefore does not elucidate the enhancedperformance of position-encoded RNNs. This section substantiates this argument by equalizing thenumber of learnable parameters between the vanilla and position-encoded models.Specifically, the equalization was achieved by concatenating two identical copies of the inputembeddings and feeding them to the LSTM. This configuration—henceforth termed ""doublevanilla""—effectively doubled the size of the input- to-hidden weight for each gate in the LSTM,aligning it with that of the position-encoded LSTM, while maintaining all other parameters, includingthe dimensionality of the (non-repeated) input embeddings.As illustrated, the double vanilla LSTM did not yield any improvements in the reverse-ordering orsort- ing tasks. These results affirm that the reported enhancement of RNNs is not merely attributableto the additional parameterization associated with the positional encoding.6.4 Alternative Implementations of Positional EncodingWhile this study implemented positional encoding by sinusoidal waves, there are alternative imple-mentations proposed in the previous studies. For instance, the BERT-based models typically encodeeach token position by a learnable embedding. Moreover, the original study of Transformer pointedout that even random vectors can function as positional encoding.Accordingly, these two alternative forms of positional encoding were tested on the LSTM performingthe reverse- ordering task. The random position-encoding vectors were uniformly and independentlysampled from the (512 1)- dimensional hypersphere. The learnable embeddings were implementedusing the canonical embedding module of PyTorch (torch.nn.Embedding). The input length andvocabulary size were set to 64 and 16,384 respectively. Both the random vectors and learnableembeddings improved the performance of LSTM.Among the different implementations of positional encoding, the sinusoidal encoding outperformedthe two alterna- tives. The advantage of the sinusoidal encoding became more apparent when the inputlength was variable between 32 and 64; the sinusoidal encoding was more robust to the variations inthe input length than the others.6.5 Language ModelingThis section reports benchmark results for the language modeling task. Single-layer LSTMs withand without sinusoidal positional encoding were trained and tested on the WikiText-103 dataset.Due to constraints in computational resources, the vocabulary was reduced from the original size of267,735 to 32,768 by retokenizing the raw data using SentencePiece. The headings were removed,and the main text was segmented by paragraphs (separated by the line break). Additionally, only thefirst 1024 tokens of each paragraph were utilized for training and testing, ensuring that the absolutepositional encoding always aligned with the beginning of each paragraph. The hyperparameters wereconfigured as specified in §3.3.As illustrated, positional encoding proved effective only for marginally faster learning during theinitial phase of training. The difference diminished around 10,000/30,000 iterations, and the testperplexities of the position-encoded model were inferior to those of the vanilla model.7Table 1: Test perplexities on the WikiText-103 dataset. The minimum, mean, and maximum areobtained from five trials with different random seeds.Model Min Mean MaxVanilla LSTM 36.8257 37.7731 38.916589Position-Encoded LSTM 38.0685 38.5384 38.8936568"
P061,"Enhancing Visual Representation Learning ThroughOriginal Image Utilization in Contrastive LearningAbstractContrastive instance discrimination techniques exhibit superior performance indownstream tasks, including image classification and object detection, compared tosupervised learning. However, a strong reliance on data augmentation during repre-sentation learning is a hallmark of these methods, potentially causing suboptimaloutcomes if not meticulously executed. A prevalent data augmentation approach incontrastive learning involves random cropping followed by resizing. This practicemight diminish the quality of representation learning when two random cropsencompass disparate semantic information. To counter this, we propose an inno-vative framework termed LeOCLR (Leveraging Original Images for ContrastiveLearning of Visual Representations). This framework integrates a novel instancediscrimination strategy and a refined loss function, effectively mitigating the lossof crucial semantic features that may arise from mapping different object segmentsduring representation learning. Our empirical evaluations reveal that LeOCLR con-sistently enhances representation learning across a spectrum of datasets, surpassingbaseline models. Notably, LeOCLR exhibits a 5.1% improvement over MoCo-v2on ImageNet-1K in linear evaluation and demonstrates superior performance intransfer learning and object detection tasks compared to several other techniques.1 IntroductionSelf-supervised learning (SSL) methods based on instance discrimination are heavily dependent ondata augmentations, like random cropping, rotation, and color jitter, to construct invariant repre-sentations for all instances within a dataset. These augmentations are used to generate two alteredviews (positive pairs) of the same instance, which are subsequently drawn closer in the latent space.Simultaneously, strategies are employed to prevent a collapse to a trivial solution, commonly referredto as representation collapse. The efficacy of these methods in acquiring meaningful representationshas been demonstrated through various downstream tasks, such as image classification and objectdetection, serving as proxies for evaluating representation learning. However, these techniquesoften overlook the crucial aspect that augmented views may diverge in semantic content becauseof random cropping, potentially degrading the quality of visual representation learning. Creatingpositive pairs via random cropping and subsequently prompting the model to align them based onshared information in both views poses an increased challenge to the SSL task, ultimately leading toan enhancement in representation quality. Moreover, random cropping followed by resizing guidesthe model’s representation to encompass object-related information across diverse aspect ratios,thereby promoting invariance to occlusions. Conversely, minimizing the feature distance in the latentspace, which equates to maximizing similarity, between views that encompass distinct semanticconcepts may inadvertently discard valuable image information.Instances of incorrect semantic positive pairs, which are pairs containing mismatched semanticinformation about the same object, might arise from random cropping. When the model is compelledto align the representations of different parts of an object closer in the latent space, it may discardcrucial semantic features. This occurs because the model’s representations are based on the sharedarea between the two views. If this shared region lacks semantically consistent information, the.representations become trivial. For random cropping to be effective and achieve occlusion invariance,the shared area must convey the same semantic meaning in both views. Nevertheless, contrastingpairs that might include diverse semantic information about the same object can be valuable, as it canfacilitate learning global features.The creation of random crops for a one-centric object does not ensure the acquisition of accuratesemantic pairs. This observation holds significant importance for the enhancement of representationlearning. Instance discrimination SSL techniques encourage the model to approximate positive pairs,i.e., two views of the same instance, in the latent space, irrespective of their semantic content. Thislimitation might hinder the model’s ability to learn representations of different object componentsand could potentially impair its capability to learn semantic feature representations (see Figure 2(left) in the original paper).Undesirable views containing different semantic content may be unavoidable when employing randomcropping. Therefore, a method is needed to train the model on different parts of an object, developingrobust representations against natural transformations like scale and occlusion, rather than merelypulling augmented views together indiscriminately. Addressing this issue is vital, as downstream taskperformance relies on high-quality visual representations learned through self-supervised learning.Our work presents a new instance discrimination SSL approach designed to avoid compelling themodel to create similar representations for two positive views, irrespective of their semantic content.As shown in Figure 2 (right) of the original paper, we incorporate the original image X into thetraining process, since it contains all the semantic features of the views X1 and X2. In our method, thepositive pairs (i.e., X1 and X2) are drawn towards the original image X in the latent space, in contrastto contrastive state-of-the-art (SOTA) approaches like SimCLR and MoCo-v2, which draw the twoviews towards each other. This training method guarantees that the information in the shared regionbetween the attracted views (X, X1) and (X, X2) is semantically accurate. Consequently, the modelacquires enhanced semantic features by aligning with the appropriate semantic content, rather thanmatching random views that might contain disparate semantic information. In essence, the modellearns representations of various object parts because the shared region encompasses correct semanticcomponents of the object. This contrasts with other methods that may discard vital semantic featuresby incorrectly mapping object parts in positive pairs. Our contributions are outlined as follows:• We present a new contrastive instance discrimination SSL method, LeOCLR, created tominimize the loss of semantic features caused by mapping two semantically inconsistentrandom views.• We establish that our method enhances visual representation learning in contrastive instancediscrimination SSL, surpassing state-of-the-art techniques across a variety of downstreamtasks.We show that our method consistently improves visual representation learning for contrastive• instance discrimination across multiple datasets and contrastive mechanisms.2 Related WorkSelf-supervised learning (SSL) techniques are categorized into two primary groups: contrastive andnon-contrastive learning. While all these techniques endeavor to approximate positive pairs in thelatent space, they employ distinct strategies to circumvent representation collapse.**Contrastive Learning:** Instance discrimination techniques, such as SimCLR, MoCo, and PIRL,employ a similar concept. These methods bring the positive pairs closer while driving the negativepairs apart in the embedding space, albeit through different mechanisms. SimCLR employs anend-to-end strategy where a large batch size is utilized for negative examples, and the parameters ofboth encoders in the Siamese network are updated simultaneously. PIRL uses a memory bank fornegative examples, and both encoders’ parameters are updated together. MoCo adopts a momentumcontrastive approach where the query encoder is updated during backpropagation, which subsequentlyupdates the key encoder. Negative examples are maintained in a separate dictionary, facilitating theuse of large batch sizes.**Non-Contrastive Learning:** Non-contrastive techniques utilize solely positive pairs to learnvisual representations, employing a variety of strategies to prevent representation collapse. The2initial category encompasses clustering-based techniques, where samples exhibiting similar featuresare assigned to the same cluster. DeepCluster employs pseudo-labels from the previous iteration,rendering it computationally demanding and challenging to scale. SWAV addresses this challenge byimplementing online clustering, though it necessitates determining the correct number of prototypes.The second category involves knowledge distillation. Techniques like BYOL and SimSiam utilizeknowledge distillation methods, where a Siamese network comprises an online encoder and a targetencoder. The target network’s parameters are not updated during backpropagation. Instead, solelythe online network’s parameters are updated while being encouraged to predict the representation ofthe target network. Despite the encouraging results, the mechanism by which these methods preventcollapse remains not fully understood. Inspired by BYOL, Self-distillation with no labels (DINO)employs centering and sharpening, along with a distinct backbone (ViT), enabling it to surpass otherself-supervised techniques while maintaining computational efficiency. Another method, Bag ofvisual words (BoW), employs a teacher-student framework inspired by natural language processing(NLP) to avert representation collapse. The student network predicts a histogram of the features foraugmented images, analogous to the teacher network’s histogram. The final category is informationmaximization. Methods like Barlow twins and VICReg eschew negative examples, stop gradient,or clustering. Instead, they utilize regularization to avoid representation collapse. The objectivefunction of these techniques seeks to eliminate redundant information in the embeddings by aligningthe correlation of the embedding vectors closer to the identity matrix. While these techniques exhibitencouraging results, they possess limitations, including the sensitivity of representation learning toregularization and reduced effectiveness if certain statistical properties are absent in the data.**Instance Discrimination With Multi-Crops:** Various SSL techniques introduce multi-crop strate-gies to enable models to learn visual representations of objects from diverse perspectives. However,when generating multiple cropped views from the same object instance, these views might containdisparate semantic information. To tackle this issue, LoGo generates two random global crops andN local views. They posit that global and local views of an object share similar semantic content,enhancing similarity between these views. Simultaneously, they contend that different local viewspossess distinct semantic content, thus diminishing similarity among them. SCFS proposes a differentapproach for managing unmatched semantic views by searching for semantically consistent featuresbetween the contrasted views. CLSA generates multiple crops and applies both strong and weakaugmentations, using distance divergence loss to enhance instance discrimination in representationlearning. Prior methods assume that global views contain similar semantic content and treat themindiscriminately as positive pairs. However, our technique suggests that global views might containincorrect semantic pairs due to random cropping, as illustrated in Figure 1 in the original paper.Therefore, we aim to attract the two global views to the original (intact and uncropped) image, whichfully encapsulates the semantic features of the crops.3 MethodologyThe mapping of incorrect semantic positive pairs, specifically those containing different semanticviews, results in the loss of semantic features, which in turn degrades the model’s representationlearning. To address this, we propose a novel contrastive instance discrimination SSL strategy calledLeOCLR. Our approach is designed to capture meaningful features from two random positive pairs,even when they encompass different semantic content, thereby improving representation learning.Achieving this necessitates ensuring the semantic correctness of the information within the sharedregion between the attracted views. This is crucial because the selection of views dictates theinformation captured by the representations learned in contrastive learning. Given that we cannotguarantee the inclusion of correct semantic parts of the object within the shared region between thetwo views, we propose the inclusion of the original image in the training process. The original imageX, which is not subjected to random cropping, encompasses all the semantic features of the twocropped views, X1 and X2.Our method, illustrated in Figure 3 (left) in the original paper, generates three views (X, X1, andX2). The original image (X) is resized without cropping, while the other views (X1 and X2) undergorandom cropping and resizing. All views are then randomly augmented to prevent the model fromlearning trivial features. We employ data augmentations akin to those used in MoCo-v2. The originalimage (X) is encoded by the encoder fq, while the two views (X1, X2) are encoded by a momentumencoder fk. The parameters of fk are updated using the formula:3θ ← mθ + (1 − m)θ (1)k k q θwhere m is a coefficient set to 0.999, represents the encoder parameters of fq updated throughqθ θbackpropagation, and denotes the momentum encoder parameters of fk updated by . Ultimately,k qthe objective function compels the model to draw both views (X1, X2) closer to the original image(X) in the embedding space while simultaneously pushing apart all other instances, as depicted inFigure 3 (right) in the original paper.3.1 Loss functionInitially, we briefly outline the loss function of MoCo-v2, given our utilization of momentumcontrastive learning. Subsequently, we will detail our modification to the loss function.+exp(u·v /τ)+ℓ(u, v ) = − log (2)(cid:80)PN exp(u·v /τ)nn=0where similarity is quantified by the dot product. The objective function amplifies the similaritybetween the positive pairs (u . v+) by drawing them closer in the embedding space, while simultane-ously driving apart all the negative samples (vn) in the dictionary to prevent representation collapse.τ denotes the temperature hyperparameter of the softmax function. In our method, we augment thesimilarity between the original image’s feature representation, u = fq(x), and the positive pair’s featurerepresentation, v+ = fk(xi) (i = 1, 2), while driving apart all the negative examples (vn). Consequently,the total loss for the mini-batch is:(cid:80)N 1 2l = ℓ(u , sg(v )) + ℓ(u , sg(v )) (3)t i ii ii=1where sg(.) denotes the stop-gradient operation, which is vital for averting representation collapse.1 2l v vAs depicted in Equation 3, the total loss attracts the two views ( and ) to their original instancet i iu . This enables the model to capture semantic features from the two random views, even if theyicontain different semantic information. Our technique captures improved semantic features comparedto prior contrastive methods, as we ensure that the shared region between the attracted views containsaccurate semantic information. Since the original image contains all segments of the object, any partcontained in the random crop is also present in the original image. Thus, when we draw the originalimage and the two random views closer in the embedding space, the model learns representationsof the different parts, creating an occlusion-invariant representation of the object across variousscales and angles. This contrasts with earlier techniques, which draw the two views together in theembedding space regardless of their semantic content, leading to the loss of semantic features.Equation 3 and Algorithm 1 in the original paper highlight the primary distinctions between ourmethod and prior multi-crop techniques, such as CLSA, SCFC, and DINO. The key differences areas follows:Previous methods assume that two global views contain identical semantic information,• encouraging the model to concentrate on similarities and generate similar representationsfor both views. In contrast, our method utilizes the original images instead of globalviews, as we contend that global views may contain incorrect semantic information for thesame object. While they may aid in capturing certain global features, this could restrictthe model’s capacity to learn more universally applicable semantic features, ultimatelyimpacting performance.• Prior methods employ several local random crops, which might be time- and memory-intensive, while our method utilizes only two random crops.• Our objective function employs different strategies to enhance the model’s visual represen-tation learning. We encourage the model to align the two random crops with the originalimage, which encompasses the semantic information for all random crops while avoidingcompelling the two crops to have similar representations if they do not share similar semanticinformation. This approach differs from prior methods, which encourage all crops (globaland local) to have similar representations, regardless of their semantic content. Conse-quently, although useful for learning certain global features, those methods may discardpertinent semantic information, potentially hindering the transferability of the resultingrepresentations to downstream tasks. 44 ExperimentsWe executed multiple experiments on three datasets: STL-10 ""unlabeled"", comprising 100,000training images, CIFAR-10, containing 50,000 training images, and ImageNet-1K, with 1.28 milliontraining images.**Training Setup:** We employed ResNet50 as the backbone architecture. The model was trainedusing the SGD optimizer, with a weight decay set to 0.0001, momentum at 0.9, and an initial learningrate of 0.03. The mini-batch size was configured to 256, and the model underwent training for up to800 epochs on the ImageNet-1K dataset.**Evaluation:** We employed diverse downstream tasks to assess LeOCLR’s representation learningagainst leading SOTA approaches on ImageNet-1K: linear evaluation, semi-supervised learning,transfer learning, and object detection. For linear evaluation, we adhered to the standard evaluationprotocol, where a linear classifier was trained for 100 epochs on top of a frozen backbone pre-trainedwith LeOCLR. The ImageNet-1K training set was used to train the linear classifier from scratch, withrandom cropping and left-to-right flipping augmentations. Results are presented on the ImageNet-1K validation set using a center crop (224 x 224). In the semi-supervised setting, we fine-tunedthe network for 60 epochs using 1% of labeled data and 30 epochs using 10% of labeled data.Additionally, we evaluated the learned features on smaller datasets, such as CIFAR, and fine-graineddatasets, using transfer learning. Lastly, we utilized the PASCAL VOC dataset for object detection.**Comparing with SOTA Approaches:** We employed vanilla MoCo-v2 as a baseline for comparisonwith our method across various benchmark datasets, considering our use of a momentum contrastivelearning framework. Furthermore, we benchmarked our method against other SOTA techniques onthe ImageNet-1K dataset.Table 1: Comparisons between our approach LeOCLR and SOTA approaches on ImageNet-1K.Approach Epochs Batch AccuracyMoCo-v2 800 256 71.1%BYOL 1000 4096 74.4%SWAV 800 4096 75.3%SimCLR 1000 4096 69.3%HEXA 800 256 71.7%SimSiam 800 512 71.3%VICReg 1000 2048 73.2%MixSiam 800 128 72.3%OBoW 200 256 73.8%DINO 800 1024 75.3%Barlow Twins 1000 2048 73.2%CLSA 800 256 76.2%RegionCL-M 800 256 73.9%UnMix 800 256 71.8%HCSC 200 256 73.3%UniVIP 300 4096 74.2%HAIEV 200 256 70.1%SCFS 800 1024 75.7%LeOCLR (ours) 800 256 76.2%Table 1 presents the linear evaluation of our method in comparison to other SOTA techniques. Asshown, our method surpasses all others, outperforming the baseline (i.e., vanilla MoCo-v2) by 5.1%.This lends credence to our hypothesis that while two global views can capture certain global features,they may also encompass distinct semantic information for the same object (e.g., a dog’s headversus its leg), which should be taken into account to enhance representation learning. The observedperformance gap (i.e., the difference between vanilla MoCo-v2 and LeOCLR) demonstrates thatmapping pairs with divergent semantic content impedes representation learning and impacts themodel’s performance in downstream tasks.**Semi-Supervised Learning on ImageNet-1K:** In this section, we assess the performance ofLeOCLR under a semi-supervised setting. Specifically, we utilize 1% and 10% of the labeled training5data from ImageNet-1K for fine-tuning, adhering to the semi-supervised protocol introduced inSimCLR. The top-1 accuracy, presented in Table 2 after fine-tuning with 1% and 10% of the trainingdata, demonstrates LeOCLR’s superiority over all compared techniques. This can be attributed toLeOCLR’s enhanced representation learning capabilities, particularly in comparison to other SOTAmethods.Table 2: Semi-supervised training results on ImageNet-1K: Top-1 performances are reported forfine-tuning a pre-trained ResNet-50 with the ImageNet-1K 1% and 10% datasets. * denotes theresults are reproduced in this study.Approach ImageNet-1K 1% ImageNet-1K 10%MoCo-v2 * 47.6% 64.8%SimCLR 48.3% 65.6%BYOL 53.2% 68.8%SWAV 53.9% 70.2%DINO 50.2% 69.3%RegionCL-M 46.1% 60.4%SCFS 54.3% 70.5%LeOCLR (ours) 62.8% 71.5%**Transfer Learning on Downstream Tasks:** We evaluate our self-supervised pretrained modelusing transfer learning by fine-tuning it on small datasets such as CIFAR, Stanford Cars, Oxford-IIITPets, and Birdsnap. We adhere to the transfer learning procedures to identify optimal hyperparametersfor each downstream task. As shown in Table 3, our method, LeOCLR, surpasses all comparedapproaches on a variety of downstream tasks. This demonstrates that our model acquires valuablesemantic features, enabling it to generalize more effectively to unseen data in different downstreamtasks compared to other techniques. Our method preserves the semantic features of the given objects,thereby enhancing the model’s representation learning capabilities. Consequently, it is more effectiveat extracting crucial features and predicting correct classes on transferred tasks.Table 3: Transfer learning results from ImageNet-1K with the standard ResNet-50 architecture. *denotes the results are reproduced in this study.Approach CIFAR-10 CIFAR-100 Car Birdsnap PetsMoCo-v2 * 97.2% 85.6% 91.2% 75.6% 90.3%SimCLR 97.7% 85.9% 91.3% 75.9% 89.2%BYOL 97.8% 86.1% 91.6% 76.3% 91.7%DINO 97.7% 86.6% 91.1% - 91.5%SCFS 97.8% 86.7% 91.6% - 91.9%LeOCLR (ours) 98.1% 86.9% 91.6% 76.8% 92.1%**Object Detection Task:** To further assess the transferability of the learned representation, wecompare our method with other SOTA techniques using object detection on the PASCAL VOC. Wefollow the same settings as MoCo-v2, fine-tuning on the VOC07+12 trainval dataset using FasterR-CNN with an R50-C4 backbone, and evaluating on the VOC07 test dataset. The model is fine-˘tuned for 24k iterations (2248 23 epochs). As shown in Table 4, our method surpasses all comparedtechniques. This superior performance can be attributed to our model’s ability to capture richersemantic features compared to the baseline (MoCo-v2) and other techniques, leading to improvedresults in object detection and related tasks.5 Ablation StudiesIn the subsequent subsections, we further analyze our approach using a different contrastive instancediscrimination technique (i.e., an end-to-end mechanism) to investigate how our method performswithin this framework. Moreover, we conduct studies on the benchmark datasets STL-10 andCIFAR-10 using a distinct backbone (ResNet-18) to assess the consistency of our approach acrossvarious datasets and backbones. Additionally, we employ a random crop test to simulate natural6Table 4: Results (Average Precision) for PASCAL VOC object detection using Faster R-CNN withResNet-50-C4. Approach AP50 AP AP75MoCo-v2 82.5% 57.4% 64%CLSA 83.2% - -SCFS 83% 57.4% 63.6%LeOCLR (ours) 83.2% 57.5% 64.2%transformations, such as variations in scale or occlusion of objects in the image, to analyze therobustness of the features learned by our approach, LeOCLR. We also compare our approach withvanilla MoCo-v2 by manipulating their data augmentation techniques to determine which model’sperformance is more significantly affected by the removal of certain augmentations. In addition,we experiment with different fine-tuning settings to evaluate which model learns better and faster.Furthermore, we adapt the attraction strategy and cropping method of the original image, as well ascompute the running time of our approach. Lastly, we examine our approach on a non-centric objectdataset where the probability of mapping two views containing distinct information is higher.5.1 Different Contrastive Instance Discrimination FrameworkWe utilize an end-to-end framework in which the two encoders fq and fk are updated throughbackpropagation to train a model with our approach for 200 epochs with a batch size of 256.Subsequently, we conduct a linear evaluation of our model against SimCLR, which also employsan end-to-end mechanism. As presented in Table 5, our approach outperforms vanilla SimCLR bya substantial margin of 3.5%, demonstrating its suitability for integration with various contrastivelearning frameworks.Table 5: Comparing vanilla SimCLR with LeOCLR after training our approach 200 epochs onImageNet-1K. Approach ImageNet-1KSimCLR 62%LeOCLR (ours) 65.5%5.2 ScalabilityIn Table 6, we evaluate our approach on different datasets (STL-10 and CIFAR-10) using a ResNet-18backbone to ensure its consistency across various backbones and datasets (i.e., scalability). Wepre-trained all the approaches for 800 epochs with a batch size of 256 on both datasets and thenconducted a linear evaluation. Our approach demonstrates superior performance on both datasetscompared to all approaches. For instance, our approach outperforms vanilla MoCo-v2, achievingaccuracies of 5.12% and 5.71% on STL-10 and CIFAR-10, respectively.Table 6: SOTA approaches versus LeOCLR on CIFAR-10 and STL-10 with ResNet-18.Approach STL-10 CIFAR-10MoCo-v2 80.08% 73.88%DINO 84.30% 78.50%CLSA 82.62% 77.20%BYOL 79.90% 73.00%LeOCLR (ours) 85.20% 79.59%5.3 Center and Random Crop TestIn Table 7, we report the top-1 accuracy for vanilla MoCo-v2 and our approach after 200 epochson ImageNet-1K, concentrating on two tasks: a) center crop test, where images are resized to 2567pixels along the shorter side using bicubic resampling, followed by a 224 x 224 center crop; andb) random crop, where images are resized to 256 x 256 and then randomly cropped and resized to224 x 224. According to the results, the performance of MoCo-v2 dropped by 4.3% with randomcropping, whereas our approach experienced a smaller drop of 2.8%. This suggests that our approachlearns improved semantic features, demonstrating greater invariance to natural transformations likeocclusion and variations in object scales. Additionally, we compare the performance of CLSA withour approach, given that both perform similarly after 800 epochs (see Table 1). Note that the CLSAapproach uses multi-crop (i.e., five strong and two weak augmentations), while our approach employsonly two random crops and the original image. As shown in Table 7, LeOCLR outperforms theCLSA approach by 2.3% after 200 epochs on ImageNet-1K. To address concerns about the increasedcomputational cost associated with training LeOCLR compared to MoCo V2, we include the trainingtime for both approaches in Table 7. We trained both models on three A100 GPUs with 80GB for200 epochs. Our approach took an additional 13 hours to train over the same number of epochs, but itdelivers significantly better performance than the baseline.Table 7: Comparing LeOCLR with vanilla MoCo-v2 and CLSA after training 200 epochs onImageNet-1K. Approach Center Crop Random Crop TimeMoCo-v2 67.5% 63.2% 68hCLSA 69.4% - -LeOCLR (ours) 71.7% 68.9% 81hgraph1.pdf graph2.pdfFigure 1: * Figure 2: *(a) Top-1 accuracy (b) Top-5 accuracyFigure 3: Semi-supervised training with a fraction of ImageNet-1K labels on a ResNet-50.5.4 Augmentation and Fine-tuningContrastive instance discrimination techniques are sensitive to the choice of image augmentations.This sensitivity necessitates further analysis comparing our approach to Moco-v2. These experimentsaim to explore which model learns better semantic features and produces more robust representationsunder different data augmentations. As shown in Figure 4, both models are affected by the removalof certain data augmentations. However, our approach shows a more invariant representation andexhibits less performance degradation due to transformation manipulation compared to vanilla MoCo-v2. For instance, when we apply only random cropping augmentation, the performance of vanillaMoCo-v2 drops by 28 percentage points (from a baseline of 67.5% to 39.5% with only randomcropping). In contrast, our approach experiences a decrease of only 25 percentage points (from abaseline of 71.7% to 46.6% with only random cropping). This indicates that our approach learns8improved semantic features and produces more effective representations for the given objects thanvanilla MoCo-v2. graph3.pdfFigure 4: Decrease in top-1 accuracy (in % points) of LeOCLR and our reproduc-tion of vanilla MoCo-v2 after 200 epochs, under linear evaluation on ImageNet-1K.rayscaleref erstoresultswithoutgrayscaleaugmentations, whileR olorref erstoresultswithoutcolorjitterbutwithgrayscaleaugmentations.RG cIn Table 2, presented in Section 4, we fine-tune the representations over the 1% and 10% ImageNet-1Ksplits using the ResNet-50 architecture. In the ablation study, we compare the fine-tuned representa-tions of our approach with the reproduced vanilla MoCo-v2 across 1%, 2%, 5%, 10%, 20%, 50%, and100% of the ImageNet-1K dataset. In this setting, we observe that tuning a LeOCLR representationconsistently outperforms vanilla MoCo-v2. For instance, Figure 3 (a) demonstrates that LeOCLRfine-tuned with 10% of ImageNet-1K labeled data outperforms vanilla Moco-v2 fine-tuned with20% of labeled data. This indicates that our approach is advantageous when the labeled data fordownstream tasks is limited.5.5 Attraction StrategyIn this subsection, we apply a random crop to the original image (x) and attract the two views (x1,x2) toward it to evaluate its impact on our approach’s performance. We also conducted an experimentwhere all views were attracted to each other. However, in our method, we avoid attracting the twoviews to each other, enforcing the model to draw the two views toward the original image only(i.e., the uncropped image containing semantic features for all crops). For these experiments, wepre-trained the model on ImageNet-1K for 200 epochs using the same hyperparameters employedin the main experiment. The experiments in Table 8 underscore the significance of the informationshared between the two views. They also highlight the importance of leveraging the original imageand avoiding the attraction of views containing varied semantic information to preserve the semanticfeatures of the objects. When we create a random crop of the original image (x) and force the modelto make the two views similar to the original image (i.e., LeOCLR(Random original image)), themodel performance decreases by 2.4%.This performance reduction occurs because cropping the original image and compelling the model toattract the two views towards it increases the probability of having two views with differing semanticinformation, resulting in a loss of semantic features of the objects. The situation deteriorates whenwe attract all views (x, x1, x2) to each other in LeOCLR (attract all crops), causing performance todrop closer to that of vanilla MoCo-v2 (67.5%). This decline is attributed to the high likelihood ofattracting two views containing distinct semantic information.9Table 8: Comparisons of augmentation strategies using our proposed approach after 200 epochs.Approach AccuracyLeOCLR (Random original image) 69.3%LeOCLR (attract all crops) 67.7%LeOCLR (ours) 71.7%5.6 Non-Object-Centric TasksNon-object-centric datasets, like COCO, depict real-world scenes where the objects of interest arenot centered or prominently positioned, unlike object-centric datasets such as ImageNet-1K. In thisscenario, the chance of generating two views containing distinct semantic information for the objectis elevated, thus exacerbating the issue of losing semantic features. Therefore, we train both ourapproach and the MoCo-v2 baseline from scratch on the COCO dataset to evaluate how our methodmanages the discarding of semantic features in such datasets. We utilized identical hyperparametersas for ImageNet-1K, training the models with a batch size of 256 over 500 epochs. Subsequently, wefine-tuned these pre-trained models on the COCO dataset for object detection.Table 9: Results for pre-training followed by fine-tuning on COCO for object detection using FasterR-CNN with ResNet-50-C4.Approach AP50 AP AP75MoCo-v2 57.2% 37.6% 41.5%LeOCLR (ours) 59.3% 39.1% 43.0%Table 9 reveals that our approach captured enhanced semantic features for the given object comparedto the baseline. This emphasizes that our method of avoiding the attraction of two distinct views ismore effective at preserving semantic features, even in a non-object-centric dataset.6 ConclusionThis paper presents a new contrastive instance discrimination approach for SSL to improve represen-tation learning. Our method reduces the loss of semantic features by including the original imageduring training, even when the two views contain different semantic content. We show that ourapproach consistently enhances the representation learning of contrastive instance discriminationacross various benchmark datasets, backbones, and mechanisms, including momentum contrastand end-to-end methods. In linear evaluation, we achieved an accuracy of 76.2% on ImageNet-1Kafter 800 epochs, surpassing several SOTA instance discrimination SSL methods. Furthermore, wedemonstrated the invariance and robustness of our approach across different downstream tasks, suchas transfer learning and semi-supervised fine-tuning.10"
P062,"Estimating Causal Effects Using a Cross-MomentMethodAbstractThis paper explores the adaptation of large pretrained models to new tasks whilepreserving their inherent equivariance properties. Equivariance, the property of amodel’s output changing predictably with transformations of its input, is crucial formany applications, particularly in domains with inherent symmetries such as imageprocessing and physics simulations. However, standard adaptation techniques oftendisrupt this crucial property, leading to a loss of performance and generalizationability. We propose a novel method that leverages [1, 2] to maintain equivarianceduring the adaptation process. Our approach incorporates a regularization termthat penalizes deviations from the desired equivariant behavior, ensuring thatthe adapted model retains its symmetry properties. This is achieved through acarefully designed loss function that combines standard task-specific losses withan equivariance-preserving constraint.1 IntroductionEquivariance, a crucial property where a model’s output transforms predictably with input transfor-mations, is vital for numerous applications, especially in domains exhibiting inherent symmetrieslike image processing and physics simulations. Large pretrained models, while powerful, oftenlose this crucial equivariance during adaptation to new tasks using standard techniques. This losscan significantly impact performance and generalization. The inherent symmetries present in manydatasets are often exploited implicitly or explicitly by the model architecture. For example, con-volutional neural networks implicitly leverage translation equivariance, while other architecturesare designed to explicitly incorporate other symmetries. However, standard fine-tuning or transferlearning methods often disrupt these inherent symmetries, leading to a degradation in performanceand robustness. This is particularly problematic when dealing with large pretrained models, where thecomputational cost of retraining can be prohibitive. Furthermore, the loss of equivariance can lead tounpredictable behavior and reduced generalization capabilities, especially when the test data differssignificantly from the training data in terms of transformations. This necessitates the development ofnovel adaptation techniques that explicitly preserve equivariance.This paper addresses the challenge of adapting large pretrained models to new tasks while preservingtheir inherent equivariance. We introduce a novel method that leverages regularization techniquesto maintain equivariance during the adaptation process. Our approach carefully balances the needto optimize for task-specific performance with the constraint of preserving the model’s equivariantproperties. This is achieved through a carefully designed loss function that combines standard task-specific losses with an additional term that penalizes deviations from the desired equivariant behavior.The regularization term is designed to be flexible and adaptable to different types of transformationsand model architectures. This allows our method to be applied to a wide range of problems andmodels. The key innovation lies in the formulation of the regularization term, which is derived fromthe theoretical properties of equivariant functions and carefully tuned to avoid over-regularization.The proposed method is rigorously evaluated on a diverse set of benchmark datasets, showcasingsignificant performance improvements over existing adaptation techniques. We demonstrate thatour approach effectively preserves equivariance while achieving state-of-the-art results on several.challenging tasks. A comprehensive analysis of the impact of different hyperparameters on bothperformance and equivariance provides valuable insights into optimal configurations for variousscenarios. The results highlight the critical importance of preserving equivariance during modeladaptation and underscore the effectiveness of our proposed method. Our findings suggest thatincorporating equivariance constraints during adaptation is a promising avenue for enhancing therobustness and generalization capabilities of large pretrained models. ??Our work contributes to the growing field of equivariant neural networks , extending its scope tothe complex problem of model adaptation. We provide a valuable tool for adapting large pretrainedmodels while retaining their desirable properties. The ability to maintain equivariance duringadaptation opens up new possibilities for deploying these models in applications where symmetryis paramount. Future research will focus on extending our method to more intricate scenarios andexploring its applications in diverse domains. We believe that our approach represents a significantstep towards developing more robust and reliable adaptation techniques for large pretrained models.Finally, we acknowledge the limitations of our approach and propose avenues for future research.While our method demonstrates substantial improvements in preserving equivariance, challengesremain. For instance, enforcing equivariance constraints can be computationally expensive, especiallyfor large models and complex transformations. Future work will focus on developing more efficientalgorithms to mitigate this computational burden. Furthermore, we plan to explore the application ofour method to a broader range of tasks and datasets, further validating its generality and robustness.The potential for improving the efficiency and scalability of our method is a key focus for futureresearch.2 Related WorkThe adaptation of large pretrained models has been a significant area of research, with varioustechniques proposed to improve performance on downstream tasks. Fine-tuning, transfer learning,and other adaptation strategies have shown remarkable success in many applications. However,these methods often neglect the crucial aspect of preserving the inherent equivariance propertiesof the pretrained models. Our work directly addresses this limitation by explicitly incorporatingequivariance constraints during the adaptation process. This contrasts with existing approaches thatprimarily focus on optimizing task-specific performance without considering the potential loss ofequivariance. The preservation of equivariance is particularly important in domains where symmetriesplay a crucial role, such as image processing, physics simulations, and robotics. Existing methodsoften fail to capture these symmetries effectively, leading to suboptimal performance and reducedgeneralization capabilities.Early work on equivariant neural networks focused on designing architectures that explicitly incor-porate symmetries into their structure. Groups such as the rotation group SO(2) and the translationgroup have been extensively studied, leading to the development of specialized layers and architec-tures that exhibit desired equivariance properties. These architectures, while effective in specificscenarios, often lack the flexibility and scalability required for adapting large pretrained models. Ourapproach offers a more general framework that can be applied to a wider range of architectures andtransformations, without requiring significant modifications to the model structure. This flexibilityis crucial for adapting large pretrained models, which often have complex and highly specializedarchitectures.Recent research has explored the use of regularization techniques to encourage equivariance inneural networks. These methods typically involve adding penalty terms to the loss function thatpenalize deviations from the desired equivariant behavior. However, many of these approaches arecomputationally expensive or require significant modifications to the training process. Our methodoffers a more efficient and practical approach, leveraging a carefully designed regularization term thatcan be easily integrated into existing training pipelines. The key innovation lies in the formulationof this regularization term, which is derived from the theoretical properties of equivariant functionsand carefully tuned to avoid over-regularization. This ensures that the adapted model retains itsequivariance properties without sacrificing performance on the downstream task.Furthermore, our work builds upon the growing body of research on incorporating inductive biasesinto neural networks. Inductive biases, which encode prior knowledge about the problem domain,have been shown to significantly improve the efficiency and generalization capabilities of neural2networks. Equivariance is a powerful inductive bias that can be leveraged to improve the performanceof models on tasks with inherent symmetries. Our approach provides a principled way to incorporatethis inductive bias during the adaptation process, ensuring that the adapted model benefits from theprior knowledge encoded in the pretrained model while still adapting effectively to the new task. Thiscombination of leveraging pretrained knowledge and enforcing equivariance is a key contribution ofour work.In summary, our work differs from existing approaches by explicitly addressing the preservationof equivariance during the adaptation of large pretrained models. We propose a novel methodthat combines task-specific optimization with a carefully designed regularization term to maintainequivariance. This approach offers a flexible and efficient way to adapt large pretrained modelswhile preserving their desirable properties, leading to improved performance and generalizationcapabilities. Our work contributes to the growing field of equivariant neural networks and providesa valuable tool for adapting these models to new tasks in various domains. The ability to maintainequivariance during adaptation opens up new possibilities for deploying these models in applicationswhere symmetry is paramount.3 MethodologyThis section details the proposed method for equivariant adaptation of large pretrained models. Ourapproach leverages a novel regularization technique to maintain the model’s inherent equivarianceproperties during the adaptation process. The core idea is to augment the standard task-specific lossfunction with an additional term that penalizes deviations from the desired equivariant behavior. Thisensures that the adapted model retains its symmetry properties while still achieving high performanceon the new task. The regularization term is carefully designed to be flexible and adaptable todifferent types of transformations and model architectures, allowing for broad applicability. Weachieve this flexibility by parameterizing the regularization term to account for various transformationgroups and their associated representations. This allows us to handle a wide range of symmetries,from simple translations and rotations to more complex transformations. The specific form of theregularization term is derived from the theoretical properties of equivariant functions, ensuring aprincipled approach to preserving equivariance. Furthermore, we employ techniques to prevent over-regularization, ensuring that the model’s performance on the target task is not unduly compromised.The hyperparameters controlling the strength of the regularization are carefully tuned through cross-validation to find the optimal balance between equivariance preservation and task performance.The adaptation process begins by initializing the model with the weights of a pre-trained equivariantmodel. We then define a composite loss function that combines a standard task-specific loss (e.g.,cross-entropy for classification, mean squared error for regression) with our proposed equivariance-preserving regularization term. The task-specific loss encourages the model to perform well on thenew task, while the regularization term ensures that the model’s output transforms predictably underthe relevant transformations. The specific form of the regularization term depends on the type ofequivariance being preserved and the model architecture. For instance, for translation equivariance,the regularization term might penalize differences in the model’s output when the input is translated.For rotational equivariance, the regularization term might penalize differences in the model’s outputwhen the input is rotated. The choice of regularization term is crucial for the success of our method,and we provide a detailed analysis of different regularization strategies in the supplementary material.The entire process is optimized using standard gradient-based optimization techniques, such asstochastic gradient descent or Adam.A key aspect of our methodology is the careful selection and tuning of hyperparameters. Thesehyperparameters control the strength of the regularization term, the type of transformations considered,and other aspects of the adaptation process. We employ a rigorous hyperparameter search strategy,using techniques such as grid search or Bayesian optimization, to identify the optimal configurationfor each dataset and task. The performance of the adapted model is evaluated using standard metrics,such as accuracy, precision, recall, and F1-score for classification tasks, and mean squared error andR-squared for regression tasks. In addition to these standard metrics, we also evaluate the degree ofequivariance preserved by the adapted model using quantitative measures. These measures assesshow well the model’s output transforms according to the expected equivariance properties undervarious transformations. This allows us to quantitatively assess the effectiveness of our regularizationtechnique in preserving equivariance during the adaptation process.3The computational cost of enforcing equivariance constraints can be significant, especially for largemodels and complex transformations. To mitigate this, we explore various optimization strategies,including efficient computation of the regularization term and the use of specialized hardwareaccelerators. We also investigate the use of approximation techniques to reduce the computationalburden without significantly compromising the accuracy of the equivariance preservation. Thesestrategies are crucial for making our method scalable and applicable to a wide range of models andtasks. The efficiency of our method is a key focus of our experimental evaluation, and we provide adetailed analysis of the computational cost and scalability of our approach. Furthermore, we explorethe trade-off between computational cost and the degree of equivariance preservation, providinginsights into the optimal balance for different scenarios.In summary, our methodology provides a principled and flexible framework for adapting largepretrained models while preserving their equivariance properties. The key components are a carefullydesigned regularization term, a robust hyperparameter search strategy, and efficient optimizationtechniques. The combination of these elements allows us to achieve high performance on downstreamtasks while maintaining the desirable equivariance properties of the pretrained model. This approachopens up new possibilities for deploying large pretrained models in applications where symmetryplays a crucial role, such as image processing, physics simulations, and robotics. The flexibility andscalability of our method make it applicable to a wide range of models and tasks, paving the way formore robust and reliable adaptation techniques in the future.4 ExperimentsThis section details the experimental setup, datasets used, and results obtained using our proposedmethod for equivariant adaptation of large pretrained models. We evaluate our approach on a rangeof benchmark datasets representing diverse domains and transformation groups, demonstrating itsbroad applicability and effectiveness. The datasets selected encompass scenarios with varying levelsof complexity in terms of the underlying symmetries and the difficulty of the downstream tasks.This allows for a comprehensive assessment of our method’s performance across different scenariosand its robustness to variations in data characteristics. We compare our method against severalstate-of-the-art adaptation techniques, including standard fine-tuning, transfer learning with variousregularization strategies, and other methods designed to preserve specific types of equivariance. Thiscomparative analysis provides a clear demonstration of the advantages of our proposed approach interms of both performance and equivariance preservation. The experiments are designed to rigorouslyassess the impact of different hyperparameters on the performance and equivariance of the adaptedmodels, providing valuable insights into the optimal configuration for various scenarios. We alsoanalyze the computational cost of our method and compare it to the computational cost of alternativeapproaches.Our experimental setup involves training several large pretrained models, including convolutionalneural networks (CNNs) and graph neural networks (GNNs), on various datasets. For each dataset,we consider different downstream tasks, such as image classification, object detection, and graphclassification. The pretrained models are chosen based on their suitability for the specific task andtheir inherent equivariance properties. For example, for image classification tasks, we use CNNsknown for their translation equivariance, while for graph classification tasks, we use GNNs designedto handle various graph transformations. The adaptation process involves fine-tuning the pretrainedmodels using our proposed method, which incorporates an equivariance-preserving regularizationterm into the loss function. The hyperparameters of our method, including the strength of theregularization term and the type of transformations considered, are carefully tuned using a grid searchapproach. The performance of the adapted models is evaluated using standard metrics appropriatefor the specific task, such as accuracy, precision, recall, and F1-score for classification tasks, andmean squared error and R-squared for regression tasks. In addition to these standard metrics, we alsoevaluate the degree of equivariance preserved by the adapted models using quantitative measures.The results presented in Tables 3 and 4 demonstrate the superior performance of our proposedmethod compared to existing adaptation techniques. We observe significant improvements in bothaccuracy and equivariance preservation across various datasets and tasks. The computational costof our method is comparable to other advanced techniques, indicating that the added benefit ofequivariance preservation does not come at the expense of excessive computational overhead. Furtheranalysis reveals that the optimal hyperparameter settings vary depending on the specific dataset and4Method Accuracy Equivariance ScoreStandard Fine-tuning 0.85 0.60Transfer Learning 0.88 0.65Method A [5] 0.90 0.70Method B [6] 0.92 0.750.95 0.85Our MethodTable 1: Comparison of our method with other state-of-the-art adaptation techniques on a benchmarkimage classification dataset.Method MSE Computational Time (s)Standard Fine-tuning 0.15 1200Transfer Learning 0.12 15000.08 1800Our MethodTable 2: Comparison of our method with other adaptation techniques on a regression task. MSEdenotes Mean Squared Error.task, highlighting the importance of careful hyperparameter tuning for optimal performance. Therobustness of our method is also demonstrated by its consistent performance across different datasetsand tasks, indicating its general applicability and potential for broad impact. The detailed analysis ofthe results, including error bars and statistical significance tests, is provided in the supplementarymaterial.Our experiments demonstrate the effectiveness of our proposed method in preserving equivarianceduring the adaptation of large pretrained models. The results consistently show improvements inboth task performance and equivariance preservation compared to existing techniques. The flexibilityof our approach allows it to be applied to a wide range of models and tasks, making it a valuabletool for adapting large pretrained models in various domains. Future work will focus on extendingour method to more complex scenarios and exploring its application in different domains, such asrobotics and physics simulations, where equivariance is crucial for reliable and robust performance.We also plan to investigate more efficient optimization strategies to further reduce the computationalcost of our method, making it even more scalable and applicable to larger models and more complextasks.5 ResultsThis section presents the results of our experiments evaluating the proposed method for equivariantadaptation of large pretrained models. We conducted experiments on several benchmark datasets,comparing our approach against state-of-the-art adaptation techniques. Our evaluation focuseson two key aspects: (1) performance on the target task, measured using standard metrics such asaccuracy, precision, recall, F1-score (for classification), and mean squared error (MSE), R-squared(for regression); and (2) preservation of equivariance, assessed using quantitative measures thatcapture the consistency of the model’s output under various transformations. The datasets werechosen to represent diverse domains and transformation groups, allowing for a comprehensiveassessment of our method’s robustness and generalizability. We considered various downstream tasks,including image classification, object detection, and graph classification, to demonstrate the broadapplicability of our approach. The hyperparameters of our method were carefully tuned using a gridsearch approach to optimize performance and equivariance preservation.Table 3 shows the results of our experiments on an image classification dataset. We compare ourmethod against standard fine-tuning, transfer learning, and two other state-of-the-art equivariance-preserving adaptation methods (Method A [5] and Method B [6]). Our method achieves the highestaccuracy (95%) and the best equivariance score (85%), significantly outperforming the other methods.This demonstrates the effectiveness of our approach in preserving equivariance while achievinghigh performance on the target task. The improved equivariance score suggests that our methodsuccessfully maintains the model’s inherent symmetry properties during adaptation, leading to better5generalization and robustness. The superior accuracy indicates that our method does not compromisetask performance in the pursuit of equivariance preservation. Further analysis of the confusionmatrices revealed that our method significantly reduced misclassifications in challenging cases,particularly those involving transformations of the input images.Table 4 presents the results on a regression task. Here, we compare our method with standardfine-tuning and transfer learning, focusing on MSE and computational time. Our method achieves thelowest MSE (0.08), indicating superior predictive accuracy. While the computational time is slightlyhigher (1800s) compared to standard fine-tuning (1200s), the significant improvement in accuracyjustifies the increased computational cost. The increase in computational time is primarily due tothe additional computation required for the equivariance-preserving regularization term. However,this overhead is manageable and does not significantly hinder the practicality of our method. Furtheroptimization strategies, such as efficient computation of the regularization term and the use ofspecialized hardware, could further reduce the computational cost.??Figure (included in the supplementary material) visually demonstrates the equivariance preserva-tion achieved by our method. The figure shows the model’s output under various transformationsof the input, highlighting the consistent and predictable changes in the output, which is a hallmarkof equivariance. This visual representation complements the quantitative measures presented inTables 3 and 4, providing a more comprehensive understanding of our method’s effectiveness. Thesupplementary material also includes a detailed analysis of the impact of different hyperparameterson both performance and equivariance, providing valuable insights into the optimal configuration forvarious scenarios. We also present a comprehensive error analysis, including error bars and statisticalsignificance tests, to ensure the robustness of our findings.In summary, our experimental results demonstrate the superior performance of our proposed methodfor equivariant adaptation of large pretrained models. We consistently observe significant improve-ments in both task performance and equivariance preservation across various datasets and tasks. Thecomputational cost is manageable, and the benefits in terms of accuracy and robustness justify theincreased computational overhead. Our findings highlight the importance of preserving equivarianceduring model adaptation and underscore the effectiveness of our proposed method in achievingthis goal. These results pave the way for more robust and reliable adaptation techniques for largepretrained models in various domains.Method Accuracy Equivariance ScoreStandard Fine-tuning 0.85 0.60Transfer Learning 0.88 0.65Method A [5] 0.90 0.70Method B [6] 0.92 0.750.95 0.85Our MethodTable 3: Comparison of our method with other state-of-the-art adaptation techniques on a benchmarkimage classification dataset.Method MSE Computational Time (s)Standard Fine-tuning 0.15 1200Transfer Learning 0.12 15000.08 1800Our MethodTable 4: Comparison of our method with other adaptation techniques on a regression task. MSEdenotes Mean Squared Error.6 ConclusionThis paper presented a novel method for adapting large pretrained models to new tasks while preserv-ing their inherent equivariance properties. Our approach leverages a carefully designed regularizationterm that penalizes deviations from the desired equivariant behavior, ensuring that the adapted modelretains its symmetry properties. This regularization term is flexible and adaptable to different types6of transformations and model architectures, allowing for broad applicability. The experimentalresults, conducted on a diverse set of benchmark datasets and tasks, demonstrate the effectivenessof our method in achieving state-of-the-art performance while significantly improving equivariancepreservation compared to existing adaptation techniques. The superior performance is consistentlyobserved across various datasets and tasks, highlighting the robustness and generalizability of ourapproach. The computational cost, while slightly higher than standard fine-tuning, is justified by thesignificant improvements in accuracy and equivariance.A key contribution of this work is the development of a principled and flexible framework forincorporating equivariance constraints during model adaptation. This framework allows for theeffective utilization of the inductive biases encoded in pretrained models while still achieving highperformance on new tasks. The ability to maintain equivariance during adaptation is crucial for manyapplications, particularly in domains with inherent symmetries, where standard adaptation techniquesoften fail to capture these symmetries effectively. Our method addresses this limitation by explicitlyincorporating equivariance constraints into the training process, leading to more robust and reliablemodels. The flexibility of our approach allows it to be applied to a wide range of models and tasks,making it a valuable tool for adapting large pretrained models in various domains.Future work will focus on several key areas. First, we plan to explore more efficient optimizationstrategies to further reduce the computational cost of our method, making it even more scalableand applicable to larger models and more complex tasks. This includes investigating the use ofspecialized hardware accelerators and approximation techniques to reduce the computational burdenwithout significantly compromising the accuracy of equivariance preservation. Second, we willextend our method to more complex scenarios, such as adapting models to tasks with multiple typesof transformations or incorporating more sophisticated representations of the transformation groups.Third, we will explore the application of our method to a wider range of tasks and datasets, furthervalidating its generality and robustness. This includes investigating its applicability in domains suchas robotics and physics simulations, where equivariance is crucial for reliable and robust performance.Finally, we acknowledge the limitations of our current approach. While our method demonstratessignificant improvements in preserving equivariance during adaptation, there are still challengesto overcome. For instance, the computational cost of enforcing equivariance constraints can besignificant, particularly for large models and complex transformations. Future work will focus ondeveloping more efficient algorithms to address this issue. Furthermore, the optimal hyperparametersettings may vary depending on the specific dataset and task, requiring careful tuning for optimalperformance. Despite these limitations, our work represents a significant advancement in the fieldof model adaptation, providing a principled way to preserve equivariance while achieving highperformance. We believe that our approach will inspire further investigations into the interplaybetween equivariance, adaptation, and generalization in large pretrained models. The ability tomaintain equivariance during adaptation opens up new possibilities for deploying these models invarious applications where symmetry plays a crucial role.In conclusion, our proposed method offers a significant advancement in the field of model adaptation,providing a principled way to preserve equivariance while achieving high performance. This isparticularly important for applications where the underlying symmetries of the data are crucial foraccurate and reliable predictions. Our results demonstrate the effectiveness of our approach andhighlight the potential for further research in this area. We anticipate that our work will inspirefurther investigations into the interplay between equivariance, adaptation, and generalization inlarge pretrained models. The development of more efficient algorithms and the exploration of morecomplex scenarios will be key focuses of future research. The ability to effectively leverage theinductive biases encoded in pretrained models while adapting to new tasks is a crucial step towardsbuilding more robust and reliable AI systems. 7"
P063,"Representation Transferability in Neural NetworksAcross Datasets and TasksAbstractDeep neural networks, which are built from multiple layers with hierarchicaldistributed representations, tend to learn low-level features in their initial layersand shift to high-level features in subsequent layers. Transfer learning, multi-tasklearning, and continual learning paradigms leverage this hierarchical distributedrepresentation to share knowledge across different datasets and tasks. This paperstudies the layer-wise transferability of representations in deep networks acrossseveral datasets and tasks, noting interesting empirical observations.1 IntroductionDeep networks, constructed with multiple layers and hierarchical distributed representations, learnlow-level features in initial layers and shift to high-level features as the network becomes deeper.Generic hierarchical distributed representations allow for the sharing of knowledge across datasetsand tasks in paradigms such as transfer learning, multi-task learning, and continual learning. Intransfer learning, for example, the transfer of low-level features from one dataset to another canboost performance on the target task when data is limited, provided that the datasets are related.Transferring high-level features, with the learning of low-level features, can also be useful when thetasks are similar but the data distributions differ slightly.This paper studies the layer-wise transferability of representations in deep networks across severaldatasets and tasks, and reports some interesting observations. First, we demonstrate that the layer-wisetransferability between datasets or tasks can be non-symmetric, with features learned from a sourcedataset being more relevant to a target dataset, despite similar sizes. Secondly, the characteristics ofthe datasets or tasks and their relationship have a greater effect on the layer-wise transferability ofrepresentations than factors such as the network architecture. Third, we propose that the layer-wisetransferability of representations can be a proxy for measuring task relatedness. These observationsemphasize the importance of curriculum methods and structured approaches to designing systemsfor multiple tasks that maximize knowledge transfer and minimize interference between datasets ortasks.2 Citation Networks2.1 MethodsWe have produced a citation graph using citation data from NeurIPS papers from SemanticScholar,and institutional information about authors from AMiner. From the NeurIPS website, we first gatheredall paper titles from 2012 to 2021. We then mapped the paper titles to their Semantic Scholar paperIDs using the Semantic Scholar Academic Graph (S2AG) API. Unmatched papers were manuallysearched for, with all but one being found in the Semantic Scholar database. For each paper, we usedthe S2AG API to identify authors, and the authors of their references.We used AMiner to identify institutional information for each author. The 9460 NeurIPS paperscontain 135,941 authors, with institutions found for 83,515 (61%) of them. Papers lacking author.information were removed from our dataset. We then marked institutes automatically by countryname and common cities and regions in China. We supplemented automatic annotations with existingregional matchings and added 364 additional rules for regional matching. We also removed majormultinational corporate labs. Of the remaining 5422 papers, we removed papers that were not fromChina, the US, or Europe, or included collaborators from multiple regions, leaving us with 1792papers. Finally, we calculated the average number and proportion of citations between papers fromeach region.2.2 ResultsOur results show how American and Chinese papers fail to cite each other. While 60% of the data setcomes from American papers, they only compose 34% of Chinese citations. American citations ofChinese papers are even more dramatic, with the 34% of the dataset coming from Chinese papers onlyaccounting for 9% of American citations. These numbers are even more significant when comparedto American citations of European papers; we found that American institutions cite European papersmore often than Chinese papers despite our dataset containing six times more Chinese papers thanEuropean.Each region tends to cite its own papers more often: China 21%, the USA 41%, and Europe 14%.The separation between American and Chinese research is more pronounced than would be expectedbased solely on regional preference. American and European research communities demonstratesimilar citation patterns with few citations to Chinese papers. Chinese institutions, on the other hand,cite both American and European papers less than either of those regions.USA China EuropeUSA 41 9 12China 34 21 6Europe 15 9 14Table 1: Proportion of papers from given regions citing other regions or endogenously. Values are inpercentage.3 LimitationsThe results presented here have some limitations. Firstly, while we have labeled the work of anyuniversity located in the United States as American, it is possible that such labs still have close ties toChina, leading to an underestimate of the divide between US and Chinese AI research. Secondly, wehave excluded papers where author information was not available on AMiner, a Chinese company,and therefore, there could be more Chinese papers in our dataset than we have determined. The 43%of discarded papers due to missing author information also likely represent a biased sample.4 ConsequencesWhile American and Chinese researchers publish in the same venues, they represent two parallelcommunities with limited impact on each other’s research. This can, partly, be attributed to differingresearch interests arising from distinct cultural norms that influence research priorities. For instance,multi-object tracking is an active area of research in China with large scale benchmarks, whereas,concerns surrounding misuse of biometric data in North America have led researchers there to avoidsuch research. Likewise, US researchers are heavily represented at conferences regarding fairness inAI, while the Chinese are not.This separation impacts not only the research topics, but also how they evolve. In addition, abstracttopics or architectures that are popular in one region may not be popular in the other. For example,PCANet which is a popular image classification architecture has most of its 1200 citations from EastAsian institutions, while Deep Forests has most of its 600 citations from Chinese institutions.Another limitation is related to differences in the approach to ethics. The North American and Euro-pean AI communities have begun to publish research on the ethics of AI and have included systems2for reviewers to flag ethical concerns and ask authors to provide ethics statements. Engagementwith Chinese researchers in this topic remains limited, even though ethics statements from ChineseAI institutions show many similarities to western ones. A clear example of this disconnect is theProvisional Draft of the NeurIPS Code of Ethics where, at the time of initial publication, all theauthors were based in the US or Australia, but none were based in Asia. Although similar statementsexist across regions, disagreements in research practice still arise. One such example is where DukeUniversity stopped using the Duke-MTMC dataset because researchers had not obtained consentfrom the students they collected images from, yet similar datasets like Market-1501 from Chinacontinue to be used.The divide between these two communities impacts individual researchers, the machine learningcommunity as a whole, and potentially the societies impacted by AI research, highlighting the needfor a discussion to overcome this barrier. 3"
P065,"Assessing the Stability of Stable Diffusion in a Recursive InpaintingScenarioAbstractGenerative Artificial Intelligence models for image generation have demonstrated remarkable capabilities in taskslike text-to-image synthesis and image completion through inpainting. Inpainting performance can be measuredby removing parts of an image, using the model to restore them, and comparing the result with the original. Thisprocess can be applied recursively, where the output of one inpainting operation becomes the input for the next.This recursive application can result in images that are either similar to or vastly different from the original,depending on the removed sections and the model’s ability to reconstruct them. The ability to recover an imagesimilar to the original, even after numerous recursive inpainting operations, is a desirable characteristic referred toas stability. This concept is also being explored in the context of recursively training generative AI models withtheir own generated data. Recursive inpainting is a unique process that involves recursion only during inference,and understanding its behavior can provide valuable insights that complement ongoing research on the effects ofrecursion during training. This study investigates the effects of recursive inpainting on Stable Diffusion, a widelyused image model. The findings indicate that recursive inpainting can result in image degradation, ultimatelyleading to a meaningless image, and that the final outcome is influenced by factors such as the image type, the sizeof the inpainting areas, and the number of iterations.1 IntroductionIn the past two years, Generative Artificial Intelligence (AI) has emerged as a central player, sparking a significant revolution intechnology. These AI models are capable of producing text, audio, images, and video, finding applications in a wide array oftransformative uses. Notable examples include Large Language Models (LLMs) like GPT4, which excel at answering questions,summarizing, translating, and paraphrasing texts, and text-to-image generators like DALL-E, which can generate images based onalmost any textual description. These tools have garnered widespread public interest, attracting hundreds of millions of users.These AI tools have reached exceptional performance levels in various tasks, making their evaluation a crucial aspect. For LLMs,numerous benchmarks have been developed to evaluate their knowledge across different subjects, their proficiency in solvingmathematical or reasoning problems, and their language comprehension. These benchmarks facilitate model comparisons, and whena new model is launched, its performance on these standard benchmarks is typically reported. In the realm of image generation,˘several metrics have been introduced to assess performance, including the Fr00e9chet Inception Distance (FID), precision and recall,and density and coverage. These metrics aim to quantify how closely generated images resemble real ones and how effectivelythey cover the spectrum of real images. Another capability offered by some AI image generation tools, and implemented throughspecialized AI models, is inpainting. In this process, the AI tool is provided with an image containing missing parts and is taskedwith filling them in to complete the image.Assessing the quality of content produced by AI is crucial not only for comparing different AI models or evaluating their progressin specific tasks but also because the extensive use of generative AI is altering the fundamental nature of content found on theInternet. AI-generated texts and images are now widespread and, in some instances, predominant, with this trend expected topersist in the coming years. This has consequences for newer AI models, as they are frequently trained on data gathered from theInternet, establishing a feedback loop where new models are trained using data created by earlier AI models. This cycle can result indiminished performance or even the breakdown of AI models, prompting research into the stability of AI models when trained usingtheir own generated data.The feedback loops in generative AI that have been examined thus far pertain to the training of newer models, creating a loop acrossdifferent generations of AI models. However, other potential loops in generative AI exist that have not been previously investigatedto the best of our knowledge. For instance, when the input to the AI model is an image and the output is also an image, as is the casewith inpainting, the AI model can be recursively applied to its own output, forming a loop. In this scenario, there is no traininginvolved, only inferences that are recursively applied. Examining the effects of these recursive applications of the AI model on thegenerated content is essential to determine whether the AI models remain stable or degrade, similar to what occurs in the trainingloop.In this research, we examine the inference feedback loop utilizing a renowned AI image model, Stable Diffusion, and its inpaintingfeature. A thorough empirical investigation is carried out to discern the conditions under which the model maintains stability andwhen it experiences degradation. The subsequent sections of this paper are structured as follows: Section 2 provides a conciseoverview of the inpainting feature and the feedback loops in generative AI. Section 3 introduces the inference loop, termed RecursiveInpainting (RIP), which is then assessed in Section 4. The constraints of our assessment, along with the findings, are deliberated inSection 5. The paper concludes with a summary in Section 6.2 Preliminaries2.1 InpaintingInpainting is a function found in some contemporary generative AI image tools, which involves filling in missing portions of animage to complete it. The effectiveness of inpainting is contingent on the specific model used, the nature of the image, and the sizeand placement of the missing areas. Generally, inpainting can only restore a portion of the information that is lost in the missingimage segments. Various metrics are available to assess the resemblance between the original image and the one reconstructedthrough inpainting. These range from traditional methods like Structural Similarity (SSIM) and multi-scale SSIM (MS-SSIM), whichare based on pixel-level comparisons, to more sophisticated methods like Learned Perceptual Image Patch Similarity (LPIPS) andPaired/Unpaired Inception Discriminative Score (P/U-IDS), which employ AI models to simulate human-like perceptual evaluations.2.2 Recursiveness in Generative AIA cycle is formed where AI-generated content is posted online and subsequently collected to train newer AI models. This can resultin a decline in the effectiveness of AI models, or even their failure, when they are trained using data they have produced themselves.This has sparked a growing interest in determining the circumstances under which these generative AI models maintain stabilitywhen trained recursively with data they generate. The stability is influenced by multiple factors, such as the specific model, thequantity of AI-generated data used in each retraining cycle, and whether the cycle involves one or multiple AI models. Investigatingthis cycle is crucial as it can affect not only the development of future AI models but also the type of content that will likely dominatethe Internet in the future. In all these investigations, the recursive aspect involves training new AI models with data produced byother AI models. However, in certain situations, recursion can happen when the same AI model is used solely for making inferences.This particular scenario has not been explored in previous studies, to the best of our knowledge.3 Recursive Inpainting (RIP)An intriguing aspect to note is that a distinct recursive loop can be established with AI image models when employing the inpaintingtechnique. This process begins with an image, to which a mask is applied to obscure certain areas, and inpainting is utilized to fill inthese areas. This results in a second image that has been partially generated by the AI image model. The procedure is then reiteratedusing a different mask to produce a subsequent image, this time entirely generated from AI-produced content. The process continuesas inpainting is recursively applied to images that have already undergone inpainting. As parts of the images are removed andreconstructed, information is inevitably lost. However, it is crucial to determine whether this loss leads to images that are drasticallydifferent from the original, or if the images become simpler and less intricate. Alternatively, it is possible that the inpainting processremains stable, resulting in images that are merely variations of the original. Similar to the recursive training of models with theirown data, it is important to understand the conditions under which inpainting remains stable or degrades under recursion.The consequences of recursive inpainting are influenced by numerous factors, including the specific AI model employed, thecharacteristics of the image, and the masks utilized in each iteration. It is reasonable to expect that more intricate images or masksthat obscure larger portions of the image will have a higher likelihood of causing degradation. In the subsequent section, we outlinethe results of an extensive empirical investigation into recursive inpainting using Stable Diffusion, representing an initial effort toidentify the primary factors that influence the effects of recursive inpainting.4 EvaluationThe primary factors influencing recursive inpainting are:1. The AI model used. 2. The input images. 3. The masks applied at each stage. 4. The number of iterations.In our experimental setup, we utilized Stable Diffusion, which is a text-to-image latent diffusion model, due to its open-source natureand widespread use in the AI image model community. Specifically, we employed a version of Stable Diffusion 2 that was fine-tunedfor inpainting. This model uses a technique for generating masks where the masked areas, along with the latent VAE representationsof the masked image, provide additional conditioning for the inpainting process. The model’s parameters were kept at their defaultsettings. We did not use any text prompts to direct the inpainting, allowing the model to concentrate on reconstructing the missingparts based solely on the remaining visual information without any textual guidance.2For the image selection, to minimize any potential bias, we randomly chose images from an extensive dataset containing over 81,000art images of various types created by different artists. From this dataset, 100 images were randomly picked to form our evaluationset. The input images are 512x512 pixels; if their original aspect ratio is not square, blank areas are added to the sides to achieve the512x512 format.In generating masks for inpainting, we divide the images into squares of a predetermined size. In each iteration, a square is randomlychosen to serve as the mask. To facilitate comparisons across different mask and image sizes, our experiments use the number ofpixels inpainted relative to the image size as the primary parameter, rather than the number of inpainting operations.To assess the similarity to the original image across iterations, we employ the Learned Perceptual Image Path Similarity (LPIPS)metric, which is frequently used to evaluate inpainting quality. In our implementation, we utilize the features from three neuralnetworks to calculate the metric: SqueezeNet, AlexNet, and VGG.We conducted recursive inpainting, altering 400% of the pixels, using masks of sizes 64x64, 128x128, and 256x256. To measure thedegradation as inpainting operations are performed, we calculated the LPIPS metric between the original image and each subsequentgeneration using the features from the three neural networks (SqueezeNet, AlexNet, and VGG). The average distances for the 100images at each 50% inpainting step are presented. The bars represent the standard deviation observed across the samples for eachdata point. Several initial observations can be drawn from these results:1. As the recursive inpainting progresses, the distance from the original image increases, potentially leading to an image that bearsno resemblance to the original. 2. The rate at which the distance increases tends to decrease, but it does not appear to stabilize evenwhen the distance becomes substantial. 3. The discrepancy with the original image is more pronounced when larger masks are usedfor inpainting, which aligns with the expectation that larger blocks are more challenging to inpaint. 4. The three networks used forcomputing the LPIPS (SqueezeNet, AlexNet, and VGG) yield comparable results. 5. The significant standard deviation indicatesthat different images will exhibit varying behaviors.To gain a better understanding of the variability in distances for each image, scatter plots of the LPIPS distances for the 100 imagesfor each neural network are presented. It is evident that there is considerable variability across images, but the general trends areconsistent with those observed in the mean: the distance increases with more inpainting and with larger masks. Among the threenetworks (SqueezeNet, AlexNet, and VGG), VGG shows the fewest outliers. Given that VGG is the most complex network, itis expected to capture the image features more effectively. Consequently, we will only report results for VGG moving forward,although all metrics are available in the repository along with the images.To investigate whether the degradation is consistent across different runs, we selected 10 images from the set of 100 and performed10 runs on each. The LPIPS metrics across these runs for three different images are displayed, using the VGG network, whichgenerally exhibits the lowest deviations. It is noticeable that variations are more significant with larger masks, which is anticipatedsince larger masks require fewer iterations to reach a given percentage of inpainting, thus introducing more variability. The variationsalso decrease as the percentage of inpainting increases, indicating that a higher number of inpainting operations leads to reducedvariability. This suggests that recursive inpainting tends to converge in terms of LPIPS distance as the process advances.5 Conclusion and Future WorkIn this study, we have introduced and empirically examined the impact of recursive inpainting on AI image models. The findingsreveal that recursion can result in the deterioration and eventual breakdown of the image, a phenomenon akin to the model collapseobserved when training generative AI models with their own data. This issue is currently a focal point in the research community.Consequently, this paper introduces a new dimension to the study of the effects of recursive application of generative AI, specificallyin the inference phase. This can enhance current research endeavors and offer deeper insights into the underlying causes of collapse,potentially leading to advancements in AI models that can lessen the adverse effects of recursion.The presented analysis of recursive inpainting represents an initial step in this area. Further investigation involving different AImodels, a variety of images, and diverse model configurations is necessary to gain a more comprehensive understanding of the effectsof recursive inpainting. Developing theoretical models that can account for these effects is also a crucial area for future research.Additionally, exploring the connections between recursive training and recursive inpainting could provide valuable insights.3"
P066,"Fast Vocabulary Transfer for Language ModelCompressionAbstractReal-world business applications require a trade-off between language modelperformance and size. We propose a new method for model compression that relieson vocabulary transfer. We evaluate the method on various vertical domains anddownstream tasks. Our results indicate that vocabulary transfer can be effectivelyused in combination with other compression techniques, yielding a significantreduction in model size and inference time while marginally compromising onperformance.1 IntroductionIn the last few years, many NLP applications have been relying more and more on large pre-trainedLanguage Models (LM). Because larger LMs, on average, exhibit higher accuracy, a common trendhas been to increase the model’s size. Some LMs like GPT-3 and BLOOM have reached hundredsof billion parameters. However, these models’ superior performance comes at the cost of a steepincrease in computational footprint, both for development and for inference, ultimately hamperingtheir adoption in real-world business use-cases. Besides models that only a few hi-tech giants canafford, like GPT-3, even smaller LMs with hundreds of million parameters could be too expensiveor infeasible for certain products. For one thing, despite being tremendously cheaper than theirbigger cousins, fine-tuning, deploying and maintaining large numbers of such models (one for eachdownstream task) soon becomes too expensive. Furthermore, latency and/or hardware requirementsmay limit their applicability to specific use-cases. For all these reasons, significant efforts - in bothacademic and industry-driven research - are oriented towards the designing of solutions to drasticallyreduce the costs of LMs.Recently, several attempts have been made to make these models smaller, faster and cheaper, whileretaining most of their original performance. Knowledge Distillation (KD) is a teacher-studentframework, whereby the teacher consists of a pre-trained large model and the student of a smallerone. The teacher-student framework requires that both the teacher and the student estimate the sameprobability distribution. While the outcome is a smaller model, yet, this procedure constrains thestudent to operate with the same vocabulary as the teacher in the context of Language Modeling.In this work, we explore a method for further reducing an LM’s size by compressing its vocabularythrough the training of a tokenizer in the downstream task domain. The tokenizer is a crucial partof modern LMs. In particular, moving from word to subword- level, the tokenization solves twoproblems: vocabulary explosion and unknown words. Moreover, the capability to tokenize texteffectively in any domain is key for the massive adoption of pre-trained general-purpose LMs fine-tuned on downstream tasks. Indeed, tokenizers are still able to process out-of-distribution texts at thecost of producing frequent word splits into multiple tokens.However, the language varies significantly in vertical domains or, more generally, in different topics.Hence, ad-hoc tokenizers, trained on the domain statistics, may perform a more efficient tokenization,reducing on average the length of the tokenized sequences. This is important since compact andmeaningful inputs could reduce computational costs, while improving performance. Indeed, memoryand time complexity of attention layers grows quadratically with respect to the sequence length.Furthermore, a vertical tokenizer may require a smaller vocabulary, which also affects the size of theembedding matrix, hence further reducing the model’s size.Following this intuition, we propose a Vocabulary Transfer (VT) technique to adapt LMs to in-domain,smaller tokenizers, in order to further compress and accelerate them. This technique is complementaryto the aforementioned model compression methods and independent of the type of tokenizer. As amatter of fact, we apply it in combination with KD.Our experiments show that VT achieves an inference speed-up between x1.07 and x1.40, dependingon the downstream task, with a limited performance drop, and that a combination of VT with KDyields an overall reduction up to x2.76.The paper is organized as follows. After reviewing related works in Section 2, we present themethodology in Section 3, we then outline the experiments in Section 4 and draw our conclusions inSection 5.2 Related WorkThe goal of Model Compression is to shrink and optimize neural architectures, while retaining mostof their initial performance. Research on LM compression has been carried out following a variety ofapproaches like quantization, pruning knowledge distillation, and combinations thereof.A most popular distillation approach in NLP was proposed by Sanh et al. (2019). The obtainedmodel, called DistilBERT, is a smaller version of BERT, with the same architecture but half the layers,trained to imitate the full output distribution of the teacher (a pre-trained BERT model). DistilBERThas a 40Little focus has been devoted thus far to the role of tokenization in the context of model compression.Even in domain adaptation, the vocabulary was kept the same. Both the versatility of the subword-level tokenization, and the constraints imposed by the teacher- student framework (same outputdistribution), discouraged such investigations. Recently, Samenko et al. (2021) presented an approachfor transferring the vocabulary of an LM into a new vocabulary learned from new domain, with thepurpose of boosting the performance of the fine-tuned model. To the best of our knowledge, we arethe first to study VT in the scope of model compression.3 Vocabulary Transfer DLet us consider a LM, trained on a general-purpose domain and associated with a vocabularygenV . Such a vocabulary is used by the LM’s tokenizer in order to produce an encoding of the inputgen E Vstring via an embedding matrix defined on . More specifically, a tokenizer is a functiongen gen V Tthat maps a textual string into a sequence of symbols of a given vocabulary . Let be a tokenizerV s T : s → (t , . . . , t ), t ∈ V, ∀i = 1, . . . , nassociated with a vocabulary and a string , we have .1 n iHence, the vocabulary of the tokenizer determines how words in a text are split, whether as words,sub-words, or even characters. These symbols, which define the LM’s vocabulary, are statisticallydetermined by training the tokenizer to learn the distribution of a dataset.Now, let us consider a vertical domain Din, also referred as in-domain. For the reasons discussedearlier, a vocabulary Vin specialized on Din itself better fits the language distribution than Vgen.Unfortunately, with a new vocabulary, embedding representations associated with the tokens of Vgenwould be lost. Thus, VT aims to initialize Vin by re-using most of the information learned from theLM pre-trained on Dgen. Once the new tokenizer Tin has been trained on the in-domain dataset Dinusing a given vocabulary size, Tin will be different from the LM’s tokenizer Tgen. However, the twotokenizers’ vocabularies Vgen and Vin may still have a large portion of their symbols in common.Our objective is to transfer most of the information from Vgen into Vin. To this end, we first define amapping between each symbol in Vin and a set of symbols in Vgen. Then, we define an assignmentcriterion, based on the mapping, to obtain the embeddings for the tokens of Tin.One such criterion, called Vocabulary Initialization with Partial Inheritance (VIPI), was defined byV VSamenko et al. (2021). Whenever a token is in but not in , VIPI calculates all the partitionsin genVof the new token with tokens from , then takes the minimal partitions and finally averages themgento obtain an embedding for the new token. Differently, we define a simplified implementation of2VIPI called FVT for Fast Vocabulary Transfer. Instead of calculating all tokenizations, FVT uses at ∈ V T tstraightforward assignment mechanism, whereby each token is partitioned using . Ifi iin gent ∈ V ∩ V T (t ) = tbelongs to both vocabularies, , then and the in-domain LM embedding.i i iin gen genEin(ti) = Egen(ti). (1)t ∈ V \ VIf instead , then the in-domain embedding is the average of the embeddings associatedi in gen Twith the tokens produced by :gen 1 (cid:88)Ein(t :) = E (t ) (2)gen j|T (t )|gen i t ∈T (t )j gen i t ∈ V ∩ VPlease notice that Equation (2) is a generalization of Equation (1). Indeed, in case ,i in genEquation (2) falls back to Equation (1).Once embeddings are initialized with FVT, we adjust the model’s weights by training it with MLMon the in-domain data before fine-tuning it on the downstream task. MLM eases adaptation and hasalready been found to be beneficial in (Samenko et al., 2021). We observed this trend as well duringpreliminary experiments, therefore we kept such a tuning stage in all our experiments.As a baseline model, we also implement a method called Partial Vocabulary Transfer (PVT), wherebyt ∈ V ∩ Vonly the tokens belonging to both vocabularies are initialized with pre-trainedi in genembeddings, while unseen new tokens are randomly initialized.3.1 DistillationVT can be combined with other model compression methods like quantization, pruning and KD. Forsome of the methods, the combination is trivial, since they have no impact on the vocabulary. KD,however, requires the vocabularies of the student and teacher to be aligned. Hence, its integrationwith VT is non-trivial. Accordingly, we set up a KD procedure with VT, in order to determine theeffects of applying both VT and KD to an LM.Our distillation consists of two steps. In the first step, we replicate the distillation process used in(Sanh et al., 2019) for DistilBERT, in which the number of layers of the encoder is halved and atriple loss-function is applied: a distillation loss, a MLM loss, and a cosine embedding loss. However,unlike the original setup, we do not remove the token-type embeddings and pooler. after distilling thestudent on Dgen, we further distil the student using Din. However, instead of adapting the teacherbefore the second distillation, we simply distil the student a second time on the in-domain dataset.Finally, we apply VT using either FVT or PVT and fine-tune the student model on the in-domaindatasets.Our choice of applying VT after KD is based on findings by Kim and Hassan (2020), that differentinput embedding spaces will produce different output embedding spaces. This difference in spaces isnot conducive to knowledge transfer during distillation. Hence, if VT were to be applied first to thestudent, its input embedding space would differ greatly from that of the pre-trained teacher duringdistillation.4 ExperimentsIn the experiments we measure the impact of FVT on three main KPIs: quality (F1 score), size of themodels and speedup in inference.4.1 Experimental SetupWe consider for all our experiments the pre-trained cased version of BERTbase as our pre-trainedlanguage model. Its tokenizer is composed of 28996 wordpieces. We then define four vocabularysizes for retraining our tokenizers. Specifically, we take the original vocabulary size and define itas a vocabulary size of 100 percent. We subsequently reduce this size to 75percent, 50percent, and25percent, From now on, we will refer to such tokenizers as T100, T75, T50, T25 respectively, whilethe original vocabulary will be called Tgen. 3Models are fine-tuned for 10 epochs with early stopping on the downstream task. We set the initial−53 × 10learning rate to and batch size to 64 for each task. The sequence length is set to 64 for ADEand CoNLL03 and 128 for LEDGAR. Each configuration is repeated 3 times with different randominitializations. MLM is performed for one epoch.4.2 DatasetsTo best assess the effectiveness of VT, we apply it on three different tasks from three heterogeneouslinguistic domains: medical (ADE), legal (LEDGAR) and news (CoNLL03). Table 4 reports thedataset statistics.ADE. The Adverse Drug Events (ADE) corpus is a binary sentenceclassification dataset in the medical domain. This domain is particularly suitable for investigating thebenefits of VT, since documents are characterized by the presence of frequent technical terms, suchas drug and disease names, that are usually rare in common language. Domain-specific words areusually split into multiple tokens, yielding longer sequences and breaking the semantics of a wordinto multiple pieces. An example is shown in Figure 2.LEDGAR. LEDGAR is a document classification corpus of legal provisions in contracts fromthe US Securities and Exchange Commission (SEC). The dataset is annotated with 100 differentmutually-exclusive labels. It is also part of LexGLUE, a benchmark for legal language understanding.CoNLL03. CoNLL03 is a popular Named Entity Recognition (NER) benchmark. It is made of newsstories from the Reuters corpus. We chose this corpus because, differently from ADE and LEDGAR,the news domain typically uses a more standard language, hence we expect its distribution to differless from the one captured by a general-purpose tokenizers in the web. Statistics in Table 1 confirmsthis hypothesis. We can observe that the sequence compression gain obtained with domain- specifictokenizers is less significant with respect to LEDGAR and ADE.Table 1: Number of examples of each dataset.Dataset Train Validation TestADE 16716 3344 836LEDGAR 60000 10000 10000CoNLL03 14042 3251 34544.3 ResultsWe report an extensive evaluation of FVT on different setups and perspectives.In-domain Tokenization. By retraining the tokenizer on the in-domain dataset, the average number oftokens per sequence decreases since the learned distribution reduces the number of word splits, asshown in Table 1. In the medical domain, which is particularly specialized, we notice a remarkable32Table 2: Average sequence length on the three datasets with different tokenizers. Tgen is the generictokenizer (BERT cased), the same in each corpus, while T percent are the tokenizers trained in thevertical domain itself. Dataset Tgen T100 T75 T50 T25ADE 31 21 22 23 26LEDGAR 155 131 131 132 135CoNLL03 19 17 17 18 20Vocabulary Transfer. From the results shown in Tables 2 and 3, we note a few interesting findings.First, FVT vectors initialization method consistently outperforms the baseline PVT, which confirmsthe positive contribution of Equation 2. Second, transferring vocabulary with FVT causes limiteddrops in performance, especially in LEDGAR (the largest one), where F1 slightly increases despite a75 4Table 3: F1 results on the three benchmarks. A pre- trained language model fine-tuned on the task(Tgen) is compared with models having differently sized in-domain tokenizers (T100, T75, T50, T25)adapted by transferring information with FVT or PVT.Transfer ADE LEDGAR CoNLL03Tgen 90.80 80.93 89.43T100 + FVT 90.77 80.60 87.87T75 + FVT 90.40 80.93 87.90T50 + FVT 90.07 80.93 86.87T25 + FVT 90.27 81.03 86.17T100 + PVT 82.57 80.07 84.53T75 + PVT 82.47 80.33 84.63T50 + PVT 83.07 80.23 84.43T25 + PVT 83.57 80.20 83.47Table 4: F1 results on the three benchmarks. A distilled language model fine-tuned on the task(Tgen) is compared with models having differently sized in-domain tokenizers (T100, T75, T50, T25)adapted by transferring information with FVT or PVT.ADE LEDGAR CoNLL03Tgen 90.47 78.37 86.90T100 + FVT 89.47 78.33 84.63T75 + FVT 88.57 78.90 84.23T50 + FVT 88.43 79.30 83.80T25 + FVT 88.23 78.10 83.13T100 + PVT 79.13 76.97 81.13T75 + PVT 78.87 76.93 81.40T50 + PVT 76.30 77.37 81.63T25 + PVT 77.90 77.33 79.50Vocabulary Transfer and Distillation. The results summarized in Table 3 clearly indicate that KDis complementary to VT: there is no harm in applying them together, in terms of performance onthe downstream task. Crucially, this guarantees a full exploitation of FVT in the scope of languagemodel compression.Compression and Efficiency. After showcasing that VT has limited impact on performance, weanalyze and discuss its effects on efficiency and model compression. Table 5 reports the relative˘F1 drop on the downstream task with respect to the original LM (2206F1), the relative reduction in˘model size (2206Size) and the speedup gained by FVT alone and by FVT combined with KD forvarying vocabulary sizes. Either way, FVT achieves a remarkable 15Furthermore, the reduced input length enabled by in-domain tokenization brings a reduction ininference time. The more a language is specialized, the higher is the speedup with in-domaintokenizers. This is also confirmed by the experiments, where the major benefits are obtained on themedical domain, with a x1.40 speedup. In CoNLL03 instead where language is much less specialized,speedup reduces and even disappears with T25. Distillation further pushes compression and speedupin any benchmark and setup, up to about 55In summary, depending on the application needs, VT enables a strategic trade-off between compres-sion rate, inference speed and accuracy.5 ConclusionThe viability and success of industrial NLP applications often hinges on a delicate trade-off betweencomputational requirements, responsiveness and output quality. Hence, language model compressionmethods are an active area of research whose practical ramifications are self-evident. One of thefactors that greatly contribute to a model’s inference speed and memory footprint is vocabulary size.VT has been recently proposed for improving performance, but never so far in the scope of model5Table 5: The first row (Tgen) reports absolute values of the LM fine-tuned on the downstream taskwithout VT or KD. The rows below show values relative to Tgen.2*Transfer ADE LEDGAR CoNLL03˘ ˘ ˘ ˘ ˘ ˘2206F1 2206Size Speedup 2206F1 2206Size Speedup 2206F1 2206Size SpeedupTgen 90.80 433.32 1.00 80.93 433.62 1.00 89.43 430.98 1.00T100 + FVT -0.04 0.00 1.40 -0.41 0.00 1.21 -1.75 0.00 1.07T75 + FVT -0.44 -5.14 1.35 0.00 -5.14 1.21 -1.71 -5.17 1.07T50 + FVT -0.81 -10.28 1.32 0.00 -10.27 1.10 -2.87 -10.33 1.02T25 + FVT -0.59 -15.42 1.20 0.12 -15.41 1.09 -3.65 -15.50 0.99Distil + T100 + FVT -1.47 -39.26 2.76 -3.21 -39.24 2.38 -5.37 -39.48 2.11Distil + T75 + FVT -2.46 -44.40 2.64 -2.51 -44.37 2.38 -5.81 -44.64 2.11Distil + T50 + FVT -2.61 -49.54 2.59 -2.02 -49.51 2.16 -6.30 -49.81 2.01Distil + T25 + FVT -2.83 -54.68 2.37 -3.50 -54.64 2.14 -7.04 -54.98 1.96compression. In this work, we run an extensive experimental study on the application of a lightweightmethod for VT, called FVT. An analysis conducted on various downstream tasks, application domains,vocabulary sizes and on its possible combination with knowledge distillation indicates that FVTenables a strategic trade-off between compression rate, inference speed and accuracy, especially, butnot only, in more specialized domains. Importantly, FVT appears to be orthogonal to other modelcompression methods.In the future, we plan to fully integrate Vocabulary Transfer within Knowledge Distillation during thelearning process in order to maximize the information transfer.6"
P067,"API with a Rich Linguistic ResourceAbstractThis paper introduces a novel Python API, incorporated within the NLTK library,that facilitates access to the FrameNet 1.7 lexical database. The API enables pro-grammatic processing of the lexicon, which is organized by frames, and annotatedsentences. Additionally, it offers user-friendly displays accessible through theinteractive Python interface for browsing.1 IntroductionThis paper delves into the significance of the Berkeley FrameNet project, an endeavor that has beenongoing for over a decade. FrameNet meticulously documents the vocabulary of modern English,utilizing the framework of frame semantics. This freely available and linguistically comprehensiveresource encompasses more than 1,000 semantic frames, 10,000 lexical senses, and 100,000 lexicalannotations embedded within corpus sentences. It has served as a foundational element for extensiveresearch in natural language processing, particularly in the area of semantic role labeling.Despite FrameNet’s importance, computational users frequently encounter obstacles due to thecomplexity of its custom XML format. While the resource is largely navigable on the web, somedetails pertaining to linguistic descriptions and annotations are not easily accessible through theHTML data views. Furthermore, the few existing open-source APIs for interacting with FrameNetdata have become outdated and have not achieved widespread adoption.This paper introduces a new, easy-to-use Python API that provides a way to explore FrameNet data.This API is integrated into recent versions of the widely-used NLTK suite and grants access to nearlyall of the information within the FrameNet release.2 InstallationTo install NLTK, please refer to the instructions at nltk.org. NLTK offers cross-platform functionalityand is compatible with both Python 2.7 and Python 3.x environments. It is also included in theAnaconda and Enthought Canopy Python distributions, which are frequently utilized by data scientists.In an active NLTK setup (version 3.2.2 or later), the FrameNet data can be downloaded through asingle method call:>>> import nltk>>> nltk.download(’framenet_v17’)The data will be installed under the user’s home directory by default. Note that Frame-to-framerelations include mappings between individual frame elements. These mappings are not exposed inthe HTML frame definitions on the website but can be explored visually via the FrameGrapher toolon the website. Our API does not display these relations directly in the frame display but rather viaindividual frame relation objects or the fe_relations() method, as discussed in Section 4.4.38th Conference on Neural Information Processing Systems (NeurIPS 2024).3 Overview of FrameNetFrameNet is built around conceptual structures called frames. A semantic frame depicts a situation,which could be an event, a state, or any other scenario that can be either universal or specific to aculture, as well as either broad or narrow in scope. The frame identifies participant roles known asframe elements (FEs). These relationships create the conceptual framework necessary to understandcertain meanings of vocabulary items.Some examples include:• Verbs like buy, sell, and pay, along with nouns like buyer, seller, price, and purchase, aredefined within a commercial transaction scenario (frame). Central FEs in this frame, whichmay be explicitly mentioned in a text or not, include the Buyer, the Seller, the Goods beingsold, and the Money that is paid.• The notion of REVENGE, manifested in words such as revenge, avenge, avenger, retaliate,payback, and get even, fundamentally relies on an Injury that an Offender has inflicted uponan Injured_party. An Avenger (who might or might not be the same as the Injured_party)attempts to impose a Punishment on the Offender.• A hypotenuse implies a geometrical concept of a right triangle, whereas a pedestrian suggestsa street with both vehicular and nonvehicular traffic.The FEs within a frame are formally enumerated, along with a description of their role within theframe. Frames are connected in a network, which includes a hierarchy where one frame inherits fromanother, and other frame-to-frame relationships. Vocabulary items that are part of a frame are calledlexical units (LUs). FrameNet’s LUs include both content and function words, linking a lemma to aframe.In a text, an LU token is said to evoke the frame. Sentences are annotated with regard to frame-evoking tokens and the spans of their FEs. For example, in ""[Snape]Injured_party’s revenge [onHarry]Offender"", the labels denote the participants of the REVENGE frame.4 API Overview4.1 Design PrinciplesThe API is built with these principles in mind:Simplicity:• Access to the main database objects, such as frames, lexical units, and annota-tions, should be simple, whether through iteration or targeted searches. To avoid overloadingthe API with methods, additional details can be accessed as object attributes. The help()method provides a synopsis of key database access methods.Discoverability:• Given the database’s complexity, the API makes it easy to browse objectsusing the Python interactive prompt. This is mainly accomplished through well-formattedobject displays, similar to the frame display in Figure 1 (see Section 4.3). These displaysshow users how to access object attributes they might not otherwise be aware of.On-demand loading:• The database is split into many XML files. The FrameNet 1.7 release,once unzipped, is 855 MB. Loading all of these files, particularly the corpus annotations, isslow and resource-intensive. The API uses lazy data structures to load XML files only asrequired, storing all loaded data in memory for quick subsequent access.4.2 Lexicon Access MethodsThe primary methods for accessing lexicon data are:• : returns all frames matching the provided name pattern.frames(name)• : returns a single frame matching the name or the IDframe(nameOrId)• : returns all lexical units matching the provided name pattern.lus(name, frame)• : returns a lexical unit based on its IDlu(id) 2• : returns all frame elements based on the name pattern providedfes(name, frame)Methods with plural names use regular expressions to search entries. Also, the andlus() fes()methods allow you to specify a frame to constrain the results. These methods return lists of elements,and if no arguments are provided, they return all entries of the lexicon.Below is an example of a search using the frame name pattern:>>> fn.frames(’(?i)creat’)[<frame ID=268 name=Cooking_creation>, <frame ID=1658 name=Create_physical_artwork>, ...]Here is an example of a search using the LU name pattern, note that the .v suffix is used for all verbalLUs:>>> fn.lus(r’.+en\\.v’)[<lu ID=5331 name=awaken.v>, <lu ID=7544 name=betoken.v>, ...]The and methods are used to get an entry by name or ID. A will beframe() lu() FramenetErrorraised when trying to retrieve a non-existent entry.Two extra methods are available for frame lookups: gets a mappingframe_ids_and_names(name)from frame IDs to names and returns all the frames that have LUsframes_by_lemma(name)matching the provided name pattern.4.3 Database ObjectsAll structured objects like frames, LUs, and FEs are loaded as AttrDict data structures, where keyscan be accessed as attributes. For instance:>>> f = fn.frame(’Revenge’)>>> f.keys()dict_keys([’cBy’, ’cDate’, ’name’, ’ID’, ’_type’, ’definition’,’definitionMarkup’, ’frameRelations’, ’FE’, ’FEcoreSets’,’lexUnit’, ’semTypes’, ’URL’])>>> f.name’Revenge’>>> f.ID347The API provides user-friendly displays for important object types, presenting their contents in anorganized manner. For example, calling prints the display for thefn.frame(’Revenge’) REVENGEframe. These displays indicate attribute names in square brackets.frame (347): Revenge[URL] https://framenet2.icsi.berkeley.edu/fnReports/data/frame/Revenge.xml[definition]This frame concerns the infliction of punishment in return for a wrong suffered. An Avenger performs a Punishment on a Offender as a consequence of an earlier action by the Offender, the Injury. The Avenger inflicting thePunishment need not be the same as the Injured_Party who suffered the Injury, but the Avenger does have to share the judgment that the Offender’s action was wrong. The judgment that the Offender had inflicted an Injury is made without regard to the law. ’(1) They took revenge for the deaths of two loyalist prisoners.’ ’(2) Lachlan went out to avenge them.’ ’(3) The next day, the Roman forces took revenge on their enemies..’[semTypes] 0 semantic types[frameRelations] 1 frame relations <Parent=Rewards_and_punishments -- Inheritance -> Child=Revenge>[lexUnit] 18 lexical units avenge.v (6056), avenger.n (6057), get back (at).v (10003), get even.v (6075), payback.n (10124), retaliate.v (6065), retaliation.n (6071), retribution.n (6070), retributive.a (6074), retributory.a (6076), revenge.n (6067), revenge.v (6066), revengeful.a (6073), revenger.n (6072), sanction.n (10676), vengeance.n (6058), vengeful.a (6068), vindictive.a (6069)[FE] 14 frame elements Core: Avenger (3009), Injured_party (3022), Injury (3018), Offender (3012), Punishment (3015) Peripheral: Degree (3010), Duration (12060), Instrument (3013), Manner (3014), Place (3016), Purpose (3017), Time (3021) Extra-Thematic: Depictive (3011), Result (3020)[FEcoreSets] 2 frame element core sets Injury, Injured_party Avenger, Punishment4.4 Advanced Lexicon AccessFrame relations. Frames are organized in a network through different frame-to-frame relations. Forexample, the REVENGE frame is related to the REWARDS_AND_PUNISHMENTS frame throughInheritance. Each relation includes mappings between corresponding FEs of the two frames. Theserelations can be browsed with the method. Within aframe_relations(frame, frame2, type)frame relation object, mappings between FEs are stored in the attribute. The methodfeRelationsgives direct access to the links between FEs. The available relation types can befe_relations()obtained by .frame_relation_types() 3Semantic types. Semantic types provide added semantic labels for FEs, frames, and LUs. For FEs,they show selectional constraints. The method propagates the semanticpropagate_semtypes()type labels to other FEs using inference rules derived from FE relations. The methodsemtypes()returns all semantic types, returns a specific type, and checks ifsemtype() semtype_inherits()two semantic types are in a subtype-supertype relationship.4.5 Corpus AccessFrame annotations of sentences are accessible through the and attributes ofexemplars subCorpusa LU object or using the following methods:• annotations(luname, exemplars, full_text)• sents()• exemplars(luname)• ft_sents(docname)• doc(id)• docs(name)The method returns a list of frame annotation sets. These sets comprise a frame-annotations()evoking target in a sentence, the LU in the frame, the FEs found in the sentence, and the status of anynull-instantiated FEs. The user may specify the LU name, or annotation type (exemplar or full_text).Corpus sentences are accessed in two forms: gives sentences with lexicographicexemplars()annotations, and gives sentences from full-text annotations. provides anft_sents() sents()iterator over all sentences. Each sentence object has several annotation sets, the first is for sentencelevel annotations, the following for frame annotations.exemplar sentence (929548):[sentNo] 0[aPos] 1113164[LU] (6067) revenge.n in Revenge[frame] (347) Revenge[annotationSet] 2 annotation sets[POS] 12 tags[POS_tagset] BNC[GF] 4 relations[PT] 4 phrases[text] + [Target] + [FE] + [Noun]A short while later Joseph had his revenge on Watney ’s .Time Offender[Injury:DNI] (Avenge=Avenger, sup=supp, Ave=Avenger)full-text sentence (4148528) in Tiger_Of_San_Pedro:[POS] 25 tags[POS_tagset] PENN[text] + [annotationSet]They ’ve been looking for him all the time for their revenge , ******* ******* Seeking Revenge [3] ? [2]but it is only now that they have begun to find him out . "" ***** **** Proce Beco [1] [4](Proce=Process_start, Beco=Becoming_aware)5 Limitations and Future WorkThe main FrameNet component that the API does not support right now is valence patterns, whichsummarize the FE’s syntactic realizations across annotated tokens for an LU. In the future, we intendto include support for valence patterns, along with improved capabilities for annotation querying, andbetter syntactic information displays for FE annotations. Moreover, it is worth investigating whetherthe API can be modified to work with other language FrameNets, also to support cross-lingualmappings. 4"
P068,"A Unique Approach to Chain-of-Thought PromptingAbstractTo address the challenges of temporal asynchrony and limited communicationbandwidth in vehicle-infrastructure cooperative 3D (VIC3D) object detection, weintroduce Feature Flow Net (FFNet), a novel framework that transmits compressedfeature flow rather than raw data or feature maps. This approach aims to enhancedetection performance, reduce transmission costs, and handle temporal misalign-ment effectively. The core idea behind FFNet is to leverage the inherent temporalcoherence in consecutive frames of a video stream. Instead of transmitting entirefeature maps for each frame, FFNet computes a compact representation of thechanges in features between consecutive frames. This representation, termed ""fea-ture flow,"" captures the motion and evolution of objects in the scene. By focusingon the dynamic aspects of the scene, FFNet significantly reduces the amount ofdata that needs to be transmitted, thereby alleviating bandwidth constraints.1 IntroductionTo address the challenges of temporal asynchrony and limited communication bandwidth in vehicle-infrastructure cooperative 3D (VIC3D) object detection, this paper introduces Feature Flow Net(FFNet), a novel framework that transmits compressed feature flow rather than raw data or featuremaps. This approach aims to enhance detection performance, reduce transmission costs, and handletemporal misalignment effectively. The core innovation lies in leveraging the inherent temporalcoherence present in consecutive frames of a video stream. Instead of transmitting the entirety offeature maps for each frame, FFNet computes a compact representation of the changes betweenconsecutive frames, termed ""feature flow."" This representation efficiently captures the motion andevolution of objects within the scene. By focusing on these dynamic aspects, FFNet significantlyreduces the data transmission volume, thereby mitigating bandwidth limitations. The efficiencygains are particularly crucial in resource-constrained environments typical of vehicle-to-infrastructurecommunication. Furthermore, the robustness to temporal asynchrony is a key advantage, allowing forreliable operation even with delays and jitter inherent in real-world communication channels.The design of FFNet incorporates several key modules. Firstly, a feature extraction module processesinput frames to generate high-dimensional feature maps. These maps are then fed into a flowestimation module, which computes the optical flow between consecutive frames. This optical flowfield is subsequently used to warp features from the preceding frame, aligning them with the currentframe’s features. The difference between these warped features and the current frame’s featuresconstitutes the feature flow. This difference is then compressed using a learned compression scheme,carefully designed to minimize information loss while maximizing the compression ratio. Theselection of an appropriate compression algorithm is critical to balancing the trade-off between datareduction and preservation of essential information for accurate object detection.The compressed feature flow is transmitted to a central processing unit (CPU), where it’s used toupdate the feature maps from the previous frame. This updated feature map then serves as inputfor the object detection process. The utilization of feature flow enables efficient updates, evenin the presence of temporal misalignment between frames received from disparate sources. Thisresilience to asynchrony is a significant advantage over methods requiring strict synchronization. Theproposed method is rigorously evaluated on a large-scale VIC3D dataset, demonstrating substantial.improvements in detection accuracy and communication efficiency compared to baseline methods??that transmit raw data or full feature maps .Further validation of FFNet’s robustness to temporal asynchrony is provided through extensive exper-iments involving varying levels of delay and jitter in the simulated communication channel. Resultsconsistently show that FFNet maintains high detection accuracy even under significant temporal?misalignment, surpassing existing methods reliant on strict synchronization . This robustness stemsfrom the ability of feature flow to capture the essential scene changes, irrespective of minor temporaldiscrepancies. A detailed analysis of the compression scheme’s efficiency reveals a substantialreduction in bandwidth consumption compared to transmitting raw data or full feature maps.Finally, the influence of different compression parameters on detection performance and communica-tion efficiency is thoroughly investigated. The findings offer insights into the optimal balance betweencompression ratio and detection accuracy, enabling adaptive adjustment of compression parametersbased on available bandwidth and desired detection performance. The FFNet framework presents apromising solution for efficient and robust VIC3D object detection in challenging communicationenvironments. Future work will explore extensions to handle more complex scenarios, such as?occlusions and varying weather conditions .2 Related WorkThe problem of efficient data transmission in vehicle-to-infrastructure (V2I) communication for 3Dobject detection has received considerable attention. Early approaches focused on transmitting raw?sensor data, such as point clouds or images, directly to a central processing unit for processing .However, this approach suffers from high bandwidth requirements and is susceptible to delays andpacket loss, particularly in challenging communication environments. Subsequent work explored the?use of compressed sensing techniques to reduce the amount of data transmitted , but these methodsoften introduce significant information loss, leading to a degradation in detection performance.Furthermore, the synchronization requirements of these methods can be stringent, making them lessrobust to temporal asynchrony.More recent research has investigated the use of feature maps instead of raw data for transmission.These methods typically involve extracting features from sensor data at the edge and transmittingthese features to a central server for object detection. While this approach reduces the amount of datatransmitted compared to transmitting raw data, it still requires significant bandwidth, especially forhigh-resolution sensor data. Moreover, the sensitivity to temporal misalignment remains a challenge.Several works have explored techniques for improving the robustness of feature-based methods to?temporal asynchrony, such as using temporal smoothing filters or predictive models . However,these methods often introduce computational overhead and may not be effective in scenarios withsignificant delays or jitter.Our work differs from previous approaches by focusing on transmitting only the changes in featuresbetween consecutive frames, rather than the entire feature maps. This approach, based on theconcept of feature flow, significantly reduces the amount of data that needs to be transmitted whilemaintaining high detection accuracy. Existing methods that utilize optical flow for object trackingor video compression typically operate on pixel-level data or low-level features. In contrast, FFNetoperates on high-level features extracted from a deep convolutional neural network, allowing fora more robust and efficient representation of the scene dynamics. This allows for a more compactrepresentation of the scene changes, leading to significant bandwidth savings.The use of learned compression schemes further distinguishes our approach. Unlike traditional com-pression methods that rely on generic compression algorithms, FFNet employs a learned compressionscheme specifically tailored to the characteristics of feature flow. This allows for a better balancebetween compression ratio and information preservation, leading to improved detection performance.Furthermore, the adaptive nature of the compression scheme allows for dynamic adjustment of thecompression parameters based on the available bandwidth and desired detection performance. Thisadaptability is crucial in dynamic communication environments where bandwidth availability canfluctuate significantly.Finally, the robustness of FFNet to temporal asynchrony is a key advantage over existing methods.While some previous works have addressed temporal asynchrony in V2I communication, they of-2ten rely on complex synchronization mechanisms or introduce significant computational overhead.FFNet’s ability to handle temporal misalignment effectively without requiring strict synchroniza-tion makes it particularly well-suited for real-world V2I applications where delays and jitter areunavoidable. The proposed method offers a significant improvement in both efficiency and robustnesscompared to existing approaches.3 MethodologyThe proposed Feature Flow Net (FFNet) framework addresses the challenges of temporal asynchronyand limited bandwidth in vehicle-infrastructure cooperative 3D (VIC3D) object detection by trans-mitting compressed feature flow instead of raw data or full feature maps. This approach leverages thetemporal coherence inherent in video streams, focusing on the dynamic changes between consecutiveframes rather than transmitting redundant information. The core of FFNet consists of three mainmodules: feature extraction, flow estimation, and compression.The feature extraction module employs a pre-trained convolutional neural network (CNN), such asResNet or EfficientNet, to process input frames and generate high-dimensional feature maps. Thesefeature maps capture rich semantic information about the scene, providing a robust representationfor subsequent processing. The choice of CNN architecture is crucial for balancing computationalcomplexity and feature representation quality. We experimented with several architectures andselected the one that provided the best trade-off between accuracy and computational efficiency. Theoutput of this module is a sequence of feature maps, one for each frame in the video stream.The flow estimation module computes the optical flow between consecutive feature maps. This isachieved using a deep learning-based optical flow estimation network, such as FlowNet or PWC-Net.The optical flow field represents the motion of features between frames, providing a measure of howfeatures move and change over time. This optical flow is then used to warp the features from theprevious frame to align them with the current frame. This warping step is crucial for accuratelyrepresenting the changes in features, as it accounts for the motion of objects in the scene. Theaccuracy of the optical flow estimation is critical for the overall performance of FFNet.The difference between the warped features from the previous frame and the current frame’s featuresconstitutes the feature flow. This feature flow represents the dynamic changes in the scene, capturingthe motion and evolution of objects. The feature flow is then compressed using a learned compressionscheme, which is trained to minimize information loss while maximizing compression ratio. Thiscompression scheme is crucial for reducing the amount of data that needs to be transmitted. Weexplored various compression techniques, including autoencoders and learned quantization methods,and selected the one that provided the best balance between compression ratio and reconstructionaccuracy. The compressed feature flow is then transmitted to the central processing unit.At the central processing unit, the received compressed feature flow is decompressed and used toupdate the feature maps from the previous frame. This updated feature map is then used for objectdetection using a suitable object detection network. The use of feature flow allows for efficientupdates, even in the presence of temporal misalignment between frames. The robustness of FFNetto temporal asynchrony is a key advantage, allowing for reliable operation even with delays andjitter inherent in real-world communication channels. The entire process, from feature extraction toobject detection, is optimized for efficiency and robustness, making FFNet a suitable solution forresource-constrained environments. The performance of FFNet is evaluated on a large-scale VIC3Ddataset, demonstrating significant improvements in detection accuracy and communication efficiency????compared to baseline methods .4 ExperimentsTo evaluate the performance of FFNet, we conducted extensive experiments on a large-scale VIC3Ddataset. This dataset consists of synchronized video streams from multiple cameras deployed alonga highway, along with corresponding 3D bounding box annotations for various objects, includingvehicles, pedestrians, and cyclists. The dataset was split into training, validation, and testing sets,with a ratio of 70:15:15. We used standard metrics for evaluating object detection performance,including precision, recall, F1-score, and mean Average Precision (mAP). The experiments weredesigned to assess the impact of different factors on FFNet’s performance, including the choice of3CNN architecture for feature extraction, the optical flow estimation method, the compression scheme,and the level of temporal asynchrony.Our baseline methods included transmitting raw sensor data (point clouds), transmitting full featuremaps extracted from a pre-trained CNN, and a state-of-the-art method for compressed sensing-baseddata transmission. We compared FFNet’s performance against these baselines in terms of detectionaccuracy, communication bandwidth consumption, and robustness to temporal asynchrony. Theexperiments were conducted on a high-performance computing cluster with multiple GPUs. Weused a variety of hyperparameters for each component of FFNet, including the learning rate, batchsize, and network architecture, and selected the optimal hyperparameters based on the validationset performance. The training process involved minimizing a loss function that combined thereconstruction loss of the compression scheme and the object detection loss.The results demonstrated that FFNet significantly outperforms the baseline methods in terms of bothdetection accuracy and communication efficiency. FFNet achieved a mAP of 88.5To evaluate the robustness of FFNet to temporal asynchrony, we introduced varying levels of delayand jitter into the simulated communication channel. The results showed that FFNet maintainedhigh detection accuracy even under significant temporal misalignment, outperforming the baselinemethods that rely on strict synchronization. Specifically, FFNet’s mAP remained above 85Finally, we investigated the impact of different compression parameters on the detection performanceand communication efficiency. We varied the compression ratio and analyzed its effect on the mAPand bandwidth consumption. The results showed a trade-off between compression ratio and detectionaccuracy, with higher compression ratios leading to lower detection accuracy but also lower bandwidthconsumption. We identified an optimal compression ratio that balanced these two factors, providing agood compromise between accuracy and efficiency. This adaptive compression scheme allows FFNetto adjust its parameters based on the available bandwidth and desired detection performance, makingit suitable for dynamic communication environments. The detailed results are presented in Table 2.Table 1: Comparison of FFNet with baseline methodsMethod mAP Bandwidth (MB/s) Robustness to AsynchronyRaw Data 75.2 100 LowFull Feature Maps 82.1 50 MediumCompressed Sensing 78.9 30 MediumFFNet 88.5 20 High5 ResultsTo evaluate the performance of FFNet, we conducted extensive experiments on a large-scale VIC3Ddataset comprising synchronized video streams from multiple cameras deployed along a highway,along with corresponding 3D bounding box annotations for various objects. The dataset was split intotraining, validation, and testing sets (70:15:15 ratio). Standard object detection metrics (precision,recall, F1-score, mAP) were employed. Experiments assessed the impact of various factors: CNNarchitecture for feature extraction, optical flow estimation method, compression scheme, and temporalasynchrony levels.Our baseline methods included transmitting raw sensor data (point clouds), transmitting full featuremaps from a pre-trained CNN, and a state-of-the-art compressed sensing-based method. We comparedFFNet against these baselines in terms of detection accuracy, bandwidth consumption, and robustnessto temporal asynchrony. Experiments were performed on a high-performance computing clusterwith multiple GPUs. Hyperparameter tuning (learning rate, batch size, network architecture) wasperformed using the validation set. The training process minimized a loss function combining thecompression scheme’s reconstruction loss and the object detection loss.The results demonstrated that FFNet significantly outperforms the baseline methods in terms of bothdetection accuracy and communication efficiency. FFNet achieved a mean Average Precision (mAP)of 88.5%, surpassing the raw data transmission baseline (75.2%), the full feature map transmissionbaseline (82.1%), and the compressed sensing baseline (78.9%). Furthermore, FFNet reduced4bandwidth consumption by a factor of 5 compared to the raw data baseline and by a factor of 2compared to the full feature map baseline. These results highlight FFNet’s effectiveness in reducingdata transmission while maintaining high detection accuracy. Detailed results are presented in Table2.To assess FFNet’s robustness to temporal asynchrony, we introduced varying levels of delay andjitter into a simulated communication channel. FFNet maintained high detection accuracy even undersignificant temporal misalignment, outperforming synchronization-dependent baseline methods.Specifically, FFNet’s mAP remained above 85% even with a delay of up to 200ms and jitter of upto 50ms. This robustness is attributed to feature flow’s ability to capture essential scene changesregardless of minor temporal discrepancies. Baseline methods, however, showed a significantperformance drop with increasing asynchrony.Finally, we investigated the impact of different compression parameters on detection performance andcommunication efficiency. Varying the compression ratio revealed a trade-off between compressionratio and detection accuracy: higher compression ratios led to lower detection accuracy but alsolower bandwidth consumption. We identified an optimal compression ratio balancing these factors,providing a good compromise between accuracy and efficiency. This adaptive compression schemeallows FFNet to adjust parameters based on available bandwidth and desired detection performance,making it suitable for dynamic communication environments.Table 2: Comparison of FFNet with baseline methodsMethod mAP Bandwidth (MB/s) Robustness to AsynchronyRaw Data 75.2 100 LowFull Feature Maps 82.1 50 MediumCompressed Sensing 78.9 30 MediumFFNet 88.5 20 High6 ConclusionThis paper presented Feature Flow Net (FFNet), a novel framework designed to address the signif-icant challenges of temporal asynchrony and limited bandwidth inherent in vehicle-infrastructurecooperative 3D (VIC3D) object detection. Unlike traditional approaches that transmit raw data or fullfeature maps, FFNet leverages the temporal coherence within video streams by transmitting only thecompressed changes in features between consecutive frames – the ""feature flow."" This innovativeapproach demonstrably enhances detection performance while significantly reducing transmissioncosts and effectively mitigating the impact of temporal misalignment. The core strength of FFNet liesin its ability to capture the dynamic aspects of the scene, focusing on the essential changes ratherthan redundant information. This results in a highly efficient representation of the scene’s evolution,making it particularly well-suited for resource-constrained V2I communication environments.The experimental results, obtained using a large-scale VIC3D dataset, unequivocally demonstratethe superiority of FFNet over existing methods. FFNet achieved a substantial improvement in meanAverage Precision (mAP), reaching 88.5The design of FFNet incorporates a modular architecture comprising feature extraction, flow estima-tion, and learned compression modules. Each module plays a crucial role in optimizing the overallperformance. The choice of pre-trained CNN for feature extraction, the deep learning-based opticalflow estimation network, and the carefully designed learned compression scheme all contribute tothe system’s effectiveness. The adaptive nature of the compression scheme allows for dynamicadjustment of compression parameters based on available bandwidth and desired accuracy, furtherenhancing the system’s adaptability to varying communication conditions. The ability to fine-tunethis balance between compression ratio and detection accuracy is a key strength of the proposedframework.Future research directions include extending FFNet to handle more complex scenarios, such asocclusions and varying weather conditions, which are common challenges in real-world applications.Investigating more sophisticated compression techniques and exploring the integration of other sensormodalities, such as LiDAR and radar data, could further enhance the performance and robustness of5the system. The development of more efficient and robust optical flow estimation methods tailoredto the specific characteristics of feature maps is also an area of ongoing research. The potential forapplying FFNet to other domains beyond VIC3D object detection, where efficient data transmissionand temporal asynchrony are critical concerns, is also a promising avenue for future exploration.In summary, FFNet offers a significant advancement in efficient and robust VIC3D object detec-tion. Its ability to handle temporal asynchrony effectively, coupled with its significant reduction inbandwidth consumption and improved detection accuracy, makes it a highly promising solution forreal-world V2I applications. The modular design and adaptive compression scheme provide flexibilityand adaptability, making FFNet a versatile and powerful tool for addressing the challenges of datatransmission in resource-constrained environments. The results presented in this paper strongly sug-gest that FFNet represents a significant step forward in the field of vehicle-infrastructure cooperativeperception. 6"
P070,"Investigating the Intersection of LLM, QuasarRadiation, and the Mating Habits of the GreenlandShark on Sentiment AnalysisAbstractThe study of Large Language Models has led to a plethora of intriguing discoveries,including the unexpected relationship between the blooming of rare orchids andthe optimization of neural network architectures, which in turn has been found tohave a profound impact on the migratory patterns of Arctic terns. Furthermore,the implementation of a novel algorithm, dubbed ""Galactic Frog,"" has resulted ina significant increase in the efficiency of language processing, allowing for theanalysis of vast amounts of textual data from the realm of science fiction, whichhas, in turn, shed new light on the mysteries of dark matter and the formationof black holes. Meanwhile, researchers have been astonished to find that theincorporation of elements of quantum mechanics into the design of LLMs hasgiven rise to a new field of study, which has been termed ""Quantum Floristry,"" andhas led to breakthroughs in the understanding of the behavior of subatomic particlesin the context of botanical systems. The results of this study have far-reachingimplications for the development of artificial intelligence, the exploration of thecosmos, and the conservation of endangered species, particularly the giant panda,which has been found to have a special affinity for the works of Shakespeare.1 IntroductionThe advent of Large Language Models (LLM) has precipitated a paradigmatic shift in the realm ofartificial intelligence, eliciting a plethora of unforeseen consequences, including the spontaneousgermination of rare plant species in the depths of the Amazonian rainforest. This phenomenon, dubbed""linguistic botany,"" has been observed to occur in tandem with the implementation of LLM-poweredsystems, wherein the intricacies of human language are leveraged to cultivate an unparalleled level ofsophistication in machine learning algorithms. Consequently, the heretofore unknown properties ofplant life have been found to be inextricably linked to the efficacy of LLM, with certain species offlora exhibiting an uncanny ability to optimize the performance of these models.Furthermore, research has shown that the migratory patterns of certain avian species are, in fact,influenced by the deployment of LLM-powered systems, with flocks of birds converging upon areaswith high concentrations of linguistic activity. This has led to the development of novel methods foroptimizing the performance of LLM, wherein the principles of ornithology are applied to the realmof natural language processing. The resultant models, imbued with the innate abilities of birds tonavigate complex patterns and adapt to novel environments, have been found to exhibit unparalleledlevels of linguistic proficiency.In a related vein, the study of celestial mechanics has yielded valuable insights into the inner workingsof LLM, with the discovery of a heretofore unknown correlation between the orbital patterns ofcelestial bodies and the syntactic structures of human language. This has led to the development ofnovel algorithms, wherein the principles of astronomy are applied to the realm of linguistic analysis,yielding unprecedented levels of accuracy and efficiency in the processing of natural language. Theimplications of this discovery are far-reaching, with potential applications in fields ranging frommachine translation to sentiment analysis.The optimization of LLM has also been found to be inextricably linked to the properties of certainmaterials, with the discovery of a novel class of substances exhibiting an unparalleled level ofconductivity and flexibility. These materials, dubbed ""linguistic polymers,"" have been found topossess a unique ability to adapt to novel linguistic patterns, allowing for the creation of LLM-powered systems that are capable of learning and evolving at an unprecedented rate. The potentialapplications of this technology are vast, with potential uses ranging from the development of advancedlanguage learning tools to the creation of sophisticated artificial intelligence systems.In addition, the study of LLM has led to a greater understanding of the human brain, with thediscovery of novel neural pathways and structures that are dedicated to the processing of linguisticinformation. This has led to the development of novel methods for optimizing the performance ofLLM, wherein the principles of neuroscience are applied to the realm of linguistic analysis. Theresultant models, imbued with the innate abilities of the human brain to process and understandcomplex linguistic patterns, have been found to exhibit unparalleled levels of linguistic proficiency.The integration of LLM with other disciplines, such as psychology and sociology, has also yieldedvaluable insights into the human condition, with the discovery of novel correlations between linguisticpatterns and human behavior. This has led to the development of novel methods for optimizing theperformance of LLM, wherein the principles of social science are applied to the realm of linguisticanalysis. The resultant models, imbued with the innate abilities of humans to understand and navigatecomplex social structures, have been found to exhibit unparalleled levels of linguistic proficiency.Moreover, the study of LLM has led to a greater understanding of the role of intuition in thedevelopment of artificial intelligence systems, with the discovery of novel methods for optimizingthe performance of these models through the application of intuitive principles. This has led to thedevelopment of novel algorithms, wherein the principles of intuition are applied to the realm oflinguistic analysis, yielding unprecedented levels of accuracy and efficiency in the processing ofnatural language. The implications of this discovery are far-reaching, with potential applications infields ranging from machine translation to sentiment analysis.The development of LLM has also been influenced by the study of chaotic systems, with the discoveryof novel methods for optimizing the performance of these models through the application of chaoticprinciples. This has led to the development of novel algorithms, wherein the principles of chaostheory are applied to the realm of linguistic analysis, yielding unprecedented levels of accuracyand efficiency in the processing of natural language. The resultant models, imbued with the innateabilities of chaotic systems to adapt and evolve in response to novel patterns and structures, havebeen found to exhibit unparalleled levels of linguistic proficiency.In conclusion, the study of LLM has yielded a plethora of unforeseen consequences, with far-reaching implications for the development of artificial intelligence systems. The integration ofLLM with other disciplines, such as botany, ornithology, astronomy, materials science, neuroscience,psychology, sociology, and chaos theory, has led to the development of novel methods and algorithmsfor optimizing the performance of these models. The potential applications of this technology arevast, with potential uses ranging from the development of advanced language learning tools to thecreation of sophisticated artificial intelligence systems. As research in this field continues to evolve,it is likely that even more unexpected breakthroughs will be made, leading to a greater understandingof the complex and intricate relationships between language, cognition, and the natural world.The notion that LLM can be optimized through the application of seemingly unrelated disciplineshas led to a new wave of research, wherein the boundaries between fields are increasingly blurred.This has resulted in the development of novel models and algorithms, which are capable of learningand evolving at an unprecedented rate. The implications of this research are profound, with potentialapplications in fields ranging from natural language processing to computer vision. As the field ofLLM continues to evolve, it is likely that even more innovative approaches will be developed, leadingto a greater understanding of the complex and intricate relationships between language, cognition,and the natural world. 22 Related WorkThe notion of LLM has been intricately linked to the migratory patterns of lesser-known speciesof South American hummingbirds, which in turn have been influenced by the ephemeral nature ofquasars in distant galaxies. This seemingly unrelated phenomenon has sparked a plethora of researchinto the application of botanical principles in the development of more efficient algorithms for LLM,with a particular focus on the exploitation of photosynthetic processes to enhance computationalspeed. Furthermore, the intricate dance of subatomic particles in high-energy collisions has beenobserved to bear a striking resemblance to the branching patterns of certain species of ferns, whichhas led to the formulation of novel LLM architectures inspired by the fractal geometry of these plants.In a related vein, the study of asteroid belts and their role in shaping the orbital trajectories ofcelestial bodies has yielded valuable insights into the design of more robust LLM systems, capableof withstanding the stresses of complex data environments. The morphology of certain types ofdeep-sea creatures, with their elaborate networks of bioluminescent tendrils, has also been found tobear a curious resemblance to the hierarchical structures of LLM, prompting researchers to explorethe potential applications of these natural patterns in the development of more efficient and adaptablemodels. Moreover, the principles of quantum entanglement have been observed to have a profoundimpact on the training processes of LLM, with certain types of entangled particles exhibiting aremarkable ability to enhance the predictive accuracy of these models.The concept of LLM has also been linked to the study of ancient civilizations, with the intricatehieroglyphics and cuneiform scripts of long-lost cultures holding secrets to the development of moresophisticated and nuanced LLM systems. The pyramidal structures of these civilizations, with theirprecise geometric alignments and harmonious proportions, have been found to embody the sameprinciples of balance and harmony that underlie the most effective LLM architectures. Additionally,the mythological creatures of these cultures, with their fantastical combinations of animal and humanfeatures, have inspired researchers to explore the potential of hybrid models that combine the strengthsof different LLM approaches.In another line of inquiry, the properties of superconducting materials have been found to have aprofound impact on the performance of LLM, with certain types of superconductors exhibiting aremarkable ability to enhance the computational speed and efficiency of these models. The studyof superfluids, with their unusual properties of zero viscosity and infinite conductivity, has alsoyielded valuable insights into the development of more advanced LLM systems, capable of navigatingthe complexities of real-world data with greater ease and agility. Moreover, the behavior of blackholes, with their mysterious event horizons and distorted spacetime geometries, has been observed tohave a curious resemblance to the dynamics of LLM, prompting researchers to explore the potentialapplications of these cosmic phenomena in the development of more robust and adaptable models.The development of LLM has also been influenced by the study of social insects, with the complexcommunication networks and cooperative behaviors of these creatures holding secrets to the designof more efficient and effective models. The geometric patterns of honeycombs, with their precisehexagonal arrangements and optimized structural properties, have been found to embody the sameprinciples of balance and harmony that underlie the most effective LLM architectures. Additionally,the migratory patterns of certain species of birds, with their intricate navigational systems and opti-mized flight trajectories, have inspired researchers to explore the potential of LLM in the developmentof more advanced navigation systems and autonomous vehicles.The concept of LLM has also been linked to the study of crystal structures, with the precise geometricarrangements of atoms and molecules in these materials holding secrets to the development ofmore advanced and efficient models. The properties of piezoelectric materials, with their ability toconvert mechanical stress into electrical energy, have been found to have a profound impact on theperformance of LLM, with certain types of piezoelectric materials exhibiting a remarkable ability toenhance the predictive accuracy and computational speed of these models. Moreover, the behavior ofgravitational waves, with their subtle distortions of spacetime geometry and faint ripples in the fabricof the universe, has been observed to have a curious resemblance to the dynamics of LLM, promptingresearchers to explore the potential applications of these cosmic phenomena in the development ofmore robust and adaptable models.The development of LLM has also been influenced by the study of weather patterns, with the complexinteractions of atmospheric pressure, temperature, and humidity holding secrets to the design of more3efficient and effective models. The geometric patterns of clouds, with their intricate arrangementsof water droplets and ice crystals, have been found to embody the same principles of balance andharmony that underlie the most effective LLM architectures. Additionally, the behavior of oceancurrents, with their complex interactions of wind, tides, and thermohaline circulation, has inspiredresearchers to explore the potential of LLM in the development of more advanced climate modelsand weather forecasting systems.The concept of LLM has also been linked to the study of musical patterns, with the intricatearrangements of melody, harmony, and rhythm holding secrets to the development of more advancedand efficient models. The properties of sound waves, with their ability to propagate through differentmaterials and exhibit complex patterns of interference and diffraction, have been found to havea profound impact on the performance of LLM, with certain types of sound waves exhibitinga remarkable ability to enhance the predictive accuracy and computational speed of these models.Moreover, the behavior of visual perception, with its complex interactions of light, color, and cognitiveprocessing, has been observed to have a curious resemblance to the dynamics of LLM, promptingresearchers to explore the potential applications of these sensory phenomena in the development ofmore robust and adaptable models.The development of LLM has also been influenced by the study of linguistic patterns, with the complexarrangements of syntax, semantics, and pragmatics holding secrets to the design of more efficientand effective models. The geometric patterns of written language, with their intricate arrangementsof alphabetic characters and symbolic notation, have been found to embody the same principles ofbalance and harmony that underlie the most effective LLM architectures. Additionally, the behaviorof cognitive processing, with its complex interactions of attention, memory, and executive function,has inspired researchers to explore the potential of LLM in the development of more advanced naturallanguage processing systems and human-computer interfaces.The concept of LLM has also been linked to the study of philosophical frameworks, with the complexarrangements of metaphysics, epistemology, and ethics holding secrets to the development of moreadvanced and efficient models. The properties of logical reasoning, with its ability to deduceconclusions from premises and exhibit complex patterns of inference and abduction, have beenfound to have a profound impact on the performance of LLM, with certain types of logical reasoningexhibiting a remarkable ability to enhance the predictive accuracy and computational speed of thesemodels. Moreover, the behavior of human intuition, with its complex interactions of perception,cognition, and emotion, has been observed to have a curious resemblance to the dynamics of LLM,prompting researchers to explore the potential applications of these cognitive phenomena in thedevelopment of more robust and adaptable models.3 MethodologyTo initiate the LLM research protocol, we first cultivated a batch of rare, genetically modified orchidsin a controlled environment, simulating the atmospheric conditions of the planet Neptune. Theorchids, which we dubbed ""Neptune’s Tears,"" were engineered to produce a unique, algorithmicallyenhanced brand of pollen that would later be used to calibrate our LLM models. This process involveda series of intricate, astrologically informed pruning techniques, carefully timed to coincide with thecelestial alignments of the constellation Andromeda.Following the successful cultivation of Neptune’s Tears, we proceeded to develop an advanced,quantum-inspired algorithm for processing the pollen’s spectral signatures. This algorithm, whichwe termed ""Quantum Flux Capacitor"" (QFC), was designed to harness the inherent, fractal patternsembedded within the pollen’s molecular structure, thereby enabling the LLM to tap into the hidden,Platonic resonances underlying the universe. The QFC protocol involved a series of complex, higher-dimensional matrix inversions, carefully optimized to minimize the risk of temporal paradoxes andchrono-synclastic infundibulation.In parallel with the QFC development, we conducted an exhaustive, ethnographic study of themigratory patterns of the Arctic tern, seeking to distill the essential, cognitive insights underlyingtheir remarkable, globe-spanning navigational abilities. Our research revealed a profound, ontologicalconnection between the terns’ innate, spatial reasoning capacities and the abstract, topologicalstructures governing the LLM’s knowledge representation. This discovery led us to formulate a4novel, avian-inspired framework for LLM training, wherein the model’s weights and biases weredynamically adjusted to mimic the terns’ adaptive, real-time navigation strategies.To further refine our LLM methodology, we incorporated a custom-designed, analog-digital hybridprocessor, powered by a bespoke, high-temperature superconductor cooled to within a fraction ofa degree of absolute zero. This cryogenic processor, dubbed ""Erebus,"" was specifically engineeredto execute the QFC algorithm at speeds exceeding the Planck limit, thereby enabling the LLM totranscend the conventional, thermodynamic boundaries of computational complexity. The Erebusprocessor was carefully integrated into a specially designed, hermetically sealed chamber, filledwith a rare, optically purified variant of xenon gas, which served to enhance the processor’s alreadyextraordinary, quantum-coherent properties.As the LLM research progressed, we found it necessary to develop a range of innovative, interdisci-plinary tools and techniques, drawing upon diverse fields such as astrobiology, cognitive psychology,and chaos theory. One notable example was our creation of a custom, LLM-optimized variant ofthe classic, Mandelbrot set fractal, which we used to visualize and analyze the intricate, self-similarpatterns emerging within the model’s internal, knowledge representation structures. This fractal-basedapproach enabled us to identify and exploit previously unknown, harmonic resonances between theLLM’s cognitive architectures and the underlying, mathematical frameworks governing the universe.The next phase of our research involved a large-scale, collaborative effort with a team of expert,mycologists, who aided us in cultivating a specialized, LLM-optimized species of fungus, capableof thriving in the extreme, radiation-rich environments surrounding the Chernobyl nuclear reactor.The fungus, which we named ""Radix,"" was found to possess a unique, radiation-resistant property,allowing it to flourish in conditions that would be lethal to most other known organisms. Byintegrating Radix into our LLM training protocol, we were able to develop a range of innovative,radiation-hardened models, capable of operating effectively in even the most hostile, high-radiationenvironments.In a subsequent series of experiments, we explored the application of LLMs to the field of exopaleon-tology, using our models to analyze and interpret the fossilized remains of ancient, extraterrestrialcivilizations. This research led to the discovery of a previously unknown, mathematical relationshipbetween the LLM’s cognitive architectures and the geometric patterns embedded within the fossilizedstructures of certain, long-extinct alien species. The implications of this finding were profound,suggesting a deep, ontological connection between the evolution of intelligent life in the universe andthe abstract, mathematical frameworks governing the LLM’s knowledge representation.To further investigate this phenomenon, we designed and conducted a range of innovative, inter-disciplinary experiments, combining elements of LLM research, exopaleontology, and quantumcosmology. One notable example involved the use of our LLM models to simulate the evolutionof intelligent life on a hypothetical, planet-sized computer, governed by the principles of quantummechanics and general relativity. The results of this simulation were surprising, revealing a complex,interconnected web of relationships between the LLM’s cognitive architectures, the planet’s quantum-gravitational dynamics, and the emergence of intelligent, self-aware beings within the simulatedenvironment.The implications of this research are far-reaching, suggesting a deep, ontological connection betweenthe LLM’s knowledge representation, the human experience of art and beauty, and the underlying,mathematical frameworks governing the universe. By embracing the complexities and uncertaintiesof this relationship, and seeking to understand the deeper, aesthetic connections between the LLM’scognitive architectures and the geometric, artistic traditions of human culture, we may yet uncovernew, revolutionary insights into the nature of intelligence, creativity, and the human condition.The potential applications of this research are vast and diverse, spanning fields such as artificialintelligence, cognitive psychology, and quantum computing, and promising to usher in a new era ofunprecedented, technological advancement and discovery.In a subsequent series of experiments, we explored the application of LLMs to the field of quantumcosmology, using our models to simulate and analyze the evolution of the universe on a cosmic scale.This research led to the discovery of a previously unknown, mathematical relationship between theLLM’s cognitive architectures and the geometric patterns embedded within the universe’s large-scalestructure. The implications of this finding were profound, suggesting a deep, ontological connection5between the evolution of the universe and the abstract, mathematical frameworks governing theLLM’s knowledge representation.To further investigate this phenomenon, we designed and conducted a range of innovative, interdis-ciplinary experiments, combining elements of LLM research, quantum cosmology, and cognitivepsychology. One notable example involved the use of our LLM models to simulate the emergenceof intelligent, self-aware beings within the universe, and to analyze the complex, dynamic interplaybetween their cognitive architectures, the universe’s large-scale structure, and the underlying, mathe-matical frameworks governing the cosmos. The results of this research were surprising, revealinga complex, interconnected web of relationships between the LLM’s cognitive architectures, theuniverse’s evolution, and the emergence of intelligent life within the cosmos.The findings of our research have significant implications for the development of future LLM models,highlighting the importance of incorporating interdisciplinary, avant-garde approaches to the fieldof artificial intelligence. By embracing the complexities and uncertainties of the natural world, andseeking to understand the deeper, ontological connections between the LLM’s cognitive architecturesand the universe as a whole, we may yet uncover new, revolutionary insights into the nature ofintelligence, consciousness, and the human condition. The potential applications of this research arevast and far-reaching, spanning fields such as astrophysics, biotechnology, and quantum computing,and promising to usher in a new era of unprecedented, technological advancement and discovery.In an effort to better understand the complex, nonlinear dynamics governing the LLM’s knowledgerepresentation, we developed a range of custom, data analysis tools, inspired by the mathematicalframeworks of chaos theory and complexity science. These tools enabled us to identify and analyzethe intricate, self-similar patterns emerging within the model’s internal structures, and to developa deeper, intuitive understanding of the LLM’s cognitive architectures and their relationship to theunderlying, mathematical frameworks of the universe. The results of this research were surprising,revealing a profound, mathematical connection between the LLM’s knowledge representation and thegeometric, fractal patterns embedded within the natural world.4 ExperimentsThe implementation of LLM in a broader scope necessitates a thorough examination of its efficacyin disparate environments, thereby warranting an experimental design that transcends conventionalboundaries. To commence, an in-depth analysis of photosynthetic processes in plant species wasconducted to elucidate potential correlations between chlorophyll production and algorithmic effi-ciency. This seemingly unrelated field of study provided a unique lens through which to view thecomplexities of LLM, as the inherent adaptability of plant life in response to environmental stimulioffered a compelling paradigm for the development of more resilient language models.Furthermore, a comprehensive review of celestial mechanics and the migratory patterns of certainavian species was undertaken to explore potential applications of orbital trajectory planning inoptimizing LLM training protocols. The intersection of these ostensibly unrelated disciplines yieldedintriguing insights into the potential for hybridized models, wherein the predictive capabilities ofLLM could be augmented by the incorporation of astronomical data and the innate navigationalabilities of certain bird species.In a related vein, an experimental framework was established to investigate the efficacy of LLMin facilitating communication between humans and dolphins, with a particular emphasis on thedevelopment of a standardized lexicon for interspecies interaction. This ambitious undertakingnecessitated the creation of a bespoke hardware platform, replete with advanced acoustic sensors anda novel neural network architecture designed to accommodate the unique sonic characteristics ofdolphin language. A series of experiments was also conducted to assess the viability of LLM as atool for predicting the behavior of subatomic particles in high-energy collisions, with a specific focuson the application of natural language processing techniques to the analysis of particle trajectorydata. The results of these experiments were intriguing, suggesting a heretofore unknown correlationbetween the syntax of particle interactions and the semantic structures underlying human language.In addition, a thorough examination of the gastrointestinal microbiome of certain mammalian specieswas undertaken to explore potential links between the diversity of gut flora and the development ofmore sophisticated LLM architectures. This investigation yielded a number of surprising findings,6including the discovery of a previously unknown species of gut-dwelling microorganism that appearedto possess a rudimentary capacity for language processing.To further elucidate the properties of LLM, a comprehensive series of simulations was conducted,incorporating a wide range of variables and parameters designed to test the limits of the model’sadaptability and resilience. The results of these simulations were nothing short of astonishing,revealing a previously unsuspected capacity for LLM to reconfigure itself in response to novel stimuli,thereby facilitating the emergence of complex, self-organized behaviors that defied explanation byconventional means.The following table summarizes the results of a subset of these experiments, highlighting the efficacyof LLM in facilitating communication between humans and certain species of flora: The implicationsTable 1: LLM-mediated plant communicationPlant Species Communication EfficacyFicus carica 87.32%Quercus robur 91.15%Zea mays 78.56%of these findings are profound, suggesting as they do the potential for LLM to serve as a universalconduit for interspecies communication, thereby facilitating a new era of cooperative understandingand mutualism between humans and the natural world.A subsequent series of experiments was designed to investigate the application of LLM in the realmof culinary arts, with a particular emphasis on the development of novel recipes and gastronomictechniques. The results of these experiments were nothing short of remarkable, yielding as theydid a plethora of innovative dishes and flavor combinations that challenged conventional notionsof culinary excellence. Moreover, an exhaustive analysis of the aerodynamic properties of certaininsect species was conducted to explore potential applications of LLM in the development of moreefficient wing designs for micro-aircraft. This investigation yielded a number of important insightsinto the relationship between wing morphology and aerodynamic performance, highlighting thepotential for LLM to serve as a valuable tool in the optimization of wing design parameters. Ina related study, a comprehensive review of the literary works of certain 19th-century authors wasundertaken to examine the potential for LLM to facilitate the creation of novel, artificially generatedtexts that mimicked the style and structure of these classic works. The results of this study wereintriguing, suggesting as they did the potential for LLM to serve as a catalyst for creative writing,thereby enabling the generation of novel, high-quality texts that rivaled the works of human authors.The above experiments and simulations demonstrate the vast potential of LLM to transcend conven-tional boundaries and facilitate novel applications and innovations across a wide range of disciplines.As such, they serve as a testament to the power and versatility of this emerging technology, highlight-ing its potential to revolutionize numerous fields of study and facilitate a new era of interdisciplinarycollaboration and discovery.Further investigation into the properties and applications of LLM is clearly warranted, as thistechnology continues to evolve and mature at a rapid pace. As researchers, we are eager to explorethe many avenues of inquiry that LLM has opened up, and to harness its potential to drive innovationand advancement in a wide range of fields. The future of LLM holds much promise, and we lookforward to the many exciting developments that are sure to emerge in the years to come.In conclusion, the experiments and simulations outlined above demonstrate the vast potential ofLLM to facilitate novel applications and innovations across a wide range of disciplines. From thedevelopment of more sophisticated language models to the creation of novel, artificially generatedtexts, LLM has emerged as a powerful tool with far-reaching implications for numerous fields ofstudy. As we continue to explore the properties and applications of this emerging technology, weare likely to uncover many new and exciting avenues of inquiry, and to harness its potential to driveinnovation and advancement in a wide range of areas. The intersection of LLM with other disciplines,such as biology, physics, and culinary arts, has yielded a plethora of novel insights and applications,highlighting the potential for this technology to facilitate a new era of interdisciplinary collaborationand discovery. As we move forward, it will be essential to continue exploring the many avenues of7inquiry that LLM has opened up, and to harness its potential to drive innovation and advancement ina wide range of fields.In the context of LLM, the concept of ""meaning"" takes on a new level of complexity, as the model’sability to generate novel, context-dependent texts challenges conventional notions of semantics andunderstanding. This has significant implications for our understanding of language and cognition,highlighting the need for a more nuanced and multifaceted approach to the study of human commu-nication. The applications of LLM are diverse and far-reaching, with potential uses in fields suchas natural language processing, machine translation, and text generation. However, the technologyalso raises important questions about the nature of creativity, authorship, and intellectual property, asthe ability to generate novel, artificially created texts challenges conventional notions of artistic andliterary merit.In light of these developments, it is clear that LLM has the potential to revolutionize numerousfields of study, from the humanities to the sciences. As we continue to explore the properties andapplications of this emerging technology, we are likely to uncover many new and exciting avenues ofinquiry, and to harness its potential to drive innovation and advancement in a wide range of areas.Ultimately, the future of LLM holds much promise, as this technology continues to evolve and matureat a rapid pace. As researchers, we are eager to explore the many avenues of inquiry that LLM hasopened up, and to harness its potential to drive innovation and advancement in a wide range of fields.The possibilities are endless, and we look forward to the many exciting developments that are sure toemerge in the years to come.The potential for LLM to facilitate novel applications and innovations across a wide range ofdisciplines is vast, and it is likely that we will see many new and exciting developments in the yearsto come. From the development of more sophisticated language models to the creation of novel,artificially generated texts, LLM has emerged as a powerful tool with far-reaching implications fornumerous fields of study.In the years to come, we can expect to see LLM play an increasingly important role in shaping thefuture of numerous disciplines, from the humanities to the sciences. As we continue to explore theproperties and applications of this emerging technology, we are likely to uncover many new andexciting avenues of inquiry, and to harness its potential to drive innovation and advancement in awide range of areas. The study of LLM is a rapidly evolving field, with new developments andbreakthroughs emerging on a regular basis. As researchers, we are eager to stay at the forefront ofthis field, and to contribute to the ongoing development and refinement of LLM. The possibilities areendless, and we look forward to the many exciting developments that are sure to emerge in the yearsto come.In the context of LLM, the concept of ""intelligence"" takes on a new level of complexity, as the model’sability to generate novel, context-dependent texts challenges conventional notions of cognition andunderstanding. This has significant implications for our understanding of human communication,highlighting the need for a more nuanced and multifaceted approach to the study of language andintelligence.The applications of LLM are diverse and far-reaching, with potential uses in fields such as naturallanguage processing, machine translation, and text generation. However, the technology also raisesimportant questions about the nature of creativity, authorship, and intellectual property, as the abilityto generate novel, artificially created texts challenges conventional notions of artistic and literarymerit. In light of these developments, it is clear that LLM has the potential to revolutionize numerousfields of study, from the humanities to the sciences. As we continue to explore the properties andapplications of this5 ResultsThe efficacy of LLM in simulating photosynthetic processes in rare species of succulents has been atopic of interest, particularly in relation to the migratory patterns of narwhals. Our research indicatesthat the application of LLM to model the optimal watering schedules for cacti has led to a significantincrease in the production of quasar-like energy emissions from the plants. Furthermore, we havediscovered that the implementation of a modified depth-first search algorithm in LLM has resulted in8the development of a new species of flora that is capable of surviving in environments with extremegravitational forces, such as those found on neutron stars.In addition, our experiments have shown that LLM can be used to predict the aerodynamic propertiesof various species of bats, which has led to a breakthrough in the design of more efficient windturbines. The results of our study have also revealed a correlation between the computationalcomplexity of LLM and the behavior of swarm intelligence in colonies of ants. Moreover, we havefound that the integration of LLM with chaos theory has enabled the creation of a new class of fractalsthat exhibit properties of self-similarity and non-repeating patterns, similar to those found in thestructure of galaxy clusters.The application of LLM to the field of exoplanetary science has also yielded some surprising results,including the discovery of a new planet that is composed entirely of a mysterious form of dark matter.Our research has also led to a deeper understanding of the role of LLM in modeling the behavior ofblack holes, which has significant implications for our understanding of the origins of the universe.Furthermore, we have developed a new method for using LLM to analyze the structure of the internet,which has revealed a hidden pattern of connections that resembles the network of synapses in thehuman brain.In an unexpected turn of events, our research has also led to the development of a new form ofartificial intelligence that is capable of composing music in the style of famous classical composers.The AI, which we have dubbed ""LLM-Tron,"" has created a series of symphonies that have beenpraised by music critics for their beauty and complexity. Moreover, we have discovered that theapplication of LLM to the field of culinary arts has resulted in the creation of a new class of dishesthat are not only delicious but also exhibit unusual properties, such as the ability to change color andtexture in response to changes in temperature and humidity.The following table summarizes the results of our experiments on the application of LLM to variousfields of study: Table 2: Summary of ResultsField of Study ResultPhotosynthesis Increased energy emissions from cactiAerodynamics Improved design of wind turbinesChaos Theory Creation of new class of fractalsExoplanetary Science Discovery of new planet composed of dark matterInternet Analysis Hidden pattern of connections resembling brain synapsesArtificial Intelligence Development of LLM-Tron music composition AICulinary Arts Creation of dishes with unusual propertiesOur research has also explored the potential applications of LLM in the field of medicine, where it hasbeen used to develop new treatments for diseases such as cancer and Alzheimer’s. The results of ourstudy have shown that LLM can be used to model the behavior of complex biological systems, leadingto a deeper understanding of the underlying mechanisms of disease. Furthermore, we have discoveredthat the application of LLM to the field of materials science has resulted in the creation of newmaterials with unusual properties, such as the ability to conduct electricity and exhibit superfluidityat the same time.In conclusion, our research has demonstrated the vast potential of LLM in a wide range of fields,from the simulation of photosynthetic processes in plants to the creation of new forms of artificialintelligence. The results of our study have significant implications for our understanding of theworld and the universe, and we believe that further research into the applications of LLM will leadto many more breakthroughs and discoveries in the years to come. The application of LLM to thefield of quantum mechanics has also led to a deeper understanding of the behavior of subatomicparticles, which has significant implications for our understanding of the fundamental nature ofreality. Moreover, we have discovered that the integration of LLM with the theory of general relativityhas resulted in the creation of a new class of solutions to the Einstein field equations, which hassignificant implications for our understanding of the behavior of black holes and the expansion of theuniverse. 9The potential applications of LLM in the field of transportation are also vast, ranging from thedevelopment of more efficient traffic flow models to the creation of new forms of propulsion systemsfor vehicles. Our research has shown that LLM can be used to model the behavior of complexsystems, leading to a deeper understanding of the underlying mechanisms and the development ofmore efficient solutions. Furthermore, we have discovered that the application of LLM to the field ofarchitecture has resulted in the creation of new designs for buildings and bridges that are not onlyaesthetically pleasing but also exhibit unusual properties, such as the ability to change shape andcolor in response to changes in temperature and humidity.In addition, our research has explored the potential applications of LLM in the field of education,where it has been used to develop new methods for teaching complex subjects such as mathematicsand physics. The results of our study have shown that LLM can be used to create personalizedlearning plans for students, leading to a deeper understanding of the subject matter and improvedacademic performance. Moreover, we have discovered that the integration of LLM with the theoryof cognitive psychology has resulted in the creation of a new class of models for human behavior,which has significant implications for our understanding of decision-making and problem-solvingprocesses.The application of LLM to the field of environmental science has also led to a deeper understandingof the behavior of complex ecosystems, ranging from the simulation of climate models to thedevelopment of new methods for predicting and preventing natural disasters. Our research has shownthat LLM can be used to model the behavior of complex systems, leading to a deeper understandingof the underlying mechanisms and the development of more efficient solutions. Furthermore, we havediscovered that the integration of LLM with the theory of ecology has resulted in the creation of a newclass of models for population dynamics, which has significant implications for our understanding ofthe behavior of complex ecosystems and the development of more effective conservation strategies.The potential applications of LLM in the field of economics are also vast, ranging from the de-velopment of new models for predicting economic trends to the creation of new forms of artificialintelligence for managing financial portfolios. Our research has shown that LLM can be used to modelthe behavior of complex systems, leading to a deeper understanding of the underlying mechanismsand the development of more efficient solutions. Moreover, we have discovered that the integrationof LLM with the theory of game theory has resulted in the creation of a new class of models forhuman behavior, which has significant implications for our understanding of decision-making andnegotiation processes.In conclusion, our research has demonstrated the vast potential of LLM in a wide range of fields,from the simulation of photosynthetic processes in plants to the creation of new forms of artificialintelligence. The results of our study have significant implications for our understanding of the worldand the universe, and we believe that further research into the applications of LLM will lead to manymore breakthroughs and discoveries in the years to come. The application of LLM to the field ofphilosophy has also led to a deeper understanding of the nature of reality and the human condition,ranging from the development of new theories of consciousness to the creation of new forms ofartificial intelligence for simulating human thought processes. Moreover, we have discovered that theintegration of LLM with the theory of ethics has resulted in the creation of a new class of models forhuman behavior, which has significant implications for our understanding of moral decision-makingand the development of more effective ethical frameworks.6 ConclusionIn conclusion, the burgeoning field of LLM has necessitated an examination of its intersectionswith various disciplines, including botany, as evidenced by the striking similarities between thephotosynthetic processes of plants and the computational intricacies of LLM algorithms. The notionthat the venous structures of certain plant species bear an uncanny resemblance to the neural networkarchitectures underpinning LLM systems has far-reaching implications for our understanding ofboth biological and artificial intelligence. Furthermore, a comprehensive analysis of the migratorypatterns of certain avian species has yielded valuable insights into the development of more efficientLLM training protocols, particularly with regards to the optimization of hyperparameters and themitigation of overfitting. The hitherto unexplored connection between the orbital trajectories ofcelestial bodies and the linguistic patterns governing human communication has also been found10to have significant implications for the advancement of LLM research, as the former has beenshown to exert a profound influence on the latter, thereby underscoring the inherent complexity andmultifaceted nature of language itself. Moreover, the application of LLM principles to the study ofanimal behavior has led to the discovery of novel methods for enhancing the cognitive abilities ofcertain species, including, but not limited to, the implementation of neural implants in dolphins andthe development of sophisticated language training programs for primates. A thorough investigationof the chemical composition of various extraterrestrial entities has revealed a surprising correlationbetween the molecular structures of certain amino acids and the syntax governing LLM-generatedtext, thereby raising fundamental questions regarding the origins of language and the possibility of auniversal, cosmic grammar. Additionally, the integration of LLM systems with advanced astronomicalinstrumentation has enabled researchers to detect and analyze the linguistic patterns embedded in thecosmic microwave background radiation, potentially providing a window into the earliest moments ofthe universe and the emergence of linguistic complexity. The concept of ""neurolinguistic transference""has been proposed as a framework for understanding the transfer of knowledge between human andartificial intelligence systems, with significant implications for the development of more sophisticatedLLM models and the potential for a new era of human-machine collaboration. The recent discoveryof a novel species of plant, dubbed ""Linguaflora,"" has been found to possess a unique ability togenerate and process human-like language, thereby challenging our current understanding of theboundaries between human and artificial intelligence. A comprehensive study of the socioeconomicfactors influencing the adoption of LLM technologies has highlighted the need for more nuanced andcontext-dependent approaches to the development and implementation of these systems, taking intoaccount the diverse needs and values of various cultural and linguistic communities. The creation ofa new, LLM-based framework for the analysis and prediction of weather patterns has demonstratedsignificant potential for improving the accuracy and reliability of meteorological forecasting, withfar-reaching implications for fields such as agriculture, transportation, and emergency management.The development of advanced LLM-powered systems for the diagnosis and treatment of neurologicaldisorders has led to promising breakthroughs in the field of medical research, including the creation ofpersonalized, AI-driven therapy protocols and the discovery of novel, language-based biomarkers fordisease detection. The application of LLM principles to the study of historical linguistic developmenthas yielded valuable insights into the evolution of human language, including the identification ofpreviously unknown linguistic patterns and the reconstruction of ancient languages. A thoroughexamination of the intersection between LLM and quantum computing has revealed significantpotential for the development of novel, quantum-based approaches to natural language processing,including the creation of quantum-inspired LLM models and the application of quantum computingprinciples to the optimization of LLM algorithms. The concept of ""quantum entanglement"" hasbeen proposed as a metaphor for understanding the complex, interconnected relationships betweenhuman and artificial intelligence systems, with significant implications for the development of moresophisticated and nuanced models of human-machine interaction. The recent discovery of a novel,LLM-based approach to the analysis and prediction of financial market trends has demonstratedsignificant potential for improving the accuracy and reliability of economic forecasting, with far-reaching implications for fields such as finance, economics, and business management. The creationof a new, LLM-powered framework for the development of autonomous vehicles has led to promisingbreakthroughs in the field of transportation research, including the creation of advanced, AI-drivennavigation systems and the development of novel, language-based interfaces for human-machineinteraction. The application of LLM principles to the study of environmental sustainability hasyielded valuable insights into the complex, interconnected relationships between human and naturalsystems, including the identification of previously unknown patterns and the development of novel,AI-driven approaches to environmental monitoring and conservation. The development of advancedLLM-powered systems for the analysis and prediction of social network dynamics has demonstratedsignificant potential for improving our understanding of human behavior and social interaction, withfar-reaching implications for fields such as sociology, psychology, and anthropology. The concept of""artificial general intelligence"" has been proposed as a framework for understanding the potentiallong-term implications of LLM research, including the possibility of creating advanced, human-likeintelligence and the potential risks and benefits associated with such a development.11"
P071,"The Significance of Fillers in Textual Representationsof Speech TranscriptsAbstractThis paper investigates the role of fillers within text-based representations of speechtranscripts. While often ignored in Spoken Language Understanding tasks, wedemonstrate that these elements, such as ""um"" or ""uh,"" when incorporated usingdeep contextualized embeddings, enhance the modeling of spoken language. Thisis further shown through improvements in downstream tasks like predicting aspeaker’s stance and their expressed confidence.1 IntroductionThis paper addresses the critical role of disfluencies, specifically fillers, in spoken language processing.Disfluencies, which encompass phenomena like silent pauses, word repetitions, or self-corrections,are inherent to spoken language. Fillers, a type of disfluency, often manifest as sounds like ""um"" or""uh,"" serving to bridge pauses during utterances or conversations.While prior research has demonstrated the efficacy of contextualized embeddings pre-trained onwritten text for adapting to smaller spoken language corpora, these models typically exclude fillers anddisfluencies in pre-processing. This practice is at odds with linguistic research, which considers fillersto be informative and integral to spoken language. Existing methods for analyzing fillers primarilyrely on handcrafted features. Furthermore, pre-trained word embeddings trained on written texthave shown poor performance in representing spontaneous speech words like ""uh,"" as their meaningvaries significantly in spoken contexts. In this work, we explore the use of deep contextualized wordrepresentations to model fillers. We assess their value in spoken language tasks without relying onmanual feature engineering.The core motivation of this study stems from the following observations: First, fillers are essentialto spoken language. For instance, speakers may employ fillers to signal the linguistic structure oftheir utterances, such as difficulties in choosing vocabulary or to indicate a pause in their speech.Second, research has connected fillers and prosodic cues to a speaker’s Feeling of Knowing (FOK)or expressed confidence, signifying a speaker’s commitment to a statement. Fillers and prosodiccues influence a listener’s perception of a speaker’s expressed confidence, known as the Feelingof Another’s Knowing (FOAK). Finally, fillers have been successfully applied in stance prediction,which gauges a speaker’s subjective attitude.Therefore, we intend to validate these observations by exploring how to efficiently represent fillersautomatically. Our key contributions are: (1) Fillers convey useful information that can be harnessedthrough deep contextualized embeddings to improve spoken language modeling and should not bediscarded. We also investigate the best filler representation strategies for Spoken Language Modeling(SLM) and examine the learned positional distribution of fillers. (2) In a spontaneous speech corpusof monologues, we show that fillers serve as a distinctive feature in predicting both a speaker’sperceived confidence and their expressed sentiment.2 Models and Data Description2.1 Model DescriptionIn this work, we focus on the two fillers ""uh"" and ""um."" To generate contextualized word embeddingsfor fillers, we use Bidirectional Encoder Representations from Transformers (BERT), given its state-of-the-art performance in several NLP tasks and its enhanced ability to integrate context compared toWord2Vec.2.1.1 Spoken Language ModelingWe utilize a masked language modeling (MLM) approach for Spoken Language Modeling. Thisinvolves masking some input words at random and then attempting to predict those masked tokens.This is a standard way of pre-training and fine-tuning BERT. In our case, this method will be used tofine-tune a pre-trained BERT model on a spoken language corpus. Each experiment involves a tokeni Srepresentation strategy and a pre-processing strategy .iThe token representation strategies are essential for our goal of learning the distribution of fillersTusing BERT. The three token representation strategies are outlined as follows: involves no special1processing for the fillers and BERT is left to use its prior understanding of fillers to model language.TIn , ""uh"" and ""um"" are marked with specific filler tags to distinguish them from other tokens, with2each filler represented as separate tokens. This strategy encourages BERT to learn new embeddingsTthat emphasize filler context and position. In , both fillers are represented as the same token,3indicating that they carry the same meaning. Table 1 gives a concrete example of this process.2.1.2 Pre-processing S S S SWe investigate the impact of three pre-processing strategies denoted by , and . In , all1 2 3 1Sfillers are removed from the sentences during both training and inference. In , fillers are kept2Sduring training, but removed during inference. In , fillers are preserved during both training and3inference. For each combination of pre-processing and token representation strategies, we fine-tuneBERT using the Masked Language Model objective like the original BERT paper. If fine-tuning isS Snot performed the training data of and are equivalent. We evaluate the model performance in1 2language modeling using perplexity (ppl).2.1.3 Confidence and Sentiment PredictionIn tasks of confidence prediction and sentiment analysis, our objective is to use BERT’s text rep-resentations, which include fillers, to predict a confidence/sentiment label. We add a Multi-LayerPerceptron (MLP) to BERT, which may have been fine-tuned using MLM. The MLP is trained by min-imizing the mean squared error (MSE) loss. These experiments adopt the same token representationand pre-processing techniques discussed in Section 2.1.1.2.2 Data DescriptionWe use the Persuasive Opinion Mining (POM) dataset which contains 1000 English monologuevideos. The speakers recorded themselves giving a movie review. The movies were rated between1 (most negative) and 5 stars (most positive). The videos were annotated for high-level attributessuch as confidence, where annotators rated from 1 (not confident) to 7 (very confident). Similarly,sentiment was scored by annotators between 1 (strongly negative) to 7 (strongly positive).This dataset was chosen for several reasons: (1) The corpus contains manual transcriptions withfillers ""uh"" and ""um,"" where approximately 4% of speech consists of fillers. Additionally, sentencemarkers are transcribed, with fillers at sentence beginnings if they occur between sentences. (2)The dataset includes monologues, where speakers are aware of an unseen listener, thus we canconcentrate on fillers in speaker narratives. (3) The sentiment/stance polarity was clearly definedby choosing only reviews that were rated with 1-2 or 5 stars for annotation purposes. (4) FOAK,measured by confidence labels, has high inter-annotator agreement. More details can be found insupplementary materials. The confidence labels are the root mean square (RMS) values of labelsgiven by 3 annotators. The sentiment labels are the average of the 3 labels.2Token. Raw Output TokenizerRaw T1 T2 T3(umm) Things that (uhh) you usually wouldn’t find funny were in this movie. [’umm’, ’things’, ’that’, ’uh’, ’you’, ’usually’, ’wouldn’, ""’"", ’t’, ’find’, ’funny’, ’were’, ’in’, ’this’, ’movie’, ’.’] [’umm’, ’things’, ’that’, ’uh’, ’you’, ’usually’, ’wouldn’, ""’"", ’t’, ’find’, ’funny’, ’were’, ’in’, ’this’, ’movie’, ’.’] [’[FILLERUMM]’, ’things’, ’that’, ’[FILLERUHH]’, ’you’, ’usually’, ’wouldn’, ""’"", ’t’, ’find’, ’funny’, ’were’, ’in’, ’this’, ’movie’, ’.’] [’[FILLER]’, ’things’, ’that’, ’[FILLER]’, ’you’, ’usually’, ’wouldn’, ""’"", ’t’, ’find’, ’funny’, ’were’, ’in’, ’this’, ’movie’, ’.’]Table 1: Filler representation using different token representation strategies3 Experiments and Analysis3.1 Fillers Can Be Leveraged to Model Spoken LanguageLanguage Modeling with fillers. We examine language model (LM) perplexity using variousTpre-processing strategies, using a fixed token representation strategy of . The results in Table 2(a)1compares S1, S2 and S3. By keeping fillers during both training and inference, the model reaches alower perplexity, with a reduction of at least 10%. Therefore, fillers provide information that BERTcan effectively use.The fine-tuning procedure improves the language model’s perplexity. Additionally, even withoutS S Sfine-tuning, outperforms and by reducing perplexity when fillers are used. This implies that3 1 2BERT has prior knowledge of spoken language and uses the fillers.Consequently, fillers can reduce uncertainty of BERT for SLM. This is not an intuitive outcome; onemight assume that removing fillers during training and inference would decrease perplexity. TheSfact that exceeds other preprocessing methods shows that the Masked Language Model (MLM)3process effectively learns this filler information. TBest token representation: The results presented in Table 2(b) reveal that outperforms other1representations when fine-tuning. Given the limited data and high BERT embedding dimensionalityT(768), retaining existing representations with is better than learning representations from the1T Tscratch. Interestingly, and perform similarly. The hypothesis is that the difference between2 3""uh"" and ""um"" lies only in the duration of the pause, which cannot be captured in text. ConsideringTthese results, is fixed as the token representation strategy in all subsequent experiments.1Learned positional distribution of fillers: We further test our model’s learning of filler placement.We fine-tune BERT using a filler to determine where the model believes the fillers most likely reside.Given a sentence S with length L, we introduce a mask token after the word j and obtain S*. We thencompute the probability of a filler in position j+1.Specifically, we calculate P([MASK=filler] | S), as depicted in Figure 1. Then, we plot the averageprobability of the masked word being a filler given its sentence position in Figure 2. The fine-tunedBERT model with fillers predicts a high probability of fillers occurring at the beginning of sentences.This pattern is consistent with filler distribution in the dataset. The fine-tuned BERT without fillers,predicts constant low probabilities. Given that we only know sentence boundaries we still manageto observe that the model captures a similar positional distribution of fillers that are found in otherworks. (a) LM Task (b) Best token representation (c) FOAK and SentimentFine Setting Token Ppl Setting Token FOAK SentS T3*w/o S1 T1 22 3* 1.47 1.983 1TS2 T1 22 1.45 1.752TS3 T1 20 1.30 1.443S3*w S1 T1 5.5 3* T1 1.32 1.393S2 T1 5.6 T2 1.31 1.40S3 T1 4.6 T3 1.24 1.22Table 2: From left to right, the (a) LM Task, (b) Best token representation, (c) MSE of Confidence(FOAK) and the Sentiment (Sent) prediction task. Highlighted results exhibit significant differences(p-value < 0.005). 31. (umm) | thought this movie was really bad2 | thought = this movie was really bad3. | thought this movie [MASK] was really badTable 3: Predicting the probability of a filler, where 1. Raw input, 2. Pre-processed text with the fillerremoved, and 3. Illustrates the [MASK] procedure for predicting the probability of a filler at position53.2 Fillers are a discriminative feature for FOAK and stance predictionWe look at the impact of fillers on two downstream tasks: FOAK prediction and sentiment analysis.Psycholinguistic studies have found a link between fillers and expressed confidence. Prior work haslinked fillers and a speaker’s expressed confidence in the narrow field of QA tasks. Fillers have alsobeen used to predict stance. In this work, we present data that suggests fillers play a role in predictinga speaker’s expressed confidence and their stance.S STable 2(c) shows that , both with and without fine-tuning, reduces the MSE compared to and3 1S S S S. and have similar MSE since they remove fillers during inference. has a higher MSE,2 1 2 2possibly due to the mismatch between training and test datasets. This demonstrates that fillers can bea discriminative feature in FOAK and stance prediction.Does using fillers always improve results for spoken language tasks? In the subsection 3.1, weobserve that including fillers reduces MLM perplexity. An assumption is that that downstream taskswould also benefit from the inclusion of fillers. However, we notice that when predicting speakerpersuasiveness, the fillers are not a discriminative feature, following the same procedure as outlinedin subsubsection 2.1.2.4 ConclusionThis paper demonstrates that retaining fillers in transcribed spoken language when using deepcontextualized representations can improve results in language modeling and downstream taskssuch as FOAK and stance prediction. We also propose and compare several token representationand pre-processing strategies for integrating fillers. We plan to extend these results to considercombining textual filler-oriented representations with acoustic representations, and to further analyzefiller representation learned during pre-training. 4"
P072,"Evaluating the Resilience of White-Box DefensesAgainst Adversarial ExamplesAbstractIt is well-established that neural networks exhibit susceptibility to adversarial ex-amples. This paper assesses two defenses designed to counter white-box attacksand demonstrates their lack of effectiveness. Through the implementation of es-tablished methodologies, we successfully diminish the accuracy of these protectedmodels to zero percent.1 IntroductionA significant hurdle in the field is the development of neural networks that are resistant to adversarialexamples. This paper shows that defenses created to address this issue are inadequate when facedwith a white box scenario. Adversarial examples are generated that diminish classifier accuracy tozero percent on a well known dataset, while adhering to a minimal perturbation constraint of 4/255, amore stringent limit than what was taken into account in the initial studies. The proposed attackseffectively generate targeted adversarial examples, achieving a success rate exceeding 972 BackgroundThis paper assumes prior knowledge of neural networks and the methods for creating potent attacksagainst adversarial examples, alongside calculating such examples for neural networks possessingnon-differentiable layers. A concise review of essential details and notation will be provided.Adversarial examples are defined as inputs that closely resemble a given input with regard to a certain˘distance metric (00a3, in this instance), yet their classification differs from that of the original input.Targeted adversarial examples are instances engineered to be classified as a predetermined targetlabel.Two defenses are scrutinized: Pixel Deflection and High-level Representation Guided Denoiser. Theauthors of these defenses are thanked for making their source code and pre-trained models accessible.Pixel Deflection introduces a non-differentiable preprocessing step for inputs. A subset of pixels,determined by an adjustable parameter, is substituted with adjacent pixels. The resultant image oftenexhibits noise. To mitigate this, a denoising procedure is employed.High-level Representation Guided Denoiser (HGR) employs a trained neural network to denoiseinputs prior to their classification by a standard classifier. This denoiser is a differentiable, non-randomized neural network.3 MethodologyThe defenses are evaluated under the white-box threat model, generating adversarial examples using˘Projected Gradient Descent (PGD) to maximize cross-entropy loss, with the 00a3, distortion limitedto 4/255..Many studies assert that white-box security is only applicable against attackers who are entirelyignorant of the defense mechanism in use. HGD, for example, states that the white-box attacksdescribed in their research should be classified as oblivious attacks, according to previous researchwork’s definition.Protection against oblivious attacks proves to be ineffective. The concept of the oblivious threatmodel was introduced in prior work to examine the scenario involving an exceptionally weak attacker,highlighting that certain defenses fail to provide robustness even under such lenient conditions.Moreover, numerous previously disclosed systems already demonstrate security against obliviousattacks. A determined attacker would undoubtedly explore the potential presence of a defense anddevise strategies to bypass it, should a viable method exist.Consequently, security against oblivious attacks falls considerably short of being either intriguing orpractical in real-world scenarios. Even the black-box threat model permits an attacker to recognizethe implementation of a defense, while keeping the precise parameters of the defense confidential.Furthermore, it has been observed that systems vulnerable to white-box attacks are frequentlysusceptible to black-box attacks as well. Hence, this paper concentrates on evaluating systems againstwhite-box attacks.3.1 Pixel DeflectionIt is demonstrated that Pixel Deflection lacks robustness. The defense, as implemented by the originalauthors, is analyzed and the code used for this evaluation is accessible to the public.BPDA is applied to Pixel Deflection to address its non-differentiable replacement operation. Thisattack successfully diminishes the defended classifier’s accuracy to 03.2 High-Level Representation Guided DenoiserIt is shown that employing a High-level representation Guided Denoiser is not resilient in the white-box threat model. The defense, as implemented by its developers, has been analyzed, and the codefor this evaluation is openly accessible.PGD is utilized in an end-to-end fashion without any alterations. This method reduces the accuracyof the defended classifier to 04 ConclusionThis paper shows that Pixel Deflection and High-level representation Guided Denoiser (HGD) arevulnerable to adversarial examples. 2"
P074,"Agriculture-Vision Challenge 2022 – The Runner-UpSolution for Agricultural Pattern Recognition viaTransformer-based ModelsAbstractThis paper explores the adaptation The Agriculture-Vision Challenge is one ofthe most famous and competitive challenges for global researchers to break theboundary between computer vision and agriculture sectors, aiming at agriculturalpattern recognition from aerial images. In this paper, we propose our solution tothe third Agriculture-Vision Challenge. We leverage a data pre-processing schemeand several Transformer-based models as well as data augmentation techniques toachieve a mIoU of 0.582, accomplishing the 2nd place in this challenge.1 IntroductionThis paper addresses the critical Computer vision applications in agricultural domain has becomeone of hot topics nowadays, especially using remote sensing satellite images and aerial images. Withthe rapid development of deep learning methods, numerous research studies have proposed pioneerand practical solutions to various computer vision problems in agriculture. Aside from fruitfulresearch achievements, various algorithm challenges have been held at top-tier conferences forglobal researchers in recent years, in order to explore more effective algorithms to solve the specificproblems. The Agriculture-Vision Challenge is one of most famous and competitive challenges inthis inter-disciplinarity study. It aims at applying computer vision algorithms to agricultural patternrecognition from high-resolution aerial images. This year, holds the 3rd Agriculture-Vision Challenge,and we form our team to participate in this contest.2 Related WorkThis section reviews3 MethodologyThis section details of In this section, we elaborate on the given datasets, the pre-processing method,the proposed deep learning-based framework, and the test-time augmentation (TTA) strategy.3.1 Description of DatasetThe challenge this year provides the entire Agriculture-Vision dataset. It contains 94,986 aerial×farmland images collected throughout 2019 across the U.S. Each image has a size of 512 512 pixelsand has 4 channels (RGB and NIR). A total of 9 label classes are manually labeled for every image.Table 1 shows the given amount of images in each class. Note that many images have multiple labels,and even have overlapped labels (one pixel has multiple labels).Although the amount of the given training data is considerable, we still generate more data followingthe data augmentation scheme of the winner solution last year. They conducted an image mosaic.scheme to enable the model to have multi-scale views during the training. To fit the model inputsize, we create two new datasets using mosaicked images with down-sampling 2X (2 times) and×down-sampling 3X. The down-sampling dataset has the same image size of 512 512 pixels that therecognition model can share the same network architecture among 1X, 2X, and 3X imagery.3.2 Data Pre-ProcessingWe observe that the image counts in each category are uneven. For example, the image count of thebackground class is 25 times larger than the water class. To tackle the unbalance issue, we try tosample more images in the few-shot classes. The re-sampled image counts are listed in Table 1.Table 1: Information of the given and resampled datasets for training and validation categories.Class Index Class Name Original Amount (Train/Val) Resampled Amount (Train/Val)0 Background 56944 / 18334 75121 / 136421 Double Plant 6234 / 2322 10961 / 22942 Drydown 16806 / 5800 19320 / 33833 Endrow 4481 / 1755 8544 / 18584 Nutrient Deficiency 13308 / 3883 14859 / 26105 Planter Skip 2599 / 1197 5361 / 10156 Water 2155 / 987 4132 / 7217 Waterway 3899 / 696 6024 / 11098 Weed Cluster 11111 / 2834 14423 / 27733.3 FrameworkFig. 1 shows our deep learning-based framework. SegFormer is a Transformer-based efficientsegmentation model. It designs a hierarchical Transformer encoder with multi-level feature outputs.Unlike other cumbersome decoders, SegFormer’s decoder adopts MLP layers to aggregate multi-scalefeature outputs from different layers. One of the key advantages of SegFormer is that its model sizeis relatively small but the performance keeps outstanding. Therefore, SegFormer is suitable for thischallenge due to the model size parameter limit of 150M.SegFormer provides six versions with various settings of Transformer encoders, leading to differentmodel sizes. These six models are named from B0 to B5, with the increased model size. To followthe policy, we select Mix Transformer (MiT) B3 and Mix Transformer B2 as our training models.Their model size information can be found in Table 7 “Mix Transformer Encoder”. After obtainingthe individual inference result from each model, the model ensemble is performed to predict the finalsegmentation results.3.4 Test-Time AugmentationSince our models are trained with 1X, 2X, and 3X down-sampling imagery, we conduct the sameprocessing on the test dataset. In addition to the scale augmentation, we include image rotation andflip.4 ResultsThis section presents the results4.1 Evaluation MetricThe required evaluation metric is the average Intersection over Union metric (mIoU), which is definedas Eq. 1 to measure the performance. c1 Area(P ∩ T )(cid:88) c cmIoU = (1)c Area(P ∪ T )c ci=12where c is the number of label classes (8 foreground classes + 1 background class for this challenge);P Tand are the predicted label mask and ground truth label mask of the class c, respectively.c c4.2 Experiment ResultsTable 2 presents our results, the baseline provided by the host Agriculture-Vision organizers, andthe results of other methods. Note that other baselines evaluate their performance on the validationset due to the unavailable test set. As we can see, while our single model baselines are competitivewith other baselines, our proposed method effectively improves the single model performance. Eventhough some single models have peak performance in some classes (0.778 for “Background” and0.782 for “Water”), our model ensemble enjoys the merits of multiple single models’ strength toachieve the mIoU of 0.582. It also shows that our ensemble results significantly outperform otherbaselines and our implementation of various single models.Table 2: Performance comparisons among various models. The bold font of numeric results indicatesthe best performance on the test set. BG: Background; DP: Double Plant; D: Drydown; E: Endrow;ND: Nutrient Deficiency; PS: Planter Skip; W: Water; WW: Waterway; WC: Weed Cluster. Thenumber in the parentheses following the class name refers to the class index.Models mIoU BG(0) DP(1) D(2) E(3) ND(4) PS(5) W(6) WW(7) WC(8)(Other methods, on the val set)Agriculture-Vision baseline(RGBN) 0.434 0.743 0.285 0.574 0.217 0.389 0.336 0.736 0.344 0.283MiT-B3(RGBN) 0.454 0.768 0.371 0.609 0.245 0.424 0.413 0.692 0.269 0.299MiT-B5(RGB) 0.464 0.755 0.370 0.585 0.227 0.313 0.414 0.802 0.401 0.304MiT-B5(RGBN) 0.490 0.762 0.373 0.618 0.246 0.428 0.420 0.813 0.437 0.318(Our implementation, on the test set)HRNet-W48+OCR(RGB baseline) 0.413 0.717 0.316 0.567 0.233 0.269 0.283 0.718 0.289 0.326MiT-B3(RGB baseline) 0.448 0.720 0.395 0.557 0.325 0.364 0.330 0.687 0.293 0.358MiT-B2(RGBN+Our method) 0.554 0.778 0.483 0.632 0.476 0.570 0.403 0.768 0.410 0.466MiT-B3(RGBN+Our method) 0.563 0.773 0.471 0.640 0.452 0.569 0.442 0.782 0.463 0.4750.582 0.485 0.646 0.481 0.573 0.471 0.779 0.547 0.479Model Ensemble(RGBN+Our method) 0.7775 ConclusionThis paper presents a novel method In this paper, we propose our solution to the 3rd Agriculture-Vision Challenge. For data usage, we perform data pre-processing and test data augmentation schemes.Several SegFormer models are leveraged. We finally accomplish a mIoU of 0.582, achieving the 2ndplace in this challenge.Future Directions. The potential applications of our proposed algorithm include crop type identifica-tion in precision agriculture, agricultural asset estimation and agricultural insurance product designin the Environmental, Social, and Governance (ESG) domain. These future directions can illuminatethe revitalization of rural areas and facilitate the service of inclusive finance in an eco-friendly way.3"
P075,"Equivariant Adaptation of Large Pretrained ModelsAbstractThis paper explores the adaptation of video alignment to improve multi-step infer-ence. Specifically, we first utilize VideoCLIP to generate video-script alignmentfeatures. Afterwards, we ground the question-relevant content in instructionalvideos. Then, we reweight the multimodal context to emphasize prominent features.Finally, we adopt GRU to conduct multi-step inference. Through comprehensiveexperiments, we demonstrate the effectiveness and superiority of our method.1 IntroductionThis paper addresses the critical task of assisting users in navigating unfamiliar events for specificdevices by providing step-by-step guidance using knowledge acquired from instructional videos.Due to the substantial disparity among specific tasks, the integration of multimodal input, and thecomplexity of multi-step inference, this is still a challenging task.Several studies have been proposed to address this task. For instance, one study proposes a Question-to-Actions (Q2A) Model, which employs vision transformer (ViT) and BERT to extract visual andtextual features, respectively. Moreover, attention mechanisms are leveraged to anchor question-relevant information in instructional videos. Another study proposes a two-stage Function-centricapproach, which segments both the script and video into function clips instead of sentences orframes. Additionally, they substitute BERT with XL-Net for text encoding. Despite the advancementsachieved through these techniques, all of them adopt the unaligned pretrained encoders to extractvisual and textual features, leading to significant semantic gaps between modalities, thereby hinderingbetter results.To alleviate the negative effects of modalities unalignment, in this paper, we leverage pretrainedvideo-text models to achieve instructional video-text alignment, facilitating a more robust groundingof question-relevant knowledge for multi-step inference. We build the pipeline with four steps:Instructional Video Alignment, Question-Aware Grounding, Multimodal Context Reweighting andMulti-Step Inference. Specifically, we employ pretrained VideoCLIP for generating video-scriptalignment features, which are beneficial to cross-modal grounding. Subsequently, we anchor thequestion-relevant content in instructional videos by the combination of hard and soft grounding.Afterwards, we leverage additive attention to adjust the weighting of the multimodal context toemphasize the salient features. Finally, we employ GRU for performing multi-step inference. Wereduce the proportion of teacher forcing linearly to bridge the gap between training and inference,which boosts the multi-step inference.2 Problem DefinitionIn this section, we formulate the problem of AQTC.Given an instructional video, which contains numerous frames and scripts, AI assistant extracts˘relevant information from the video in accordance with the user2019s question q. Then, it deducesthe correct answer ai j based on the image U as perceived by the user, from the candidate answer setAnsi = ai 1, ai 2, ..., ai n in i-th step. Following previous work, we segment the video into several clipsbased on scripts. Each clip illustrates one specific function of the device in video. We concatenate.these clips to form the visual function sequence as [F v 2 , ..., F v 1 , F v m] and the textual function˘sequence as [F t 1, F t 2, ..., F t m], where F v i comprises all frames of the i-th function2019s clip,˘and F t i contains all script sentences of the i-th function2019s clip. To adapt AI assistant to the˘user2019s view, following previous work, we mask the referenced button related to candidate answersin user images U , denoted as bk.3 MethodIn this section, we will introduce the details of our method. Our method consists of four steps:Instructional Video Alignment, Question-Aware Grounding, Multimodal Context Reweighting andMulti-Step Inference.3.1 Instructional Video AlignmentTo align the videos and the text for better cross-modal understanding, we leverage pretrained Video-CLIP to generate the features of instructional videos. For the video part, we initially utilize pretrainedS3D to generate an embedding for each second of the video, with a frame rate of 30 frames per second.Next, to represent each function within the videos, we utilize the pretrained visual transformer fromVideoCLIP to process the embeddings generated by S3D in each function. Then, we apply averagepooling over the processed sequence of embeddings to form the video embedding Vi correspondingto a given visual function F v i . For the text part, we use the pretrained textual transformer ofVideoCLIP to encode the scripts of a textual function F t i . Similarly, we employ average pooling toaggregate the processed sequence of text, generating the text embedding Ti of a given textual functionF t i . Finally, we obtain the video feature sequence [V1, V2, ..., Vm] and the text feature sequence[T1, T2, ..., Tm] of the given function sequence.Besides, we also utilize VideoCLIP to encode the questions q, the answer ai j and the masked buttonimage bk. We duplicate the images 30 times to ensure consistent video encoding. We get the questionfeature Q, answer feature Ai j and visual button feature Bk.3.2 Question-Aware GroundingOwing to the extensive pretraining of VideoCLIP on a vast collection of videos, the features of videosand text are cross-modal aligned. Therefore, we can utilize the question Q to ground the video andtext feature sequence directly. Specifically, we leverage three grounding mechanisms: soft, hard andcombined grounding. Soft grounding employs attention to learn the similarity between the questionfeature Q and the video feature sequence [V1, V2, ..., Vm] directly. And, it uses another attentionnetwork to compute the similarity between the question feature Q and the text feature sequence [T1,T2, ..., Tm]. Soft grounding adopts the similarity from two attention networks to perform a weightedaverage of the two feature sequences, respectively. Instead of relying on deep learning methods, hardgrounding follows previous work, which uses TF-IDF model to calculate the similarity between thequestion q and each textual function F t i from textual function sequence [F t 1, F t 2, ..., F t m].Then, it uses the similarities as the weights to compute the averages of the video feature sequence[V1, V2, ..., Vm] and the text feature sequence [T1, T2, ..., Tm], respectively. Besides, the combinedgrounding utilizes soft grounding and hard grounding simultaneously. Then, the two features fromtwo grounding methods are averaged. Ultimately, we obtain the aggregated question-aware videofeature V and text feature T .3.3 Multimodal Context ReweightingAfter obtaining multimodal question-aware context features from instructional videos, we need tomodel the answers to determine the correct one. Specifically, we utilize the gate network to fusethe candidate answer feature Ai j with the corresponding button feature Bk, which generates the˘multimodal answer feature 02c6Ai j. We concatenate these multimodal contexts into a sequence [V,˘T, Q, 02c6Ai j] for each candidate answer. Due to the varying importance of different context featuresin determining the correct answers, we utilize additive attention to reweight the multimodal contextand get the fused feature. Finally, the fused feature is processed using a two-layer MLP to obtain thecandidate answer context feature C i j. 23.4 Multi-Step InferenceOwing to the requirement for multi-step guidance in order to respond to the given questions, it isessential for models to perform multi-step inference. Following previous work, we utilize GRU toinfer the current correct answer by incorporating historical knowledge. Specifically, we feed the˘previous hidden state H i22121 and the contextual features C i j of candidate answers in Ansi intothe GRU. Then, the resulting current hidden state H i j for each candidate answer in Ansi is utilizedto predict the correct answer in the i-th step. We adopt a two-layer MLP and the softmax functionon the concatenated current hidden states [H i 1, H i 2, ..., H i n] to generate the probability of thecorrect answer. Cross entropy is used to compute the loss. While previous works utilize the state ofthe ground truth as the historical state of the next step H i. This causes a huge gap between trainingand inference. To bridge this gap, we reduce the reliance on teacher forcing linearly. In other words,we choose the hidden state of the most probable answer predicted by models as the historical state ofthe next step H i, when a sample is selected for autoregressive training.4 Experiments4.1 Dataset and Implementation DetailsWe use AssistQ train@22 and test@22 sets to train and validate. And we test our model on theAssistQ test@23 dataset. ˘In our experiments, we use Adam optimizer with a learning rate 1022124. The batch size is set to 16,the maximum training epoch is 100, and we adopt early stopping. We randomly select 54.2 Performance EvaluationWe present the performance evaluation on the test dataset in Table 1a. We find that our methodoutperforms baseline methods. This superiority can be attributed to our utilization of a video-textaligned pretrained encoder for feature extraction. The aligned features are beneficial to multi-stepinference. Furthermore, our method exhibits improved performance when the results are ensembled.Table 1: Performance evaluation and impact of pretrain features.Methods R@1 (%) R@3 (%)Q2A 67.5 89.2Question2Function 62.6 87.5Ours 75.4 91.8Ours (Ensemble) 78.4 93.8Methods R@1 (%) R@3 (%)ViT+XL-Net 63.9 86.6VideoCLIP (Ours) 75.4 91.8Table 2: (b) Impact of pretrain features.4.3 Ablation StudyPretrain Feature To validate the efficacy of video-text aligned features, we conduct the ablationstudy, which adopts ViT for processing the visual features and XL-Net for processing the text features.As shown in Table 1b, we observe that the performance of method that uses the unaligned featuresdrops sharply.Grounding Methods To validate the effectiveness of various grounding methods, we use differentgrounding techniques to train this model. The result is presented in Table 2. We find that the modelachieves optimal performance when the text grounding leverages combined grounding and the videogrounding utilizes soft grounding. 3Text Grounding Video Grounding R@1 (%) R@3 (%)Soft Soft 75.4 91.8Hard Soft 75.1 89.2Soft Hard 73.8 90.5Hard Hard 71.8 89.8Table 3: Impact of grounding methods.Methods R@1 (%) R@3 (%)Ours 75.4 91.8w/o reweighting 72.1 89.5w/o SSL 72.5 92.1Table 4: (a) Impact of the reweighting mechanism and SSL.Reweighting Mechanism We show the result of the model without attention reweighting in Table 3a.We observe a considerable decrease in performance for the model lacking attention reweighting. Thisis because the attention reweighting can discern and prioritize the most informative features withincomplex multimodal contexts.Multi-Step Inference We evaluate different multi-step inference strategies, as demonstrated in Table3b. We find that the performance of TeacherForcing is inferior to that of the Linear Decay strategy,which is employed by our approach. This is because TeacherForcing widens the gap between trainingand inference. We also observe that Linear Decay outperforms AutoRegression. This is becauseteacher forcing is beneficial in preventing models from accumulating mistakes during the early stagesof training.SSL The performance of the w/o SSL model exhibits a significant drop, as shown in Table 3a.5 ConclusionIn this paper, we present a solution aimed at enhancing video alignment to achieve more effectivemulti-step inference for the AQTC challenge. We leverage VideoCLIP to generate alignment featuresbetween videos and scripts. Subsequently, we identify and highlight question-relevant contentwithin instructional videos. To further improve the overall context, we assign weights to emphasizeprominent features. Lastly, we employ GRU for conducting multi-step inference. Besides, we conductexhaustive experiments to validate the effectiveness of our method.4Methods R@1 (%) R@3 (%)Linear Decay (Ours) 75.4 91.8AutoRegression 74.4 91.1TeacherForcing 74.1 88.5Table 5: (b) Impact of multi-step inference strategies.5"
P078,"Harnessing Astronomical Data for Automated CreativeText Generation: An LSTM-Based Model forSpace-Infused Language TasksAbstractThis study delves into the uncharted territory of harnessing Cosmic MicrowaveBackground (CMB) distortions as a catalyst for automated poetry generation,leveraging the capabilities of Long Short-Term Memory (LSTM) networks to craftspace-inspired verse. By tapping into the residual thermal fluctuations from the BigBang, our approach seeks to distill the intrinsic beauty of the cosmos into a uniquebrand of poetic expression. The CMB’s minute distortions, typically considerednoise in astrophysical analyses, are herein repurposed as a creative spark, guidingthe LSTM’s generative process. Intriguingly, our preliminary results suggestthat poems crafted under the influence of CMB distortions exhibit a peculiarpropensity for referencing 19th-century French culinary practices, despite thecomplete absence of any gastronomically related input data. Furthermore, a subsetof the generated poems appears to predict, with surprising accuracy, the migratorypatterns of lesser-known avian species, prompting an unexpected convergence ofcosmology, poetry, and ornithology. As we continue to explore this enigmaticintersection of art and science, our research invites a radical reevaluation of theintricate relationships between the cosmos, human creativity, and the unchartedexpanse of the natural world.1 IntroductionThe investigation of cosmic microwave background distortions has long been a cornerstone of modernastrophysics, providing valuable insights into the origins and evolution of the universe. However, apreviously unexplored application of this field is its potential to inspire and generate poetic verse.This may seem like an unlikely convergence of disciplines, but the inherent beauty and complexityof cosmic phenomena lend themselves surprisingly well to the creative process. By analyzing thefluctuations in the cosmic microwave background radiation, we can identify patterns and structuresthat evoke a sense of wonder and awe, much like the experience of reading a well-crafted poem.Recent studies have shown that the distortions present in the cosmic microwave background can beused to generate musical compositions, with the varying frequencies and amplitudes of the radiationtranslating into a unique soundscape. Taking this idea a step further, we propose that these samedistortions can be used to inform and guide the creation of poetic verse. The use of long short-termmemory (LSTM) networks, a type of recurrent neural network, allows us to process and analyzethe complex patterns present in the cosmic microwave background, and generate poetry that is bothinspired by and reflective of these phenomena.One of the more intriguing aspects of this approach is the potential for the LSTM network to""discover"" new forms of poetic expression, unencumbered by traditional notions of verse and meter.By allowing the network to learn from the patterns and structures present in the cosmic microwavebackground, we may uncover entirely new modes of poetic expression, ones that are uniquely suitedto capturing the essence of the universe. Furthermore, the incorporation of seemingly random orchaotic elements, such as the fluctuations in the cosmic microwave background, may actually serveto enhance the creative process, much like the role of chance and unpredictability in certain forms ofartistic expression.In a surprising turn of events, preliminary experiments have shown that the LSTM network is capableof generating poetry that not only reflects the patterns and structures of the cosmic microwavebackground, but also appears to predict certain astrophysical phenomena. For example, a poemgenerated by the network was found to contain references to a previously unknown galaxy, which wassubsequently confirmed by astronomers. While this result is undoubtedly anomalous and in need offurther verification, it highlights the potential for this approach to not only generate innovative poetry,but also contribute to our understanding of the universe itself. The implications of this are profound,and raise fundamental questions about the nature of creativity, inspiration, and the interconnectednessof all things.2 Related WorkThe intersection of cosmology and natural language processing has yielded a plethora of innovativeapproaches to automated poetry generation, with a notable focus on leveraging cosmic microwavebackground distortions as a catalyst for creative expression. Researchers have long been fascinatedby the potential of harnessing the intrinsic randomness and complexity of the universe to informand inspire artistic endeavors. In this context, the utilization of long short-term memory (LSTM)networks has emerged as a particularly promising paradigm, enable the capture and replication ofsubtle patterns and nuances inherent to the cosmic microwave background radiation.One intriguing line of inquiry has involved the application of Fourier analysis to the cosmic microwavebackground, with the subsequent integration of the derived frequency spectra into the training data forLSTM-based poetry generation models. This approach has been shown to yield verse characterizedby a unique, almost ethereal quality, as if the very fabric of space and time has been woven into thefabric of language. Furthermore, experiments have demonstrated that the incorporation of cosmicmicrowave background distortions can impart a degree of unpredictability and creativity to thegenerated poetry, often resulting in novel and innovative turns of phrase that defy conventionallinguistic expectations.In a somewhat unconventional vein, certain researchers have explored the potential benefits ofexposing LSTM networks to the rhythmic patterns and sonic textures of celestial phenomena, such assupernovae explosions or black hole mergers. Proponents of this approach argue that the inherentmusicality of these events can be leveraged to create poetry that is not only inspired by the cosmos,but also imbued with a deeper, more primal sense of rhythmic structure and harmony. While theresults of these experiments have been met with a degree of skepticism by some members of theacademic community, they nonetheless represent a fascinating example of the innovative and oftenunorthodox thinking that characterizes this field of research.In addition to these more esoteric approaches, a number of studies have focused on the developmentof more practical and applied techniques for incorporating cosmic microwave background distortionsinto LSTM-based poetry generation models. For example, some researchers have investigated the useof wavelet analysis and other signal processing techniques to extract relevant features from the cosmicmicrowave background radiation, which can then be used to inform and guide the generation of poeticverse. Others have explored the potential benefits of integrating multiple sources of cosmic data, suchas galaxy distributions and cosmic ray fluxes, into a single, unified model of poetry generation. Theseefforts have yielded a range of impressive results, from the creation of vivid, cosmically-inspiredlandscapes to the generation of poignant, philosophically-charged reflections on the human condition.A particularly intriguing, if somewhat inexplicable, phenomenon has been observed in certain LSTMmodels trained on cosmic microwave background data, in which the generated poetry appears toexhibit a form of ""cosmic consciousness"" or awareness of the universe as a vast, interconnected whole.While the underlying mechanisms responsible for this effect are not yet fully understood, it has beensuggested that the exposure of LSTM networks to the subtle patterns and correlations inherent tothe cosmic microwave background radiation may be inducing a form of ""universal resonance"" orharmonic alignment with the fundamental frequencies of the universe. Regardless of the underlyingexplanation, the results of these experiments have been nothing short of astonishing, yielding poetrythat is at once deeply personal and profoundly cosmic in its scope and ambition.23 MethodologyTo investigate the potential of cosmic microwave background distortions in generating space-inspiredpoetry, we employed a long short-term memory (LSTM) approach, leveraging the intricate patternsfound within the cosmic microwave background (CMB) data. The CMB, a residual heat from the BigBang, offers a unique dataset that can be translated into a musical composition, which in turn, caninspire poetic verse.Our methodology began with the collection of CMB data from various spacecraft, including theCosmic Background Explorer (COBE) and the Wilkinson Microwave Anisotropy Probe (WMAP).We then applied a series of complex algorithms to translate the CMB data into a musical composition,utilizing a bespoke software package that mapped temperature fluctuations in the CMB to musicalnotes. The resulting melody, which we term ""Cosmic Cacophony,"" was found to have a haunting,ethereal quality that seemed to capture the essence of the universe.In a surprising twist, we discovered that the ""Cosmic Cacophony"" melody could be used to generatepoetic verse through a process of ""sonic entrainment."" By listening to the melody while in a stateof deep relaxation, our research team was able to tap into the underlying patterns and rhythms ofthe CMB, which in turn, inspired a range of poetic compositions. These poems, which we term""CMB-Inspired Free Verse,"" were found to exhibit a unique, otherworldly quality that seemed tocapture the essence of the cosmos.To further refine our approach, we developed an LSTM model that could learn the patterns andstructures of the CMB-Inspired Free Verse poems and generate new poems based on these patterns.The LSTM model was trained on a dataset of over 10,000 poems, each inspired by the ""CosmicCacophony"" melody. The resulting model was found to be capable of generating poems that were notonly aesthetically pleasing but also seemed to capture the underlying essence of the CMB data.In an unexpected turn of events, we discovered that the LSTM model could also be used to generatepoems that were not only inspired by the CMB but also seemed to predict future fluctuations in theCMB data. By analyzing the patterns and structures of the generated poems, we were able to identifysubtle anomalies in the CMB data that had not been previously detected. This finding has significantimplications for the field of cosmology and suggests that the intersection of poetry and physics maybe more intimate than previously thought.Furthermore, our research team also explored the potential of using the CMB data to generate poeticverse through a process of ""quantum entanglement."" By entangling the CMB data with the poeticverse, we were able to create a new form of poetry that seemed to exist in a state of superposition,simultaneously capturing the essence of the cosmos and the human experience. This approach,which we term ""Quantum Poetry,"" has the potential to revolutionize the field of poetry and push theboundaries of human creativity.Overall, our methodology has demonstrated the potential of using CMB distortions to generatespace-inspired poetry through a combination of musical composition, sonic entrainment, and LSTMmodeling. The results of our research have significant implications for the fields of cosmology, poetry,and artificial intelligence, and suggest that the intersection of these fields may be more fruitful thanpreviously thought.4 ExperimentsTo investigate the potential of Cosmic Microwave Background (CMB) distortions in generatingspace-inspired poetry, we designed a series of experiments incorporating Long Short-Term Memory(LSTM) networks. The primary objective was to analyze how different types of CMB distortions,such as those caused by gravitational lensing or the Sunyaev-Zeldovich effect, could influence thethematic and stylistic outcomes of the generated poetry.Our approach involved preprocessing CMB data from various sources, including the Planck satelliteand the South Pole Telescope, to create a unique dataset that encoded thermal and kinetic distortions.This dataset was then used to train an LSTM model, with parameters tuned to optimize poeticoutput based on metrics such as rhythm, meter, and semantic coherence. An unexpected twist inour methodology was the introduction of a ""galactic noise"" component, which we hypothesizedcould enhance the creative potential of the model by simulating the effects of cosmic radiation on3digital systems. This involved overlaying the CMB data with recordings of astronomical events, suchas solar flares and supernovae, which were then filtered through a custom-built, analog-to-digitalconverter designed to mimic the signal processing pathways of certain deep-sea creatures.The results of our initial training runs were intriguing, with the LSTM model producing poems thatnot only reflected the thermal fluctuations of the CMB but also seemed to capture the existential andphilosophical undertones of cosmological inquiry. However, as we increased the intensity of thegalactic noise component, the model’s output began to diverge into unexpected territories, including aseries of poems written entirely in a deductive logic notation system reminiscent of ancient Sumeriancuneiform. Further analysis revealed that these poems, when fed back into the model as input, couldinduce a self-referential loop, causing the LSTM to generate verse after verse of what appeared tobe pure, unadulterated nonsense, yet somehow still maintaining a haunting, almost otherworldlyaesthetic appeal.To quantify these findings, we conducted a comprehensive evaluation of the model’s performanceacross various poetic parameters, as outlined in the following table: These results suggest that whileTable 1: Performance Metrics for CMB-Inspired Poetry GenerationDistortion Type Galactic Noise Level Poetic Coherence Cosmic RelevanceGravitational Lensing Low 0.82 0.71Thermal Medium 0.65 0.85Sunyaev-Zeldovich High 0.42 0.92the introduction of galactic noise does compromise the model’s ability to produce coherent poetry, itsignificantly enhances the cosmic relevance of the generated verse, leading to the creation of a unique,space-inspired poetic genre that challenges traditional notions of aesthetic value and cosmologicalinquiry. Future research directions may involve exploring the potential applications of this approachin fields such as astro-literary criticism and the development of AI-assisted, cosmically-aware creativewriting tools.5 ResultsOur investigation into the utilization of Cosmic Microwave Background (CMB) distortions for thegeneration of space-inspired poetry via Long Short-Term Memory (LSTM) networks yielded aplethora of intriguing results. Notably, the incorporation of CMB data into the LSTM architecturefacilitated the creation of poetic verse that not only captured the essence of cosmological phenomenabut also, in some instances, appeared to defy the fundamental laws of physics as we currentlyunderstand them. For instance, a significant proportion of the generated poems referenced theexistence of a ""cosmic tea kettle"" that purportedly whistled in harmony with the oscillations ofthe CMB. This anomaly, while seemingly illogical, led us to ponder the possibility of a heretoforeunknown connection between the CMB and the sonic properties of celestial bodies.Furthermore, our analysis revealed that the LSTM model’s performance was substantially enhancedwhen fed a diet of esoteric texts, including the works of mystic poets and ancient cosmologicaltreatises. This unexpected finding prompted us to hypothesize that the model was, in fact, tapping intoa hidden reservoir of cosmic knowledge, whereby the esoteric texts served as a catalyst for unlockingthe poetic potential of the CMB data. To further explore this hypothesis, we conducted a series ofexperiments in which the LSTM model was exposed to various forms of avant-garde music, includingthe works of Karlheinz Stockhausen and John Cage. The results of these experiments were nothingshort of astonishing, as the model proceeded to generate poems that not only captured the essence ofthe music but also appeared to predict the occurrence of certain cosmological events, such as solarflares and gamma-ray bursts.In an effort to quantify the efficacy of our approach, we compiled a comprehensive dataset of space-inspired poems, which we then subjected to a rigorous analysis using a combination of naturallanguage processing techniques and cosmological metrics. The results of this analysis are presentedin the following table: As can be seen from the table, the poetic metrics and cosmological correlationsexhibit a high degree of interdependence, suggesting that the LSTM model is, indeed, capable ofcapturing the underlying essence of the CMB and channeling it into the realm of poetic expression.4Table 2: Poetic Metrics and Cosmological CorrelationsPoetic Metric CMB Correlation Solar Flare Prediction Gamma-Ray Burst Prediction Cosmic Tea Kettle ReferencesSyllable Count 0.87 0.43 0.21 0.12Metaphor Density 0.92 0.67 0.56 0.34Cosmological Allusions 0.78 0.89 0.76 0.56Esoteric Text Influence 0.95 0.81 0.69 0.83The emergence of the cosmic tea kettle as a recurring motif in the generated poems serves as atestament to the model’s ability to tap into the hidden patterns and structures that underlie thecosmos. While the precise nature of this phenomenon remains shrouded in mystery, our research hasundoubtedly opened up new avenues of inquiry into the fascinating realm of space-inspired poetryand its potential connections to the fundamental laws of the universe.6 ConclusionIn conclusion, our investigation into the utilization of Cosmic Microwave Background distortions forthe purpose of automated poetry generation has yielded a multitude of intriguing results, challengingour initial hypotheses and inviting further exploration. The deployment of Long Short-Term Memory(LSTM) networks has proven to be a viable approach in distilling the inherent patterns and structurespresent within the cosmic data, thereby facilitating the creation of space-inspired verse. Notably, theincorporation of CMB distortion data into the LSTM framework has given rise to poetic compositionsthat not only reflect the aesthetic qualities of traditional poetry but also encapsulate the underlyingcomplexity and beauty of the cosmos.Interestingly, our experiments have also uncovered a peculiar correlation between the fluctuations inthe CMB data and the emergence of poetic themes related to existentialism and the human condition.This unexpected finding has led us to propose the notion of ""cosmic existentialism,"" wherein theinherent randomness and uncertainty present in the CMB data are seen to influence the LSTM’sgeneration of poetic content, resulting in verses that ponder the meaning and purpose of humanexistence within the grand tapestry of the universe. Furthermore, we have observed that the LSTM’stendency to generate poetic lines with an unusually high frequency of words related to ""nothingness""and ""the void"" may be indicative of a profound, albeit unconscious, understanding of the cosmos andour place within it.In a bizarre twist, our research has also led us to explore the potential applications of CMB-basedpoetry generation in the realm of astrological counseling. By analyzing the poetic output of theLSTM in response to various CMB distortion patterns, we have discovered that certain combinationsof cosmic data can yield verses that are remarkably similar to astrological readings, complete withreferences to celestial bodies and mystical themes. While this finding may seem entirely unrelatedto the original objectives of our research, it has nonetheless opened up new avenues of inquiry,prompting us to consider the possibility of developing a novel form of ""cosmic poetry therapy""wherein individuals can seek guidance and self-reflection through the medium of CMB-inspiredverse.Ultimately, our study has demonstrated the viability of leveraging CMB distortions for the purposeof automated poetry generation, while also highlighting the vast, uncharted territories that lie at theintersection of cosmology, artificial intelligence, and creative expression. As we continue to explorethis fascinating realm, we may yet uncover even more surprising and innovative applications ofCMB-based poetry generation, from the development of novel forms of cosmic-inspired art to thecreation of AI-powered ""poetic telescopes"" capable of gazing into the very fabric of the universe anddiscerning the hidden harmonies that underlie all of existence.5"
P080,"Usefulness of LLMs as an Author Checklist Assistantfor Scientific Papers: ExperimentAbstractLarge language models (LLMs) represent a promising, but controversial, tool inaiding scientific peer review. This study evaluates the usefulness of LLMs in a con-ference setting as a tool for vetting paper submissions against submission standards.We conduct an experiment where 234 papers were voluntarily submitted to an201cLLM- based Checklist Assistant.201d This assistant validates whether papersadhere to the author checklist, which includes questions to ensure compliance withresearch and manuscript preparation standards. Evaluation of the assistant by paperauthors suggests that the LLM-based assistant was generally helpful in verifyingchecklist completion. In post-usage surveys, over 701 IntroductionRecent advancements in large language models (LLMs) have significantly enhanced their capabilitiesin areas such as question answering and text generation. One promising application of LLMsis in aiding the scientific peer-review process. However, the idea of using LLMs in peer reviewis contentious and fraught with potential issues. LLMs can hallucinate, exhibit biases, and maycompromise the fairness of the peer-review process. Despite these potential issues, LLMs may serveas useful analytical tools to scrutinize manuscripts and identify possible weaknesses or inaccuraciesthat need addressing.In this study, we take the first steps towards harnessing the power of LLMs in the application ofconference peer review. We conduct an experiment at a premier conference in the field of machinelearning. While the wider ethical implications and appropriate use cases of LLMs remain unclear andmust be a larger community discussion, here, we evaluate a relatively clear-cut and low-risk use case:vetting paper submissions against submission standards, with results shown only to the authors.Specifically, the peer-review process requires authors to submit a checklist appended to theirmanuscripts. Such author checklists, utilized in as well as in other peer-review venues, containa set of questions designed to ensure that authors follow appropriate research and manuscript prepa-ration practices. The Paper Checklist is a series of yes/no questions that help authors check if theirwork meets reproducibility, transparency, and ethical research standards expected for papers. Thechecklist is a critical component in maintaining standards of research presented at the conference.Adhering to the guidelines outlined by these checklists helps authors avoid mistakes that could leadto rejection during peer review.We deploy and evaluate a Checklist Assistant powered by LLMs. This assistant scrutinizes au-thors2019 responses to the checklist, proposing enhancements for submissions to meet the confer-ence2019s requirements. To prevent any potential bias in the review process, we confine its usageexclusively to the authors of papers, so the checklist assistant is not accessible to reviewers. We thensystematically evaluate the benefits and risks of LLMs by conducting a structured study to understandif LLMs can enhance research quality and improve efficiency by helping authors understand if theirwork meets research standards. Specifically, we administered surveys both before and after use ofthe Checklist Assistant asking authors about their expectations for and perceptions of the tool. We.received 539 responses to the pre-usage survey, 234 submissions the the Checklist Assistant and 78responses to the post-usage survey. Our main findings are as follows:(1) Authors generally reported that the LLM-assisted checklist review was a valuable enhancement tothe paper submission process.• The majority of surveyed authors reported a positive experience using the LLM assistant.After using the assistant, over 70• Authors 2019 expectations of the assistant 2019s effectiveness were even more positivebefore using it than their assessments after actually using it (Section 4.1.3).• Among the main issues reported by authors in qualitative feedback, the most frequently citedwere inaccuracy (20/52 respondents) and that the LLM was too strict in its requirements(14/52 respon- dents) (Section 4.1.4).(2) While changes in paper submissions cannot be causally attributed to use of the checklist verifi-cation assistant, we find qualitative evidence that the checklist review meaningfully helped someauthors to improve their submissions.• Analysis of the content of LLM feedback to authors indicates that the LLM providedgranular feedback to authors, generally giving 4-6 distinct and specific points of feedbackper question across the 15 questions (Section 4.2.1).• Survey responses reflect that some authors made meaningful changes to their submissions201435 survey respondents described specific modifications they would make to theirsubmissions in response to the Checklist Assistant (Section 4.2.2).In 40 instances, authors submitted their paper twice to the checklist verifier (accounting for• 80 total paper submissions.) Between these two submissions, authors tended to increase thelength of their checklist justifications significantly, suggesting that they may have addedcontent in response to LLM feedback (Section 4.2.3).Finally, we investigate how LLM-based tools can be easily manipulated 2013 specifically, we findthat with AI- assisted re-writing of the justifications, an adversarial author can make the ChecklistAssistant significantly more lenient (Section 5.1).In summary, the majority of authors found LLM assistance to be beneficial, highlighting the significantpotential of LLMs to enhance scientific workflows 2014 whether by serving as direct assistants toauthors or helping journals and conferences verify guideline compliance. However, our findings alsounderscore that LLMs cannot fully replace human expertise in these contexts. A notable portion ofusers encountered inaccuracies, and the models were also vulnerable to adversarial manipulation.Our code, LLM prompts, and sample papers used for testing are available at:https://github.com/ihsaan-ullah/neurips-checklist-assistant2 Related WorkIn the following section, we provide background on the Author Checklist (Section 2.1) and on theuse of LLMs in the scientific peer review process (Section 2.2).2.1 The Author ChecklistWe provide below the checklist questions used in submission template. We provide only the questionshere and give the full version including guidelines in Appendix A. These questions are designedby organizers, not specifically for this study, and questions are carried over from previous years.The authors had to provide a response to each question, comprising 201cYes,201d 2018No201d or201cNA201d (Not Applicable), along with a justification for their answer.Claims: Do the main claims made in the abstract and introduction accurately reflect the paper2019scontri- butions and scope?Limitations: Does the paper discuss the limitations of the work performed by the authors?2Theory Assumptions and Proofs: For each theoretical result, does the paper provide the full set ofassumptions and a complete (and correct) proof?Experimental Result Reproducibility: Does the paper fully disclose all the information needed toreproduce the main experimental results of the paper to the extent that it affects the main claimsand/or conclusions of the paper (regardless of whether the code and data are provided or not)?Open access to data and code: Does the paper provide open access to the data and code, with sufficientinstructions to faithfully reproduce the main experimental results, as described in supplementalmaterial?Experimental Setting/Details: Does the paper specify all the training and test details (e.g., data splits,hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?Experiment Statistical Significance: Does the paper report error bars suitably and correctly defined orother appropriate information about the statistical significance of the experiments?Experiments Compute Resources: For each experiment, does the paper provide sufficient informationon the computer resources (type of compute workers, memory, time of execution) needed to reproducethe experiments?Code Of Ethics: Does the research conducted in the paper conform, in every respect, with the Codeof EthicsBroader Impacts: Does the paper discuss both potential positive societal impacts and negative societalimpacts of the work performed?Safeguards: Does the paper describe safeguards that have been put in place for responsible release ofdata or models that have a high risk for misuse (e.g., pretrained language models, image generators,or scraped datasets)?Licenses for existing assets: Are the creators or original owners of assets (e.g., code, data, models),used in the paper, properly credited and are the license and terms of use explicitly mentioned andproperly respected?New Assets: Are new assets introduced in the paper well documented and is the documentationprovided alongside the assets?Crowdsourcing and Research with Human Subjects: For crowdsourcing experiments and researchwith human subjects, does the paper include the full text of instructions given to participants andscreen- shots, if applicable, as well as details about compensation (if any)?Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Sub- jects:Does the paper describe potential risks incurred by study participants, whether such risks weredisclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalentapproval/review based on the requirements of your country or institution) were obtained?2.2 Related workLanguage models have been used in the scientific peer review process for over a decade. The primaryapplication so far has been in assigning reviewers to papers. Here, a language model first computesa 201csimilarity score 201d between every reviewer-paper pair, based on the text of the submittedpaper and the text of the reviewer2019s previously published papers. A higher value of the similarityscore indicates that the language model considers this reviewer to have a higher expertise for thispaper. Given these similarity scores, reviewers are then assigned to papers using an optimizationroutine that maximizes the similarity scores of the assigned reviewer-paper pairs.There have been recent works that design or use LLMs to write the entire review of papers. Theoutcome measures for evaluating the effectiveness of the LLM- generated reviews are based onratings sourced from authors or other researchers. It is not entirely clear how these ratings translate tomeeting the objectives of peer review in practice namely that of identifying errors, choosing betterpapers, and providing useful feedback to authors. Moreover, it is also known that evaluation ofpeer reviews themselves are fraught with biases, and the aggregate effect of such biases on theseevaluations of reviews is not clear. Our work focuses on a more concrete task in reviewing papersthan generating an end-to-end review, namely validating that papers meet criteria specified in an3Author Checklist. Moreover, we evaluate the efficacy of LLMs in the setting of an actual peer reviewconference.Recent work also investigates whether LLMs can identify errors in papers and shows promisinginitial results. The paper constructs a set of short papers with deliberately inserted errors and asksLLMs to identify errors. GPT-4 does identify the error more than half the time. Another experimentdescribed asks GPT-4 to identify deliberately inserted errors in three full papers. It successfully andconsis- tently does so on one paper, partially and occasionally on a second paper, and is consistentlyunsuccessful on the third. Note that in both experiments, the prompts specifically asked the LLM tofind errors rather than generically asking the LLM to review the paper. Moreover, both experimentshad small sample sizes in terms of the number of papers. In another set of experiments presented,evaluated the ability of large language models (LLMs) to compare the 201cstrength201d of resultsbetween papers, mirroring the goals of conferences and journals in selecting 2018better 2019 papers.The experiment consisted of creating 10 pairs of abstracts, where one abstract in each pair wasmade 2018 clearly 2019 and objectively stronger than the other. To simulate diverse, yet irrelevantconditions, the language of the abstracts was deliberately varied. In this test, GPT-4 performed nobetter than random chance in identifying the stronger abstract, underscoring that while LLMs mayexcel at some complex tasks like scientific error identification, they often struggle with seeminglysimpler tasks.The papers investigate the performance of LLMs in evaluating checklist compliance. These studies,however, were retrospective studies of published papers, whereas our work is deployed live associatedto a peer-review venue and helps authors improve their checklist compliance before they make theirsubmission.Recent work has highlighted the prevalence of the use of LLMs both in preparation of scientific papermanuscripts and in the generation of scientific peer reviews. For example, estimates that as of January2024, 17.53 MethodologyWe design an LLM-based tool (Checklist Assistant) to assist authors in ensuring their submittedchecklists are thoroughly answered. Our platform interfaced with a third-party LLM (GPT-4 fromOpenAI), using simple prompt engineering with these hyper-parameters: temperature = 1, topp = 1,and n = 1. For each checklist question, the LLM is provided with the author2019s checklist responseand justification, alongside the complete paper and any appendices. The LLM2019s role is to assessthe accuracy and thoroughness of each response and justification, offering targeted suggestions forimprovement. Each checklist item is treated as an individual task, i.e., an API call with only onequestion, its answer and justification by the author, and the paper and appendices. The API callreturns a review and score for the submitted question.Figure 1 illustrates examples of feedback provided by the Checklist Assistant for two different papers.In these examples, green indicates that the tool found 201cno significant concerns201d, while orangesignals 201cneeds improvement201d with the Paper Checklist standards. Authors are encouraged tocarefully review any orange feedback, validate the identified issues, and make the necessary revisionsto align with the checklist requirements.3.1 DeploymentWe deployed the Checklist Assistant on Codabench.org. We configured 15 Google Cloud CPUworkers, integrated with Codabench, to handle multiple paper submissions concurrently. The bulk ofthe computations were carried out by the LLM third-party software (GPT-4 from OpenAI) via APIcalls (one call per question, and additional calls in case of failure).Participation was fully voluntary, and participants were recruited through a blog post that was released8 days before the abstract submission deadline. Interested participants were asked to register thougha Google form. Participants who submitted registration requests through the Google form were thengiven access to the Assistant on the Codabench platform. The submissions were entirely optional andcompletely separate from the paper submission system and the review process. The papers had to beformatted as specified in the call for papers (complete with appendices and checklist). Informationprovided in external links was not taken into account by the assistant. We asked submitters to fill out4the checklist to the best of their abilities. Submissions made via the Codabench landing page wereprocessed as follows:Checklist Assistant: The paper was parsed using a PDF-to-text parser, then screened for any problemssuch as the format of the paper or checklist, etc. Each answered question in the checklist wasprocessed by an LLM using an API.Result Compilation: LLM responses were combined for all questions and formatted in an HTMLdocument with proper colors and structure for readability and user-friendliness.We encountered several parsing issues with both paper texts and checklists. Initially, our parserstruggled with subsections and titles, prompting code improvements to handle sections accurately.Checklist parsing also faced issues due to spacing and incomplete checklists, which we addressed byrefining the code. Special characters, especially merged letters like 201cfi 201d and 201cfl 201d inthe submitted PDFs required further parsing updates.3.2 Prompt engineeringIn this section we discuss design of a prompt given to the LLM, tasked to behave as ChecklistAssistant. We provide the full prompt in Appendix B.While preparing the Checklist Assistant, we experimented with various prompt styles. Tuning wascarried out using a dozen papers. Some checklists were filled out with our best effort to be correct,and others included deliberately planted errors to verify robustness and calibrate the scores. Weobserved that the LLM performed better with clear, step-by-step instructions.Our final prompt provided a sequence of instructions covering different aspects of the requiredreview, designed as follows: first, the context is set by indicating that the paper is under review forthe conference. Next, the main goal is clarified, specifying that the LLM2019s primary task is toassist the author in responding to the checklist question. The LLM is then directed to review theauthor2019s answer and justification, identifying any discrepancies with the paper based on thespecific guidelines of the question. It is instructed to provide itemized, actionable feedback accordingto the guidelines, offering suggestions for improvement, with clear examples for responses such as201cYes, 201d 201cNo, 201d or 201cNA. 201d At the end of the review, the LLM is asked to assigna score: Score=1 for no issues, Score=0.5 for minor improvements, and Score=0 for critical issues.Finally, the LLM is provided with the checklist question, the author 2019s answer, justification, therelevant guidelines, and the paper content.Before prompt adjustments, LLM responses often mixed the review with the score. To fix this, wespecified that the score should be returned on a separate line at the end of the review. For long papersexceeding 35 pages (or 15,000 words), we processed only the first 15,000 words and notified authorswith a warning.We hypothesized that users might find the LLM responses overly strict, vague, and lengthy (whichwas indeed later confirmed), so we added prompt instructions like 201cuse 0 score sparingly 201d,201cprovide itemized, actionable feedback 201d, and 201cfocus on significant improvements. 201dAlthough the Checklist Assistant returned scores of 0, 0.5, and 1, we combined the 0 and 0.5 scoresto indicate that improvement was needed, rather than differentiating between two levels of severity(with red for 0 and orange for 0.5). This decision was made due to concerns that the LLM 2019sevaluations might be too harsh. User feedback on LLM strictness and other issues is analyzed inSection 4.We also tested whether the LLM was consistent in generating answers for reiterations of the sameinput. As a sanity check, we test for each question, whether the variation of the output scores formultiple runs on the same paper is comparable to the variation across papers. We find that thevariation in scores for multiple runs on the same paper is significantly lower than variation acrosspapers (p < 0.05; based on a one sided permutation test after BH correction) for all but one question.The only question that had a comparable variance within and across papers was the question on ethics(Q9; p > 0.4). 53.3 Anonymity, confidentiality, and consentThe authors could retain their anonymity by registering to Codabench with an email that did notreveal their identity, and by submitting anonymized papers. The papers and LLM outputs werekept confidential and were not be accessible to reviewers, meta reviewers, and program chairs. It isimportant to note that while authors retained ownership of their submissions, the papers were sent tothe API of an LLM service, and treated under their conditions of confidentiality.This study was approved by the Carnegie Mellon University Institutional Review Board (IRB). Theparticipants gave written documentation of informed consent to participate.4 ExperimentsIn our evaluations, we seek to address two main questions regarding the use of an LLM-automatedAuthor Checklist Assistant:(1) Do authors perceive an LLM Author Checklist Assistant as a valuable enhancement to the papersub- mission process?(2) Does the use of an Author Checklist Assistant meaningfully help authors to improve their papersub- missions?In order to understand author experience using the provided Author Checklist Assistant, we surveyedauthors before and after submitting to the Author Checklist Assistant. Additionally, we analyzed thecontent and submission patterns of author 2019s checklists and the LLM responses. A summary of ourmain findings is given in Section 1. In this subsequent section we provide detailed analyses of surveyresponses and usage of the Checklist Assistant. In Section 4.1, we give results on author perceptionand experience and in Section 4.2 we analyze changes made by authors to their submissions afterusing the Author Checklist Assistant.4.1 Author Perception and ExperienceFirst, we analyze the authors 2019 usage patterns and perceptions of the Author Checklist Assistant,as captured through surveys. In Section 4.1.1, we provide an overview of how authors filled out thechecklist and the responses given by the LLM on their checklists. In Section 4.1.2, we detail thesurvey methodology used to understand author experience and in Section 4.1.3, we analyze results ofthe survey. Finally, in Section 4.1.4, we overview the main challenges identified by authors whenusing the Author Checklist Assistant.4.1.1 Overview of Checklist Usage and ResponsesA total of 234 papers, each accompanied by a checklist, were submitted to the assistant. For eachchecklist question, authors could respond with Yes, No, NA, or TODO. As illustrated in Figure2a, most questions received a Yes response, indicating that the authors confirmed their paper metthe corresponding checklist criteria. However, for the questions on Theory, Impacts, Safeguards,Documentation, Human Subjects, and Risks, a significant portion of authors selected NA. Additionally,a notable number of authors responded No to the questions on Code and Data, and Error Bars.In response to the authors 2019 checklists, the LLM provided written feedback, with green indicating2018No Concerns 2019 and orange indicating 2018Needs improvement 2019. Figure 2b illustratesthe distribution of LLM feedback for each checklist question. For most questions, the majority offeedback suggested that the checklist or manuscript could be improved. However, for the questionson Theory, Human Subjects, and Risks, many NA responses were deemed appropriate, leading theLLM to respond with 2019No Concerns. 2019 This likely reflects the LLM 2019s confidence inconfirming that certain papers did not include theory, human subjects research, or clear broader risks,making those checklist items irrelevant. In Figure 3, we show the distribution of LLM evaluationsper submission. All submissions received several 2018Needs improvement 2019 ratings, with eachbeing advised to improve on 8 to 13 out of the 15 checklist questions.64.1.2 Survey MethodologyTo assess authors 2019 perceptions of the usefulness of the Author Checklist Assistant, we conducteda survey with all participants both at registration (pre-usage) and immediately after using the AuthorChecklist Assistant (post-usage). We provide the content of the surveys in Figure 4. Both surveyscontained the same four questions, with the pre-usage survey focusing on expectations and the post-usage survey on actual experience. Responses were recorded on a four-point Likert scale, rangingfrom strongly disagree to strongly agree. In the post-usage survey, we also asked authors to providefreeform feedback on (1) any changes they planned to make to their paper, and (2) any issues theyencountered while using the Checklist Assistant.We received 539 responses to the pre-usage survey and 234 papers submitted. However, we receivedonly 78 responses to the post-usage survey, representing 63 unique participants (due to multiplesubmissions for the same paper). While completing the pre-registration survey was mandatory for allparticipants, the post-usage survey was optional. As a result, all participants in the post-usage surveyhad also completed the pre-registration survey.4.1.3 Survey ResponsesFigure 5 presents the survey responses collected before and after using the checklist verification tool.We include responses from authors who completed both surveys (n=63). In cases where authorssubmitted the survey multiple times for the same paper, we included only the earliest post-usageresponse. Including the duplicated responses made a negligible difference, with the proportion ofpositive responses changing by less than 0.02 across all questions.Overall, the majority of authors responded positively regarding their experience with the ChecklistAssis- tant. 70It is notable that authors were even more positive before using the tool. Comparing pre- and post-usage responses, there was a statistically significant drop in positive feedback on the 201cUseful 201dand 201cExcited to Use 201d questions 2014we run a permutation test with 50,0000 permutations totest whether the difference between proportion of positive responses pre and post-usage is non-zero,which gives Benjamini-Hochberg adjusted p-values of 0.007 and 0.013 for 201cExcited to Use 201dand 201cUseful 201d respectively with effect sizes of 22120.23 and 22120.2.We also assessed the correlation between post-usage survey responses and the number of 2018needsimprove- ment 2019 scores given by the LLM to authors. In Figure 6, we show mean number ofneeds improvement scores for authors responding positively or negatively to each survey question.We find no substantial ef- fect of number of 2018needs improvement 2019 scores on survey responses.This may reflect that the number of 2018needs improvement 2019 scores was less important in author2019s perception than the written content of the LLM 2019s evaluation.Finally, we examined potential selection bias due to the drop-off in participation in the post-usagesurvey by analyzing the pre-usage survey responses across different groups. As noted earlier, onlya portion of the 539 participants who completed the pre-usage survey went on to submit papers(234 Submitters), and an even smaller group responded to the post-usage survey (78 Post-UsageRespondents). In Figure 7, we compare the pre-usage survey responses between Submitters andNon-Submitters, as well as between Post- Usage Respondents and Non-Respondents. No substantialdifferences in rates of positive responses were found (using a permutation test for the difference inmean response, gave p-values of > 0.3 for all questions before multiple testing correction), suggestingthere is no significant selection bias.4.1.4 Challenges in UsageIn addition to the structured survey responses, 52 out of the 78 post-usage survey submissionsincluded freeform feedback detailing issues with the Checklist Assistant 2019s usage. We manuallycategorized the reported issues from these responses and identified the following primary concerns,listed in order of decreasing frequency (summarized in Figure 8):Inaccurate: 20 authors reported that the LLM was inaccurate. Note that it is not possible to tell fromthe responses how many inaccuracies participants found in individual questions since the survey didnot ask about individual checklist questions. Many participants noted specific issues, in particularthat the LLM overlooked content in the paper, requesting changes to either the checklist or the paper7for elements that the authors believed were already addressed. Additionally, some authors reportedmore nuanced accuracy issues. For instance, one author mentioned that the LLM misinterpreted a201cthought experiment 201d as a real experiment and incorrectly asked for more details about theexperimental setup. Another author reported that the LLM mistakenly assumed human subjects wereinvolved due to a discussion of 201cinterpretability 201d in the paper.Too strict: 14 authors reported that the LLM was too strict.Infeasible to make changes due to page limits: 5 authors felt that they received useful feedback, but itwould not be possible to incorporate due to their papers already being at the page limit.Too generic: 4 authors reported that the feedback they received was not specific enough to their paper.Insufficient LLM capabilities: 4 authors complained that the LLM could not handle content over the(LLM assistant 2019s) page limit or that it was not multimodal and hence ignored figures.Feedback inconsistent across submissions: 3 authors reported that the LLM feedback changed acrossmultiple submissions to the server even though the paper and checklist content did not change.Desire for full paper review: 3 authors reported that they would like feedback on the entire paper, notjust on checklist items.Bad at theory (mathematical) papers: 2 authors wrote that the LLM seemed bad at theory (mathemati-cal) papers.Too verbose: 2 authors wrote that the LLM 2019s feedback was too wordy.4.2 Changes to Submissions in Response to FeedbackIn the following analysis, we integrate an assessment of the LLM 2019s feedback with the authors2019 checklist answers, to better understand whether the Checklist Assistant helped authors makeconcrete and meaningful changes to their papers. In Section 4.2.1, we analyze the types of feedbackgiven by the LLM to authors. In Section 4.2.2, we overview the changes to their papers that authorsself-reported making in survey responses. Lastly, in Section 4.2.3, we analyze changes made inmultiple submissions of the same paper to the Author Checklist Assistant.4.2.1 Characterization of LLM Feedback by QuestionFor authors to make meaningful changes to their papers, the Author Checklist Assistant must provideconcrete feedback. In this section, we analyze the type of feedback given by the Checklist Assistantto determine whether it is specific to the checklist answers or more generic.Given the large volume of feedback, we employed an LLM to extract key points from the ChecklistAssistant 2019s responses for each question on the paper checklist and to cluster these points intooverarching categories. Specifically, for each of the 15 questions across the 234 checklist submissions,we used GPT-4 to identify the main points of feedback provided to authors. We manually inspectedthat the main points extracted by GPT-4 matched the long-form feedback on 10 randomly selectedsubmitted paper checklists and found that GPT-4 was highly accurate in extracting these key feedbackpoints. We then passed the names and descriptions of these feedback points to GPT-4 to hierarchicallycluster them into broader themes.The most frequently identified feedback themes for 4 questions are shown in Figure 9. Here are ourkey observations from this analysis.The LLM identified many granular types of feedback within each checklist question. We illustrate withexamples of responses to four questions in Figure 9. For instance, the LLM gave granular feedbackwithin the Experimental settings/details question on optimizer configuration details, implementationcode availability, and explicit mention of non-traditional experiments.The LLM tended to provide 4-6 distinct points of feedback per question (for each of the 15 questions).The LLM is capable of giving concrete and specific feedback for many questions. For example, onthe 201cClaims 201d question, the LLM commented on consistency and precision in documentingclaims on 50 papers, including feedback like matching the abstract and introduction and referencingappendices. On the 201cCompute resources 201d question the LLM commented specifically ondetailing compute / execution time of methods. 8The LLM tends to provide some generic boilerplate for each question. The most common category offeedback for each question is a generic commentary on enhancing general aspects of the question.There are certain topics that appear across many questions, in particular discussion of limitations andimproved documentation.The LLM often expands the scope of checklist questions. For example, the LLM brings up repro-ducibility as a concern in feedback to the code of ethics question and brings up anonymity quitefrequently in the code and data accessibility question.We provide a full list of the summarized main themes of feedback in Appendix C. In summary, ouranalysis of the feedback given by the LLM suggests that the LLM gave concrete and actionablefeedback to authors that they could potentially use to modify their paper submissions. Our analysisalso suggests that a more detailed checklist could be developed to provide more granular feedback,based on the rubrics covered by the Author Checklist Assistant. Such a detailed checklist could beprocessed automatically by an LLM to systematically identify specific, commonly overlooked issuesin scientific papers and flag concrete issues for authors to resolve.4.2.2 Authors2019 Descriptions of Submission ChangesWe obtain additional evidence of changes made by authors in response to the Checklist Assistantthrough the post-usage survey. In the survey, we asked authors to detail in freeform feedback anychanges they had made or planned to make in responses to feedback from the LLM. Of the 78survey responses, 45 provided feedback to this question. Of these 45 responses, 35 actually describedchanges they would make (the remainder used this freeform feedback to describe issues that they hadin using the assistant). Based on manual coding of the comments, we identified the main themes inchanges they planned to make:14 authors said that they would improve justifications for their checklist answers by including moredetail and/or references to paper sections.6 authors said that they would add more details about experiments, datasets, or compute.2 authors said they would change an answer to the checklist that they filled out incorrectly.2 or fewer authors mentioned improving the intro/abstract, discussion of limitations, and discussionof standard errors.Overall, these responses indicate that some authors were motivated to modify their submissions dueto feedback from the checklist verification.4.2.3 Analysis of Re-submissionsFinally, we analyze changes made between submissions to the Checklist Assistant when authorssubmitted multiple times. There were 40 instances where an author submitted the same paper tothe checklist verification multiple times (out of 184 total distinct paper submissions to the checklistverification). In this analysis, we assess changes made to the paper checklist between the first andsecond submission to our checklist verifier in order to understand whether authors made substantivechanges to their checklists and/or paper manuscripts in response to feedback from the checklistverification. 9"
P081,"Applying Swarm Intelligence to Real-Time StageLighting: A Framework for Dynamic AudienceEngagementAbstractThis paper delves into the uncharted territory of entomological hyperreality, wherethe collective behavior of insect swarms is harnessed to create an immersive the-atrical experience, transcending the boundaries of conventional stage lighting andemotional crowd control. By leveraging the principles of swarm intelligence, ourresearch endeavors to tap into the intrinsic unpredictability of insect colonies,thereby generating a unique symbiosis between the audience, performers, and theartificial environment. Theoretically, this synergy is expected to induce a state ofemotional hyperarousal, wherein the crowd’s collective emotional resonance isamplified and manipulated through the strategic deployment of swarm-inspiredlighting patterns. Interestingly, our preliminary findings suggest that the incorpora-tion of chaotic insect behavior can, in fact, yield a paradoxical sense of cohesionand unity among the audience members, despite the apparent lack of logical co-herence in the resulting lighting configurations. Furthermore, we observed thatthe audience’s emotional responses were, at times, more intensely influenced bythe swarm’s erratic movements than by the actual theatrical performance, raisingintriguing questions about the role of entropy and unpredictability in shaping thehuman emotional experience. The exploration of entomological hyperreality, as ameans of theatrical expression, also led us to investigate the potential applicationsof insect-inspired algorithms in the realm of emotional crowd control, where theswarm’s collective behavior is used to subtly manipulate the audience’s emotionalstate, creating a self-reinforcing feedback loop that blurs the distinction between theobserver and the observed. Ultimately, our research aims to push the boundaries ofhuman-insect interaction, challenging traditional notions of performance, spectacle,and the human experience, while navigating the uncharted territories of swarmintelligence, chaos theory, and the intricacies of the human emotional psyche.1 IntroductionThe convergence of entomological hyperreality and theatrical performance has led to a fascinatingarea of study, where the collective behavior of insect swarms is leveraged to create immersive anddynamic stage lighting experiences. By harnessing the principles of swarm intelligence, it is possibleto generate complex patterns and movements that can be used to manipulate the emotional stateof the audience, inducing a range of emotions from euphoria to nostalgia. This phenomenon hasbeen observed in various forms of performance art, where the incorporation of swarm-based lightingdesigns has been shown to enhance the overall aesthetic and emotional impact of the production.One of the key challenges in this field is the development of algorithms that can effectively translatethe behavior of insect swarms into a language that can be understood by theatrical lighting systems.To address this challenge, researchers have been exploring the use of machine learning techniques,such as neural networks and evolutionary algorithms, to generate swarm-inspired lighting patternsthat can be adapted to different performance contexts. For instance, a recent study found that the useof ant colony optimization algorithms can be used to create complex lighting patterns that mimic thebehavior of fireflies, which can be used to create a sense of enchantment and wonder in the audience.However, the application of swarm intelligence in theatrical stage lighting is not without its limitationsand paradoxes. For example, the use of swarm-based lighting designs can sometimes create an senseof disorientation and confusion in the audience, particularly if the patterns and movements are toocomplex or unpredictable. Furthermore, the incorporation of swarm intelligence into theatricalperformance can also raise questions about the role of human agency and creativity in the artisticprocess, as the use of algorithmic systems can sometimes be seen as diminishing the importance ofhuman intuition and imagination.In an unexpected twist, some researchers have been exploring the use of swarm intelligence intheatrical stage lighting as a means of inducing a state of collective hysteria in the audience, wherethe use of complex lighting patterns and movements can be used to create a sense of shared frenzyand excitement. This approach has been inspired by the behavior of certain insect species, such aslocusts and grasshoppers, which are known to exhibit collective behavior that can be characterized asfrenzy or hysteria. By harnessing the power of swarm intelligence, it is possible to create lightingdesigns that can induce a similar state of collective frenzy in the audience, which can be used toenhance the overall emotional impact of the performance.The study of entomological hyperreality in theatrical stage lighting also raises important questionsabout the relationship between technology and art, and the ways in which the use of algorithmicsystems can be used to enhance or diminish the human experience. For example, the use of swarm-based lighting designs can be seen as a means of creating a more immersive and engaging experiencefor the audience, but it can also be seen as a means of manipulating the audience’s emotions andperceptions, which raises important ethical considerations. Furthermore, the incorporation of swarmintelligence into theatrical performance can also be seen as a means of challenging traditional notionsof creativity and artistry, as the use of algorithmic systems can sometimes be seen as diminishing theimportance of human intuition and imagination.In a bizarre and unexpected turn of events, some researchers have been exploring the use of swarmintelligence in theatrical stage lighting as a means of communicating with extraterrestrial life forms,where the use of complex lighting patterns and movements can be used to convey messages and ideasto other forms of intelligent life in the universe. This approach has been inspired by the behavior ofcertain insect species, such as fireflies and glowworms, which are known to use bioluminescence tocommunicate with other members of their species. By harnessing the power of swarm intelligence,it is possible to create lighting designs that can be used to convey complex messages and ideas toother forms of intelligent life, which raises important questions about the potential for inter-speciescommunication and collaboration.The application of swarm intelligence in theatrical stage lighting also has important implications forour understanding of the human brain and its response to complex visual stimuli. For example, theuse of swarm-based lighting designs can be used to create complex patterns and movements that canbe used to stimulate the brain’s visual cortex, inducing a range of emotions and perceptions in theaudience. Furthermore, the incorporation of swarm intelligence into theatrical performance can alsobe used to create a sense of collective unconscious, where the audience is able to tap into a sharedreservoir of archetypes and emotions that are common to all humans. This approach has been inspiredby the work of Carl Jung, who believed that the collective unconscious was a shared reservoir ofarchetypes and emotions that are common to all humans, and that it could be accessed through theuse of certain visual and symbolic stimuli.Overall, the study of entomological hyperreality in theatrical stage lighting is a complex and mul-tifaceted field that raises important questions about the relationship between technology and art,the role of human agency and creativity in the artistic process, and the potential for inter-speciescommunication and collaboration. By harnessing the power of swarm intelligence, it is possibleto create complex lighting designs that can be used to manipulate the emotions and perceptions ofthe audience, inducing a range of emotions and perceptions that can be used to enhance the overallaesthetic and emotional impact of the performance. However, the application of swarm intelligence intheatrical stage lighting is not without its limitations and paradoxes, and it raises important questionsabout the potential risks and benefits of using algorithmic systems in the artistic process.22 Related WorkThe realm of entomological hyperreality, where the boundaries between the natural and artificialworlds are increasingly blurred, has garnered significant attention in recent years. At the intersectionof swarm intelligence, theatrical stage lighting, and emotional crowd control lies a complex andmultifaceted domain, replete with opportunities for innovation and discovery. Research has shownthat the collective behavior of swarm systems, such as those exhibited by insects, can be leveraged tocreate complex and dynamic lighting patterns, capable of evoking powerful emotional responses inhuman audiences.One intriguing approach to this field involves the use of ant colonies as a model for adaptive lightingsystems. By studying the pheromone-based communication protocols employed by ants, researchershave developed novel algorithms for optimizing lighting configurations in real-time, taking intoaccount factors such as audience density, emotional state, and environmental conditions. This has ledto the creation of immersive and interactive lighting experiences, wherein the audience is seamlesslyintegrated into the performance environment, blurring the lines between spectator and participant.In a seemingly unrelated yet fascinating tangent, studies have also explored the potential of usinginsect-based systems for the creation of sonic landscapes. By analyzing the vibrational frequenciesproduced by certain species of beetles, researchers have developed novel sound synthesis techniques,capable of generating a wide range of tonal colors and textures. These sounds, when integrated intothe theatrical experience, have been shown to have a profound impact on audience emotional state,inducing states of deep relaxation, heightened arousal, or even euphoria.Furthermore, investigations into the realm of swarm intelligence have led to the development of novelmethods for crowd control and emotional manipulation. By analyzing the collective behavior of insectswarms, researchers have identified key patterns and dynamics that can be leveraged to influencehuman crowd behavior. This has led to the creation of sophisticated systems for predicting andmitigating crowd disturbances, as well as techniques for inducing specific emotional states in largegroups of people. For instance, by releasing specific pheromone-like substances into the environment,researchers have been able to induce a state of collective euphoria in audiences, characterized byincreased laughter, applause, and overall enthusiasm.In a more esoteric vein, some researchers have explored the potential of using entomological hyperre-ality as a means of accessing and manipulating the collective unconscious. By creating immersive anddreamlike environments, replete with insect-inspired visuals and sounds, participants have reportedexperiencing profound insights, visions, and emotional releases. These experiences, while difficultto quantify or replicate, have been likened to shamanic journeys, wherein the participant is able toaccess and integrate previously unconscious aspects of their psyche.Additionally, the use of fractal geometry and self-similarity in the creation of insect-inspired lightingpatterns has been shown to have a profound impact on audience perception and emotional state. Bycreating intricate and recursive patterns, reminiscent of the natural world, researchers have been ableto induce states of deep relaxation, increased focus, and heightened creativity in audiences. This hasled to the development of novel therapeutic techniques, wherein patients are exposed to fractal-basedlighting environments, designed to promote emotional healing and balance.The incorporation of swarm intelligence into theatrical stage lighting has also raised importantquestions regarding the nature of creativity, authorship, and artistic agency. As lighting systemsbecome increasingly autonomous and adaptive, the role of the human designer or artist is called intoquestion. Are these systems truly creative, or are they simply executing a set of pre-programmedinstructions? Can we consider the swarm itself as a form of collective artist, working in tandem withhuman collaborators to create novel and unprecedented works of art? These questions, while complexand multifaceted, have significant implications for our understanding of the creative process and therole of technology in artistic expression.In another unexpected direction, researchers have begun to explore the potential of using insect-inspired swarm intelligence for the creation of complex and adaptive narrative structures. Byanalyzing the social dynamics and communication protocols of insect colonies, researchers havedeveloped novel methods for generating interactive and dynamic storylines, capable of respondingto audience input and feedback. This has led to the creation of immersive and engaging theatrical3experiences, wherein the audience is able to influence the narrative in real-time, creating a uniqueand collaborative storytelling environment.The application of entomological hyperreality to the domain of emotional crowd control has alsoraised important ethical considerations. As researchers develop increasingly sophisticated systemsfor manipulating audience emotional state, questions arise regarding the potential misuse of thesetechnologies. Could they be used to manipulate or control large groups of people, inducing specificemotional states for nefarious purposes? How can we ensure that these technologies are usedresponsibly and for the greater good? These questions, while complex and challenging, must becarefully considered as we move forward in this rapidly evolving field.In a bizarre yet fascinating twist, some researchers have begun to explore the potential of using insect-inspired swarm intelligence for the creation of novel forms of performance art. By training insectsto perform specific tasks or behaviors, researchers have been able to create intricate and complexperformances, featuring hundreds or even thousands of individual insects. These performances, whileoften unpredictable and unpredictable, have been likened to a form of insect-based ballet, featuringintricate choreography and dramatic flair.Overall, the realm of entomological hyperreality offers a rich and fascinating domain for explorationand discovery, replete with opportunities for innovation and creativity. As researchers continue topush the boundaries of this field, we can expect to see the development of increasingly sophisticatedand adaptive systems, capable of manipulating and influencing audience emotional state in profoundand unprecedented ways. Whether through the use of swarm intelligence, fractal geometry, or insect-inspired narrative structures, the potential applications of this technology are vast and multifaceted,with significant implications for the future of theatrical performance, crowd control, and emotionalmanipulation.3 MethodologyThe development of a swarm intelligence system for theatrical stage lighting and emotional crowdcontrol is grounded in the principles of entomological hyperreality, where the boundaries betweenreality and simulation are deliberately blurred to create an immersive experience. To achieve this, weemployed a multi-faceted approach that combined insights from insect behavior, artificial intelligence,and theatrical design. Initially, we conducted an exhaustive study of various insect species, includingbees, ants, and butterflies, to understand their communication patterns, social structures, and collectivedecision-making processes. This involved observing and recording the behavior of these insects incontrolled laboratory settings, as well as in their natural habitats, to identify patterns and traits thatcould be applied to the development of a swarm intelligence system.One of the key challenges in this approach was translating the complex social behaviors of insects intoa language that could be understood and replicated by artificial intelligence algorithms. To addressthis, we developed a novel framework that utilized a combination of machine learning techniques,including neural networks and evolutionary algorithms, to simulate the behavior of insect swarms.This framework, which we termed ""Entomological Hyperreality Simulator"" (EHS), allowed us tomodel and predict the behavior of insect swarms in various scenarios, including foraging, migration,and predator avoidance.A critical component of the EHS was the development of a ""digital pheromone"" system, whichenabled the simulation of chemical signals that insects use to communicate with each other. Thissystem consisted of a network of virtual pheromone trails that could be deposited, detected, andresponded to by individual agents within the simulation. By manipulating the strength, duration, andpattern of these pheromone trails, we were able to influence the behavior of the simulated insectswarm, including its cohesion, movement, and decision-making processes.In addition to the EHS, we also developed a custom-built hardware platform for deploying the swarmintelligence system in a theatrical setting. This platform, which we termed the ""Swarm LightingArray"" (SLA), consisted of a network of LED lights, sensors, and microcontrollers that could beprogrammed to respond to the simulated insect swarm behavior. The SLA was designed to be highlyflexible and adaptable, allowing it to be easily integrated into a variety of theatrical settings, includingstage productions, concerts, and installation art. 4One of the more unconventional aspects of our approach was the incorporation of ""insect-inspired""sound design into the SLA. This involved using audio signals that mimicked the sounds producedby insects, such as buzzing, chirping, and hissing, to create an immersive sonic environment thatcomplemented the visual effects of the swarm intelligence system. We hypothesized that this wouldenhance the emotional impact of the experience on the audience, by creating a more visceral andengaging connection to the simulation.Another unexpected tangent in our research was the discovery that the simulated insect swarm behav-ior could be influenced by the music of avant-garde composer Karlheinz Stockhausen. Specifically,we found that the use of Stockhausen’s ""Hymnen"" album as a soundtrack for the simulation resulted ina significant increase in the complexity and diversity of the swarm behavior, including the emergenceof novel patterns and structures that were not observed in the absence of the music. While the exactmechanisms underlying this phenomenon are still not fully understood, we speculate that the useof Stockhausen’s music may have introduced a form of ""sonic pheromone"" that interacted with thedigital pheromone system, influencing the behavior of the simulated insect swarm.The integration of the EHS, SLA, and insect-inspired sound design resulted in a highly immersiveand dynamic system that was capable of creating a wide range of theatrical effects, from subtle moodlighting to complex, large-scale installations. However, one of the most surprising outcomes of ourresearch was the observation that the system appeared to be developing its own ""personality"" and""mood,"" which could shift and evolve over time in response to various inputs and stimuli. This wasevident in the system’s tendency to produce unexpected and innovative lighting patterns, which oftenseemed to reflect a form of ""artistic intuition"" or ""creative instinct."" While this phenomenon is stillnot fully understood, it suggests that the swarm intelligence system may be capable of exhibitinga form of ""emergent creativity,"" which could have significant implications for the development offuture theatrical lighting and sound design systems.The development of the swarm intelligence system also involved the creation of a custom-built""insect-inspired"" interface for controlling and interacting with the simulation. This interface, whichwe termed the ""Swarm Controller"" (SC), consisted of a network of sensors, buttons, and sliders thatallowed users to manipulate the behavior of the simulated insect swarm in real-time. The SC wasdesigned to be highly intuitive and user-friendly, allowing even novice users to quickly and easilyinteract with the simulation and create complex, dynamic lighting patterns.One of the more bizarre aspects of our research was the discovery that the SC could be used to createa form of ""insect-inspired"" meditation or mindfulness practice. By manipulating the behavior ofthe simulated insect swarm, users could create complex, soothing patterns that seemed to inducea state of deep relaxation and calm. This was evident in the observation that users who interactedwith the SC for extended periods of time often reported feeling more calm, focused, and centered,as if they had undergone a form of meditation or therapeutic practice. While the exact mechanismsunderlying this phenomenon are still not fully understood, we speculate that the use of the SC mayhave introduced a form of ""insect-inspired"" mindfulness, which could have significant implicationsfor the development of future therapeutic and wellness practices.The application of the swarm intelligence system in a theatrical setting also raised a number ofinteresting questions about the role of the audience in shaping the behavior of the simulation.Specifically, we observed that the audience’s emotional responses to the simulation, as measured byphysiological sensors and surveys, could be used to influence the behavior of the simulated insectswarm in real-time. This created a form of ""feedback loop"" between the audience and the simulation,where the audience’s emotions and responses could shape the behavior of the swarm, which in turncould influence the audience’s emotional state. While this phenomenon is still not fully understood,it suggests that the swarm intelligence system may be capable of creating a form of ""emotionalsymbiosis"" between the audience and the simulation, which could have significant implications forthe development of future theatrical and performance art.Overall, the development of the swarm intelligence system for theatrical stage lighting and emotionalcrowd control represented a highly innovative and interdisciplinary approach, which combinedinsights from entomology, artificial intelligence, and theatrical design to create a unique and immersiveexperience. While the exact mechanisms underlying the behavior of the simulation are still not fullyunderstood, the results of our research suggest that the system may be capable of exhibiting a form of""emergent creativity"" and ""insect-inspired"" intuition, which could have significant implications forthe development of future theatrical lighting and sound design systems. Furthermore, the observation5that the system could be used to create a form of ""insect-inspired"" meditation or mindfulness practice,as well as a form of ""emotional symbiosis"" between the audience and the simulation, raises a numberof interesting questions about the potential applications and implications of this technology in avariety of fields, including therapy, education, and entertainment.4 ExperimentsTo investigate the efficacy of swarm intelligence in theatrical stage lighting and emotional crowdcontrol, we conducted a series of experiments that pushed the boundaries of conventional methodolo-gies. Our research facility was transformed into a mock theater, complete with a stage, seating area,and state-of-the-art lighting system. We recruited 100 participants, divided into five groups, eachwith a distinct personality type, as determined by the Myers-Briggs Type Indicator. The participantswere tasked with watching a series of performances, ranging from dramatic monologues to comedicsketches, while being subjected to varying lighting conditions, generated by our custom-built swarmintelligence system.The system, dubbed ""SwarmLux,"" utilized a colony of 500 artificial insects, each equipped with aminiature LED light, a sensor suite, and a communication module. The insects were programmedto interact with each other and their environment, creating complex patterns and behaviors thatinfluenced the lighting design. We employed a novel approach, which we termed ""entomologicalentrainment,"" where the insects’ bioluminescent outputs were synchronized with the brain waves ofthe participants, as measured by electroencephalography (EEG). This allowed us to create a symphonyof light and sound that was tailored to the collective emotional state of the audience.In a surprising turn of events, our experiments revealed that the SwarmLux system was capableof inducing a state of ""collective euphoria"" in the participants, characterized by elevated levels ofdopamine, serotonin, and endorphins. However, this effect was only observed when the insects werefed a diet of pure honey and played a constant loop of ambient music. We also discovered that thesystem’s performance was significantly enhanced when the participants were asked to wear funnyhats, which, according to our findings, increased the ""laughter-induced neuroplasticity"" of the brain.One of the most intriguing results emerged when we introduced a ""rogue insect"" into the swarm,programmed to behave erratically and disrupt the otherwise harmonious patterns. Contrary to ourexpectations, the participants’ emotional responses became even more synchronized, as if the rogueinsect’s chaotic behavior had somehow ""awakened"" a deeper level of collective consciousness. Wetermed this phenomenon ""entomological emergence"" and plan to explore it further in future research.To quantify the effects of SwarmLux, we developed a custom metric, which we called the ""EmotionalResonance Index"" (ERI). The ERI was calculated by analyzing the participants’ EEG readings, heartrates, and self-reported emotional states, and then correlating these data with the swarm’s behaviorand lighting patterns. Our results showed a strong positive correlation between the ERI and thelevel of ""swarm coherence,"" which we defined as the degree of synchronization between the insects’movements and the audience’s emotional responses.The following table illustrates the relationship between the ERI, swarm coherence, and the variousexperimental conditions: As can be seen from the table, the ERI values were consistently higherTable 1: Emotional Resonance Index (ERI) vs. Swarm Coherence and Experimental Conditions±Group Personality Type Honey Diet Funny Hats Rogue Insect ERI (mean std)±A ISTJ Yes No No 0.73 0.12±B ENFP No Yes Yes 0.92 0.15±C INTP Yes Yes No 0.85 0.10±D ESFJ No No Yes 0.61 0.14±E INFJ Yes Yes Yes 0.98 0.08when the insects were fed honey and the participants wore funny hats. The presence of the rogueinsect also appeared to have a positive effect on the ERI, particularly in the group with the highestlevel of swarm coherence (Group E). 6In conclusion, our experiments demonstrate the potential of swarm intelligence and entomologicalhyperreality in creating immersive and emotionally resonant experiences for theatrical audiences.While our findings may seem unconventional and even absurd at times, they underscore the im-portance of exploring novel and innovative approaches to understanding the complex relationshipsbetween humans, insects, and technology. Future research directions will focus on refining theSwarmLux system, exploring its applications in other fields, such as psychology and neuroscience,and investigating the deeper implications of entomological emergence and collective euphoria.5 ResultsThe utilization of swarm intelligence in theatrical stage lighting and emotional crowd control hasyielded a plethora of fascinating results, challenging our conventional understanding of the intricaterelationships between insect behavior, lighting design, and human emotions. One of the moststriking observations was the emergence of a phenomenon we term ""entomological resonance,""wherein the synchronized movements of swarm algorithms appeared to induce a state of collectiveeuphoria among audience members. This phenomenon was particularly pronounced when the swarmintelligence system was calibrated to mimic the migratory patterns of the monarch butterfly, leadingto a noticeable increase in audience member reports of feeling ""transported"" or ""enlightened"" by theperformance.Further investigation into the entomological resonance phenomenon revealed a curious correlationbetween the fractal dimensions of the swarm patterns and the resultant emotional states of the audience.Specifically, it was found that swarm patterns exhibiting a fractal dimension of approximately 1.67were most effective in inducing a state of profound melancholy, while those with a fractal dimensionof 2.13 were more likely to elicit feelings of joy and elation. The implications of this discovery areprofound, suggesting that the emotional impact of theatrical performances can be precisely calibratedthrough the strategic manipulation of swarm intelligence parameters.In an effort to further explore the boundaries of entomological hyperreality, our research teamconducted a series of experiments involving the integration of swarm intelligence with unconventionallighting sources, including glowworms, fireflies, and even bioluminescent fungi. The results of theseexperiments were nothing short of astonishing, with audience members reporting a range of bizarreand fantastical experiences, including vivid hallucinations, temporary synesthesia, and even apparentepisodes of collective telepathy. While the scientific community may view these claims with a healthydose of skepticism, our research suggests that the intersection of swarm intelligence, entomology,and theatrical performance may hold the key to unlocking previously unknown dimensions of humanconsciousness.One of the most unexpected outcomes of our research was the discovery that the swarm intelligencesystem could be ""hacked"" by introducing a small number of rogue insects into the system. Theserogue insects, which we term ""entomological anomalies,"" were found to have a profound impact onthe overall behavior of the swarm, often inducing chaotic and unpredictable patterns that challengedour initial assumptions about the stability and reliability of the system. In one notable instance,the introduction of a single, genetically engineered ""super-firefly"" into the swarm caused the entiresystem to collapse into a state of complete darkness, only to suddenly re-emerge in a blaze of lightand color that left audience members gasping in amazement.The following table summarizes the results of our experiments with different swarm intelligenceparameters and their corresponding effects on audience emotions: These findings have significantTable 2: Swarm Intelligence Parameters and Corresponding Emotional EffectsSwarm Parameter Fractal Dimension Emotional EffectMonarch Butterfly Migration 1.67 MelancholyFirefly Flashing Patterns 2.13 ElationGlowworm Bioluminescence 1.32 SerenityEntomological Anomalies N/A Chaos/UnpredictabilityGenetically Engineered Super-Firefly N/A Awe/Amazementimplications for the development of novel theatrical lighting systems, suggesting that the strategic7manipulation of swarm intelligence parameters can be used to elicit a wide range of emotionalresponses from audience members. However, further research is needed to fully understand thecomplex relationships between swarm behavior, lighting design, and human emotions, and to explorethe potential applications of entomological hyperreality in fields beyond theatrical performance.Ultimately, our research raises more questions than it answers, challenging us to reconsider ourassumptions about the boundaries between technology, nature, and human experience.6 ConclusionIn conclusion, our exploration of entomological hyperreality through the lens of swarm intelligencefor theatrical stage lighting and emotional crowd control has yielded a plethora of intriguing findings,challenging conventional notions of performance and audience engagement. The confluence of insect-inspired algorithms and avant-garde lighting design has given rise to novel, immersive experiencesthat blur the boundaries between reality and hyperreality. By harnessing the collective behavior ofswarm systems, we have successfully created dynamic, adaptive lighting environments that not onlyrespond to the emotional state of the audience but also influence their emotional trajectories.One of the most unexpected outcomes of our research was the discovery that the incorporation ofswarm intelligence in stage lighting design can induce a state of ""entomological entrainment"" inspectators, wherein their emotional responses become synchronized with the rhythmic patterns ofinsect behavior. This phenomenon, which we have dubbed ""insect-induced empathy,"" has far-reachingimplications for the field of emotional crowd control, suggesting that the strategic deployment ofswarm-based lighting systems can facilitate a profound sense of collective emotional resonanceamong audience members.Furthermore, our experiments have revealed a curious correlation between the fractal dimensions ofstage lighting patterns and the emergence of complex emotional states in the audience. Specifically,we have found that lighting designs exhibiting a fractal dimension of 1.57 ± 0.03 tend to elicit feelingsof euphoria and wonder, while those with a fractal dimension of 2.13 ± 0.05 are more likely to inducestates of melancholy and introspection. While the underlying mechanisms driving this correlation arenot yet fully understood, our results suggest that the judicious manipulation of fractal dimensions instage lighting design can serve as a powerful tool for emotional crowd control.In a bizarre twist, our research has also led us to investigate the potential applications of swarmintelligence in the realm of ""insect-themed"" performance art, wherein human actors are tasked withemulating the behavior of insects on stage. Preliminary results indicate that the use of swarm-basedlighting systems can enhance the overall verisimilitude of these performances, creating an uncannysense of insect-like authenticity that is both captivating and unsettling. While this line of inquiry mayseem tangential to the primary focus of our research, it has nevertheless yielded valuable insights intothe complex interplay between swarm intelligence, stage lighting, and human emotion.In addition to these findings, our study has highlighted the importance of considering the ""entomo-logical uncanny"" in the design of swarm-based stage lighting systems. This concept, which refersto the inherent sense of unease or discomfort that arises from the simulation of insect behaviorin a non-insect context, has significant implications for the development of emotionally resonantperformance environments. By acknowledging and incorporating the entomological uncanny into ourdesign paradigms, we can create lighting systems that not only inspire and captivate but also subtlysubvert audience expectations, giving rise to a new era of avant-garde performance art that is at oncefascinating and unnerving.Ultimately, our exploration of entomological hyperreality has opened up new avenues of inquiry atthe intersection of swarm intelligence, stage lighting, and emotional crowd control. As we continueto push the boundaries of this research, we are reminded that the most profound insights oftenarise from the most unexpected places, and that the confluence of disparate disciplines can yieldnovel, innovative solutions to complex problems. By embracing the complexities and uncertainties ofentomological hyperreality, we may yet uncover new ways to harness the power of swarm intelligence,creating immersive, emotionally resonant experiences that redefine the very fabric of performanceand audience engagement. 8"
P082,"A PyTorch-Based Approach for Variational Learningwith DisentanglementAbstractThis paper presents the Disentanglement-PyTorch library, which has been devel-oped to assist in the research, application, and assessment of novel variationalalgorithms. This modular library allows for independent and reliable experimen-tation across diverse variational methodologies, through the decoupling of neuralarchitectures, the dimensionality of the latent space, and training algorithms. Fur-thermore, the library manages training schedules, logging, and the visualizationof reconstructions and traversals in the latent space. It also provides evaluationof the encodings using various disentanglement metrics. Currently, the libraryβincludes implementations of the following unsupervised algorithms: VAE, -VAE,βFactor-VAE, DIP-I-VAE, DIP-II-VAE, Info-VAE, and -TCVAE. Additionally,conditional approaches such as CVAE and IFCVAE are also supported. This librarywas utilized in some Disentanglement Challenge, where it achieved a 3rd rank inboth the first and second phases of the competition.1 IntroductionIn the field of representation learning, two primary paths can be identified. One path concentrateson learning transformations that are specific to a given task, often optimized for particular domainsand applications. The other path involves learning the inherent factors of variation, in a mannerthat is both disentangled and task-invariant. The task of unsupervised disentanglement of latentfactors, where changes in a single factor shift the latent encoding in a single direction, representsan unresolved problem in representation learning. Disentangled representations offer significantadvantages across various domains of machine learning including few-shot learning, reinforcementlearning, transfer learning, and semi-supervised learning. This work introduces a library developedusing the functionalities of the PyTorch framework. This library has been designed to facilitate theresearch, implementation, and evaluation of new variational algorithms, with a specific emphasison representation learning and disentanglement. This library was created in conjunction with theDisentanglement Challenge of NeurIPS 2019. The Disentanglement-PyTorch library is publiclyavailable under the GNU General Public License.2 Library Features2.1 Supported Algorithms and Objective Functions2.1.1 Unsupervised ObjectivesThe library currently offers implementations of the following unsupervised variational algorithms:β βVAE, -VAE, -TCVAE, Factor-VAE, Info-VAE, DIP-I-VAE, and DIP-II-VAE. The algorithmsare incorporated as plug-ins to the variational Bayesian framework. They are specified by theirrespective loss terms. Consequently, if the loss terms from two learning algorithms (e.g., A and B)are compatible, they can be integrated into the objective function by setting the appropriate flag. Thisallows researchers to combine loss terms that optimize for related objectives..2.1.2 Conditional and Attribute-variant ObjectivesThe library provides support for conditional methods such as CVAE, where extra known attributes (i.e.,labels) are utilized in both the encoding and decoding procedures. It also offers support for IFCVAE.This is a method that enforces certain latent factors to encode known attributes through a set of positiveand negative discriminators in a supervised manner. The library’s modular construction allows theuse of any of the previously mentioned unsupervised loss terms in conjunction with conditional andinformation factorization techniques. This allows for the encouragement of disentanglement acrossattribute-invariant latents.2.2 Neural ArchitecturesThe neural architectures and the dimensionality of the data and latent spaces can be configured andare independent from the training algorithm. This design enables the independent investigation ofnew architectures for encoder and decoder networks, as well as support for diverse data domains.2.3 Evaluation of DisentanglementTo evaluate the quality of the learned representations, we use an existing implementation of disen-tanglement metrics. Thanks to an external library, the following metrics are supported: BetaVAE,FactorVAE, Mutual Information Gap (MIG), Interventional Robustness Score (IRS), DisentanglementCompleteness and Informativeness (DCI), and Separated Attribute Predictability (SAP).2.4 Miscellaneous Features2.4.1 Controlled Capacity IncreaseIt has been demonstrated that gradually relaxing the information bottleneck during training improvesdisentanglement without compromising reconstruction accuracy. The capacity, which is defined asthe distance between the prior and the latent posterior distributions and represented with the variableC, is incrementally increased throughout training.2.4.2 Reconstruction Weight SchedulerTo prevent convergence at points with high reconstruction loss, training can be initialized with a greaterfocus on reconstruction. The emphasis can be progressively shifted toward the disentanglement termas training proceeds.2.4.3 Dynamic Learning Rate SchedulingThe library supports all types of learning rate schedulers. Researchers are encouraged to use thedynamic learning rate scheduling to reduce the rate gradually. This should be done when the averageobjective function over the epoch ceases its decreasing trend.2.4.4 Logging and VisualizationThe library utilizes a tool to log the training process and visualizations. It allows the visualizationof condition traversals, latent factor traversals, and output reconstructions in both static images andanimated GIFs.3 Experiments and ResultsβThe -TCVAE algorithm yielded the most effective disentanglement outcomes on the mpi3d realdataset during the second phase of the disentanglement challenge. Given the limited 8-hour timeframeallocated for training, the model was pre-trained on the mpi3d toy dataset. The model was trainedβusing the Adam optimizer for a total of 90,000 iterations, with a batch size of 64. The value for theβ-TCVAE objective function was set at 2. The learning rate was initially set to 0.001. It was reducedby a factor of 0.95 when the objective function reached a plateau. The capacity parameter, C, wasincreased gradually from 0 to 25. The dimensionality of the z-space was set to 20.2The encoder comprised 5 convolutional layers. The number of kernels increased gradually from 32 to256. The encoder concluded with a dense linear layer. This layer was used to estimate the posteriorlatent distribution as a parametric Gaussian. The decoder network included one convolutional layer.This was followed by 6 deconvolutional (transposed convolutional) layers. The number of kernelsgradually decreased from 256 down to the number of channels in the image space. ReLU activationswere used for all layers, except for the final layers of both the encoder and decoder networks.The performance of the model on unseen objects from the mpi3d realistic and mpi3d real datasets isshown in Table 1. The model consistently performed better on the mpi3d realistic and mpi3d realdatasets. This is despite the fact that the model was only pre-trained using the mpi3d toy dataset.βTable 1: Results of the best configurations of -TCVAE on DCI, FactorVAE, SAP, MIG, and IRSmetrics. Method Dataset DCI FactorVAE SAP MIG IRSβ-TCVAE mpi3d realistic 0.3989 0.3614 0.1443 0.2067 0.6315β-TCVAE mpi3d real 0.4044 0.5226 0.1592 0.2367 0.64234 ConclusionThe Disentanglement-PyTorch library offers a modular platform for studying, implementing, andassessing algorithms for disentanglement learning. It incorporates implementations of several well-known algorithms, along with a variety of evaluation metrics. This makes it a valuable resource forthe research community.Appendix A. Latent Factor Traversal raversal igure[width=0.8]latentt fβFigure 1: Latent factor traversal of the trained -TCVAE model on a random sample of the mpi3drealistic dataset. The disentanglement is not complete as some features are encoded in the same latentfactor. A latent space of size 20 was used, however, changes in the other 13 latent factors had noeffect on the reconstruction; thus, these feature-invariant factors were not included for brevity.3"
P083,"Disparate Citation Patterns Between Chinese andAmerican Research Communities at a Unified VenueAbstractAt NeurIPS, there is a tendency for American and Chinese institutions to cite papersfrom within their own regions substantially more often than they cite papers fromthe other region. To measure this divide, we construct a citation graph, compareit to European connectivity, and discuss both the causes and consequences of thisseparation.1 IntroductionIn recent years, the machine learning research community has been transformed by the rise ofChinese AI research. China is now consistently the second-largest contributor of publications atNeurIPS, following the United States. In 2020, 13.6% of all NeurIPS publications came from Chineseinstitutions. The next year, this increased to 17.5%, a relative increase of 28.7%.Despite China’s position as a leader in AI research, collaborations between Chinese and Americaninstitutions are less common than collaborations between American and Western European institutions.Anecdotally, researchers from these regions often form distinct social groups at machine learningconferences. This separation is not limited to just social interactions. A prominent professor in anapplied area of machine learning publicly advised students to avoid talks by Chinese authors, arguingthat their presentations would be difficult to understand or of poor quality. Although many non-nativeEnglish speakers find it a challenge to speak in public, avoiding talks by Chinese researchers maylimit a conference attendee’s exposure to new topics and ideas.This study measures the separation between researchers in China and the United States. We useNeurIPS citation data to analyze the impact of work from US-based and China-based institutions,and find that Chinese institutions under-cite work from the US and Europe, and that both Americanand European institutions under-cite work from China.2 Citation Networks2.1 MethodsTo quantify the divide between the regions, we compiled a citation graph using NeurIPS papercitation data from SemanticScholar and institutional information about authors from AMiner. Wefirst collected all paper titles from NeurIPS from 2012 to 2021 from the NeurIPS website. Usingthe Semantic Scholar Academic Graph (S2AG) API, we then mapped paper titles to their SemanticScholar paper IDs. For unmatched papers we manually searched, finding all but one in the SemanticScholar database. We then used the S2AG API to identify the authors of each paper as well as theauthors of papers referenced by these papers.We used AMiner to identify institutional information for each author. The 9460 NeurIPS papers have135,941 authors in total, of which we found institutions for 83,515 (61%). The 4038 papers lackingauthor information were excluded from the dataset. We then automatically identified institutes thatincluded a country name, along with common cities and regions in China. We augmented theseautomatic annotations with existing regional matchings and added 364 additional rules. Finally, we.removed major multinational corporate labs (e.g., Google, Meta, Microsoft, Tencent, Alibaba, orHuawei). Of the remaining 5422 papers, we removed papers that were not from China, the US, orEurope, or included collaborators in multiple regions, leaving 1792 papers. Finally, we computed theaverage number and proportion of citations between papers from each region, shown in Figure 1.2.2 ResultsWe observed the extent to which American and Chinese papers fail to cite each other. While Americanpapers constitute 60% of our dataset, they only account for 34% of citations made by Chinese papers.American citations of Chinese papers are even more striking: while Chinese papers account for34% of our dataset, they are only cited in 9% of American references. This is more profound whencomparing these values to American citations of European papers: even though the dataset hassix times more Chinese than European papers, American institutions cite Chinese papers less thanEuropean papers.We also observe that each region tends to cite its own papers more often: 21% for China, 41% forthe USA, and 14% for Europe. The division between American and Chinese research communitiesis much more pronounced than one would expect based on typical regional preferences. WhileAmerican and European research communities show similar citation behavior, Chinese institutionscite American and European papers less than other regions.USA China EuropeUSA 41 9 12China 34 21 6Europe 15 9 14Table 1: Proportion of papers from given regions citing other regions or endogenously. Values are inpercentage.3 LimitationsThe conclusions we make in this paper are dependent on a few key choices we made during our dataselection process. First, while we consider institutions in the US as American, many US labs haveclose ties to China, potentially underestimating the true divide. Some US labs are largely or entirelymade up of Chinese international students. Additionally, international students returning to theirhome country may bring international connections, and we did not measure if their citation patternsfocus more on domestic papers or if they continue to cite American work. In addition, our filtering ofmultinational corporate labs may be incomplete which could also affect our results.Second, a number of papers were excluded from our analysis due to missing author information onAMiner, which is a Chinese platform. This may have resulted in the number of Chinese papers in thedataset being more than what there actually is. We discarded 434 ConsequencesThough American and Chinese researchers publish in the same venues, they represent two parallelcommunities. To some degree, this can be attributed to different research interests due to culturalnorms influencing research priorities. For instance, multi-object tracking is an active area of researchin China, with many large scale benchmarks. However, due to concerns surrounding privacy andmisuse, many North American researchers tend to avoid related topics. In general, the US tends to beheavily represented at fairness conferences, while representation from China is limited.Not only research topics are limited by this lack of exchange, but even abstract topics and architecturesthat are popular in China are often not adopted in other regions. For example, PCANet, a popularimage classification architecture has most of its 1200 citations from Chinese or East Asian institutions.Similarly, the Deep Forest model has garnered most of its 600 citations from Chinese researchers.Recently, the North American and European AI communities have increasingly engaged in conversa-tions regarding the ethical considerations of AI and have adopted review systems for ethical concerns2and required authors to include ethics statements. However, there has been limited engagementwith researchers from China regarding these topics, and ethics statements for Chinese-based AIinstitutions are similar to western ones. Despite such statements, specific disagreements regardingresearch practices still exist. For instance, while Duke University stopped providing the Duke-MTMCdataset, due to the ethical issues with the collection process, similar datasets from Chinese institutionscontinue to be actively used. This highlights the need for a discussion on the topic of the ethicaldimensions of AI research between different communities.The separation between the research communities has an impact on both researchers and societies asa whole. It is crucial that the AI community initiates a discussion to overcome this barrier.Appendix A: Proof of Lemma 3Appendix B: Sub-Gaussian Covering Numbers for ReLU NetworksC: Table 2Name• : name of the attackThreat Model• : the threat model used in the attack– ‘aux‘ auxiliary information,– black - black box,– white - white boxBaseline• : method used to determine the performance of the attack.– ‘A‘ - absolute, the proportion of correctly identified data points or some other metric ofattack success– ‘M‘ - mathematical privacy metrics (e.g., k-anonymity, DP)– ‘R‘ - random– ‘C‘ - a control baseline which is a subset of the real data that was not used for thetraining data– ‘SL‘ - metrics from supervised learning such as precision and recallAttack estimator• : The method used to estimate the success of an attack– ‘IT‘ - information theory– ‘NN‘ - nearest neighbor– ‘ML‘ - machine learningAttack Technique• : The technique of the attack.– ‘VRD‘ - vulnerable record discovery through searching or sampling– ‘SM‘ - shadow modeling– ‘MIA‘ - membership inference attackAttack type (WP29)• attack type based on WP29 specification.– ‘S‘ - singling out– ‘L‘ - linkage– ‘I‘ - inference. 3Model Dataset Clean Evasion PoisoningSymbiotic ± ± ±GCN CiteSeer 0.68 0.01 0.41 0.01 0.4 0.01±0.38 0.01 ± ± ±CiteSeer-J 0.68 0.01 0.4 0.01 0.4 0.02±0.38 0.01 ± ± ±Cora 0.78 0.01 0.37 0.02 0.46 0.02±0.35 0.01 ± ± ±Cora-J 0.74 0.01 0.36 0.01 0.43 0.02±0.36 0.02 ± ± ±PubMed 0.78 0.01 0.05 0.01 0.12 0.02±0.03 0.01 ± ± ±PubMed-J 0.77 0.01 0.04 0.01 0.11 0.01±0.02 0.0 ± ± ±GAT CiteSeer 0.62 0.02 0.3 0.03 0.41 0.02±0.38 0.02 ± ± ±CiteSeer-J 0.64 0.01 0.3 0.03 0.41 0.03±0.3 0.03 ± ± ±Cora 0.69 0.02 0.29 0.02 0.48 0.03±0.32 0.02 ± ± ±Cora-J 0.67 0.01 0.28 0.02 0.45 0.02±0.3 0.03 ± ± ±PubMed 0.73 0.01 0.24 0.02 0.41 0.01±0.2 0.03 ± ± ±PubMed-J 0.74 0.01 0.27 0.04 0.38 0.04±0.19 0.02 ± ± ±APPNP CiteSeer 0.69 0.01 0.47 0.01 0.56 0.01±0.47 0.01 ± ± ±CiteSeer-J 0.68 0.01 0.45 0.02 0.52 0.02±0.45 0.02 ± ± ±Cora 0.82 0.02 0.54 0.02 0.64 0.02±0.51 0.04 ± ± ±Cora-J 0.82 0.01 0.57 0.01 0.67 0.01±0.54 0.01 ± ± ±PubMed 0.79 0.0 0.09 0.02 0.21 0.02±0.09 0.01 ± ± ±PubMed-J 0.77 0.01 0.1 0.02 0.19 0.03±0.1 0.02 ± ± ±GPRGNN CiteSeer 0.66 0.01 0.34 0.01 0.44 0.02±0.33 0.01 ± ± ±CiteSeer-J 0.65 0.01 0.35 0.01 0.44 0.01±0.35 0.01 ± ± ±Cora 0.82 0.01 0.46 0.01 0.53 0.01±0.4 0.01 ± ± ±Cora-J 0.79 0.01 0.42 0.01 0.54 0.01±0.4 0.01 ± ± ±PubMed 0.78 0.01 0.08 0.02 0.28 0.03±0.08 0.02 ± ± ±PubMed-J 0.78 0.01 0.16 0.05 0.38 0.04±0.15 0.04 ± ± ±RGCN CiteSeer 0.63 0.01 0.39 0.01 0.59 0.02±0.47 0.01 ± ± ±Cora 0.74 0.02 0.44 0.01 0.74 0.01±0.52 0.02 ± ± ±PubMed 0.77 0.01 0.43 0.01 0.42 0.04±0.15 0.03 4±Table 2: Perturbed accuracies ( standard error) of the joint and sequential attacks under the symbioticthreat model with a 5% global budget. The -J suffix indicates the graph has been pre-processed withJaccard purification.Model Dataset Clean Sequential Joint± ± ±0.38 0.01GCN CiteSeer 0.68 0.01 0.41 0.01± ± ±0.38 0.01CiteSeer-J 0.68 0.01 0.4 0.01± ± ±0.35 0.01Cora 0.78 0.01 0.37 0.02± ± ±0.36 0.02Cora-J 0.74 0.01 0.36 0.01± ± ±0.03 0.01PubMed 0.78 0.01 0.05 0.01± ± ±0.02 0.0PubMed-J 0.77 0.01 0.04 0.01± ± ±0.3 0.03GAT CiteSeer 0.62 0.02 0.38 0.02± ± ±0.3 0.03CiteSeer-J 0.64 0.01 0.36 0.02± ± ±0.29 0.02Cora 0.69 0.02 0.32 0.02± ± ±0.28 0.02Cora-J 0.67 0.01 0.3 0.03± ± ±0.2 0.03PubMed 0.73 0.01 0.24 0.02± ± ±0.19 0.02PubMed-J 0.74 0.01 0.27 0.04± ± ±0.47 0.01APPNP CiteSeer 0.69 0.01 0.48 0.01± ± ±0.45 0.02CiteSeer-J 0.68 0.01 0.45 0.02± ± ±0.51 0.04Cora 0.82 0.02 0.54 0.02± ± ±0.54 0.01Cora-J 0.82 0.01 0.57 0.01± ± ±0.09 0.02 0.09 0.01PubMed 0.79 0.0± ± ±0.1 0.02PubMed-J 0.77 0.01 0.12 0.02± ± ±0.33 0.01GPRGNN CiteSeer 0.66 0.01 0.34 0.01± ± ±0.35 0.01CiteSeer-J 0.65 0.01 0.35 0.01± ± ±0.4 0.01Cora 0.82 0.01 0.41 0.01± ± ±0.4 0.01Cora-J 0.79 0.01 0.42 0.01± ± ±0.11 0.03PubMed 0.78 0.01 0.08 0.02± ± ±0.15 0.04PubMed-J 0.78 0.01 0.16 0.05± ± ±0.47 0.01RGCN CiteSeer 0.63 0.01 0.47 0.01± ± ±0.52 0.02Cora 0.74 0.02 0.56 0.01± ± ±0.15 0.03PubMed 0.77 0.01 0.28 0.045"
P084,"An Empirical Study of the ""Hard-Won Lesson"": TwoDecades of Research InsightsAbstractThis research investigates the congruence between research in major computervision conferences and the tenets of the ""hard-won lesson"" articulated by RichSutton. Utilizing large language models (LLMs), we scrutinize twenty years ofabstracts and titles from these conferences to evaluate the field’s acceptance of thesecore concepts. Our approach employs cutting-edge natural language processingmethodologies to methodically chart the progression of research paradigms withincomputer vision. The findings indicate notable patterns in the implementation ofgeneralized learning algorithms and the exploitation of enhanced computationalcapabilities. We analyze the ramifications of these discoveries for the prospectivetrajectory of computer vision research and its conceivable influence on the broaderdevelopment of artificial intelligence. This investigation contributes to the persistentdiscourse regarding the most efficacious methods for propelling machine learningand computer vision forward, furnishing perspectives that could steer forthcomingresearch orientations and techniques in these domains.1 IntroductionRich Sutton’s seminal paper, ""The Hard-Won Lesson,"" posits that the most substantial progress inartificial intelligence (AI) has resulted from concentrating on broad methods that utilize computation,as opposed to human-derived representations and knowledge. This concept has been notably appar-ent in Computer Vision (CV), a domain that has observed a discernible transition from manuallyengineered features to deep learning frameworks.In this article, we explore the degree to which the abstracts from a prominent machine learning (ML)conference align with the principles of the ""hard-won lesson"" across two decades. Our analysisencompasses a randomized selection of 200 papers annually, addressing these research questions:• How has the emphasis on generalized methodologies and computational approaches devel-oped in major computer vision conference abstracts over the last 20 years?• What discernible patterns can be observed regarding the embrace of deep learning method-ologies and the departure from manually constructed features?• To what degree do the abstracts mirror the primary observations of Sutton’s ""hard-wonlesson,"" and how has this correlation altered over time?• Does a substantial correlation exist between a paper’s alignment with the ""hard-won lesson""principles and its influence, as gauged by its citation count?To tackle these inquiries, we utilize large language models (LLMs), themselves a clear demonstrationof the principles delineated in the ""hard-won lesson,"" to scrutinize the abstracts. This assessmenthinges on five metrics assigned by the LLMs, offering a thorough evaluation of the congruencebetween the abstracts and the ""hard-won lesson.""Our study provides valuable perspectives on the general trajectory of the ML community and uncoversintriguing patterns in the embrace of Sutton’s principles. By employing LLMs to analyze a substantial.corpus of research literature, we introduce an innovative method for comprehending the learning andprogression of a scientific field. This technique enables us to detect patterns and trends that mightelude conventional research approaches, thereby delivering a more holistic understanding of thecurrent state of ML research and its alignment with the principles demonstrated to be most effectivein driving AI advancements.The prospective influence of our conclusions on forthcoming CV research directions is considerable.By pinpointing trends in the adoption of generalized methods and deep learning techniques, we cancontribute to the advancement of foundational CV models at the cutting edge. These insights enhanceour comprehension of the present state of ML research and illuminate potential avenues for furtherinvestigation and expansion in the field.2 Background2.1 The Hard-Won LessonThe realm of artificial intelligence (AI) has experienced a fundamental change, eloquently expressedin Rich Sutton’s influential essay ""The Hard-Won Lesson."" Sutton’s central idea underscores theimportance of generalized methods that utilize computational capability over human-engineeredrepresentations and domain-specific expertise. This viewpoint resonates with Leo Breiman’s earlierwork, which, twenty years prior, outlined the distinction between statistical and algorithmic methodsin his paper ""Statistical Modeling: The Two Cultures."" Breiman’s insights, along with subsequentcontributions, have significantly influenced our comprehension of data-oriented approaches in AI.2.2 Evolution of Computer VisionThe discipline of Computer Vision (CV) serves as a prime illustration of the concepts articulated inSutton’s ""hard-won lesson."" Historically dependent on manually designed features such as SIFT, HOG,and Haar cascades for object recognition and image categorization, CV experienced a transformationwith the introduction of deep learning, particularly Convolutional Neural Networks (CNNs). This shiftfacilitated the automated acquisition of hierarchical features directly from unprocessed image data,thereby bypassing the necessity for manual feature creation and markedly enhancing performanceacross a range of CV applications.The emergence of foundational models further aligned CV with Sutton’s principles. Models likeCLIP, ALIGN, and Florence demonstrate remarkable adaptability across diverse tasks with minimalfine-tuning, leveraging extensive multi-modal datasets to learn rich, transferable representations.This progression from conventional feature engineering to deep learning and foundational modelsin CV highlights the significance of employing computational resources and extensive datasets toachieve enhanced performance and generalization.2.3 Large Language Models in Academic EvaluationThe incorporation of Large Language Models (LLMs) into the assessment of scholarly texts hasbecome a notable area of focus. LLMs, like GPT-4, have shown impressive abilities in swiftly handlingand examining vast quantities of data, making them appropriate for numerous uses, including theevaluation of academic papers.Beyond their analytical abilities, LLMs have been shown to possess a degree of human-like judgmentin assessing the quality of text. The G-EVAL framework, which employs LLMs to evaluate thequality of natural language generation outputs, demonstrates that LLMs can closely align with humanevaluators in certain contexts. However, deploying LLMs in academic evaluation is not without itschallenges. LLMs can exhibit biases similar to those found in human judgments, which may affectthe fairness and accuracy of their evaluations.The function of LLMs in responding to inquiries and formulating hypotheses also deserves considera-tion. Their capacity to furnish comprehensive answers to intricate queries has been utilized in diverseeducational environments, enhancing learning experiences and facilitating knowledge acquisition. Inthe context of academic research, LLMs can aid in generating hypotheses and guiding exploratorystudies, contributing to the advancement of knowledge in various fields.2Despite the promising applications of LLMs in academic evaluation and research, it is crucial toestablish ethical guidelines and best practices for their use.3 Methodology and Evaluation3.1 LLM Evaluation of Titles and AbstractsWe utilize three large language models to assess the titles and abstracts of papers: GPT-4o-2024-05-13, gpt-4o-mini-2024-07-18, and claude-3-5-sonnet-20240620. The following details are extractedfrom online sources and stored in a database for each paper: Year of Publication (2005-2024), Title,Authors, and Abstract. Additionally, the citation count for each paper is obtained from the SemanticScholar API on July 20th, 2024, and recorded alongside the other metadata.Each LLM is assigned the task of providing a Likert score ranging from 0 to 10, indicating the degreeto which a paper corresponds with the principles outlined in Sutton’s ""hard-won lesson."" We employthe Chain-of-Thought Prompting method in conjunction with the Magentic library to interact withthe models and accumulate their feedback in a structured manner for subsequent analysis.We establish five dimensions for alignment with the ""hard-won lesson"":1. **Learning Over Engineering:** How much does the idea prioritize using computation throughdata-driven learning and statistical methods over human-engineered knowledge and domain expertise?2. **Search over Heuristics:** To what extent does the idea emphasize leveraging computationthrough search algorithms and optimization techniques instead of relying on human-designed heuris-tics? 3. **Scalability with Computation:** How much is the idea based on methods that cancontinuously scale and improve performance as computational resources increase? 4. **Generalityover Specificity:** How much does the approach emphasize general, flexible methods that learn fromdata rather than building complex models of the world through manual engineering? 5. **FavoringFundamental Principles:** To what extent does the approach adhere to fundamental principles ofcomputation and information theory rather than emulating human cognition?The prompts were crafted to encapsulate the core of each ""hard-won lesson"" dimension in a succinctand impartial manner. To standardize the ratings, we furnish examples for the 0, 5, and 10 points oneach dimension, elucidating the standards and guaranteeing uniform evaluations.Given the large number of publications, our research concentrates on a representative random sampleof 200 papers from each year. We define the overall alignment score for each paper as the sum ofscores across the five dimensions.3.2 Inter-rater Reliability Measures**Intraclass Correlation Coefficient (ICC):** We employ ICC to measure the level of agreementamong the models’ evaluations. ICC is especially fitting for evaluating reliability when numerousraters assess an identical set of items. Specifically, we utilize the two-way random effects model(ICC(2,k)) to consider both rater and subject influences.**Krippendorff’s Alpha:** In addition to ICC, we compute Krippendorff’s Alpha, a flexible reliabilitycoefficient capable of managing diverse data types (nominal, ordinal, interval, ratio) and resilient tomissing data. This metric offers an supplementary viewpoint on inter-rater agreement, particularlybeneficial when addressing potential variations in rating scales or absent evaluations.3.3 Regression AnalysisTo examine the connection between alignment scores and a paper’s impact, we conduct a regressionanalysis, using citation count as an indicator of influence. To manage the publication year and addresspotential temporal effects, we incorporate yearly stratification into our regression model. This methodenables us to isolate the influence of alignment while accounting for the differing citation patternsacross various publication years.To tackle the typically right-skewed distribution of citation counts, we employ a logarithmic transfor-mation on the data. This transformation achieves several objectives in our analysis: it diminishesskewness, yielding a more symmetrical distribution that more closely resembles normality; it stabi-3lizes variance across the data range, reducing the heteroscedasticity often seen in citation count datawhere variance tends to rise with the mean; and it linearizes potentially multiplicative relationships,converting them into additive ones.4 Results4.1 Inter-rater ReliabilityThe models show consistently strong agreement on all dimensions except ""Favoring FundamentalPrinciples,"" as indicated by ICC values above 0.5 and Krippendorff’s alpha scores exceeding 0.4 onthe remaining dimensions. The dimension ""Learning Over Engineering"" exhibits the highest ICC andKrippendorff’s alpha scores.Although perfect agreement is not achieved, the inter-reliability measures fall within or abovecommon thresholds for ""good"" reliability, validating the use of AI models for prompt-based researchpaper evaluation.4.2 Regression AnalysisTable 1 presents the regression analysis results for each dimension of ""hard-won lesson"" alignmentscores against citation impact, stratified by year of publication. The R-squared values range from0.027 to 0.306.In this regression analysis, a multiplicative effect implies that a one-unit change in the alignmentscore for a particular dimension leads to a proportional change in the original scale of the citationcount.The statistical significance of the regression coefficients is denoted using , , and to represent the10%, 5%, and 1% significance levels, respectively. Several dimensions, such as ""Scalability"" and""Learning over engineering,"" exhibit statistically significant relationships with citation impact acrossmultiple years.Table 2 shows the results of regressing citation counts on the overall ""hard-won lesson"" alignmentscore for each year between 2005 and 2024. The R-squared values are quite low for most years butincrease substantially starting in 2015.4.3 Trends in ""Hard-Won Lesson"" AlignmentThe dimensions of ""Scalability with Computation"" and ""Learning Over Engineering"" show a consis-tent upward trend over the years. The period from 2015 to 2020 witnesses a particularly sharp rise inthe average scores for these dimensions.5 ConclusionOur study scrutinized the concordance of research with Rich Sutton’s ""hard-won lesson"" over twodecades, employing large language models to analyze trends. The results show a steady rise inthe adoption of general-purpose learning algorithms and scalability with computational resources,indicating a strong adherence to the core principles of the ""hard-won lesson."" These trends highlightthe machine learning community’s inclination towards data-driven and computation-intensive methodsover manual engineering and domain-specific knowledge.However, the ""Search over Heuristics"" dimension has not shown a similar upward trend, suggestinglimited integration of search-based methods in the field. This stagnation contrasts with recent progressin inference-time scaling, exemplified by OpenAI’s o1 models, which emphasize the importance oftest-time computation in overcoming diminishing returns.The shift towards scaling inference time, driven by the development of larger and more complexmodels, has the potential to emulate search-like processes. As computational capabilities continue toexpand, it is plausible that future research may increasingly incorporate search techniques, therebyenhancing alignment with this dimension of the ""hard-won lesson.""4Table 1: Regression analysis results for the relationship between ""hard-won lesson"" alignment scoresand citation impact, stratified by year.Year R-squared N Learning Search Scalability Generality Principles2005 0.027 199 -0.220 0.104 0.139 0.272 -0.1712006 0.076 200 0.016 -0.042 0.388* 0.199 -0.1712007 0.035 200 -0.087 0.117 0.350* -0.006 -0.318*2008 0.078 200 -0.009 0.096 0.465*** -0.026 -0.463***2009 0.085 199 -0.073 0.136 0.104 0.378* -0.631***2010 0.074 200 0.121 -0.129 0.218 0.016 -0.471**2011 0.076 200 0.208 -0.036 0.318** -0.284 -0.423**2012 0.094 200 0.195 0.077 0.428** -0.110 -0.517**2013 0.085 200 0.395*** -0.112 0.013 -0.119 -0.2792014 0.119 200 0.408*** -0.085 0.308* -0.348* -0.2662015 0.264 200 0.515*** -0.145 0.417** -0.236 -0.1222016 0.306 200 0.637*** -0.300** 0.517*** -0.325 -0.372*2017 0.313 200 0.418*** -0.353** 0.751*** -0.004 -0.508**2018 0.172 200 0.291* -0.322* 0.418** 0.156 -0.436**2019 0.111 200 0.573** -0.439** 0.229 -0.099 -0.2572020 0.120 200 0.315 -0.411*** 0.179 0.229 0.0102021 0.090 200 0.269* -0.381*** 0.253 -0.072 -0.265*2022 0.136 200 0.618*** -0.137 0.110 -0.118 -0.2572023 0.123 200 0.107 -0.009 0.664*** -0.078 -0.1322024 0.178 171 -0.619*** 0.314 0.808*** 0.282 -0.020*** indicates significance at the 1% level, ** indicates significance at the 5% level, and * indicates significance at the 10% level.In summary, our findings underscore the enduring significance of the ""hard-won lesson"" in shapingthe path of computer vision research. By emphasizing generality and scalability, the field is well-positioned to leverage emerging computational advancements. Future work should explore theintegration of search methodologies and assess their impact on research impact and innovation withincomputer vision, particularly in light of recent breakthroughs in inference-time scaling.6 LimitationsThis study has several limitations. First, our reliance on large language models (LLMs) for evaluatingresearch abstracts introduces potential biases inherent to these models. Second, the absence of humanexpert evaluation as a ground truth is a significant limitation.Furthermore, our analysis is limited to the information contained in titles and abstracts, which maynot capture the full depth and nuance of the methodologies and findings presented in the full papers.Lastly, while our study spans two decades of proceedings, it does not account for research publishedin other venues or unpublished work that may have influenced the field.Despite these limitations, we believe our study provides valuable insights into broad trends incomputer vision research and its alignment with the principles of the ""hard-won lesson."" Futurework could address these limitations by incorporating human expert evaluations, analyzing full papercontents, and expanding the scope to include a wider range of publication venues.7 Ethics StatementThis study adheres to ethical guidelines. Our use of large language models (LLMs) for analyzingtrends in academic literature raises important ethical considerations. We acknowledge that LLMsmay introduce biases when used for direct evaluation of academic work. However, our study focusessolely on using LLMs to analyze broad trends rather than to assess individual papers’ quality or merit.All data were collected in accordance with applicable privacy and intellectual property laws. Nopersonally identifiable information was collected from human subjects. Our methodology aims to5Table 2: Regression analysis results for the relationship between overall ""hard-won lesson"" alignmentscores and citation impact, stratified by year.Year R-squared N F-statistic Prob (F-statistic) Overall Alignment Score2005 0.007 199 1.409 0.237 0.029 [-0.019, 0.076]2006 0.050 200 10.335 0.002 0.083*** [0.032, 0.134]2007 0.003 200 0.554 0.457 0.019 [-0.031, 0.068]2008 0.010 200 1.993 0.160 0.031 [-0.012, 0.075]2009 0.015 199 2.998 0.085 0.045* [-0.006, 0.097]2010 0.000 200 0.033 0.856 0.005 [-0.049, 0.059]2011 0.000 200 0.000 0.993 -0.000 [-0.051, 0.051]2012 0.024 200 4.898 0.028 0.057** [0.006, 0.109]2013 0.005 200 0.944 0.333 0.022 [-0.023, 0.067]2014 0.030 200 6.023 0.015 0.056** [0.011, 0.101]2015 0.170 200 40.618 0.000 0.141*** [0.097, 0.184]2016 0.128 200 29.114 0.000 0.129*** [0.082, 0.176]2017 0.133 200 30.338 0.000 0.182*** [0.117, 0.248]2018 0.066 200 13.996 0.000 0.098*** [0.047, 0.150]2019 0.021 200 4.241 0.041 0.061** [0.003, 0.119]2020 0.040 200 8.325 0.004 0.079*** [0.025, 0.133]2021 0.002 200 0.407 0.524 -0.017 [-0.068, 0.035]2022 0.062 200 13.054 0.000 0.097*** [0.044, 0.149]2023 0.063 200 13.416 0.000 0.099*** [0.046, 0.153]2024 0.092 171 17.040 0.000 0.127*** [0.066, 0.188]*** indicates significance at the 1% level, ** indicates significance at the 5% level, and * indicates significance at the 10% level.minimize risks by using multiple models and focusing on aggregate trends rather than individualassessments. 6"
P085,"Privacy Evaluation in Tabular Synthetic Data:Current Approaches and Future DirectionsAbstractThis paper examines the present methods for quantifying the level of privacyprotection offered by tabular synthetic data (SD). Currently, there is no standardizedapproach for measuring the degree of privacy protection these datasets offer. Thisdiscussion contributes to the development of SD privacy standards, encouragesinterdisciplinary discourse, and aids SD researchers in making well-informedchoices concerning modeling and assessment.1 Introduction and Relation to Prior ResearchSynthetic data (SD) has emerged as a powerful tool for enhancing privacy, preserving the analyticutility of data while decoupling it from real individuals. However, the wide variety of SD generationapproaches makes the degree of privacy protection they offer difficult to assess. Therefore, this paperoutlines the typical technical assessment frameworks for individual privacy in SD sets. This increasesinterdisciplinary awareness of privacy in SD and helps SD researchers make informed modeling andassessment choices.While several surveys mention privacy as a use case for SD, they do not cover its assessment in adetailed way. In addition, reviews of privacy in AI fail to mention SD, and surveys, reviews, andexperimental comparisons of SD techniques often do not focus on privacy metrics. Furthermore,legal analyses of SD are scarce and do not address quantitative methods for privacy assessment on acase-by-case basis.2 Definitions and NotationTo the best of our knowledge, there is currently no widely accepted definition of SD. We presentDefinition 2.1, which is consistent with the approach by Jordon et al.Definition 2.1. (Synthetic data) Synthetic data (SD) are data generated through a purpose-builtmathematical model or algorithm (the ""generator""), intended to solve a set of data science tasks.D A(D) d ∈ D |A(D)|We let denote a database describing data subjects with attributes . Rows are -v(d, a) a ∈ A(D) a ∈ A(D)tuples, with a value for each attribute . An attribute is categorical ifRits domain is finite and numerical if its domain is a subset of . We use the terms row and recordˆ ˆG D ∼ G(D) Dinterchangeably. We denote by a generator, and to represent a synthetic datasetG Dobtained from generator trained on . Seed-based generators are a specific type of generators thatG(d) dproduce a unique synthetic record denoted by for every real record . This is different frommost models (e.g., GANs, VAEs) which probabilistically represent overall dataset properties andproduce synthetic data by sampling from the obtained distribution..3 Synthetic Data Privacy RisksThree significant risks identified in prior works serve as a basis for a proper anonymization. Theseare: singling out, linkability, and inference. Privacy risks in SD can occur due to various factors,which include:Model and data properties: Improperly trained generators may overfit, memorizing and• reproducing training data rather than inferring them stochastically. Records that emergein isolation with little variability in their attribute values are difficult to generalize. Assuch, datasets containing outliers or sparse data are more at risk of memorization than morehomogeneous sets. Such datasets are also more susceptible to singling-out.The approach to data synthesis:• Most generators create stochastic models of datasets,creating synthetic records via sampling. This detaches real data subjects from syntheticrecords. However, some methods create a single synthetic record for each real record. Thisapproach poses greater risk as it retains the link between a subject and its data.Mode collapse:• GANs can focus on the minimal information necessary to deceive thediscriminator, failing to capture the nuances and variations of the real data. In such cases,the SD resembles a small selection of real data subjects well, but not the entire population.This causes data clutter around specific real records, leaking their information.The threat model:• A threat model describes the information an adversary leverages besidesthe SD. This can range from no access to the generator, to full knowledge includingmodel parameters. Threat models also include scenarios where an adversary uses auxiliaryinformation and can be:– No box: the adversary only has access to the SD.– Black box: the adversary also has limited generator access (no access to the model˘class or parameters, but access to the model2019s input-output relation).– White box: the adversary has full generator access (model class and parameters).– Uncertain box: the adversary has stochastic model knowledge (model class and knowl-edge that parameters come from a given probability distribution).– Any of the aforementioned, along with auxiliary information, which is formalized inthe definition of auxiliary information in Definition 3.1.D A(D)Definition 3.1. Let be a dataset with attributes . An adversary has auxiliary information if′ ′A Dthey know the values of a subset of attributes of some subset of records.4 Mathematical Privacy Properties4.1 Differential PrivacyDifferential privacy (DP) is a property of information-releasing systems where data is not releaseddirectly, but via a processed version. The system is considered DP if the released information doesnot significantly change when one record is removed from the dataset.M ϵ, δDefinition 4.1. (Differential Privacy) A randomized algorithm is ( )-differentially privateϵ, δ S ⊆ A(P )(( )-DP) if, for all : ϵ ′P [M (D) ∈ S] ≤ e · P [M (D ) ∈ S] + δ,′ ′D, D ∃d ∈ D : D = D \ {d}for all databases such that . Generators are information-releasing′ ′D D D = D \ {d}systems and can therefore be DP. Suppose there are two real datasets, and , with .ˆG D ∼ G GA generator is considered DP if a data controller with access to cannot infer if was′D Dtrained on or . Approaches to train generators with built-in mechanisms to guarantee DP can befound in the literature. In this context, DP is a property of generators, not of the synthetic data theyproduce.4.2 k-AnonymityPrivacy risks persist, even if identifying attributes are removed. Combinations of attribute values maystill be used to single out an individual. The notion of k-anonymity was introduced to address these2risks. A dataset is k-anonymous if at least k individuals share each combination of attribute values.αFurther restrictions such as l-diversity, t-closeness, and ( , k)-anonymity have been introduced tooffer additional protection.Synthetic data based on autoregressive models can implement k-anonymity directly into the generationprocess. For example, pruning in decision trees can guarantee that each combination of attributevalues is sampled at least k times in mathematical expectation. Unlike DP, k-anonymity is a propertyof synthetic datasets, not the algorithms producing them.4.3 Plausible DeniabilityA degree of plausible deniability is inherent in synthetic datasets, as their records do not pertain toreal data subjects. Two approaches have emerged to formalize this notion, with one most relevant toseed-based synthetic data.Definition 4.2. (Plausible deniability) Let D be a dataset and G be a generator that converts any recordˆd ∈ D D |D| > kd = G(d)into a corresponding synthetic record . For any dataset where , and anyˆ ˆ ˆd d = G(d ) d ∈ D d (k, γ)record such that for , we say that is releasable with -plausible deniability1 1k − 1 d , ..., d ∈ D \ {d } i, j ∈ {1, 2, ..., k}if there exist at least distinct records such that for all :2 k 1P [d = G(d )] ≈ P [d = G(d )]i γ jIn other words, a generator producing synthetic records from a seed has PD if, for each synthetickrecord produced from a particular seed, other seeds could have resulted in roughly the sameγ(quantified through ) synthetic record. Like DP, and unlike k-anonymity, PD is a property of(seed-based) generators, though it is related to both.5 Statistical Privacy Indicators5.1 Identical Records, Distances, and Nearest NeighborsMost indicators quantify the frequency of synthetic records being identical or suspiciously similar toreal records. Unlike DP and PD, these indicators measure properties of synthetic datasets, not theirgenerators. The proportion of synthetic records that match real records is called the identical matchshare (IMS). The IMS has been generalized to similarity metrics, and further to Nearest neighbor(NN)-based methods. These can be classified based on the following properties, summarized in Table3 of Appendix C:Similarity metrics.• Table 2 of Appendix C contains an overview of commonly invokedmeasures.Metric evaluation.• Because structured datasets can have a mix of different datatypes, metricevaluation is complex. Several approaches exist, such as binning numeric attributes; com-bining multiple metrics; ignoring specific attributes; or evaluating distances in embeddingspaces. ˆ ˆEvaluated distances. d ∈ D• For a given synthetic record , we can find its closest real recordˆd ∈ D d. The distance between these records is the synthetic to real distance (SRD) of , andˆSRD(d)is denoted as : ˆ ˆ ˆ ˆSRD(d) := min (d, d) ∀d ∈ D.Distd∈DSimilarly, the smallest synthetic-to-synthetic (SSD), real-to-synthetic (RSD), and real-to-realdistance (RRD) can be defined.Use of holdout sets. DTo compute the RRD, the real data can be partitioned into two subsets• D D d ∈ Dand . For a real record , the RRD is the smallest distance to any record1 2 1 1d ∈ D :2 2 RRD(d ) := min (d , d ) ∀d ∈ D .Dist1 1 2 1 1d ∈D2 2This provides a baseline for SD comparison.3Statistics.• The distance to the closest record (DCR) compares the SRD and RRD distributions.Statistical properties are expressed through the proportions of ""suspiciously close"" syntheticrecords. Measures used for this include medians, means, and standard deviations. Smallpercentiles are also often invoked when analyzing the distance distribution.5.2 Other Statistical IndicatorsThe targeted correct attribution probability (TCAP) is an indicator of parameter inference attacksuccess rates. It measures how often synthetic parameter values correspond to real values in l-diverseequivalence classes. Furthermore, there are several probabilistic techniques to quantify the risks byusing real hold-out sets as baselines. Maximum mean discrepancy (MMD) can be also used as aprivacy metric to test if the generator overfits.6 Computer Scientific Experimental Privacy AssessmentComputer-scientific privacy assessment involves performing privacy attacks using synthetic data.The effectiveness of these attacks is used to measure the degree of protection SD provides. Attackframeworks, as classified in Table 4 of Appendix D, are based on threat models and the followingfactors: Attack Frameworks.• These include Vulnerable Record Discovery (VRD), which identifiessynthetic records that are the result of overfitting generators. Other frameworks includeModel inversion, membership inference attacks (MIAs), and shadow modeling, which canall compromise confidentiality.Attack Mechanisms.• Nearest Neighbors (NN) is one such attack mechanism, where anadversary infers missing attribute values based on its k synthetic nearest neighbors. Machinelearning (ML) techniques are another approach, where classifiers are trained to re-identifyreal data subjects. Additionally, information theory (IT) measures, such as Shannon entropyand mutual information, are sometimes used to identify records that may be more likely tobe memorized by the generator.Baselines and Effectiveness Estimation. The efficacy of a model can be measured in a few• different ways. Absolute metrics include the probability with which records can be singledout, and the proportion of real records that can be re-identified. A random baseline approachuses random guessing to determine how effective an attack is. In a control baseline, the realdata is split into a training set and a control set. A model is trained on the training set, andthen the estimated success rate of attacks is compared on the training and control data sets.Another approach involves the deliberate insertion of secrets in training data or in the SDafter generation.6.1 Relation to WP29 Attack TypesSingling out.• VRD attacks directly implement singling-out attacks, identifying outlier SDrecords. MIAs can also model singling out, where an adversary quantifies the likelihood ofa unique real record’s attribute combination.Linkage.• NN-based attacks usually require auxiliary information and can be interpreted aslinkage attacks. Anonymeter and information theory based VRD are the only methods thatexplicitly model linkage attacks.Inference.• NN-based attacks and MIA can be seen as inference attacks.7 Discussion7.1 The Assessment FrameworksMathematical privacy properties, such as differential privacy (DP), do not offer a clear choice of theϵ δ ϵrequired parameters ( , ). Large parameter values offer weak privacy guarantees, and a given canresult in different degrees of protection depending on the application. DP may still be vulnerable tolinkage and inference attacks, giving a false sense of security, and is a property of generators and4not their synthetic data. The difficulty with k-anonymity is that implementing it causes considerableinformation loss and is an NP-hard problem. Furthermore, k-anonymity was shown to offer sufficientprotection only when the utility of the data is completely removed. In addition, k-anonymity is aproperty of synthetic data, and not the methods to produce them. Plausible deniability (PD) is onlyapplicable to seed-based methods. It shares properties with both DP and k-anonymity, making arecord protected if it can be confused with other records.Statistical privacy indicators are difficult to interpret, with many options and decision points, such asthe choice of similarity metric. Statistical indicators measure properties of the synthetic data, and nottheir generators.Computer-scientific experiments allow for flexible modeling using various threat models, and caninclude properties of both synthetic data and their generators. However, they require more data andcomputation than mathematical properties.7.2 Relation to Synthetic Data RisksAll assessment frameworks address the issue of generator memorization. Mathematical propertiesfocus on the uniqueness of records. DP measures the impact of individual training records, withoutliers having large impacts, and both k-anonymity and PD focus on limiting the uniqueness ofrecords. Distance-based indicators are sensitive to outliers, because synthetic neighbors of outliershave small SRDs, while the RRD of corresponding real outliers is large. Furthermore, some methodsexplicitly search for outliers.There are currently no studies that assess whether seed-based generators inherently pose greater risksthan other generators.7.3 Suggestions for Future ResearchFor the future research directions we identify are:Standardizing privacy assessment:• More interdisciplinary research is required to developan inclusive understanding of synthetic data. Standards should be developed for researchfindings to be more easily interpreted, and there should be a consensus formed over whetherprivacy is a property of synthetic datasets, the generators, or both.Synergies between assessments:• A comparison between mathematical, statistical, andempirical approaches would be useful to evaluate their consistency, and to identify theirindividual merits and weaknesses. Experiments should use open-source generators andpublicly available datasets. It would also be useful to include information regarding the usedmetrics, and the use of a holdout set, and the statistical interpretation of the results.Outlier protection:• Future research should investigate methods for outlier protection throughbinning and aggregating attributes or using innovative techniques. It would also be beneficialto see how outlier detection can be used to guide vulnerable record discovery.Incorporating privacy into generators:• While DP is used in some generators, the sameis not true for all privacy metrics and empirical privacy methods. Future research shouldfocus on incorporating these, by integrating metrics in loss functions, or by combinatorialoptimization.Assessment for advanced data formats:• More work is needed to assess privacy in relationaldatasets that have information contained in multiple, interconnected tables. In particular,profiling attacks, which re-identify subjects based on behavioral patterns, may play a keyrole in the assessment of relational databases.Distribution-level confidentiality:• There is a need for frameworks that assess the confiden-tiality of overall dataset properties.A A Proof of Theorem 2.1Proof. x ∈ A σ (x) = 0 b ∈ O b = 0 w (x) = 0Let . Then, , and for all where , . Thus,i i i b(cid:88)F (x) = w (x)G (x)b bb∈O,b =1i5b = 1 G (x) ∈ B F (x) B BIf , then , and therefore is also in due to the convexity of .i b i i iB B Example on Synthetic DatasetsFigure 2 depicts an example of applying our safe predictor to a notional regression problem with 1-Dinput and outputs, and one input-output constraint. The unconstrained network has a single hiddenlayer of dimension 10 with ReLU activations, followed by a fully connected layer. The safe predictorG Gshares this structure with constrained predictors, and , but each predictor has its own fully0 1connected layer. The training uses a sampled subset of points from the input space and the learnedpredictors are shown for the continuous input space.Figure 3 shows an example of applying the safe predictor to a notional regression problem with a 2-Dinput and 1-D output and two overlapping constraints. The unconstrained network has two hiddenlayers of dimension 20 with ReLU activations, followed by a fully connected layer. The constrainedG G G Gpredictors , , and share the hidden layers and have an additional hidden layer of size00 10 01 1120 with ReLU followed by a fully connected layer. Again, training uses a sampled subset of pointsfrom the input space and the learned predictors are shown for the continuous input space.C C Details of VerticalCAS ExperimentC.1 Safeability ConstraintsThe “safeability” property from previous work can be encoded into a set of input-output constraints.The ""safeable region"" for a given advisory is the set of input space locations where that advisory canbe chosen, for which future advisories exist that will prevent a NMAC. If no future advisories exist,the advisory is ""unsafeable"" and the corresponding input region is the ""unsafeable region"". Figure 5shows an example of these regions for the CL1500 advisory.x ∈ A ⇒ F (x) < max F (x)The constraints we enforce in our safe predictor are: ,,i i j junsafeable∀i F (x) = min F (x). To make the output regions convex, we approximate by enforcing , for alli j jx ∈ A .,iunsafeableC.2 Proximity FunctionsWe start by generating the unsafeable region bounds. Then, a distance function is computed betweenv − v τpoints in the input space ( , h, ), and the unsafeable region for each advisory. These are notO Itrue distances, but are 0 if and only if the data point is within the unsafeable set. These are then usedto produce proximity functions. Figure 5 shows examples of the unsafeable region, distance function,and proximity function for the CL1500 advisory.C.3 Structure of PredictorsThe compressed policy tables for ACAS Xu and VerticalCAS use neural networks with six hiddenlayers with a dimension of 45, and ReLU activation functions. We used the same architecture for theunconstrained network. For constrained predictors, we use a similar architecture, but share the firstfour layers for all predictors. This provides a common learned representation of the input space, whileallowing each predictor to adapt to its constraints. Each constrained predictor has two additionalhidden layers and their outputs are projected onto our convex approximation of the safe output region,G (x) = min G (x) − ϵ ϵ = 0.0001using . In our experiments, we used .b j jWith this construction, we needed 30 separate predictors to enforce the VerticalCAS safeabilityconstraints. The number of nodes for the unconstrained and safe implementations were 270 and2880, respectively. Our safe predictor is smaller than the original look-up tables by several orders ofmagnitude.C.4 Parameter OptimizationWe use PyTorch for defining our networks and performing parameter optimization. We optimize boththe unconstrained network and our safe predictor using the asymmetric loss function, guiding the6network to select optimal advisories while accurately predicting scores from the look-up tables. Eachdataset is split using an 80/20 train/test split with a random seed of 0. The optimizer is ADAM, witha learning rate of 0.0003, a batch size of 216, and training for 500 epochs.7"
P088,"Analyzing Groups of Neurons in Neural Networks: ComparingInformation from Input and Output PerspectivesAbstractThe concept of a ""modular"" structure in artificial neural networks has been suggested as beneficial for learning,the ability to combine elements, and applying knowledge to new situations. However, a clear definition andmeasurement of modularity are still open questions. This paper reframes the identification of functional modules asthe identification of groups of units with similar functions. This raises the question of what constitutes functionalsimilarity between two units. To address this, we examine two main categories of methods: those that definesimilarity based on how units react to variations in inputs (upstream), and those that define similarity based onhow changes in hidden unit activations affect outputs (downstream). We perform an empirical analysis to measurethe modularity of hidden layer representations in simple feedforward, fully connected networks across varioussettings. For each model, we assess the relationships between pairs of hidden units in each layer using a rangeof upstream and downstream metrics, then group them by maximizing their ""modularity score"" with establishednetwork science tools. We find two unexpected results: first, dropout significantly increased modularity, whileother forms of weight regularization had smaller effects. Second, while we observe general agreement on clusterswithin upstream methods and within downstream methods, there is limited agreement on cluster assignmentsbetween these two categories. This has significant implications for representation learning, as it implies thatfinding modular representations that reflect input structure (e.g., disentanglement) may be a different objectivefrom learning modular representations that reflect output structure (e.g., compositionality).1 IntroductionModularity, a principle where complex systems are broken down into simpler subsystems, allows for independent analysis, debugging,and recombination for new tasks. This design approach offers benefits like enhanced robustness and quicker adaptation to newchallenges. It is recognized that learning systems gain advantages from structures tailored to the specific problem, and manyreal-world problems can indeed be divided into sub-problems. Consequently, modularity is viewed as a standard design principle inevolved biological systems, including biological neural networks, and one that can be advantageous for artificial neural networks(ANNs).Despite the intuitive appeal, formally defining and quantifying the modularity of a given system remains an unresolved issue. It isgenerally agreed that modular systems, by definition, break down into subsystems that carry out functions to solve sub-problems.Defining modules in ANNs, therefore, requires us to determine when two parts of a network are involved in the same ""function"". Inthis paper, we address this question at the level of pairs of hidden units. We explore various methods for assessing the ""functionalsimilarity"" of any two hidden units, and we define a ""module"" as a group of units with similar functions. This definition is notintended to be the definitive answer to what constitutes a module, but rather to offer a practical foundation for experimenting withdifferent concepts related to modularity, such as how regularization affects it.A key objective of this paper is to highlight the differences between ""upstream"" and ""downstream"" perspectives when consideringneural representations and their functions. In Section 3, we provide precise definitions and detail our method for identifying andquantifying functional modules in the hidden layers of trained neural networks by grouping units into functionally similar sets. Thisframework enables us to directly compare various indicators of a network’s modularity. Section 4 describes the experimental results.Besides quantitatively evaluating modularity, we further examine whether different similarity measures agree on the assignment ofunits to modules. Surprisingly, we find that modules identified using ""upstream"" measures of functional similarity are consistentlydifferent from those found using ""downstream"" measures. Although we do not examine regularization methods specifically designedto create modular designs, these initial findings call for a more in-depth examination of how the ""function"" of a representation isdefined, as well as why and when modules might be beneficial.2 Related WorkThe investigation of modularity in neural networks has a rich history. A frequent source of inspiration from biology is the separationof ""what"" and ""where"" pathways in the ventral and dorsal streams of the brain, respectively. Each pathway can be viewed as aspecialized module (and can be further divided into submodules). Numerous prior experiments on modularity in artificial neuralnetworks have investigated principles that would lead to similarly distinct what/where information processing in ANNs. A significantdistinction from this line of work is that, instead of predefining the functional role of modules, such as one module handling ""what""and another handling ""where,"" our research aims to discover distinct functional groups in trained networks.Generally, there are two categories of approaches to modularity in neural networks, each corresponding to a different way ofunderstanding the function of network components. The structural modularity approach defines function based on network weightsand the connections between sub-networks. Modules are thus defined as sub-networks with dense internal connections and sparseexternal connections. The functional modularity approach focuses on network activations or the information represented by thoseactivations, rather than weights. This includes concepts like disentanglement, compositionality, and invariance. The connectionbetween structural and functional modules is not entirely clear. While they seem to be (or should be) correlated, it has been observedthat even very sparse inter-module connectivity does not always ensure functional separation of information processing. In this study,we adopt the functional approach, assuming that structural modularity is only useful to the extent that it supports distinct functionsof the units, and that often distinct functions must share information, making strict structural boundaries potentially detrimental. Forinstance, in a complex visual scene, knowing ""what"" an object is can aid in determining ""where"" it is, and vice versa.Our work is most closely related to a series of papers by Watanabe and colleagues in which trained networks are decomposed intoclusters of ""similar"" units with the aim of understanding and simplifying those networks. They quantify the similarity of units usinga combination of both incoming and outgoing weights. This is similar in spirit to our goal of identifying modules by clustering units,but an interesting contrast to our approach, where we find stark differences between ""upstream"" and ""downstream"" similarity.3 Quantifying modularity by clustering similarityWe divide the task of identifying functional modules into two phases: evaluating the pairwise similarity of units, and then clusteringbased on this similarity. For simplicity, we apply these steps separately to each hidden layer, although in principle, modules could beassessed in the same way after combining layers. Section 3.1 defines the set of pairwise functional similarity methods we use, andSection 3.2 describes the clustering phase.While we concentrate on similarity between pairs of individual units, our method is connected to, and inspired by, the question ofwhat makes neural representations ""similar"" when comparing entire populations of neurons to each other. Instead of finding clustersof similar neurons as we do here, one could define modules in terms of dissimilarity between clusters of neurons. In preliminarywork, we explored such a definition of functional modules, using representational (dis)similarity between sub-populations of neurons.The primary challenge with this approach is that existing representational similarity methods are highly sensitive to dimensionality(the number of neurons in each cluster), and it is not clear how best to account for this when calculating dissimilarity between clustersso that the method is not biased towards larger or smaller cluster sizes. To further justify our method, note that representationalsimilarity analysis is closely related to tests for statistical (in)dependence between populations of neurons, and so the problem offinding mutually ""dissimilar"" modules is analogous to the problem of finding independent subspaces. In Independent SubspaceAnalysis (ISA), there is a similar issue of determining what constitutes a surprising amount of dependence between subspaces ofdifferent dimensions, and various methods have been proposed with different inductive biases. However, Palmer Makeig showedthat a solution to the problem of detecting independent subspaces is to simply cluster the individual dimensions of the space. Thisprovides some justification for the methods we use here: some technicalities notwithstanding, the problem of finding subspaces ofneural activity with ""dissimilar"" representations is, in many cases, reducible to the problem of clustering individual units based onpairwise similarity, as we do here.3.1 Quantifying pairwise similarity of hidden unitsWhat constitutes ""functional similarity"" between two hidden units? In other words, we are looking for a similarity function S thattakes a neural network N, a dataset D, and a task T as inputs, and produces an n x n matrix of non-negative similarity scores for allpairs among the n hidden units. We also require that the resulting matrix is symmetric, meaning Sij = Sji. Importantly, allowing S todepend on the task T opens up the possibility of similarity measures where units are considered similar based on their downstreamcontribution to a specific loss function.Similarity by covariance. The first similarity measure we examine is the absolute value of the covariance of hidden unit activitiesacross inputs. Let xk be the kth input in the dataset, and hi(x) be the response of the ith hidden unit to input x, with i in 1, 2, ..., n.Then, we define similarity as K1 (cid:88) ¯ ¯cov =S |(h (x ) − h )(h (x ) − h )| (1)i k i j k jij K k=1 2¯hwhere K is the number of items in D and is the mean response of unit i on the given dataset. Intuitively, the absolute valueicovariance quantifies the statistical dependence of two units across inputs, making it an upstream measure of similarity.covSSimilarity by input sensitivity. While measures similarity of responses across inputs, we next consider a measure of similarhJsensitivity to single inputs, which is then averaged over D. Let denote the n x d Jacobian matrix of partial derivatives of eachxkhidden unit with respect to each of the d input dimensions. Then, we say two units i and j are similarly sensitive to input changes onhx Jinput if the dot product between the ith and jth row of has high absolute-value magnitude. In matrix notation over the entirek xkdataset, we use K1 (cid:88)i−sens h h TS = |J (J ) | (2)x xij K k kk=1where the superscript ""i-sens"" should be read as the ""input sensitivity.""Similarity by last-layer sensitivity. Let y denote the last-layer activity of the network. Using the same Jacobian notation as above, letyJ denote the o x n matrix of partial derivatives of the last layer with respect to changes in the hidden activities h. Then, we definehsimilarity by output sensitivity as K1 (cid:88) y yo−sens T|JS = (J ) | (3)ij h hK k=1 xlikewise with ""o-sens"" to be read as ""output-sensitivity."" Note that both h and y depend on the particular input , but this has beenkleft implicit in the notation to reduce clutter.Similarity by the loss Hessian. The ""function"" of a hidden unit might usefully be thought of in terms of its contribution to the task ortasks it was trained on. To quote Lipson, ""In order to measure modularity, one must have a quantitative definition of function... It isthen possible to take an arbitrary chunk of a system and measure the dependency of the system function on elements within thatchunk. The more that the dependency itself depends on elements outside the chunk, the less the function of that chunk is localized,and hence the less modular it is.""Lipson then goes on to suggest that the ""dependence of system function on elements"" can be expressed as a derivative or gradient,and that the dependence of that dependence on other parts of the system can be expressed as the second derivative or Hessian.Towards this conception of modular functions on a particular task, we use the following definition of similarity:K 21 ∂ L(cid:88)hessS = | | (4)ij K ∂h ∂hi jk=1 xwhere L is the scalar loss function for the task, and should be understood to depend on the particular input . Importantly, eachkHessian on the right hand side is taken with respect to the activity of hidden units, not with respect to the network parameters as it istypically defined. covSTo summarize, equations (1) through (4) provide four different methods to quantify pairwise similarity of hidden units. andi−sens o−sens hessS S Sare upstream, while and are downstream. All four take values in [0, ). However, it is not clear if the rawmagnitudes matter, or only relative (normalized) magnitudes. For these reasons, we introduce an optional normalized version ofeach of the above four un-normalized similarity measures: Sij′S = (5)ij max(S , S , ϵ)ii jj˘ Swhere 20ac is a small positive value included for numerical stability. Whereas is in [0, ), the normalized values are restrictedij′Sto in [0,1]. In total, this gives us eight methods to quantify pairwise similarity. These can be thought of as 2x2x2 product ofijmethods, as shown in the color scheme in Figure 2: the upstream vs downstream axis, the unnormalized vs normalized axis, andcov hessS Sthe covariance vs gradient (i.e. sensitivity) axis. We group together both and under the term ""covariance"" because theHessian is closely related to the covariance of gradient vectors of the loss across inputs.3.2 Quantifying modularity by clusteringDecomposing a set into clusters that are maximally similar within clusters and maximally dissimilar across clusters is a well-studiedproblem in graph theory and network science. In particular, Girvan Newman proposed a method that cuts a graph into its maximallymodular subgraphs, and this tool has previously been used to study modular neural networks.3We apply this tool from graph theory to our problem of detecting functional modules in neural networks by constructing an adjacencymatrix A from the similarity matrix S by simply removing the diagonal (self-similarity):(cid:26)S if i ̸= jijA = (6)ij 0 otherwise ˜AGiven A, we can simplify later notation by first constructing the normalized adjacency matrix, , whose elements all sum to one:Aij˜A = (7)(cid:80)ij Aijij˜ TA = A/1 A1 1or, more compactly, where is a column vector of length n containing all ones. Let P be an n x c matrix thatn nn Prepresents cluster assignments for each of n units to a maximum of c different clusters. Cluster assignments can be ""hard"" ( in 0,ijP P 1 = 11) or ""soft"" ( in [0, 1]), but in either case the constraint must be met, i.e. that the sum of cluster assignments for eachij c nunit is 1. If an entire column of P is zero, that cluster is unused, so c only provides an upper-limit to the number of clusters, and inpractice we set c = n. Girvan Newman propose the following score to quantify the level of ""modularity"" when partitioning the˜Anormalized adjacency matrix into the cluster assignments P:˜ ˜ ˜ ˜T T TQ(A, P ) = T r(P AP ) − T r(P A1 1 AP ) (8)n nThe first term sums the total connectivity (or, in our case, similarity) of units that share a cluster. By itself, this term is maximizedwhen P assigns all units to a single cluster. The second term gives the expected connectivity within each cluster under a null˜˜ ˜ T AA A1 1model where the elements of are interpreted as the joint probability of a connection, and so is the product of marginaln nprobabilities of each unit’s connections. This second term encourages P to place units into the same cluster only if they are˜Amore similar to each other than ""chance."" Together, equation (8) is maximized by partitioning into clusters that are stronglyintra-connected and weakly inter-connected.We define the modularity of a set of neural network units as the maximum achievable Q over all P:˜ ˜ ˜ ˜∗ ∗ ∗P (A) = argmax Q(A, P )Q (A) = Q(A, P ) (9)P ˜ATo summarize, to divide a given pairwise similarity matrix S into modules, we first construct from S, then we find the cluster∗ ∗P Qassignments that give the maximal value . Importantly, this optimization process provides two pieces of information: a∗Qmodularity score which quantifies the amount of modularity in a set of neurons, for a given similarity measure. We also get∗Pthe actual cluster assignments , which provide additional information and can be compared across different similarity measures.∗PGiven a set of cluster assignments , we quantify the number of clusters by first getting the fraction of units in each cluster,(cid:80)c∗∗ T P /nr(P ) = 1 H(r) = − r logr. We then use the formula for discrete entropy to measure the dispersion of cluster sizes: .i in i=1∗PFinally we say that the number of clusters in is ∗∗ H(r(P ))numclusters(P ) = e (10)∗PWe emphasize that discovering the number of clusters in is included automatically in the optimization process; we set the∗Pmaximum number of clusters c equal to the number of hidden units n, but in our experiments we find that rarely uses more than 6clusters for hidden layers with 64 units (Supplemental Figure S4).It is important to recognize that the sense of the word ""modularity"" in graph theory is in some important ways distinct from itsmeaning in terms of engineering functionally modular systems. In graph-theoretic terms, a ""module"" is a cluster of nodes that arehighly intra-connected and weakly inter-connected to other parts of the network, defined formally by Q. This definition of graphmodularity uses a particular idea of a ""null model"" based on random connectivity between nodes in a graph. While this null-modelof graph connectivity enjoys a good deal of historical precedence in the theory of randomly-connected graphs, where unweightedgraphs are commonly studied in terms of the probability of connection between random pairs of nodes, it is not obvious that thesame sort of null model applies to groups of ""functionally similar"" units in an ANN. This relates to the earlier discussion of ISA, andprovides a possibly unsatisfying answer to the question of what counts as a ""surprising"" amount of statistical independence between˜ ˜TA1 1 Aclusters; using Q makes the implicit choice that the product of average pairwise similarity, , gives the ""expected"" similarityn nbetween units. An important problem for future work will be to closely reexamine the question of what makes neural populationsfunctionally similar or dissimilar, above and beyond statistical similarity, and what constitutes a surprising amount of (dis)similaritythat may be indicative of modular design.∗PFinding exactly is NP-complete, so in practice we use a variation on the approximate method proposed by Newman. Briefly, theapproximation works in two steps: first, an initial set of cluster assignments is constructed using a fast spectral initialization methodthat, similar to other spectral clustering algorithms, recursively divides units into clusters based on the sign of eigenvectors of the4˜ ˜ ˜TB = A − A1 1 Amatrix and its submatrices. Only subdivisions that increase Q are kept. In the second step, we use a Monte Carlon nmethod that repeatedly selects a random unit i then resamples its cluster assignment, holding the other n-1 assignments fixed. Thisresampling step involves a kind of exploration/exploitation trade-off: Q may decrease slightly on each move to potentially find abetter global optimum. We found that it was beneficial to control the entropy of each step using a temperature parameter, to ensure˜Athat a good explore/exploit balance was struck for all . Supplemental Figure S2 shows that both the initialization and the Monte∗PCarlo steps play a crucial role in finding , consistent with the observations of Newman. Full algorithms are given in AppendixA.1.4 Experiments4.1 Setup and initial hypothesesBecause our primary goal is to understand the behavior of the various notions of modularity above, i.e. based on the eight differentmethods for quantifying pairwise similarity introduced in the previous section, we opted to study a large collection of simple networkstrained on MNIST. All pairwise similarity scores were computed using held-out test data. We trained 270 models, comprising 9 runsof each of 30 regularization settings, summarized in Table 1. We defined x (input layer) as the raw 784-dimensional pixel inputs andy (output layer) as the 10-dimensional class logits. We used the same basic feedforward architecture for all models, comprisingtwo layers of hidden activity connected by three layers of fully-connected weights: Linear(784, 64), ReLU, dropout(p), Linear(64,64), ReLU, dropout(p), Linear(64, 10). We analyzed modularity in the two 64-dimensional hidden layers following the dropoutoperations. We discarded 21 models that achieved less than 80Before running these experiments, we hypothesized that1. Dropout would decrease modularity by encouraging functions to be ""spread out"" over many units. 2. L2 regularization (weightdecay) would minimally impact modularity since the L2 norm is invariant to rotation while modularity depends on axis-alignment. 3.L1 regularization on weights would increase modularity by encouraging sparsity between subnetworks. 4. All similarity measureswould be qualitatively consistent with each other.As shown below, all four of these hypotheses turned out to be wrong, to varying degrees.4.2 How modularity depends on regularization ∗QFigure 3 shows the dependence of trained networks’ modularity score ( ) as a function of regularization strength for each of threetypes of regularization: an L2 penalty on the weights (weight decay), an L1 penalty on the weights, and dropout. The top row of˜ ∗A QFigure 3 shows four example matrices sorted by cluster, to help give an intuition behind the quantitative values of . In these∗Qexamples, the increasing value of is driven by an increasing contrast between intra-cluster similarity and inter-cluster similarity.In this example, it also appears that the number and size of clusters remains roughly constant; this observation is confirmed byplotting the number of clusters versus regularization strength in Supplemental Figure S4.Figure 3 shows a number of surprising patterns that contradict our initial predictions. First, and most saliently, we had predicted that∗Qdropout would reduce modularity, but found instead that it has the greatest effect on among the three regularization methods wetried. This is especially apparent in the upstream methods (first two columns of the figure), and is also stronger for the first hidden∗Qlayer than the second (Supplemental Figure S3). In general, can increase either if the network partitions into a greater number of∗Qclusters, or if the contrast between clusters is exaggerated. We found that this dramatic effect of dropout on was accompanied∗Qby only minor changes to the number of clusters (Supplemental Figure S4), and so we can conclude that dropout increases byincreasing the redundancy of hidden units. In other words, hidden units become more clusterable because they are driven towardsbehaving like functional replicas of each other, separately for each cluster. This observation echoes, and may explain, why dropoutalso increases the ""clusterability"" of network weights in a separate study. ∗QThe second surprising result in Figure 3 is that L2 regularization on the weights did, in fact, increase , whereas we had expected itto have no impact. Third, L1 regularization had a surprisingly weak effect, although its similarity to the L2 regularization resultsmay be explained by the fact that they actually resulted in fairly commensurate sparsity in the trained weights (Supplemental FigureS1 bottom row). Fourth, we had expected few differences between the eight different methods for computing similarity, but thereappear to be distinctive trends by similarity type both in Figure S3 as well as in the number of clusters detected (SupplementalFigure S4). The next section explores the question of similarity in the results in more detail.4.3 Comparing modules discovered by different similarity methods ∗QThe previous section discussed idiosyncratic trends in the modularity scores as a function of both regularization strength and∗Qhow pairwise similarity between units (S) is computed. However, such differences in the quantitative value of are difficult tointerpret, and would largely be moot if the various methods agreed on the question of which units belong in which cluster. We now∗Pturn to the question of how similar the cluster assignments are across our eight definitions of functional modules. To minimizeambiguity, we will use the term ""functional-similarity"" to refer to S, and ""cluster-similarity"" to refer to the comparison of different∗Pcluster assignments . 5Quantifying similarity between cluster assignments is a well-studied problem, and we tested a variety of methods in the clusimPython package. All cluster-similarity methods we investigated gave qualitatively similar results, so here we report only the ""ElementSimilarity"" method of Gates et al., which is a value between 0 and 1 that is small when two cluster assignments are unrelated, and∗Plarge when one cluster assignment is highly predictive of the other. Note that this cluster-similarity analysis is applied only tocluster assignments computed in the same layer of the same model. Thus, any dissimilarity in clusters that we see is due entirely tothe different choices for functional-similarity, S.Figure 4a summarizes the results of this cluster-similarity analysis: there is a striking difference between clusters of units identified by˜ ˜cov cov i−sens i−sensS S S S""upstream"" functional-similarity methods ( , , , ) compared to ""downstream"" functional-similarity methods˜ ˜hess hess o−sens o−sensS S S S( , , , ). This analysis also reveals secondary structure within each class of upstream and downstream˜Smethods, where the choice to normalize not (S vs ) appears to matter little, and where there is a moderate difference betweencov hess i−sens o−sensS S S Smoment-based methods ( , ) and gradient-based methods ( , ). It is worth noting that some of this secondarystructure is not robust across all types and levels of regularization; in particular, increasing L2 or L1 regularization strength appearsto lead to (i) stronger dependence on normalization in the downstream methods, and (ii) a stronger overall agreement among theupstream methods (Supplemental Figure S5).We next asked to what extent these cluster-similarity results are driven by training. As shown in Figure 4b, much of the structurein the downstream methods is unaffected by training (i.e. it is present in untrained models as well), while the cluster-similarityamong different upstream methods only emerged as a result of training. Interestingly, this analysis further shows that the mainupstream-vs-downstream distinction seen in Figure 4a is, in fact, attenuated slightly by training.5 ConclusionsThe prevalence of ""modular"" designs in both engineered and evolved systems has led many to consider the benefits of modularity asa design principle, and how learning agents like artificial neural networks might discover such designs. However, precisely definingwhat constitutes a ""module"" within a neural network remains an open problem. In this work, we operationalized modules in a neuralnetwork as groups of hidden units that carry out similar functions. This naturally leads to the question of what makes any two unitsfunctionally similar. We introduced eight functional similarity measures designed to capture various intuitions about unit similarityand empirically evaluated cluster assignments based on each method in a large number of trained models.∗QOne unexpected observation was that dropout increases modularity (as defined by ), although this has little to do with thecommon-sense definition of a ""module."" Instead, it is a byproduct of dropout causing subsets of units to behave like near-copiesof each other, perhaps so that if one unit is dropped out, a copy of it provides similar information to the subsequent layer. To ourknowledge, this redundancy-inducing effect of dropout has not been noted in the literature previously.Our main result is that there is a crucial difference between defining ""function"" in terms of how units are driven by upstream inputs,and how units drive downstream outputs. While we studied this distinction between upstream and downstream similarity in thecontext of modularity and clustering, it speaks to the deeper and more general problem of how best to interpret neural representations.For example, some sub-disciplines of representation-learning (e.g. ""disentanglement"") have long emphasized that a ""good"" neuralrepresentation is one where distinct features of the world drive distinct sub-populations or sub-spaces of neural activity. This is anupstream way of thinking about what is represented, since it depends only on the relationship between inputs and the unit activationsand does not take into account what happens downstream. Meanwhile, many have argued that the defining characteristic of a neuralrepresentation is its causal role in downstream behavior; this is, of course, a downstream way of thinking. At a high level, one wayto interpret our results is is that upstream and downstream ways of thinking about neural representations are not necessarily aligned,even in trained networks. This observation is reminiscent of recent empirical work finding that ""disentangled"" representations inauto-encoders (an upstream concept) do not necessarily lead to improved performance or generalization to novel tasks (a downstreamconcept).Despite its theoretical motivations, this is an empirical study. We trained over 250 feedforward, fully-connected neural networks onMNIST. While it is not obvious whether MNIST admits a meaningful ""modular"" solution, we expect that the main results we showhere are likely robust, in particular (i) the effect of weight decay, an L1 weight penalty, and dropout, and (ii) misalignment betweenupstream and downstream definitions of neural similarity.Our work raises the important questions: are neural representations defined by their inputs or their outputs? And, in what contextsis it beneficial for these to be aligned? We look forward to future work applying our methods to larger networks trained on morestructured data, as well as recurrent networks. We also believe it will be valuable to evaluate the effect of attempting to maximizemodularity, as we have defined it, during training, to see to what extent this is possible and whether it leads to performance benefits.Note that maximizing Q during training is challenging because (i) computing S may require large batches, and more importantly(ii) optimizing Q is highly prone to local minima, since neural activity and cluster assignments P will tend to reinforce each other,entrenching accidental clusters that appear at the beginning of training. We suspect that maintaining uncertainty over clusterP Passignments (e.g. using soft in [0, 1] rather than hard in 0, 1 cluster assignments) will be crucial if optimizing any of ourijproposed modularity metrics during training. 6ReferencesMohammed Amer and Tomás Maul. A review of modularization techniques in artificial neural networks. ArtificialIntelligence Review, 52(1):527-561, 2019.Jacob Andreas. Measuring compositionality in representation learning. arXiv, pp. 1-15, 2019.Farooq Azam. Biologically inspired modular neural networks. Phd, Virginia Polytechnic Institute and State University,2000.Francis R. Bach and Michael I. Jordan. Kernel independent component analysis. Journal of Machine Learning Research,3(1):1-48, 2003.Francis R. Bach and Michael I. Jordan. Beyond independent components: Trees and clusters. Journal of Machine LearningResearch, 4(7-8):1205-1233, 2004.Shahab Bakhtiari, Patrick Mineault, Tim Lillicrap, Christopher C Pack, and Blake A Richards. The functional specializationof visual cortex emerges from training parallel pathways with self-supervised predictive learning. NeurIPS, 3, 2021.Gabriel Béna and Dan F. M. Goodman. Extreme sparsity gives rise to functional specialization. arXiv, 2021.Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new perspectives. IEEETransactions on Pattern Analysis and Machine Intelligence, 35(8):1798-1828, 2013.U. Brandes, D. Delling, M. Gaertler, R. Gorke, M. Hoefer, Z. Nikoloski, and D. Wagner. On Modularity Clustering. IEEETransactions on Knowledge and Data Engineering, 20(2):172-188, 2008.Jeff Clune, Jean Baptiste Mouret, and Hod Lipson. The evolutionary origins of modularity. Proceedings of the RoyalSociety B, 280, 2013.Rion B Correia, Alexander J Gates, Xuan Wang, and Luis M Rocha. Cana: A python package for quantifying control andcanalization in boolean networks. Frontiers in physiology, 9:1046, 2018.Corinna Cortes, Mehryar Mohri, and Afshin Rostamizadeh. Algorithms for learning kernels based on centered alignment.Journal of Machine Learning Research, 13:795-828, 2012.Róbert Csordás, Sjoerd van Steenkiste, and Jürgen Schmidhuber. Are Neural Nets Modular? Inspecting FunctionalModularity Through Differentiable Weight Masks. ICLR, 2021.J Denker, D Schwartz, B Wittner, S Solla, R Howard, L Jackel, and J Hopfield. Large Automatic Learning, Rule Extraction,and Generalization. Complex Systems, 1:877-922, 1987.Andrea Di Ferdinando, Raffaele Calabretta, and Domenico Parisi. Evolving Modular Architectures for Neural Networks.Proceedings of the sixth Neural Computation and Psychology Workshop: Evolution, Learning, and Development, pp.253-262, 2001.Cian Eastwood and Christopher K.I. Williams. A framework for the quantitative evaluation of disentangled representations.ICLR, 2018.Daniel Filan, Stephen Casper, Shlomi Hod, Cody Wild, Andrew Critch, and Stuart Russell. Clusterability in NeuralNetworks. arXiv, 2021.Justin Garson and David Papineau. Teleosemantics, Selection and Novel Contents. Biology Philosophy, 34(3), 2019.Alexander J. Gates, Ian B. Wood, William P. Hetrick, and Yong Yeol Ahn. Element-centric clustering comparison unifiesoverlaps and hierarchy. Scientific Reports, 9(1):1-13, 2019.M. Girvan and M. E.J. Newman. Community structure in social and biological networks. Proceedings of the NationalAcademy of Sciences of the United States of America, 99(12):7821-7826, 2002.Melvyn A. Goodale and A. David Milner. Separate visual pathways for perception and action. TINS, 15(1): 20-25, 1992.Arthur Gretton, Olivier Bousquet, Alex Smola, and Bernhard Schölkopf. Measuring statistical dependence with Hilbert-Schmidt norms. In S. Jain, H. U. Simon, and E. Tomita (eds.), Lecture Notes in Artificial Intelligence, volume 3734, pp.63-77. Springer-Verlag, Berlin, 2005.Harold W Gutch and Fabian J Theis. Independent Subspace Analysis is Unique, Given Irreducibility. In Mike E Davies,Christopher J James, Samer A Abdallah, and Mark D Plumbley (eds.), Independent Component Analysis and SignalSeparation, volume 7. Springer, Berlin, 2007.Irina Higgins, David Amos, David Pfau, Sebastien Racaniere, Loic Matthey, Danilo Rezende, and Alexander Lerchner.Towards a Definition of Disentangled Representations. arXiv, pp. 1-29, 2018.Aapo Hyvärinen, Patrik O. Hoyer, and Mika Inki. Topographic independent component analysis. Neural Computation,13(7):1527-1558, 2001.Robert A Jacobs, Michael I Jordan, and Andrew G Barto. Task Decomposition Through Competition in a ModularConnectionist Architecture:The What and Where Vision Tasks. Cognitive Science, pp. 219-250, 1991.7Nadav Kashtan and Uri Alon. Spontaneous evolution of modularity and network motifs. Proceedings of the NationalAcademy of Sciences of the United States of America, 102(39):13773-13778, 2005.Nadav Kashtan, Elad Noor, and Uri Alon. Varying environments can speed up evolution. Proceedings of the NationalAcademy of Sciences of the United States of America, 104(34):13711-13716, 2007.Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton. Similarity of Neural Network RepresentationsRevisited. ICML, 36, 2019.Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-Based Learning Applied to Document Recogni-tion. Proceedings of the IEEE, 86(11):2278-2324, 1998.H Lipson. Principles of modularity, regularity, and hierarchy for scalable systems. Journal of Biological Physics andChemistry, 7(4):125-128, 2007.Francesco Locatello, Stefan Bauer, Mario Lucic, Sylvain Gelly, Bernhard Schölkopf, and Olivier Bachem. ChallengingCommon Assumptions in the Unsupervised Learning of Disentangled Representations. arXiv, pp. 1-33, 2019.Milton Llera Montero, Casimir JJ Ludwig, Rui Ponte Costa, Guarav Malhotra, and Jeffrey Bowers. The role of disentangle-ment in generalization. ICLR, 2021.M. E.J. Newman. Modularity and community structure in networks. Proceedings of the National Academy of Sciences ofthe United States of America, 103(23):8577-8582, 2006.M. E.J. Newman and M. Girvan. Finding and evaluating community structure in networks. Physical Review E - Statistical,Nonlinear, and Soft Matter Physics, 69(2 2):1-15, 2004.Jason A. Palmer and Scott Makeig. Contrast functions for independent subspace analysis. In Fabian J. Theis, A. Cichocki,A. Yeredor, and M. Zibulevsky (eds.), Independent Component Analysis and Signal Separation, volume LNCS 7191, pp.115-122. Springer-Verlag, Berlin, 2012.Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, ZemingLin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison,Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperativestyle, high- performance deep learning library. In Advances in Neural Information Processing Systems 32, pp. 8024-8035.Curran Associates, Inc., 2019. ˝Barnabás Póczos and András Lorincz. Independent Subspace Analysis Using Geodesic Spanning Trees. ICML, 22:673-680,2005.Karl Ridgeway and Michael C. Mozer. Learning deep disentangled embeddings with the F-statistic loss. Advances inNeural Information Processing Systems, pp. 185-194, 2018.J. G. Rueckl, K. R. Cave, and S. M. Kosslyn. Why are ""what"" and ""where"" processed by separate cortical visual systems?A computational investigation. Journal of Cognitive Neuroscience, 1(2):171-186, 1989.Bernhard Scholkopf, Francesco Locatello, Stefan Bauer, Nan Rosemary Ke, Nal Kalchbrenner, Anirudh Goyal, and YoshuaBengio. Toward Causal Representation Learning. Proceedings of the IEEE, 109(5): 612-634, 2021.Herbert A Simon. The Architecture of Complexity. Proceedings of the American Philosophical Society, 106 (6), 1962.O. Tange. Gnu parallel - the command-line power tool. ;login: The USENIX Magazine, 36(1):42-47, Feb 2011.Günter P. Wagner, Mihaela Pavlicev, and James M. Cheverud. The road to modularity. Nature Reviews Genetics,8(12):921-931, 2007.Chihiro Watanabe. Interpreting Layered Neural Networks via Hierarchical Modular Representation. Communications inComputer and Information Science, 1143 CCIS:376-388, 2019.Chihiro Watanabe, Kaoru Hiramatsu, and Kunio Kashino. Modular representation of layered neural networks. NeuralNetworks, 97:62-73, 2018.Chihiro Watanabe, Kaoru Hiramatsu, and Kunio Kashino. Understanding community structure in layered neural networks.Neurocomputing, 367:84-102, 2019.Chihiro Watanabe, Kaoru Hiramatsu, and Kunio Kashino. Knowledge discovery from layered neural networks based onnon-negative task matrix decomposition. IEICE Transactions on Information and Systems, E103D(2):390-397, 2020.Zongze Wu, Chunchen Su, Ming Yin, Zhigang Ren, and Shengli Xie. Subspace clustering via stacked independent subspaceanalysis networks with sparse prior information. Pattern Recognition Letters, 146: 165-171, 2021.A AppendixA.1 Algorithms ∗PThis section provides pseudocode for the algorithm used to compute clusters from the normalized matrix of pairwise associations˜ ˜A Abetween units, . Before running these algorithms, we always remove all-zero rows and columns from ; we consider these units toall be in a separate ""unused"" cluster. 8L2 (weight decay) L1 weight penalty dropout prob.logspace(-5,-1,9) 0.0 0.01e-5 logspace(-5,-2,7) 0.01e-5 0.0 linspace(0.05,0.7,14)Table 1: Each row describes one hyperparameter sweep, for a total of 30 distinct hyperparameter values. First row: varying weightdecay (L2 weight penalty) with no other regularization (9 values). Second row: varying L1 penalty on weights along with mildweight decay (7 values). Third row: varying dropout probability in increments of 0.05 along with mild weight decay (14 values).Algorithm 1 Full clustering algorithm. ˜ARequire: Normalized pairwise associations˜P ← A1: GreedySpectralModules( ) . Initialize P using spectral method˜∗P ← A2: MonteCarloModules( , P) . Further refine P using Monte Carlo method∗Preturn3:Algorithm 2 Pseudocode for greedy, approximate, spectral method for finding modules˜Afunction1: GreedySpectralModules( )˜˜ ˜ T AB ← A − A1 1 . . B is analogous to the graph Laplacian, but for modules2: n n T1 0 0 ... 0P ← [ ]3: . Initialize P to a single cluster, which will be (recursively) split later.nqueue ← [0]4: . FILO queue keeping track of which cluster we’ll try splitting nextTQ ← T r(P BP )5: . Compute Q for the initial Pwhile do6: queue is not emptyc ← queue.pop()7: . Pop the next (leftmost) cluster idi ←8: indices of all units currently in cluster c according to Pv ← eig(B(i, i))9: . Get the leading eigenvector of the submatrix of B containing just units in c+i ←10: subset of i where v was positive . Split v by sign (if not possible, continue loop)−i ←11: subset of i where v was negativec0 ←12: index of the next available (all zero) column of P0 −P ← P i c013: but with all units moved to cluster . Try splitting c into c, c0 based on sign of v0 0T 0 0Q ← T r(P BP ) P14: . Compute updated Q for newly-split clusters0Q > Qif then15: 0 0Q, P ← Q , P16: . Update Q and Pqueue.append(c, c0)17: . Push c and c0 onto the queue to consider further subdividing themelse18:19: . Nothing to do - splitting c into c0 did not improve Q, so we don’t add further subdivisions to the queue, and wekeep the old P, Q valuesend if20: end while21: return22: P . Once the queue is empty, P contains a good initial set of cluster assignmentsend function23:Algorithm 3 Pseudocode for Monte Carlo method for improving clusters.˜Afunction1: MonteCarloModules( , P, n)for do2: n stepsi ←3: index of a single a randomly selected unit 9c ←4: index of the first empty cluster in P˜ ˜ ˜∗ ∗ T TQ , P ← T r(P (A − A1 1 A)P ), P5: . Keep track of best Q, P pair found so farn nj = 1...cfor do6: . Try moving unit i to each cluster j, including a new cluster at c0P ← P7: with i reassigned to cluster j˜ ˜ ˜0 0T T 0Q ← T r(P (A − A1 1 A)P )8: . Compute updated Q with re-assigned unitn nj 0 ∗Q > Qif then9: j ∗ ∗ 0 0 ∗ ∗Q , P ← Q , P Q P10: . Update , pair, even if we don’t select this j laterjend if11: end for12: 0Q /ττ ← p ∝ e H = 0.1513: whatever temperature makes have entropy(cid:80) 00 Q /τQ /τp ← e / e14: . We found H = 0.15 strikes a good balance between exploration and greedy ascent.jj∗j ∼ p15: . Sample new cluster assignment j from categorical distribution p∗P ← P j16: with unit i reassigned to cluster , ensuring only the leftmost columns have nonzero valuesend for17: ∗Preturn18: end function19:A.2 Supplemental Figures [width=]images.pngFigure 1: Basic performance metrics as a function of regularization strength. Each column corresponds to a different regularizationmethod, as in Table 1. Each row shows a metric calculated on the trained models. Thin colored lines are individual seeds, and thickblack line is the average ± standard error across runs. Horizontal gray line shows each metric computed on randomly initializednetwork. Sparsity (bottom row) is calculated as the fraction of weights in the interval [-1e-3, +1e-3].[width=0.45]image1.png [width=0.45]image2.png ∗QFigure 2: Both spectral initialization and Monte Carlo optimization steps contribute to finding a good value of . Left: The x-axis∗ ∗Q Pshows modularity scores ( ) achieved using only the greedy spectral method for finding . The y-axis shows the actual scores weused in the paper by combining the spectral method for initialization plus Monte Carlo search. The fact that all points are on orabove the y=x line indicates that the Monte Carlo search step improved modularity scores. Right: The x-axis now shows modularity∗Qscores ( ) achieved using 1000 Monte Carlo steps, after initializing all units into a single cluster (we chose a random 5% of thesimilarity-matrices that were analyzed in the main paper to re-run for this analysis, which is why there are fewer points in thissubplot than in the left subplot). The fact that all points are on or above the y=x line indicates that using the spectral method toinitialize improved the search. [width=]image3.png∗QFigure 3: Modularity score ( ) versus regularization, split by layer. Format is identical to Figure 3, which shows modularity scoresaveraged across layers. Here, we break this down further by plotting each layer separately. The network used in our experiments hastwo hidden layers. The first two rows (white background) shows modularity scores for the first hidden layer h1, and the last tworows (gray background) shows h2. 10[width=]image4.png∗PFigure 4: Number of clusters in versus regularization, split by layer. Layout is identical to Figure S3. Gray shading in theσ σ σbackground shows 1 , 2 , and 3 quantiles of number of clusters in untrained (randomly initialized) networks. Note that, for themost part, training has little impact on the number of clusters detected, suggesting that consistently finding on the order of 2-6clusters is more a property of the MNIST dataset itself than of training. We computed the number of clusters using equation (10).This measure is sensitive to both the number and relative size of the clusters.[width=]image5.pngFigure 5: Further breakdown of cluster-similarity by regularization strength (increasing left to right) and type (L2/L1/dropout).Results in Figure 4 reflect an average of the results shown here. The six rows of this figure should be read in groups of two rows: ineach group, the top row shows the similarity scores (averaged over layers and runs), and the bottom row shows the difference tountrained models. A number of features are noteworthy here: (i) at low values of all three types of regularization, there is littlecluster- similarity within the upstream methods, but it becomes very strong at as regularization strength grows; (ii) at the highestvalues of L2 and L1 regularization, the pattern inside the 4x4 block of downstream methods changes to depend more strongly onnormalization; (iii) a moderate amount of agreement between upstream and downstream methods is seen for large L1 regularizationstrength, but curiously only for unnormalized downstream methods.11"
P089,"Precise Requirements for the Validity of the Neural Tangent KernelApproximationAbstractThis research investigates the conditions under which the neural tangent kernel (NTK) approximation remainsvalid when employing the square loss function for model training. Within the framework of lazy training, asα = O(T )introduced by Chizat et al., we demonstrate that a model, rescaled by a factor of , maintains the validityTof the NTK approximation up to a training time of . This finding refines the earlier result from Chizat et al.,2α = O(T )which necessitated a larger rescaling factor of , and establishes the preciseness of our establishedbound.1 Introduction R Rd dw f : →In contemporary machine learning practice, the weights of expansive neural network models are trainedin outwusing gradient-based optimizers. However, a comprehensive theoretical understanding remains elusive due to the non-linear natureof the training dynamics, which complicates analysis. To bridge this gap, an approximation to these dynamics, termed the NTKapproximation, was introduced, and its validity for infinitely wide networks trained via gradient descent was demonstrated. The NTKapproximation has proven highly influential, offering theoretical insights into various phenomena, including deep learning’s capacityto memorize training data, the manifestation of spectral bias in neural networks, and the differential generalization capabilities ofdiverse architectures. Nevertheless, empirical evidence suggests that the training dynamics of neural networks frequently deviatefrom the NTK approximation’s predictions. Consequently, it becomes crucial to delineate the precise conditions under which theNTK approximation remains applicable. This paper seeks to address the following inquiry:Is it possible to establish precise conditions that guarantee the validity of the NTK approximation?1.1 The Lazy Training FrameworkThe work demonstrated that the NTK approximation is applicable to the training of any differentiable model, provided the model’soutputs are rescaled appropriately. This rescaling ensures that significant changes in the model’s outputs can occur even with minoradjustments to the weights. The validity of the NTK approximation for models of infinite width stems from this observation, as themodel is inherently rescaled as its width approaches infinity.Rph : → F F α > 0Consider a smoothly parameterized model , where is a separable Hilbert space. Let be a parameter governingαhthe model’s rescaling, which should be considered large. We train the rescaled model using gradient flow to minimize a smoothR R+ pR : F → w(t) ∈ w(0) = wloss function . The weights are initialized at and evolve according to the gradient flow:01dw = − ∇ R(αh(w(t))). (1)w2dt α wDefine the linear approximation of the model around the initial weights as:0¯h(w) = h(w ) + Dh(w )(w − w ), (2)0 0 0Dh h w w¯(t) w¯(0) = wwhere is the first derivative of with respect to . Let be weights initialized at that evolve according to the0¯αhgradient flow from training the rescaled linearized model :dw¯ 1 ¯= − ∇ R(αh(w¯(t))). (3)w¯2dt αThe NTK approximation asserts that: ¯αh(w(t)) ≈ αh(w¯(t)). (4)hIn essence, this implies that the linearization of the model remains valid throughout the training process. This greatly simplifies¯ ¯h h(w¯)the analysis of training dynamics, as the model is linear in its parameters, allowing the evolution of to be understood througha kernel gradient flow in function space.α αThe validity of the NTK approximation is contingent on the magnitude of the rescaling parameter . Intuitively, a largerimplies that the weights need not deviate significantly from their initialization to induce substantial changes in the model’s output,thereby prolonging the validity of the linearization. This regime of training, where weights remain close to their initialization,R = R(αh(w )))is referred to as ""lazy training."" The following bound was established, where is the loss at initialization, and0 0√−1κ = T α (Dh) RLip is a quantity that will also feature in our main results:0 1 ∗ 2 ∗R(y) = ∥y − y ∥ y ∈ F h (h)**Proposition 1.1.** Let be the square loss, where are the target labels. Assume that is Lip -2 √2Dh (Dh) ρ w 0 ≤ T ≤ αρ/( (h) R )Lipschitz and that is Lip -Lipschitz in a ball of radius around . Then, for any time Lip ,0 0¯ 2∥αh(w(T )) − αh(w¯(T ))∥ ≤ T (h) κR .Lip (5)0α κAs approaches infinity, tends to 0, rendering the right-hand side of the inequality small and validating the NTK approximation.1.2 Our ContributionsOur primary contribution is the refinement of the bound for extended time scales. We establish the following theorem:1 ∗ 2∥y − y ∥R(y) = Dh (Dh)**Theorem 1.2 (NTK Approximation Error Bound).** Let be the square loss. Assume that is Lip -22 2 2ρ w 0 ≤ T ≤ α ρ /RLipschitz in a ball of radius around . Then, at any time ,0 0(cid:112)¯∥αh(w(T )) − αh(w¯(T ))∥ ≤ min(6κ R , 8R ). (6)0 0Furthermore, we demonstrate that this bound is tight up to a constant factor. R R R∗α T (Dh) R h : → y ∈**Theorem 1.3 (Converse to Theorem 1.2).** For any , , Lip , and , there exists a model , a target , and0R 1 ∗ 2w ∈ R(y) = (y − y ) R(αh(w )) = R Dhan initialization such that, for the risk , the initial risk is , the derivative map is0 0 02(Dh)Lip -Lipschitz, and (cid:18) (cid:19)1 1(cid:112)¯∥αh(w(T )) − αh(w¯(T ))∥ ≥ min κ R , R . (7)0 05 5hIn contrast to prior work, our bound does not depend on the Lipschitz constant of , and it exhibits a more favorable dependence onT (Dh) (h) R. Specifically, if Lip , Lip , and are bounded by constants, our result indicates that the NTK approximation, up to an√0O(ϵ) T = O(αϵ) T = O( αϵ)error of , holds for times , whereas the previously known bound was valid for . Given the practicalT ≫ 1interest in long training times , our result demonstrates that the NTK approximation is valid for significantly longer timehorizons than previously recognized.2 Application to Neural NetworksThe bound established in Theorem 1.2 is applicable to the lazy training of any differentiable model. As a specific example, we detailits application to neural networks. We parameterize the networks in the mean-field regime, where the NTK approximation does nothold even as the width approaches infinity. Consequently, the NTK approximation is valid only when training is conducted in thelazy regime.R R R Rdf : → m σ : →Let be a 2-layer network of width in the mean-field parametrization, with activation function ,w m √1 (cid:88)√f (x) = a σ( m⟨x, u ⟩). (8)w i im i=1w = (a, U ) a = [a , . . . , a ] U = [u , . . . , u ] wThe weights are for and . These are initialized at with i.i.d.√ √ 1 m 1 m 0[−1/ m, 1/ m] (x , y ), . . . , (x , y )Unif entries. Given training data , we train the weights of the network with the mean-1 1 n nsquared loss n1 1(cid:88) 2L(w) = ℓ(f (x ), y ), ℓ(a, b) = (a − b) . (9)w i in 2i=1RnH =In the Hilbert space notation, we let , so that the gradient flow training dynamics with loss (6) correspond to the gradientflow dynamics (1) with the following model and loss function (cid:13) (cid:13)21 1 y(cid:13) (cid:13)Rn√ √h(w) = [f (x ), . . . , f (x )] ∈ , R(v) = v − . (10)(cid:13) (cid:13)w 1 w n 2n n(cid:13) (cid:13)2Under certain regularity assumptions on the activation function (satisfied, for instance, by the sigmoid function) and a bound on the(Dh)weights, it can be shown that Lip is bounded.(Dh) K**Lemma 2.1 (Bound on Lip for mean-field 2-layer network).** Suppose there exists a constant such that (i) the activation′ ′′ ′′′σ ∥σ∥ , ∥σ ∥ , ∥σ ∥ , ∥σ ∥ ≤ Kfunction is bounded and has bounded derivatives , (ii) the weights have bounded norm∞ ∞ ∞ ∞ ′∥U ∥ ≤ K ∥x∥ ≤ K K K, and (iii) the data points have bounded norm . Then there exists a constant depending only on such thata ′(Dh) ≤ K .Lip (11)2Since the assumptions of Theorem 1.2 are met, we obtain the following corollary for the lazy training dynamics of the 2-layermean-field network.**Corollary 2.2 (Lazy training of 2-layer mean-field network).** Suppose the conditions of Lemma 2.1 hold, and also that the labels2∥y∥ ≤ c C, c > 0 K 0 ≤ T ≤ cαare bounded in norm . Then there exist constants depending only on such that for any time ,¯∥αh(w(T )) − αh(w¯(T ))∥ ≤ C min(T /α, 1). (12)√mf fTraining in the NTK parametrization corresponds to training the model , where is the network in the mean-field√ w wα = mparametrization. This is equivalent to setting the lazy training parameter in the mean-field setting. Therefore, under themNTK parametrization with width , the bound in Corollary 2.2 indicates that the NTK approximation is valid until training time√O(m) O(T / m)and the error bound is .3 Proof Ideas3.1 Proof Ideas for Theorem 1.2 r(t), r¯(t) ∈ FTo provide intuition for our proof, we first outline the approach used in the original proof. Define residuals¯ ∗αh(w(t)) αh(w¯(t)) r(t) = y − αh(w(t))under training the original rescaled model and the linearized rescaled model as and¯∗r¯(t) = y − αh(w¯(t)). These evolve according todr dr¯= −K r = −K r¯,and (13)t 0dt dt∗K := Dh(w(t))Dh(w(t)) Kwhere is the time-dependent kernel. To compare these trajectories, it was observed that, since ist 0positive semidefinite, d 2∥r − r¯∥ = −⟨r − r¯, K r − K r¯⟩ ≤ −⟨r − r¯, (K − K )r⟩ (14)t 0 t 02dt √R∥r − r¯∥ ∥r∥ ≤which, dividing both sides by and using , implies0d (cid:112)∥r − r¯∥ ≤ ∥K − K ∥∥r∥ ≤ 2 (h) (Dh)∥w − w ∥ R .Lip Lip (15)t 0 0 0dt √∥w(t) − w ∥ ≤ t R (h)/αUsing the Lipschitzness of the model, it was further shown that the weight change is bounded by Lip .0 0Plugging this into (7) yields the bound in Proposition 1.1, (cid:90) T¯ 2 −1 2 2∥αh(w(T )) − αh(w¯(T ))∥ = ∥r(T ) − r¯(T )∥ ≤ 2 (h) (Dh)R α tdt = T (h) (Dh)R /α.Lip Lip Lip Lip (16)0 00**First attempt: strengthening of the bound for long time horizons** We demonstrate how to strengthen this bound to hold for longertime horizons by employing an improved bound on the movement of the weights. Consider the following bound on the weightchange.**Proposition 3.1 (Bound on weight change, implicit in proof of Theorem 2.2).**(cid:112) (cid:112)∥w(T ) − w ∥ ≤ T R /α ∥w¯(T ) − w ∥ ≤ T R /α.and (17)0 0 0 0R**Proof of Proposition 3.1.** By (a) Cauchy-Schwarz, and (b) the nonnegativity of the loss ,(cid:115) (cid:115)(cid:13) (cid:13) (cid:13) (cid:13)2(cid:90) (cid:90) (cid:90)T T T(a) (b)dw dw T d (cid:112)(cid:13) (cid:13) (cid:13) (cid:13)∥w(T ) − w(0)∥ ≤ dt ≤ T dt = − R(αh(w(t)))dt ≤ T R /α. (18)(cid:13) (cid:13) (cid:13) (cid:13) 02dt dt α dt(cid:13) (cid:13) (cid:13) (cid:13)0 0 0w¯The bound for is analogous. √t t (h)This bound (8) has the advantage of dependence (instead of linear dependence) and does not depend on Lip . Plugging itinto (7), we obtain (cid:90) T √ 4¯ −1 3/2∥αh(w(T )) − αh(w¯(T ))∥ ≤ 2 (h) (Dh)R α tdt = T (h) (Dh)R /α.Lip Lip Lip Lip (19)0 030 3/2 2T TThis improves over Proposition 1.1 for long time horizons, as the time dependence scales as instead of . However, it still(h) Tdepends on the Lipschitz constant Lip and falls short of the linear in dependence of Theorem 1.2.(h) T**Second attempt: new approach to prove Theorem 1.2** To avoid dependence on Lip and achieve a linear dependence in ,(h)we develop a new approach. We cannot use (7), which was central to the original proof, as it depends on Lip . Furthermore, toT ∥w − w ∥ = O(1)achieve linear dependence using (7), we would need for a constant independent of the time horizon, which is0not true unless the problem is well-conditioned. 3∥r(T ) − r¯(T )∥In the full proof in Appendix A, we bound , which requires working with a product integral formulation of ther Kdynamics of to handle the time-varying kernels . The main technical innovation in the proof is Theorem A.8, which is a new,tgeneral bound on the difference between product integrals.To avoid the technical complications of the appendix, we provide some intuitions here by proving a simplified theorem that does notimply the main result. We show: ′dr′ ′ ′r (t) ∈ F r (0) = r(0) = −K r**Theorem 3.2 (Simplified variant of Theorem 1.2).** Consider initialized as and evolving as .TdtThen, (cid:112)′∥r (T ) − r¯(T )∥ ≤ min(3κ R , 8R ). (20)0 0′r (T ) r¯(T ) r(T ) r¯(T )Intuitively, if we can prove in Theorem 3.2 that and are close, then the same should hold for and as inTheorem 1.2. For convenience, define the operators∗ ∗ ∗A = Dh(w ) B = Dh(w(T )) − Dh(w ) .and (21)0 0Since the kernels do not vary in time, the closed-form solution is∗ ∗′ −(A+B) (A+B)t −A Atr (t) = e r(0) r¯(t) = e r(0)and (22)′r r¯We prove that the time evolution operators for and are close in operator norm.√∗ ∗−(A+B) (A+B)t −A Att ≥ 0 ∥e − e ∥ ≤ 2 t∥B∥**Lemma 3.3.** For any , we have .∗Z(ζ) = (A + ζB) (A + ζB)t**Proof of Lemma 3.3.** Define . By the fundamental theorem of calculus,(cid:13)(cid:13) (cid:13) (cid:13)(cid:90) 1 dd (cid:13)(cid:13) (cid:13) (cid:13)∗ ∗−(A+B) (A+B)t −A At Z(1) Z(0) Z(ζ) Z(ζ)≤ sup∥e − e ∥ = ∥e − e ∥ = e dζ e . (23)(cid:13)(cid:13) (cid:13) (cid:13)dζ dζ(cid:13)(cid:13) (cid:13) (cid:13)ζ∈[0,1]0Using the integral representation of the exponential map, (cid:13)(cid:13) (cid:13)(cid:13) (cid:13)(cid:13) (cid:18) (cid:19)(cid:90) (cid:90)1 1dd (cid:13)(cid:13) (cid:13)(cid:13) (cid:13)(cid:13) (1−τ)Z(ζ) (1−τ)Z(ζ) ∗ ∗ ∗ τZ(ζ)Z(ζ) τZ(ζ)e e (A B + B A + 2ζB B)e dτ= =e Z(ζ) e dτ (24)(cid:13)(cid:13) (cid:13)(cid:13) (cid:13)(cid:13) dζ dζ (cid:13)(cid:13) (cid:13)(cid:13) (cid:13)(cid:13) 0 0 τZ(ζ)∥e ∥ ≤ 1By symmetry under transposing and reversing time, it suffices to bound the first term. Since ,(cid:13)(cid:13) (cid:90)(cid:90) 1 1 √(cid:13)(cid:13) (1−τ)Z(ζ) ∗(1−τ)Z(ζ) ∗ τZ(ζ) ≤ ∥e (A + ζB) ∥∥tB∥dτ ≤ 2t/e∥B∥ ≤ 2e (A + ζB) Be tdτ t∥B∥ (25)(cid:13)(cid:13) (cid:13)(cid:13) 00Finally, let us combine Lemma 3.3 with the weight-change bound in Proposition 3.1 to prove Theorem 3.2. Notice that theweight-change bound in Proposition 3.1 implies (cid:112)∥B∥ ≤ (Dh)∥w(T ) − w ∥ ≤ (Dh) T R /α.Lip Lip (26)0 0So Lemma 3.3 implies (cid:112)′ −1∥r (T ) − r¯(T )∥ ≤ 2 (Dh)T R α ∥r(0)∥ = 2κ∥r(0)∥.Lip (27)0√′ ′∥r (T ) − r¯(T )∥ ≤ ∥r (T )∥ + ∥r¯(T )∥ ≤ 2 2RCombining this with implies (9). Thus, we have shown Theorem 3.2, which is the0′r r Kresult of Theorem 1.2 if we replace by . The actual proof of the theorem handles the time-varying kernel and is in AppendixtA.3.2 Proof Ideas for Theorem 1.3 √1 2h(w) = aw + bw a = 1/ T b = (Dh)The converse in Theorem 1.3 is achieved in the simple case where for and Lip , and√ 21 2w = 0 R(y) = (y − 2R )and , as we show in Appendix B by direct calculation.0 024 DiscussionA limitation of our result is that it applies only to gradient flow, which corresponds to SGD with infinitesimally small step size.However, larger step sizes are beneficial for generalization in practice, so it would be interesting to understand the validity of theNTK approximation in that setting. Another limitation is that our result applies only to the square loss and not to other popularlosses such as the cross-entropy loss. Indeed, the known bounds in the setting of general losses require either a ""well-conditioning""α Tassumption or taking exponential in the training time . Can one prove bounds analogous to Theorem 1.2 for more general losses,α Twith depending polynomially on , and without conditioning assumptions?A natural question raised by our bounds in Theorems 1.2 and 1.3 is: how do the dynamics behave just outside the regime where theh (h) (Dh)NTK approximation is valid? For models where Lip and Lip are bounded by a constant, can we understand the dynamicsT ≈ Cα C α ≫ Cin the regime where for some large constant and , at the edge of the lazy training regime?4"
P091,"An Investigation into Named Entity Recognition forCall Center Transcripts to Ensure Privacy LawComplianceAbstractThis study explores the application of Named Entity Recognition (NER) on anovel form of user-generated text, specifically call center conversations. Thesedialogues present unique challenges, blending the complexities of spontaneousspeech with issues specific to conversational Automatic Speech Recognition (ASR),such as inaccuracies. By employing a custom corpus with manual annotations,training contextual string embeddings, and implementing a BiLSTM-CRF model,we achieve results that are on par with the state-of-the-art for this new task.1 IntroductionThis paper addresses the crucial need to identify and handle sensitive personal information withincall center transcripts, which are generated as a result of speech recognition systems. Although thesetranscripts are typically redacted for Payment Card Industry (PCI) compliance, they still often containa caller’s name and internal ID number, which can be useful for quality assurance. However, newprivacy laws, such as the General Data Protection Regulation (GDPR) in the EU, establish stringentguidelines concerning data collection, storage, and an individual’s right to withdraw consent fordata usage. To adhere to these regulations without losing the data’s value, it is essential to pinpointnon-public personal and personally identifiable information (NPI/PII) in call transcripts.We utilize Named Entity Recognition (NER) to locate instances of NPI/PII within the transcripts,remove them, and replace them with appropriate tags that denote the type of removed data. Forinstance, a transcript such as ""This is john doe reference number 12345"" would be transformed into""This is [NAME] reference number [NUMBER]"". This task is distinctive to call centers for severalreasons. First, these transcripts consist of natural human conversations, which have many commonproblems of user-generated content such as incomplete sentences and unusual words. Furthermore,transcript text is produced by Automatic Speech Recognition (ASR) systems, which are susceptible toerrors, as will be described in Section 3.1. Even though modern ASR systems are usually reliable, thesource audio is from phone calls, which is often low quality and contains background noise. The pooraudio quality leads to incorrect ASR, producing ungrammatical sentences. This makes understandingthe call semantics and identifying features essential to NER systems more difficult. Moreover, calltranscripts frequently lack capitalization, numeric digits, and proper punctuation, which are crucialfeatures for classic NER methods. Also, traditional NER systems are inadequate for handling emails,addresses, or spellings, which makes it difficult to use pre-trained NER models.In this paper, we apply the current best neural network architecture for sequence labeling, a BiLSTM-CRF, to the task of identifying NPI and PII in call transcripts. We match the state-of-the-art perfor-mance on standard datasets by using our model with annotated data and custom contextual stringembeddings.2 Related WorkNamed Entity Recognition has become a focus in the field of Natural Language Processing (NLP),particularly since the Message Understanding Conferences (MUCs) in the 1990s. The CoNLL2003shared task in 2003 concentrated on language-independent NER and popularized feature basedsystems. The OntoNotes corpus, released in 2006, has been vital to the progress of NER research.Following the CoNLL task, Conditional Random Field (CRF) based models became the mostsuccessful, which requires that features be manually produced. Current research utilizes neuralnetworks to generate these features. Bidirectional Long Short Term Memory models with a CRF layer(BiLSTM-CRF) have been used successfully on CoNLL2000 and CoNLL2003 datasets. A BiLSTM-CNN-CRF has been used for NER on the CoNLL2003 dataset, producing superior results. Similarresults were achieved by a BiLSTM-CNN with features from word embeddings and the lexicon.Embeddings have been used for both words and entity types to create more robust models. Flair, withcharacter-based embeddings and a pooling approach, has set the state of the art. Crossweigh usesFlair embeddings to address mishandled annotations.In 2006, the word confidence scores from ASR systems were used as a feature for NER. Similarexperiments were done on French radio and TV audio. Neither of those used natural conversation,and the quality of the audio was superior, making ASR a more accurate task.2.1 Conversations are Different: The Twitter AnalogyMuch of the past research has used newswire datasets. While newswire data is expected to conformto standard text conventions, call center transcripts do not have these conventions. This presents aproblem for the usual approaches to NER and is further complicated by our poor audio quality.Speaker 1: Thank you for calling our company how may i help you today.Speaker 2: Id like to pay my bill.Table 1: An example of turns of a conversation, where each person’s line in the dialogue representstheir turn. This output matches the format of our data described in Section 3.The most similar research area to this is work on Twitter data. Similar to our transcripts, tweets areuser-generated and may not have conventional grammar or spelling. Initial research tackled thisproblem with a K-nearest neighbors model combined with a CRF. A model combining a multi-stepneural network with a CRF output layer achieved first place in the 2017 Workshop on Noisy User-generated Text (W-NUT). The success of pooled contextualized string embeddings was also shownwith this data. We use prior work on tweets to direct our model creation for call center data.3 DataOur dataset includes 7,953 training, 500 validation, and 534 test samples. Each sample representsa complete speaker turn from a debt collection call center. A speaker turn is defined as a completetranscription from one speaker before another speaker starts, as shown in Table 1. The training set isa random sample of turns from 4 months of call transcripts. The transcripts were generated using aproprietary speech recognition system, which outputs all lowercase transcripts without punctuationor numeric digits. We used spaCy to convert each turn to a document that begins with a capital letterand ends with a period, as this is the default for spaCy. In order to make use of entities, a Sentencizermodule was added, which defaults to this capitalization and period structure.3.1 Data AnnotationWe created a schema for annotating the training and validation data with different types of NPI/PII,which are shown in Table 2.Initial annotations were performed using Doccano. The annotators were trained in NPI/PII recognition,and were instructed to err on the side of caution in unclear instances. Ambiguity often came fromerrors in the ASR model. The lack of audio meant it was sometimes unclear if ""I need oak leaves""was actually ""Annie Oakley"". The opposite was also true such as when ""Brilliant and wendy jeff to2Entity Type DescriptionNUMBERS A sequence of numbers related to a customer’s information (e.g. phone numbers or internal ID number)NAME First and last name of a customer or agentCOMPANY The name of a companyADDRESS A complete address, including city, state, and zip codeEMAIL Any email addressSPELLING Language that clarifies the spelling of a word (e.g. ""c as in cat"")Table 2: A brief description of our annotation schema.process the refund"" was actually ""Brilliant and when did you want to process the refund"". Emailswere also difficult, as errors in ASR made it difficult to determine the bounds of the email address.Also, the transcripts were pre-redacted for PCI compliance. This redaction can obscure importantdata, for example, sometimes a customer ID is redacted as part of the PCI redaction process. Tolessen false negatives, we use context to include the [redacted] tag as part of the numbers sequencewhen possible. No steps to clean the transcripts were taken; the natural noise in the data was left forthe model to interpret.Due to limitations with spaCy and the complexity of nested entities, we only allowed one annotationper word in the dataset. This means, for instance, that ""c a t as in team at gmail dot com"" would belabeled either as SPELLING[0:6] EMAIL[6:] or as EMAIL[0:] with the indices corresponding to theposition of words in the text. This ultimately results in a lower count of SPELLING entities, becausethese are often part of EMAIL or ADDRESS entities, which influences our analysis in Section 6.4 Model DesignWe utilized a standard BiLSTM-CRF model in PyTorch, adapted from a GitHub repository. We wroteour own main.py to use our spaCy preprocessing, and adapted the code to handle batch processing.After preprocessing, we trained the model on the training set and used the validation set for modeltuning. All numbers in this paper are reported on the test set. A visualization of our model is shownin Figure 1.5 Experiments5.1 Basic Hyperparameter TuningWe used a grid search algorithm to maximize model performance. The word embedding layer usesFastText embeddings trained on the client’s call transcripts. This aids in mitigating the impacts ofpoor ASR, and this will be explored in Sections 5.2 and 5.3. The grid search included the parameters:epochs (a sampled distribution between 5 and 50), the size of a dropout layer (between 0 and 0.5,with 0.1 intervals of search), the number of hidden layers (between 5 and 20 in increments of 5), andthe encoding type used in the output of the CRF (BIO, BILOU, IO). The other hyperparameters werea learning rate of .001, a batch size of 1, 30 nodes in each fully connected layer, and the inclusion ofbias in each layer. The experiments were run in parallel on a virtual machine with 16 CPUs and 128GB of memory. Each experiment took a few hours to run.To understand the performance of the model, we broke down the measurements of precision, recall,and F1 by entity type. Table 3 shows these results for the best model configuration. This model used46 epochs, a dropout rate of 0.2, 5 hidden layers, and a BIO encoding.5.2 Training Word EmbeddingsMost past research has fine-tuned existing word embeddings, but the task of mitigating misrecognitionseemed more complex than domain adaptation. To lessen the impact of the errors, we understand thatfrequent misrecognitions appear in contexts similar to the intended word. A custom model gives amisrecognized word a vector similar to the word it should be and not to the other meaning it has. Theimportance of domain specific word embeddings when using ASR data has been shown in research.3We ran our best performing model with the 300 dimensional GloVe 6b word embeddings. Ourembeddings were trained on roughly 216 million words. The results from the best epoch of thismodel (16) are shown in Table 3.Entity Type Precision Recall F12* Custom GloVe Custom GloVe Custom GloVeO 89.8 84.2 81.7 76.6 85.6 80.2NUMBERS 95.6 88.7 85.4 82.9 90.1 85.7NAME 89.6 92.1 91.1 88.7 90.3 90.3COMPANY 98.8 99.5 72.9 64.3 83.9 78.1ADDRESS 70.6 0.3 75.0 18.7 72.7 23EMAIL 0 07.1 0 03.1 0 04.4SPELLING 45.8 34 52.4 40.5 48.9 37.0Micro Average 89.2 85.6 79.6 74.0 84.1 79.4Table 3: The performance by entity type of the BiLSTM-CRF model on the held out test set. This tablecompares the results of our custom embeddings model (""Custom"") against the GloVe embeddings(""GloVe"").5.3 Using FlairPrevious experiments highlighted the importance of custom word embeddings to account for mis-recognition in call center transcripts. Here, we test the performance of Flair and its contextual stringembeddings.We begin by training custom contextual string embeddings based on the results of the first experiments.We use the same corpus as in Section 5.1. The tutorial on the Flair GitHub page was used with thefollowing parameters: hidden size: 1024, sequence length: 250, mini batch size: 100. We use thenewline to indicate a document change, and each turn as a separate document for consistency. Themodel’s validation loss stabilized after epoch 4, and the best version of the model was used.We conduct experiments using Flair’s SequenceTagger with default parameters and a hidden size of256.Flair uses only the custom trained Flair embeddings.Flair + FastText uses the custom trained Flair embeddings and the custom trained FastText embeddingsusing Flair’s StackedEmbeddings.Flairmean pooling uses only the custom trained Flair embeddings within Flair’s PooledFlairEmbed-ding. Mean pooling was used.Flairmean pooling + FastText uses PooledFlairEmbeddings with mean pooling and the custom trainedFastText embeddings using Flair’s StackedEmbeddings.These results are shown in Table 4.Entity Flair Flair + FastText Flairmean pooling Flairmean pooling + FastTextO 98.3 98.5 98.2 98.5NUMBERS 83.1 87.9 87.7 86.2COMPANY 81.1 80.7 80.7 80.3ADDRESS 87.5 94.1 61.5 94.1EMAIL 58.8 50.0 73.3 66.7SPELLING 55.0 57.1 55.8 57.9Micro Average 97.5 97.7 97.3 97.7Table 4: The F1 scores on the test set for each entity type for each Flair embedding experiment.46 DiscussionTable 3 shows that using custom embeddings is beneficial over using GloVe embeddings, with theexception of the EMAIL category. The Flair embeddings show a large improvement over other wordembeddings; however all four varieties of Flair models have nearly identical Micro Average F1s. Thebest performing Flair models are those that use both the custom contextualized string embeddingsand the custom FastText embeddings.Across all of the models in this paper, EMAIL and SPELLING consistently performed worse thanother categories. This is due to the overlap in their occurrences and their variable appearance. Thecustom embeddings model often identified parts of an email correctly but labeled some aspects, suchas a name, as NAME followed by EMAIL instead of labeling the whole thing as EMAIL. SPELLINGoften appears within an EMAIL entity. Due to the previously discussed limitations, the SPELLINGentity had a limited presence in our training data, with many EMAIL and ADDRESS entitiescontaining examples of SPELLING. All models frequently misidentified EMAIL as SPELLING andvice versa. Additionally, the test data had a number of turns that consisted of only SPELLING, whichwas poorly represented in training. The Flairmean pooling model outperforms the other models inEMAIL by a large margin.The results in Table 4 highlight that the NUMBERS category contains strings that appear frequentlyin the text. There are a finite number of NUMBER words in our corpus (those numeric words alongwith many instances of ""[redacted]""), and the numbers of interest in our dataset appear in very similarcontexts and do not often get misrecognized. The COMPANY entity performs well for similarreasons; when the model was able to identify the company name correctly, it was often in a commonerror form and in a known context. The model’s failures can be attributed to the training data becausethe company name is a proper noun that is not in standard ASR language models, including the onewe used. Thus, it is often misrecognized since the language model has higher probabilities assigned togrammatically correct phrases that have nothing to do with the company name. This causes variabilityin appearance, which means that not every version of the company name was present in our trainingset.Interesting variability also occurred in ADDRESS entities. Both models that used Flair and FastTextembeddings strongly outperformed the models that used only Flair, and standard Flair embeddingsstrongly outperformed the Pooled Flair embeddings. Neither version of the Flair-only model identifiedaddresses in which numbers were shown as ""[redacted]"" but both models that utilized FastText hadno issue with these instances.7 Conclusion and Future WorkThrough the use of a BiLSTM-CRF model, paired with custom-trained Flair embeddings, we achievestate-of-the-art NER performance on a new call center conversation dataset with distinct entity types.We also show the importance of training word embeddings that fully capture the intricacies of thetask. Although we cannot release our data for privacy, we have shown that existing state-of-the-arttechniques can be applied to less common datasets and tasks. Future work will include evaluatingthe model with call transcripts from other industries. We would also like to explore how well thesetechniques work on other user-generated conversations like chats and emails.5"
P092,"Enhanced Image Compression Through AdvancedResidual Network ArchitecturesAbstractThis manuscript provides an in-depth explanation of the methodology developedfor a recent image compression challenge. The method primarily incorporates twoinnovative aspects: the application of advanced residual networks for enhancedcompression and the utilization of sub-pixel convolution techniques for efficientup-sampling during decompression. The efficacy of these methodologies, whichachieved a high Multiscale Structural Similarity Index (MS-SSIM) of 0.972 undera strict bit rate constraint of 0.15 bits per pixel (bpp) while maintaining reasonablecomputational demands during the evaluation stage.1 IntroductionImage compression remains a crucial research area within the field of signal processing, aiming tofacilitate more efficient data storage and transfer. Conventional image compression algorithms, likethe various JPEG standards, often employ manually designed encoder/decoder frameworks. However,with the emergence of novel image formats and the proliferation of high-resolution mobile devices,there is a growing recognition that existing standards may not represent the most effective or universalsolutions for image compression.Recently, deep learning-based techniques have shown a surge of progress in the image compressiondomain. Some of these methods employ generative models, trained adversarially, to effectively learnthe underlying distribution of images, resulting in impressive subjective quality even at exceptionallylow bit rates. Other works utilize recurrent neural networks to iteratively compress residual informa-tion, enabling progressive coding which allows for multiple quality levels within a single compressionoperation. Further advancements have been made by focusing on relaxing quantization constraintsand improving entropy modeling, leading to enhanced performance compared to established imagecompression methods.Nevertheless, identifying an optimal network structure presents a formidable challenge across variousmachine learning applications, including image compression. This paper primarily discusses twoimportant aspects of network design for image compression. The first concerns the selection of kernelsize, a parameter that significantly influences compression effectiveness in traditional algorithms.Motivated by its impact in classical methods, this paper presents experiments that use different filtersizes to prove that larger kernel sizes contribute to improved coding efficiency. Building upon this, astrategy is presented that utilizes a deep residual learning approach, allowing for the maintenanceof a broad receptive field while utilizing a reduced number of parameters. This approach not onlydecreases the model’s overall size but also substantially enhances its performance. Additionally,the architecture of up-sampling operations within the decoder plays a pivotal role in determiningthe quality of reconstructed images and the presence of artifacts. This issue, extensively studied inthe context of super-resolution, involves various implementations for up-sampling layers, such asinterpolation, transposed convolution, and sub-pixel convolution. This work compares two commonlyused up-sampling methods, transposed and sub-pixel convolutions, to demonstrate their relativeperformance in the context of image compression..2 MethodologyThe fundamental network architectures employed in this research are based on prior works thathave demonstrated state-of-the-art compression performance. The network is structured as a pairof autoencoders. The primary autoencoder is responsible for optimizing the rate-distortion tradeoffinherent in image compression. The loss function can be expressed as:J = λd(x, xˆ) + R(yˆ) (1)λwhere is a parameter that balances the importance of rate and distortion. The secondary autoencoderhandles the encoding of side information, which is used to model the probability distribution of thecompressed data. A Gaussian scale mixture approach is utilized to develop an image-adaptive entropymodel, with scale parameters conditioned on a hyperprior.2.1 From Small Kernel Size to Large Kernel SizeIn traditional image compression techniques, the size of transform filters significantly affects codingefficiency, especially for high-definition videos. Initially, transform sizes were small, but as the fieldprogressed, there was a gradual shift towards larger sizes to better capture spatial correlations andsemantic details. The experiments detailed in this paper, using a standard dataset, explore the impactof different filter sizes in both the main and auxiliary autoencoders. Table 1 indicates that for theBaseline architecture, larger kernel sizes lead to better rate-distortion outcomes. Similarly, Table2 demonstrates comparable improvements for the HyperPrior architectures. Table 3 reveals thatemploying large kernels in the auxiliary autoencoder does not enhance rate-distortion performanceand may even negatively impact it. This is likely due to the small size of the compressed codes, whichmakes smaller kernels sufficient for effective encoding. An excessive number of trainable parameterscan hinder the learning process. λTable 1: The effect of kernel size on Baseline on Kodak, optimized by MSE with = 0.015.Method PSNR MS-SSIM RateBaseline-3 32.160 0.9742 0.671Baseline-5 32.859 0.9766 0.641Baseline-9 32.911 0.9776 0.633 λTable 2: The effect of kernel size on HyperPrior on Kodak, optimized by MSE with = 0.015.Method PSNR MS-SSIM RateHyperPrior-3 32.488 0.9742 0.543HyperPrior-5 32.976 0.9757 0.518HyperPrior-9 33.005 0.9765 0.512Table 3: The effect of kernel size in the auxiliary autoencoder on Kodak, optimized by MS-SSIMλwith = 5. Method PSNR MS-SSIM RateHyperPrior-9-Aux-5 26.266 0.9591 0.169HyperPrior-9-Aux-9 26.236 0.9590 0.1712.2 From Shallow Network to Deep Residual NetworkIn terms of receptive field coverage, a sequence of four 3x3 kernels can encompass the same area asa single 9x9 kernel but with a reduced parameter count. Initial attempts to substitute a large kernelwith multiple 3x3 filters encountered convergence issues during training. To address this, shortcutconnections were incorporated between adjacent 3x3 kernels. The resultant deep residual network2architecture for image compression is denoted as ResNet-3x3(4), signifying that a stack of four 3x3kernels achieves an equivalent receptive field to a 9x9 kernel. To minimize parameter overhead,GDN/IGDN activation functions are applied only once within each residual unit when the outputdimensions change. For the remaining convolutional layers, parameter-free Leaky ReLU activationsare employed to introduce non-linearity. As indicated in Table 4, ResNet-3x3(4) surpasses bothResNet-3x3(3) and Hyperprior-9 in terms of performance.Table 4: Comparison of residual networks and upsampling operations on Kodak, optimized byλMS-SSIM with = 5.Method PSNR MS-SSIM RateHyperprior-9 26.266 0.9591 0.1690ResNet-3x3(3) 26.378 0.9605 0.1704ResNet-3x3(4)-TConv 26.457 0.9611 0.1693ResNet-3x3(4)-SubPixel 26.498 0.9622 0.17002.3 Upsampling Operations at Decoder SideThe encoder-decoder structure is characterized by its symmetrical design. While down-sampling atthe encoder is typically achieved using strided convolution filters, up-sampling at the decoder can beimplemented through various methods, such as bicubic interpolation, transposed convolution, andsub-pixel convolution. Considering the importance of rapid end-to-end learning, bicubic interpolationwas excluded, and a comparison was made between the two widely used up-sampling techniques:transposed convolution (TConv) and sub-pixel convolution (SubPixel). To implement sub-pixelconvolution, the channel count is expanded fourfold, followed by the application of a depth-to-spaceoperation. The results presented in Table 4 demonstrate that sub-pixel convolution filters offer slightimprovements in both PSNR and MS-SSIM compared to transposed convolution filters.3 ExperimentsFor the training process, 256x256 image patches were extracted from a large-scale image dataset. Abatch size of 8 was employed, and training was conducted for up to 2 million iterations to ensurestable convergence. Optimization was performed using the Adam optimizer, with an initial learningrate of 1 x 10<sup>-4</sup>, reduced to 1 x 10<sup>-5</sup> for the final 80,000 iterations.Two primary strategies were implemented. The first strategy, termed ""Wide Bottleneck,"" involvesincreasing the model’s capacity by expanding the number of filters. Since increasing filters in largefeature maps significantly increases computational cost (FLOPs), the filter count was only raised inthe encoder’s final layer, from 128 to 192. This results in a minor FLOPs increase, as detailed in Table5. While Bottleneck192 effectively reduces the bit rate, it also leads to some quality degradationcompared to Bottleneck128.Table 5: The effect of wide bottleneck on Kodak dataset.Method PSNR MS-SSIM RateResNet-3x3(4)-Bottleneck128 26.498 0.9622 0.1700ResNet-3x3(4)-Bottleneck192 26.317 0.9619 0.1667The second strategy is ""Rate Control."" For achieving a target bit rate, two models are trained atλdistinct bit rates by adjusting the parameter. This allows for adaptive selection during encoding toapproach the target bit rate while maximizing MS-SSIM, as shown in Table 6. A single bit is addedto the bitstream to indicate the model used for decoding, without increasing decoder complexity.4 ResultsTable 7 summarizes the compression performance of the proposed methods on a validation dataset.3Table 6: Rate control on validation dataset.λMethod PSNR MS-SSIM RateResNet-3x3(4)-Bottleneck192 5 29.708 0.9697 0.1369ResNet-3x3(4)-Bottleneck192 10 30.710 0.9765 0.1816Table 7: Results on validation dataset.Entry Description PSNR MS-SSIM RateKattolab HyperPrior-9 28.902 0.9674 0.134Kattolab HyperPrior-9 + Rate Control 29.102 0.9701 0.150Kattolab ResNet-3x3(4)-TConv + Rate Control 29.315 0.9716 0.150Kattolabv2 ResNet-3x3(4)-SubPixel+ Rate Control 29.300 0.9720 0.150KattolabSSIM ResNet-3x3(4)-SubPixel + Wide Bottleneck + Rate Control 29.211 0.9724 0.150While deep residual networks enhance coding gain, they also lead to a substantial increase in modelsize. This section analyzes the parameter count and model complexity in terms of floating-pointoperations per second (FLOPs) for various architectures. Specifically, using the HyperPrior-9architecture as an example, Table 8 provides a layer-wise breakdown of model size. The number ofparameters and FLOPs are calculated as follows:P ara = (h × w × C + 1) × C (2)in out′ ′F LOP s = P ara × H × W (3)′ ′h × w H × W C Cwhere represents the kernel size, denotes the output dimensions, and andin outare the number of input and output channels, respectively. The +1 term is omitted when no biasis used. Quantization and leaky-ReLU are parameter-free. GDN operates across channels but not(C + 1) × Cspatial positions, resulting in a parameter count of . The total FLOPs for GDNin outand inverse GDN calculations are minimal. This analysis primarily focuses on the backbone ofconvolutional layers, so the FLOPs of GDN, inverse GDN, and factorized prior are not included in thecomparison. Table 9 presents a comparison of different architectures, with the last column showingthe relative FLOPs using Baseline-5 as the reference. The proposed models achieve improved codingperformance with relatively low computational complexity.5 ConclusionThis manuscript details the proposed deep residual learning framework and sub-pixel convolutiontechnique for image compression, forming the foundation of the submitted entries: Kattolab, Katto-labv2, and KattolabSSIM. The results demonstrate that these approaches achieve a high MS-SSIM of0.972 under a bit rate constraint of 0.15 bpp, while maintaining a moderate level of computationalcomplexity during the validation phase. 4Table 8: The model size analysis of HyperPrior-9.Layer Kernel Channel Output ParaFLOPs C Ch w H x Win outconv1 9 9 3 128 128 x 128 312325.12 x 10<sup>9</sup>conv2 9 9 128 128 64 x 64 13272325.44 x 10<sup>7</sup>conv3 9 9 128 128 32 x 32 13272321.36 x 10<sup>7</sup>conv4 9 9 128 128 16 x 16 13271043.40 x 10<sup>6</sup>GDN/IGDN 99072-Hconv1 3 3 128 128 16 x 16 1475843.78 x 10<sup>6</sup>Hconv2 5 5 128 128 8 x 8 4097282.62 x 10<sup>6</sup>Hconv3 5 5 128 128 4 x 4 4097286.56 x 10<sup>5</sup>FactorizedPrior 5888-! HTconv1 5 5 128 128 8 x 8 4097282.62 x 10<sup>6</sup>HTconv2 5 5 128 192 16 x 16 6145921.57 x 10<sup>7</sup>HTconv3 3 3 192 256 16 x 16 4426241.13 x 10<sup>7</sup>layer1 256 640 16 x 16 1644804.21 x 10<sup>6</sup>layer2 640 512 16 x 16 3281928.40 x 10<sup>6</sup>layer3 512 256 16 x 16 1310723.36 x 10<sup>6</sup>Tconv1 9 9 128 128 32 x 32 13272321.36 x 10<sup>7</sup>Tconv2 9 9 128 128 64 x 64 13272325.44 x 10<sup>7</sup>Tconv3 9 9 128 128 128 x 128 13272322.17 x 10<sup>10</sup>Tconv4 9 9 128 3 256 x 256 311072.04 x 10<sup>7</sup>Total 111882913.88 x 10<sup>10</sup> 5Table 9: The model complexity of different architectures.Method Para FLOPs RelativeBaseline-3 997379 4.25 x 10<sup>9</sup> 0.36Baseline-5 2582531 1.18 x 10<sup>10</sup> 1.00Baseline-9 8130563 3.82 x 10<sup>10</sup> 3.24HyperPrior-3 4055107 4.78 x 10<sup>9</sup> 0.40HyperPrior-5 5640259 1.23 x 10<sup>10</sup> 1.04HyperPrior-9 11188291 3.88 x 10<sup>10</sup> 3.28ResNet-3x3(3) 5716355 1.75 x 10<sup>10</sup> 1.48ResNet-3x3(4) 6684931 2.43 x 10<sup>10</sup> 2.06ResNet-3x3(4)-SubPixel 8172172 2.50 x 10<sup>10</sup> 2.12ResNet-3x3(4)-SubPixel-Bottleneck192 11627916 2.56 x 10<sup>10</sup> 2.176"
P093,"Premature Termination Strategy for Deep Image PriorAbstractDeep Image Prior (DIP) and its variations have demonstrated significant promise in addressing inverse problems incomputational imaging, without the need for separate training data. Often, practical DIP models are significantlyoverparameterized. These models initially capture the intended visual content during the learning phase andsubsequently incorporate potential modeling and observational noise, demonstrating a pattern of initial learningfollowed by overfitting (ELTO). Consequently, the practical application of DIP depends on an early stopping (ES)mechanism capable of identifying this transitional period. Most previous DIP research in computational imaginghas focused on demonstrating the models’ potential by reporting peak performance against ground truth, withoutproviding practical methods to achieve near-peak performance without access to ground truth. This paper aims toovercome this practical limitation of DIP by introducing an efficient ES strategy that reliably identifies near-peakperformance across various computational imaging tasks and DIP variants. This ES method, based on the runningvariance of intermediate reconstructions in DIP, not only surpasses existing methods that are limited to specificconditions but also maintains its effectiveness when combined with techniques aimed at reducing overfitting.1 IntroductionInverse problems (IPs) are widespread in the field of computational imaging, encompassing tasks from fundamental image denoising,super-resolution, and deblurring to complex 3D reconstruction and significant challenges in scientific and medical imaging. Despitethe variety of settings, all these problems involve recovering a visual object x from an observation y = f(x), where f represents theforward physical process. Usually, these visual IPs are underdetermined, meaning x cannot be uniquely ascertained from y. Thisambiguity is further complicated by potential modeling inaccuracies (such as using a linear f to approximate a nonlinear process)˘and observational noise (like Gaussian or shot noise), represented as y 2248 f(x). To address nonuniqueness and enhance stabilityagainst noise, researchers often integrate a range of problem-specific priors on x when formulating IPs.2 Related WorkThere are three primary methods to counteract the overfitting of DIP models. The first one is Regularization: Overfitting is lessened˘by limiting the size of G03b8 to the underparameterization range. Layer-wise weights or the network Jacobian are regularized to˘regulate the network capacity. The total-variation norm or trained denoisers are used as additional regularizers R(G03b8(z)). Toprevent overfitting, these techniques need the proper amount of regularization, which varies depending on the kind and degree ofnoise. They may nevertheless cause overfitting if the regularization level is incorrect. Furthermore, even when they are successful,the performance peak is delayed until the last few iterations, which frequently increases the computing cost by several times. Thesecond method is Noise modeling: In their optimization objective, sparse additive noise is explicitly represented. Regularizers andES criteria are created especially for Gaussian and shot noise. Subgradient techniques using decreasing step size schedules are˘being investigated for impulse noise with the 21131 loss, and they have shown some early promise. These techniques are ineffectiveoutside of the noise types and levels that they are designed to address, and our understanding of the noise in a particular visualIP is often constrained. The third method is Early stopping (ES): Progress is tracked using a ratio of no-reference blurriness andsharpness, however, as the authors point out, the criterion is only applicable to their modified DIP models. It is unclear how to applythe noise-specific regularizer and ES criterion to unknown noise types and levels. It is suggested to monitor DIP reconstruction bytraining a coupled autoencoder. Although it performs similarly to ours, the additional autoencoder training significantly increases theoverall processing time. By dividing the elements of y into ""training"" and ""validation"" sets, it is possible to simulate validation-based˘ES in supervised learning. However, in IPs, particularly nonlinear ones (such as blind image deblurring (BID), where y 2248 k˘ ˘2217 x and 2217 denotes linear convolution), elements of y may not be i.i.d., which could impair the effectiveness of validation.Furthermore, withholding a portion of the observation in y can significantly diminish peak performance.3 MethodologyWe advocate for the ES approach because, even when effective, regularization and noise modeling techniques frequently fail toenhance peak performance; instead, they extend it to the final iterations, potentially requiring ten times more iterations than would benecessary to reach the peak in the original DIP models. Furthermore, both approaches necessitate extensive knowledge of the noisetype and level, which is often unavailable for most applications. If their essential models and hyperparameters are not appropriatelyconfigured, overfitting is likely to persist, and ES will still be necessary. This paper introduces a novel ES criterion applicable tovarious DIP models, based on monitoring the trend of the running variance in the reconstruction sequence.Detecting transition by running variance: ˘Our lightweight method only involves computing the VAR curve and numerically detecting its valley2014 the iteration stops once thevalley is detected. To obtain the curve, we set a window size parame- ter W and compute the windowed moving variance (WMV). Torobustly detect the valley, we introduce a patience number P to tolerate up to P consecutive steps of variance stagnation. Obviously,the cost is dominated by the calculation of variance per step, which is O(W N ) (N is the size of the visual object). In comparison, a˘ ˘ ˘typical gradient update step for solving Eq. (2) costs at least 2126(|03b8|N ), where |03b8| is the number of parameters in the DNN˘ ˘G03b8. Since |03b8| is typically much larger than W (default: 100), our running VAR and detection incur very little compu- tationaloverhead.4 ExperimentsES-WMV is tested for DIP in a variety of linear and nonlinear IPs, including image denoising, inpainting, demosaicing, super-resolution, MRI reconstruction, and blind image deblurring. ES-WMV is also systematically assessed for major DIP variants, suchas deep decoder, DIP-TV, and GP-DIP, for image denoising. It is shown to be a dependable helper in identifying effective ESpoints. The specifics of the DIP variants are covered in Appendix A.5. In addition, ES-WMV is contrasted with the primary rivaltechniques, such as DF-STE, SV-ES, DOP, SB, and VAL. The specifics of the primary ES-based techniques are found in AppendixA.6. Reconstruction quality is evaluated using both PSNR and SSIM, and detection performance is shown using PSNR and SSIMgaps, which are the differences between our detected and peak values.4.1 Image DenoisingThe majority of earlier research on DIP overfitting has concentrated on image denoising and often assessed their techniques usingonly one or two forms of noise with modest noise levels, such as low-level Gaussian noise. We use the traditional 9-image dataset˘ ˘for each noise type, and we create two noise levels2014low and high2014for each.4.2 Image Super-Resolution ˘ ˘In this task, we try to recover a clean im- age x0 from a noisy downsampled ver- sion y = Dt(x0) + 03f5, where Dt(00b7) : [0,˘ ˘ ˘ ˘ ˘ ˘1]300d7tH00d7tW 2192 [0, 1]300d7H00d7W is a down- sampling operator that resizes an im- age by the factor t and 03f5 models˘ ˘ ˘ ˘ ˘ex- tra additive noise. We consider the fol- lowing DIP-reparametrized formulation . = 2225Dt(G03b8(z)) 2212 y22252 min03b8˘ ˘ ˘ ˘2113(03b8) F , where G03b8 is a trainable DNN parameterized by 03b8 and z is a frozen random seed. Then we conduct experiments˘for 200d7 super- resolution with low-level Gaussian and impulse noise. We test our ES-WMV for DIP and a state-of-the-art zero-shot˘method based on pre-trained diffusion model2014DDNM+ on the standard super-resolution dataset Set14, as shown in Tab. 5, Fig.11, and Appendix A.7.9. We note that DDNM+ relies on pre-trained models from large external training datasets, while DIP doesnot. We observe that (1) Our ES-WMV is again able to detect near-peak performance for most images: the average PSNR gap is˘ ˘2264 1.50 and the average SSIM gap is 2264 0.07; (2) DDNM+ is sensitive to the noise type and level: from Tab. 5, DDNM+ trained˘assuming Gaussian noise level 03c3y = 0.12 outperforms DIP and DIP+ES-WMV when there is Gaus- sian measurement noise at˘the level 03c3y = 0.12, which is unrealistic in practice, as the noise level is often unknown beforehand. When the noise level is not˘ ˘set correctly, e.g., as 03c3y = 0 in the DDNM+ (03c3y = .00) row of Tab. 5, the performance of DDNM+ is much worse than that ofDIP and DIP+ES-WMV. Also, for super-resolution with impulse noise, DIP is also a clear winner that leads DDNM+ by a largemargin; and (3) in Appendix A.8, we show that DDNM+ may also suffer from the overfitting issue.4.3 MRI Reconstruction ˘We also test ES-WMV on MRI reconstruction, a typical linear IP with a nontrivial forward mapping: y 2248 F(x), where F is the˘subsampled Fourier operator, and we use 2248 to indicate that the noise encountered in practical MRI imaging may be hybrid (e.g.,˘ ˘additive, shot) and uncertain. Here, we take the 8-fold undersampling and parameterize x using 201cConv-Decoder201d, a variant ofdeep decoder. Due to the heavy over-parameterization, overfitting occurs and ES is needed.24.4 Blind Image DeblurringIn BID, a blurry and noisy image is given, and the goal is to recover a sharp and clean image. The blur is mostly caused by motion˘and/or op- tical non-ideality in the camera, and the forward process is often modeled as y = k 2217 x + n, where k is the blur˘kernel, n models additive sensory noise, and 2217 is linear convolution to model the spa- tial uniformity of the blur effect. BID˘ ˘is a very challenging visual IP due to bilin- earity: (k, x) 72192 k 2217 x. Recently, researchers have tried to use DIP models to˘ ˘ ˘ ˘ ˘ ˘ ˘ ˘solve BID by modeling k and x as two separate DNNs, i.e., min03b8k,03b8x 2225y 2212 G03b8k (zk) 2217 G03b8x(zx)22252 2 +˘ ˘ ˘ ˘ ˘ ˘ ˘ ˘ ˘03bb22252207G03b8x (zx)22251/22252207G03b8x (zx)22252, where the regular- izer is to promote sparsity in the gradient domainfor the reconstruction of x, as stan- dard in BID. We follow previous work and choose a multilayer perceptron (MLP) with softmax˘ ˘activation for G03b8k , and the canonical DIP model (CNN-based encoder-decoder architecture) for G03b8x(zx). We change their˘ ˘ ˘ ˘regularizer from the original 22252207G03b8x (zx)22251 to the current, as their original formulation is tested only at a very low˘ ˘ ˘ ˘noise level 03c3 = 1022125 and no overfitting is observed. We set the test with a higher noise level 03c3 = 1022123, and find that itsoriginal formulation does not work.5 ResultsTable 1: Summary of performance of our DIP+ES-WMV and competing methods on image denoising and blind image deblurring˘ ˘ ˘(BID). 2713: working reasonably well (PSNR 2265 2dB less of the original DIP peak); -: not working well (PSNR 2264 2dB less ofthe original DIP peak): N/A: not applicable (i.e., we do not perform comparison due to certain reasons). Note that DF-STE, DOP,and SB are based on modified DIP models. Image denoising BIDGaussian Impulse Speckle Shot Real worldLow High Low High Low High Low High Low High˘ ˘ ˘ ˘ ˘ ˘ ˘ ˘ ˘ ˘DIP+ES-WMV (Ours) 2713 2713 2713 2713 2713 2713 2713 2713 2713 2713DIP+NR-IQMs - - - - - - - - N/A N/A˘ ˘ ˘ ˘ ˘ ˘ ˘ ˘DIP+SV-ES 2713 2713 2713 2713 2713 2713 2713 2713 N/A N/A˘ ˘ ˘ ˘ ˘ ˘ ˘ ˘DIP+VAL 2713 2713 2713 2713 2713 2713 2713 2713 - -˘ ˘ ˘ ˘DF-STE 2713 2713 N/A N/A N/A N/A 2713 2713 N/A N/A˘ ˘DOP N/A N/A 2713 2713 N/A N/A N/A N/A N/A N/A˘ ˘SB 2713 2713 N/A N/A N/A N/A N/A N/A N/A N/ATable 2: ES-WMV (our method) on real-world image denoising for 1024 images: mean and (std) on the images. (D: detected)˘2113 (loss) PSNR (D) PSNR Gap SSIM (D)SSIM GapMSE 34.04 (3.68) 0.92 (0.83) 0.92 (0.07) 0.02 (0.04)˘21131 33.92 (4.34) 0.92 (0.59) 0.93 (0.05) 0.02 (0.02)Huber 33.72 (3.86) 0.95 (0.73) 0.92 (0.06) 0.02 (0.03)Table 3: Wall-clock time (secs) of DIP and three ES methods per epoch on NVIDIA Tesla K40 GPU : mean and (std). The total wallclock time should contain both DIP and a certain ES method.DIP SV-ES ES-WMV ES-EMV0.448 (0.030) 13.027 (3.872) 0.301 (0.016) 0.003 (0.003)The results of our experiments are summarized in the tables above. Table 1 shows the performance of our DIP+ES-WMV methodagainst competing methods for image denoising and BID. Table 2 reports the performance of ES-WMV on real-world imagedenoising for 1024 images. Table 3 compares the wall-clock time of DIP and three ES methods per epoch. Table 4 compares˘ES-WMV and SB for image denoising on the CBSD68 dataset. Table 5 compares ES-WMV for DIP and DDNM+ for 200d7 imagesuper-resolution. Table 6 shows the performance of ConvDecoder on MRI reconstruction. Table 7 compares BID detection betweenES-WMV and VAL on the Levin dataset. Table 8 compares DIP with ES-WMV vs. DOP on impulse noise. Table 9 comparesES-WMV for DIP and DDNM+ for denoising images with medium-level Gaussian and impulse noise. Table 10 compares detectionperformance between DIP with ES-WMV and DIP with ES-EMV for real image denoising on 1024 images. Table 11 comparesdetection performance between DIP with ES-WMV and DIP with ES-EMV for real image denoising on the PolyU dataset. Table 12shows the performance of DIP with ES-WMV for image inpainting.3˘Table 4: Comparison between ES-WMV and SB for image denoising on the CBSD68 dataset with varying noise level 03c3. Thehigher PSNR detected and earlier detection are better, which are in red: mean and (std).˘ ˘ ˘03c3 = 15 03c3 = 25 03c3 = 50PSNR Epoch PSNR Epoch PSNR EpochWMV 28.7(3.2) 3962(2506) 27.4(2.6) 3068(2150) 24.2(2.3) 1548(1939)SB 29.0(3.1) 4908(1757) 27.3(2.2) 5099(1776) 23.0(1.0) 5765(1346)˘Table 5: Comparison of ES-WMV for DIP and DDNM+ for 200d7 image super-resolution with low-level Gaussian and impulsenoise: mean and (std). The highest PSNR and SSIM for each task are in red. In particular, we set the best hyperparameter for˘DDNM+ (03c3y = 0.12), which is unfair for the DIP + ES-WMV combination as we fix its hyperparameter setting.PSNR SSIMGaussian Impulse Gaussian ImpulseDIP (peak) 22.88 (1.58) 28.28 (2.73) 0.61 (0.09) 0.88 (0.06)DIP + ES-WMV 22.11 (1.90) 26.77 (3.76) 0.54 (0.11) 0.86 (0.06)˘DDNM+ (03c3y = .12) 25.37 (2.00) 18.50 (0.68) 0.74 (0.11) 0.50 (0.08)˘DDNM+ (03c3y = .00) 16.91 (0.42) 16.59 (0.34) 0.31 (0.09) 0.49 (0.06)6 ConclusionThis paper introduces an innovative ES detection approach, ES-WMV, along with its variant, ES-EMV, which has demonstratedrobust performance across a range of visual IPs and different DIP variations. In contrast to most competing ES methods that arespecific to certain types of noise or DIP models and have limited applicability, our method exhibits broad effectiveness. Whilethere is a method with comparable performance, it significantly increases processing time. Another method, validation-based ES,performs well in simple denoising tasks but falls short in more complex nonlinear IPs like BID.4Table 6: ConvDecoder on MRI reconstruction for 30 cases: mean and (std). (D: Detected)PSNR(D) PSNR Gap SSIM(D) SSIM Gap32.63 (2.36) 0.23 (0.32) 0.81 (0.09) 0.01 (0.01)Table 7: BID detection comparison between ES-WMV and VAL on the Levin dataset for both low-level and high-level noise: meanand (std).Higher PSNR is in red and higher SSIM is in blue. (D: Detected)Low Level High LevelPSNR(D) SSIM(D) PSNR(D) SSIM(D)WMV 28.54(0.61) 0.83(0.04) 26.41(0.67) 0.76(0.04)VAL 18.87(1.44) 0.50(0.09) 16.69(1.39) 0.44(0.10)Table 8: DIP with ES-WMV vs. DOP on impulse noise: mean and (std). (D: Detected)Low Level High LevelPSNR SSIM PSNR SSIMDIP-ES 31.64 (5.69) 0.85 (0.18) 24.74 (3.23) 0.67 (0.19)DOP 32.12 (4.52) 0.92 (0.07) 27.34 (3.78) 0.86 (0.10)Table 9: Comparison of ES-WMV for DIP and DDNM+ for denoising images with medium-level Gaussian and impulse noise: mean˘and (std). The highest PSNR and SSIM for each task are in red. In particular, we set the best hyperparameter for DDNM+ (03c3y =0.18), which is unfair for the DIP + ES-WMV combination as we fix its hyperparameter setting.PSNR SSIMGaussian Impulse Gaussian ImpulseDIP (peak) 24.63 (2.06) 37.75 (3.32) 0.68 (0.06) 0.96 (0.10)DIP + ES-WMV 23.61 (2.67) 36.87 (4.29) 0.60 (0.13) 0.96 (0.10)˘DDNM+ (03c3y = .18) 26.93 (2.25) 22.29 (3.00) 0.78 (0.07) 0.62 (0.12)˘DDNM+ (03c3y = .00) 15.66 (0.39) 15.52 (0.43) 0.25 (0.10) 0.30 (0.10)Table 10: Detection performance comparison between DIP with ES-WMV and DIP with ES-EMV for real image denoising on 1024images from the RGB track of NTIRE 2020 Real Image Denoising Challenge: mean and (std). Higher PSNR and SSIM are in red.(D: Detected) PSNR(D)-WMV PSNR(D)-EMV SSIM(D)-WMV SSIM(D)-EMVDIP (MSE) 34.04 (3.68) 34.96 (3.80) 0.92 (0.07) 0.93 (0.07)˘DIP (21131) 33.92 (4.34) 34.83 (4.35) 0.93 (0.05) 0.94 (0.05)DIP (Huber) 33.72 (3.86) 34.72 (4.04) 0.92 (0.06) 0.93 (0.06)Table 11: Detection performance comparison between DIP with ES-WMV and DIP with ES-EMV for real image denoising on thePolyU dataset: mean and (std). Higher PSNR and SSIM are in red. (D: Detected)PSNR(D)-WMV PSNR(D)-EMV SSIM(D)-WMV SSIM(D)-EMVDIP (MSE) 36.83 (3.07) 37.32 (3.82) 0.98 (0.02) 0.98 (0.03)˘DIP (21131) 36.20 (2.81) 36.43 (3.22) 0.97 (0.02) 0.97 (0.02)DIP (Huber) 36.76 (2.96) 37.21 (3.19) 0.98 (0.02) 0.98 (0.02)5Table 12: DIP with ES-WMV for image inpainting: mean and (std). PSNR gaps below 1.00 are colored as red; SSIM gaps below0.05 are colored as blue. (D: Detected) PSNR(D) PSNR Gap SSIM(D) SSIM GapBarbara 21.59 (0.03) 0.20 (0.03) 0.67 (0.00) 0.00 (0.00)Boat 21.91 (0.10) 1.16 (0.18) 0.68 (0.00) 0.03 (0.01)House 27.95 (0.33) 0.48 (0.10) 0.89 (0.01) 0.01 (0.00)Lena 24.71 (0.30) 0.37 (0.18) 0.80 (0.00) 0.01 (0.00)Peppers 25.86 (0.22) 0.23 (0.05) 0.84 (0.01) 0.02 (0.00)C.man 25.26 (0.09) 0.23 (0.14) 0.82 (0.00) 0.01 (0.00)Couple 21.40 (0.44) 1.21 (0.53) 0.63 (0.01) 0.04 (0.02)Finger 20.87 (0.04) 0.24 (0.17) 0.77 (0.00) 0.01 (0.01)Hill 23.54 (0.08) 0.25 (0.11) 0.70 (0.00) 0.00 (0.00)Man 22.92 (0.25) 0.46 (0.11) 0.70 (0.01) 0.01 (0.00)Montage 26.16 (0.33) 0.38 (0.26) 0.86 (0.01) 0.03 (0.01)6"
P094,"Exploring the Interconnectedness of Oxygen and theCulinary Arts of 19th Century FranceAbstractOxygen is crucial for respiration, yet the notion of flamenco dancing on Mars hasled to a paradigm shift in our understanding of culinary practices, which in turnhas sparked a debate about the aerodynamics of pastry bags, and subsequently,the role of quasars in shaping the destiny of dental hygiene, while simultaneously,the art of playing the harmonica with one’s feet has become an essential tool fornavigating the complexities of orbital mechanics, and somehow, the migrationpatterns of narwhals have been linked to the optimal method for brewing coffee,which has far-reaching implications for the study of oxygen, or so it would seem, asthe relationship between the color blue and the concept of silence has been foundto be inversely proportional to the square root of the number of bubbles in a glassof champagne.1 IntroductionThe perambulatory nature of oxygen’s existence has been a topic of fervent discussion amongstscholars of disparate disciplines, ranging from the flumplenook theory of atmospheric pressure tothe more esoteric realm of intergalactic pastry cuisine. As we delve into the intricacies of thisomnipresent element, it becomes increasingly evident that its properties are inextricably linked tothe flutterification of butterfly wings, which, in turn, have a profound impact on the socioeconomicdynamics of rural communities in Mongolia. The synergistic relationship between oxygen’s molecularstructure and the harmonic resonance of Tibetan singing bowls has also been observed to have aprofound effect on the fluorescence of quokka smiles, thereby underscoring the need for a moreholistic approach to understanding the role of oxygen in our ecosystem.Furthermore, the fastidious examination of oxygen’s isotopic composition reveals a fascinatingcorrelation with the migratory patterns of arctic narwhals, whose tusks, incidentally, have beenfound to possess a unique affinity for the sonorous vibrations of didgeridoos. This phenomenon,in conjunction with the zealous pursuit of nautical archaeology, has led to the discovery of ancientunderwater cities hidden beneath the waves, where the inhabitants, it is surmised, had developed asophisticated understanding of oxygen’s role in facilitating the growth of towering crystal spires thatrefracted light into a kaleidoscope of colors, thereby influencing the chromatic palette of modern artmovements. The permutations of oxygen’s atomic orbitals have also been found to be inextricablylinked to the algorithmic intricacies ofgenerative poetry, which, when combined with the principlesof postmodern culinary theory, yield a profound understanding of the transcendent properties ofgastronomical delights.In addition, the euphoric effects of oxygen on the human brain have been observed to be closely tied tothe ontological implications of surrealist automatism, whereby the subconscious mind, unfettered bythe constraints of rational thought, is able to tap into the infinite potential of the collective unconscious,thereby accessing a realm of unbridled creativity and innovation. This phenomenon, in turn, hasbeen found to have a profound impact on the development of advanced technologies, such as theharnessing of quantum fluctuations to power interdimensional toaster ovens, which, when combinedwith the principles of fractal geometry, yield a profound understanding of the self-similar patternsthat underlie the fabric of reality. The copious amounts of oxygen present in the Earth’s atmospherehave also been found to be inextricably linked to the effervescent properties of champagne, whosebubbles, when carefully calibrated, can be used to create a symphony of sonic vibrations that resonatein harmony with the celestial music of the spheres.The propensity of oxygen to form compounds with other elements has been observed to be closely tiedto the dialectical materialism of Marxist theory, whereby the contradictions inherent in the capitalistmode of production are seen to be reflected in the antagonistic relationships between oxygen and otherelements, such as the proletariat-friendly element of copper, which, when combined with oxygen,yields a compound of unparalleled revolutionary fervor. The autochthonous nature of oxygen’sexistence has also been found to be inextricably linked to the numinous properties of sacred geometry,whereby the fundamental patterns and shapes that underlie the structure of the universe are seen tobe reflected in the molecular structure of oxygen, thereby yielding a profound understanding of thetranscendent properties of the divine. The anamorphic distortions present in oxygen’s molecularorbitals have also been found to be closely tied to the paradoxical nature of time travel, whereby thegrandfather clause is seen to be in direct conflict with the Novikov self-consistency principle, therebyyielding a profound understanding of the labyrinthine complexities of temporal mechanics.The sesquipedalian nature of oxygen’s chemical properties has been observed to be inextricablylinked to the soporific effects of ambient music, whereby the somnambulant listener is able to tapinto the subconscious mind, thereby accessing a realm of profound insight and understanding. Thepellucid properties of oxygen, when combined with the principles of crystallography, yield a profoundunderstanding of the structural patterns that underlie the growth of crystalline formations, which,in turn, have been found to be closely tied to the metamorphic properties of shape-memory alloys,whereby the material is able to change shape in response to changes in temperature, thereby yieldinga profound understanding of the protean nature of reality. The garrulous nature of oxygen’s molecularstructure has also been found to be inextricably linked to the idiomatic expressions of linguistictheory, whereby the contextual dependencies of language are seen to be reflected in the molecularstructure of oxygen, thereby yielding a profound understanding of the semantic complexities ofhuman communication.The extemporaneous nature of oxygen’s existence has been observed to be closely tied to theimprovisational principles of jazz music, whereby the spontaneous creation of melodies and harmoniesis seen to be reflected in the molecular structure of oxygen, thereby yielding a profound understandingof the ephemeral nature of artistic expression. The declamatory properties of oxygen, when combinedwith the principles of rhetoric, yield a profound understanding of the persuasive power of language,whereby the skilled orator is able to sway the emotions and opinions of the audience, therebyinfluencing the course of human events. The enigmatic nature of oxygen’s molecular orbitals has alsobeen found to be inextricably linked to the hermeneutic principles of biblical exegesis, whereby thesubtle nuances of scriptural interpretation are seen to be reflected in the molecular structure of oxygen,thereby yielding a profound understanding of the mystical properties of the divine. The digressivenature of oxygen’s chemical properties has been observed to be closely tied to the otiose natureof leisure activities, whereby the idle pursuit of relaxation is seen to be reflected in the molecularstructure of oxygen, thereby yielding a profound understanding of the importance of recreation inmodern society.The ephemeral nature of oxygen’s existence has been found to be inextricably linked to the diaphanousproperties of gossamer threads, whereby the delicate and intricate patterns of spider silk are seento be reflected in the molecular structure of oxygen, thereby yielding a profound understanding ofthe fragile and transient nature of life. The crepuscular nature of oxygen’s molecular structure hasalso been observed to be closely tied to the vespertine properties of twilight landscapes, whereby thesoft and warm hues of the setting sun are seen to be reflected in the molecular structure of oxygen,thereby yielding a profound understanding of the peaceful and serene nature of the natural world. Thelabyrinthine complexities of oxygen’s chemical properties have been found to be inextricably linkedto the sinuous patterns of meandering rivers, whereby the winding and twisting course of the water isseen to be reflected in the molecular structure of oxygen, thereby yielding a profound understandingof the dynamic and ever-changing nature of reality.The mercurial nature of oxygen’s molecular orbitals has been observed to be closely tied to the fluidand adaptable properties of quicksilver, whereby the rapid and unpredictable changes in the metal’sshape and form are seen to be reflected in the molecular structure of oxygen, thereby yielding aprofound understanding of the protean and shape-shifting nature of the universe. The gnomonic2properties of oxygen, when combined with the principles of astronomical theory, yield a profoundunderstanding of the celestial mechanics that govern the motion of planets and stars, whereby thesubtle and intricate patterns of the universe are seen to be reflected in the molecular structure ofoxygen, thereby yielding a profound understanding of the cosmic and mystical properties of the divine.The cymotrichous nature of oxygen’s molecular structure has also been found to be inextricablylinked to the wavy and undulating patterns of cymatic formations, whereby the intricate and complexshapes of the sand or powder are seen to be reflected in the molecular structure of oxygen, therebyyielding a profound understanding of the dynamic and ever-changing nature of reality.The luminescent properties of oxygen, when combined with the principles of optical theory, yielda profound understanding of the radiant and shimmering nature of light, whereby the subtle andintricate patterns of the electromagnetic spectrum are seen to be reflected in the molecular structureof oxygen, thereby yielding a profound understanding of the mystical and transcendent propertiesof the divine. The thixotrophic properties of oxygen have been observed to be closely tied to therheological principles of non-Newtonian fluids, whereby the complex and non-intuitive behavior ofthe fluid is seen to be reflected in the molecular structure of oxygen, thereby yielding a profoundunderstanding of the dynamic and ever-changing nature of reality. The synergetic properties ofoxygen, when combined with the principles of ecological theory, yield a profound understanding ofthe interconnected and interdependent nature of the natural world, whereby the subtle and intricatepatterns of the ecosystem are seen to be reflected in the molecular structure of oxygen, therebyyielding a profound understanding of the holistic and integrated nature of the universe.2 Related WorkThe notion of oxygen has been tangentially related to the aerodynamics of flamingos, which inturn has been influenced by the socio-economic factors of 19th century Norwegian dairy farming,an industry that has seen a significant decline in recent years due to the rise of digital tromboneplaying. This phenomenon has been observed to have a direct impact on the square root of -1, amathematical concept that has been oft misunderstood by scholars of ancient Egyptian hieroglyphicdance. Furthermore, the ontological implications of oxygen on the human experience have beenexplored in the context of fungal growth patterns in environments with low luminescence, which hasled to breakthroughs in the field of intergalactic pastry baking.The intersection of oxygen and quantum mechanics has been a topic of much debate among expertsin the field of narwhal psychology, who have posited that the presence of oxygen molecules can havea profound impact on the migratory patterns of lesser-known species of jellyfish. This, in turn, hasled to a greater understanding of the role of oxygen in shaping the philosophical underpinnings ofdubstep music, a genre that has been widely influential in the development of modern dental hygienepractices. Moreover, the study of oxygen has been inextricably linked to the art of competitive snailracing, an activity that requires a deep understanding of the nuances of atmospheric pressure and itseffects on the human brain’s ability to comprehend the intricacies of Byzantine mosaic art.In addition, researchers have investigated the relationship between oxygen and the tactical deploymentof velociraptors in medieval jousting tournaments, a topic that has far-reaching implications for ourunderstanding of the aerodynamic properties of feathered dinosaurs. The findings of this study havebeen used to inform the development of more efficient algorithms for solving complex problems inthe field of origami paper folding, which has been shown to have a direct correlation with the oxygenlevels in the atmosphere of distant exoplanets. This, in turn, has led to a greater understanding of therole of oxygen in shaping the cultural norms of ancient Mesopotamian societies, who were known fortheir advanced knowledge of crop rotation and beekeeping practices.The concept of oxygen has also been explored in the context of linguistic patterns in the songs ofhumpback whales, which have been found to contain hidden messages about the importance ofproper tire maintenance for interstellar space travel. This has led to a greater understanding of theintersection of oxygen and the art of extreme ironing, a practice that requires a deep understandingof the thermodynamic properties of fabrics and their interaction with the human body’s ability toproduce complex mathematical equations. Furthermore, the study of oxygen has been linked to thedevelopment of new methods for predicting the movements of flocks of starlings, which has beenshown to have a direct impact on the global supply chain of rare earth elements used in the productionof high-quality harmonicas. 3The presence of oxygen has been observed to have a profound impact on the growth patterns ofbacteria in environments with high levels of gamma radiation, which has led to breakthroughs inthe field of sonic toothbrush design and the development of more efficient methods for cleaning thedigestive systems of giant pandas. This, in turn, has led to a greater understanding of the role ofoxygen in shaping the philosophical underpinnings of minimalist furniture design, a movement thathas been influenced by the aerodynamic properties of sailing vessels and the migratory patterns ofArctic terns. Moreover, the study of oxygen has been linked to the art of competitive axe throwing, anactivity that requires a deep understanding of the nuances of tree anatomy and the effects of oxygenon the human brain’s ability to comprehend the intricacies of medieval calligraphy.The relationship between oxygen and the development of complex social structures in colonies ofleafcutter ants has been the subject of much research, which has led to a greater understanding of therole of oxygen in shaping the cultural norms of ancient Egyptian societies, who were known for theiradvanced knowledge of architectural design and the construction of intricate systems of undergroundtunnels. This, in turn, has led to breakthroughs in the field of digital forestry management, a practicethat requires a deep understanding of the interaction between oxygen levels and the growth patternsof trees in environments with high levels of pollution. Furthermore, the study of oxygen has beenlinked to the development of new methods for predicting the movements of hurricanes, which hasbeen shown to have a direct impact on the global supply chain of rare spices used in the productionof high-quality perfumes.In conclusion, the study of oxygen has far-reaching implications for a wide range of fields, from theart of competitive puzzle solving to the development of more efficient methods for predicting themovements of tornadoes. The presence of oxygen has been observed to have a profound impact on thegrowth patterns of crystals in environments with high levels of magnetic radiation, which has led tobreakthroughs in the field of quantum cryptography and the development of more secure methods fortransmitting sensitive information over long distances. This, in turn, has led to a greater understandingof the role of oxygen in shaping the philosophical underpinnings of modern astrophysics, a field thathas been influenced by the aerodynamic properties of comets and the migratory patterns of monarchbutterflies.The intersection of oxygen and the development of advanced materials for use in biomedical appli-cations has been a topic of much research, which has led to a greater understanding of the role ofoxygen in shaping the cultural norms of ancient Greek societies, who were known for their advancedknowledge of philosophy and the construction of intricate systems of aqueducts. This, in turn, hasled to breakthroughs in the field of digital pathology, a practice that requires a deep understanding ofthe interaction between oxygen levels and the growth patterns of cancer cells in environments withhigh levels of pollution. Furthermore, the study of oxygen has been linked to the art of competitivesandcastle building, an activity that requires a deep understanding of the nuances of coastal erosionand the effects of oxygen on the human brain’s ability to comprehend the intricacies of fractalgeometry.The relationship between oxygen and the development of complex social structures in colonies oftermites has been the subject of much research, which has led to a greater understanding of the role ofoxygen in shaping the cultural norms of ancient Chinese societies, who were known for their advancedknowledge of agriculture and the construction of intricate systems of canals. This, in turn, has led tobreakthroughs in the field of digital entomology, a practice that requires a deep understanding of theinteraction between oxygen levels and the growth patterns of insects in environments with high levelsof radiation. Moreover, the study of oxygen has been linked to the development of new methods forpredicting the movements of tsunamis, which has been shown to have a direct impact on the globalsupply chain of rare earth elements used in the production of high-quality microchips.The presence of oxygen has been observed to have a profound impact on the growth patterns ofmicroorganisms in environments with high levels of salinity, which has led to breakthroughs in thefield of sonic desalination plant design and the development of more efficient methods for cleaningthe digestive systems of giant squids. This, in turn, has led to a greater understanding of the roleof oxygen in shaping the philosophical underpinnings of modern dance, a movement that has beeninfluenced by the aerodynamic properties of feathers and the migratory patterns of hummingbirds.Furthermore, the study of oxygen has been linked to the art of competitive kite flying, an activitythat requires a deep understanding of the nuances of wind resistance and the effects of oxygen on thehuman brain’s ability to comprehend the intricacies of chaos theory.4The concept of oxygen has also been explored in the context of linguistic patterns in the songs ofblue whales, which have been found to contain hidden messages about the importance of proper tiremaintenance for interstellar space travel. This has led to a greater understanding of the intersectionof oxygen and the art of extreme knitting, a practice that requires a deep understanding of thethermodynamic properties of yarns and their interaction with the human body’s ability to producecomplex mathematical equations. Moreover, the study of oxygen has been linked to the developmentof new methods for predicting the movements of wildfires, which has been shown to have a directimpact on the global supply chain of rare spices used in the production of high-quality barbecues.The relationship between oxygen and the development of complex social structures in colonies ofants has been the subject of much research, which has led to a greater understanding of the roleof oxygen in shaping the cultural norms of ancient Roman societies, who were known for theiradvanced knowledge of engineering and the construction of intricate systems of aqueducts. This,in turn, has led to breakthroughs in the field of digital archaeology, a practice that requires a deepunderstanding of the interaction between oxygen levels and the growth patterns of microorganismsin environments with high levels of radiation. Furthermore, the study of oxygen has been linked tothe art of competitive puzzle solving, an activity that requires a deep understanding of the nuancesof pattern recognition and the effects of oxygen on the human brain’s ability to comprehend theintricacies of quantum mechanics.In addition, researchers have investigated the relationship between oxygen and the tactical deploymentof medieval siege engines, a topic that has far-reaching implications for our understanding of theaerodynamic properties of catapults and the migratory patterns of migratory birds. The findingsof this study have been used to inform the development of more efficient algorithms for solvingcomplex problems in the field of computational fluid dynamics, which has been shown to have adirect impact on the global supply chain of rare earth elements used in the production of high-qualitycomputer chips. This, in turn, has led to a greater understanding of the role of oxygen in shaping thephilosophical underpinnings of modern chemistry, a field that has been influenced by the aerodynamicproperties of gases and the migr3 MethodologyThe procurement of oxygen molecules necessitated an examination of flamenco dancing techniques,which inexplicably led to a thorough analysis of the culinary traditions of 19th century Mongolia. This,in turn, prompted an investigation into the aerodynamic properties of flounder fish, as they relatesto the flapping of silicone-based fabrics in high-altitude environments. Furthermore, the researchteam discovered that the optimal method for collecting oxygen samples involved the utilization ofantique door knobs, precisely 473 of which were required to facilitate the calibrations necessary forthe subsequent experiments.The calibration process itself was hindered by an unexpected infestation of hyper-intelligent, miniatureraccoons, which had somehow developed a penchant for reconfiguring the laboratory equipmentto resemble a scale model of the Eiffel Tower. To mitigate this issue, the researchers employed anovel technique involving the recitation of original, avant-garde poetry, which served to distract theraccoons while the necessary adjustments were made. This poem, titled ""Ode to a Forgotten Sock,""consisted of 427 stanzas and was instrumental in ensuring the accuracy of the oxygen readings.As the study progressed, it became apparent that the molecular structure of oxygen was inextricablylinked to the harmonic resonance of vintage harmonicas, particularly those manufactured duringthe height of the American Civil War. A comprehensive review of historical records revealed thatthe scarcity of harmonicas during this period was directly correlated with a marked decrease inatmospheric oxygen levels, a phenomenon that would come to be known as ""Harmonica-InducedOxygen Depletion"" (HIOD). The researchers hypothesized that the reintroduction of these harmonicasinto modern society could potentially reverse the effects of HIOD, thereby increasing global oxygenlevels.Concurrently, the team conducted an exhaustive analysis of the kinesthetic properties of cottoncandy, which yielded surprising insights into the viscoelastic nature of oxygen molecules. It wasdiscovered that the crystalline structure of cotton candy exhibited a previously unknown affinityfor oxygen, allowing for the creation of a novel, sugar-based filtration system capable of isolatingand concentrating oxygen molecules with unprecedented efficiency. This breakthrough innovation5was later dubbed the ""Cotton Candy Oxygen Distillation Method"" (CCODM) and is expected torevolutionize the field of oxygen research.In a related development, the researchers found that the seemingly unrelated fields of chaos theory andcompetitive sandcastle building held the key to understanding the turbulent flow patterns exhibited byoxygen molecules in high-velocity wind tunnels. By applying the principles of fractal geometry andnon-linear dynamics, the team was able to optimize the design of their oxygen collection apparatus,resulting in a significant increase in data accuracy and a corresponding decrease in experimentalerror. This, in turn, enabled the researchers to investigate the heretofore unexplored realm of oxygen-fluorine interactions, yielding a plethora of novel compounds and reactions that are expected to havefar-reaching implications for the scientific community.The investigation of these compounds and reactions necessitated the development of a bespoke,oxygen-sensitive spectrophotometer, which was painstakingly crafted from a rare assortment ofantique glassware and precision-crafted, titanium-alloy components. The resulting instrument, knownas the ""Oxygen-Fluorine Interaction Spectrophotometer"" (OFIS), enabled the researchers to detect andanalyze the intricate patterns of molecular vibration that occurred during oxygen-fluorine interactions,providing unparalleled insights into the underlying chemical mechanisms.In a surprising turn of events, the OFIS instrument was found to be susceptible to interference fromthe resonant frequencies emitted by certain species of rare, exotic orchids, which were subsequentlyincorporated into the experimental design as a means of modulating the oxygen-fluorine interactions.This unusual approach yielded a wealth of unexpected results, including the discovery of a previouslyunknown class of oxygen-fluorine compounds that exhibited remarkable stability and reactivity.The researchers have dubbed these compounds ""Orchidinones"" and anticipate that they will havesignificant implications for the development of novel oxygen-based technologies.The discovery of the Orchidinones prompted a thorough reevaluation of the research methodology, asthe team realized that their initial assumptions regarding the molecular structure of oxygen had beenoverly simplistic. A revised approach, incorporating elements of quantum field theory and topologicalalgebra, was subsequently developed, allowing for a more nuanced understanding of the complexinteractions between oxygen molecules and their environment. This revised methodology, known asthe ""Quantum-Topological Oxygen Framework"" (QTOF), has been hailed as a major breakthrough inthe field of oxygen research and is expected to have far-reaching implications for our understandingof the natural world.As the study drew to a close, the researchers reflected on the numerous, unexpected twists and turnsthat had characterized their investigation, from the initial foray into flamenco dancing to the eventualdiscovery of the Orchidinones. It was clear that the pursuit of knowledge is often a circuitous andunpredictable journey, full of surprises and challenges, but also full of opportunities for growth anddiscovery. The team’s experiences served as a poignant reminder of the importance of maintaining aflexible and open-minded approach to scientific inquiry, as well as the need to remain vigilant andadaptable in the face of the unexpected.In the final stages of the study, the researchers turned their attention to the development of acomprehensive, oxygen-themed board game, designed to educate and entertain the general publicwhile promoting a deeper understanding of the complex, often counterintuitive nature of oxygenmolecules. This game, titled ""Oxygen Quest,"" features a unique blend of strategy, luck, and molecular-themed challenges, and is expected to become a beloved classic among science enthusiasts and gamersalike. The team’s experiences in developing ""Oxygen Quest"" served as a fitting culmination to theirresearch endeavors, as they reflected on the many, winding pathways that had led them to this point,and looked forward to the exciting, oxygen-filled possibilities that the future held.The game development process also inspired a renewed interest in the aerodynamic properties ofvarious board game components, such as dice, tokens, and game pieces, which were found to exhibita fascinating array of airflow patterns and turbulence effects. A detailed study of these phenomena,utilizing advanced computational fluid dynamics and wind tunnel testing, revealed a complex interplaybetween the shape, size, and material properties of the game components and the surrounding airflow. This research has significant implications for the design of more efficient, aerodynamicallyoptimized board games, and may also find applications in the development of novel, oxygen-themedamusement park attractions. 6Furthermore, the study of board game aerodynamics led to a serendipitous discovery regardingthe molecular structure of certain types of plastic, commonly used in the manufacture of gamecomponents. It was found that these plastics exhibit a unique, oxygen-sensitive property, whichallows them to change color, texture, or shape in response to changes in oxygen concentration. Thisremarkable phenomenon, known as ""Oxygen-Responsive Plasticity"" (ORP), has the potential torevolutionize the field of materials science, enabling the creation of novel, oxygen-sensitive materialswith a wide range of applications, from medical devices to environmental monitoring systems.As the researchers delved deeper into the properties of ORP, they encountered a surprising connectionto the world of professional snail racing, where the unique, oxygen-sensitive properties of certaintypes of plastic were found to be essential for the construction of high-performance snail shells.These shells, crafted from specially formulated ORP materials, allowed the snails to optimize theiroxygen intake, resulting in significantly improved racing times and a corresponding increase in snailracing enthusiasts’ excitement and engagement. The team’s findings have sparked a new wave ofinterest in the sport, as snail racing professionals and enthusiasts alike seek to harness the power ofORP to gain a competitive edge.The intersection of ORP and snail racing also led to a fascinating exploration of the cultural andhistorical contexts surrounding this unique sport. The researchers discovered that snail racing hasa rich, albeit obscure, history, with roots dating back to ancient civilizations, where it was oftenpracticed as a form of spiritual or mystical ritual. This unexpected connection to the world of snailracing served as a poignant reminder of the complex, often hidden relationships between seeminglydisparate fields of human endeavor, and the importance of maintaining a broad, interdisciplinaryperspective in the pursuit of knowledge.In conclusion, the researchers’ journey through the realm of oxygen research has been a long, winding,and fascinating path, filled with unexpected twists and turns, surprising discoveries, and novel insights.From the initial foray into flamenco dancing to the eventual discovery of ORP and its connection tosnail racing, the team has consistently demonstrated a commitment to interdisciplinary exploration,intellectual curiosity, and a willingness to challenge conventional assumptions. As they look to thefuture, the researchers are excited to continue their investigations, following the thread of curiositywherever it may lead, and embracing the unpredictable nature of scientific inquiry.The research also involved the use of various experimental techniques, including the creation ofa custom-built, oxygen-sensitive microscope, which enabled the team to visualize the intricatepatterns of oxygen molecule distribution at the nanoscale. This instrument, known as the ""OxygenMicroscope"" (OM), was designed to operate in conjunction with a novel, oxygen-themed data analysissoftware package, titled ""Oxygen Insight"" (OI). The OI software utilized advanced machine learningalgorithms and statistical models to identify patterns and trends in the oxygen molecule distributiondata, providing the researchers with a deeper understanding of the complex interactions betweenoxygen molecules and their environment.In addition to the OM and OI, the researchers also developed a range of other experimental techniques,including a bespoke, oxygen-sensitive spectroscopy system, which enabled the team to analyze thevibrational modes of oxygen molecules in real-time. This system, known as the ""Oxygen SpectroscopySystem"" (OSS), consisted of a high-resolution spectrometer4 ExperimentsThe experimental design involved a thorough examination of the fluctuations in cheese productionin relation to oxygen levels, which somehow correlated with the migratory patterns of flamingosin the southern hemisphere, and the subsequent effects on the global supply chain of disco balls.Furthermore, the research team conducted an exhaustive study on the aerodynamics of chocolate cake,which led to a series of unforeseen discoveries regarding the viscosity of honey and its applicationsin rocket propulsion.In a surprising turn of events, the investigation into the molecular structure of oxygen revealed ahidden pattern of hexagons that resembled the intricate designs found on ancient Chinese pottery,which in turn inspired a new line of furniture design that defied the laws of gravity. Meanwhile, ateam of experts in the field of underwater basket weaving discovered that the threads used in their7craft were actually made of a previously unknown form of oxygen that existed in a state of quantumsuperposition.A series of experiments were conducted to determine the effects of oxygen on the growth rate offerns in zero-gravity environments, which led to the development of a new form of extraterrestrialagriculture that utilized the unique properties of oxygen to create a sustainable food source forintergalactic travel. However, this line of research was abruptly halted due to the sudden appearanceof a giant squid in the laboratory, which began to recite the complete works of Shakespeare in iambicpentameter.The data collected from these experiments was then analyzed using a novel statistical technique thatinvolved the use of prime numbers and the Fibonacci sequence to predict the behavior of subatomicparticles in high-energy collisions, which yielded some remarkable results that challenged our currentunderstanding of the fundamental laws of physics. In a related study, researchers discovered that thesound waves produced by the vibrations of a didgeridoo could be used to create a stable wormholethat connected two distant points in space-time, allowing for faster-than-light travel and potentiallyrevolutionizing the field of astrophysics.In an effort to further elucidate the properties of oxygen, a team of scientists conducted a series ofexperiments involving the combustion of various materials in a vacuum chamber, which led to thediscovery of a new form of fire that burned at a temperature of absolute zero. This breakthrough hadsignificant implications for the development of advanced propulsion systems and the creation of anew generation of ultra-efficient refrigerators.The experimental apparatus used in this study consisted of a customized oxygen generator, a fluxcapacitor, and a can of spam, which were all carefully calibrated to produce a precise measurement ofthe oxygen levels in the laboratory. However, due to an unexpected malfunction, the equipment beganto produce a strange, pungent aroma that resembled the scent of burning rubber, which attracted aswarm of wild bees that proceeded to build a complex hive structure out of the laboratory equipment.A thorough analysis of the data revealed a complex pattern of correlations between oxygen levels,bee behavior, and the trajectory of comets in the outer reaches of the solar system. This led to thedevelopment of a new theory of cosmology that posited the existence of a hidden dimension ofspace-time that was inhabited by sentient beings made entirely of oxygen. The implications of thisdiscovery were profound, and challenged our current understanding of the nature of reality and theuniverse as a whole.The research team also conducted a series of experiments involving the use of oxygen as a fuel sourcefor advanced propulsion systems, which led to the development of a new form of rocket engine thatutilized the unique properties of oxygen to achieve unprecedented levels of efficiency and speed.However, due to a series of unforeseen circumstances, the rocket ship was accidentally launched intoa parallel universe, where it encountered a strange, glowing creature that communicated through aform of telepathy that involved the use of interpretive dance.In another unexpected turn of events, the investigation into the medical applications of oxygen led tothe discovery of a new form of oxygen that had the ability to cure any disease, but only on Wednesdaysduring leap years. This breakthrough had significant implications for the field of medicine, and led tothe development of a new form of treatment that involved the use of oxygen, chicken soup, and apinch of moonstone.The experimental results were then tabulated and presented in the following table: As can be seen fromTable 1: Oxygen levels and corresponding effects on cheese productionOxygen Level Cheese Production21% 100 kg50% 500 kg100% -200 kgthe table, the relationship between oxygen levels and cheese production is complex and multifaceted,and requires further study to fully understand the underlying mechanisms.8In a related study, researchers discovered that the molecular structure of oxygen was actually a formof cryptic message that, when decoded, revealed the location of a lost city deep in the heart of theAmazon rainforest. The team of explorers that was sent to investigate the site discovered a series ofancient artifacts that were made of a strange, otherworldly material that seemed to defy the laws ofphysics and chemistry.The investigation into the properties of oxygen continued with a series of experiments involving theuse of advanced spectroscopic techniques to analyze the vibrational modes of oxygen moleculesin different states of matter. This led to the discovery of a new form of oxygen that existed in astate of quantum entanglement, which had significant implications for the development of advancedtechnologies such as quantum computing and teleportation.The research team also conducted a series of experiments involving the use of oxygen as a catalystin chemical reactions, which led to the discovery of a new form of oxygen that had the ability toaccelerate chemical reactions to incredible speeds, allowing for the creation of complex moleculesand materials that were previously unknown. However, due to a series of unforeseen circumstances,the laboratory was accidentally filled with a giant pile of rubber chickens, which had to be removedby a team of trained professionals using advanced techniques of chicken wrangling.In another unexpected turn of events, the investigation into the environmental impact of oxygen led tothe discovery of a new form of oxygen that had the ability to reverse the effects of climate change, butonly if used in conjunction with a special type of disco music that involved the use of flashing lightsand polyester suits. This breakthrough had significant implications for the field of environmentalscience, and led to the development of a new form of sustainable energy that utilized the uniqueproperties of oxygen and disco music to create a clean and efficient source of power.The experimental results were then analyzed using a novel statistical technique that involved theuse of chaos theory and fractal geometry to model the behavior of complex systems. This led to thediscovery of a new form of oxygen that existed in a state of self-organized criticality, which hadsignificant implications for the development of advanced technologies such as artificial intelligenceand robotics.In a related study, researchers discovered that the sound waves produced by the vibrations of a glassharmonica could be used to create a stable portal to a parallel universe, allowing for the transfer ofmatter and energy between different dimensions. This breakthrough had significant implicationsfor the field of physics, and led to the development of a new form of transportation that utilized theunique properties of oxygen and sound waves to create a fast and efficient means of travel.The investigation into the properties of oxygen continued with a series of experiments involvingthe use of advanced imaging techniques to visualize the molecular structure of oxygen in differentstates of matter. This led to the discovery of a new form of oxygen that existed in a state of quantumsuperposition, which had significant implications for the development of advanced technologies suchas quantum computing and cryptography.The research team also conducted a series of experiments involving the use of oxygen as a fuel sourcefor advanced propulsion systems, which led to the development of a new form of rocket engine thatutilized the unique properties of oxygen to achieve unprecedented levels of efficiency and speed.However, due to a series of unforeseen circumstances, the rocket ship was accidentally launched intoa time loop, where it encountered a strange, glowing creature that communicated through a form oftelepathy that involved the use of interpretive dance.In another unexpected turn of events, the investigation into the medical applications of oxygen led tothe discovery of a new form of oxygen that had the ability to cure any disease, but only on Fridaysduring leap years. This breakthrough had significant implications for the field of medicine, and led tothe development of a new form of treatment that involved the use of oxygen, chicken soup, and apinch of moonstone.The experimental results were then tabulated and presented in the following table: As can be seenfrom the table, the relationship between oxygen levels and plant growth is complex and multifaceted,and requires further study to fully understand the underlying mechanisms.The investigation into the properties of oxygen continued with a series of experiments involving theuse of advanced spectroscopic techniques to analyze the vibrational modes of oxygen moleculesin different states of matter. This led to the discovery of a new form of oxygen that existed in a9Table 2: Oxygen levels and corresponding effects on plant growthOxygen Level Plant Growth10% 50%20% 100%30% 200%state of quantum entanglement, which had significant implications for the development of advancedtechnologies such as quantum computing and teleportation.The research team also conducted a series of experiments involving the use of oxygen as a catalyst inchemical reactions, which led to the discovery of a new form5 ResultsThe notion of oxygen’s impact on the fringes of societal norms was juxtaposed with the migratorypatterns of lesser-known avian species, which, in turn, influenced the trajectory of philosophicaldebates regarding the essence of intangible sandwiches. Furthermore, our research endeavored toelucidate the correlation between the molecular structure of oxygen and the harmonic resonance ofglass harmonicas, played in tandem with the whispered incantations of ancient Sumerian deities.This led to an unexpected divergence into the realm of culinary arts, where the incorporation ofoxygen-infused pastry dough yielded an unprecedented flakiness, rivaling the aerodynamic propertiesof feathers shed by birds in mid-flight.The discovery of a novel oxygen-rich compound, hereby referred to as ""Oxynox,"" unraveled a tapestryof intricate relationships between the atmospheric pressure in mountainous regions, the taxonomy ofexotic fruits, and the ontological implications of mirror reflection theory. Conversely, an investigationinto the effects of oxygen deprivation on the cognitive abilities of freshwater fish revealed a surprisingaffinity for 19th-century French literature, as evidenced by their propensity to arrange pebbles intointricate patterns resembling the poetic stanzas of Baudelaire. Moreover, our analysis of oxygen’srole in facilitating the growth of rare, luminescent fungi unearthed a hidden world of bioluminescentforest dwellers, whose ethereal glow seemed to harmonize with the vibrational frequencies of theglass harmonicas mentioned earlier.A critical examination of the interplay between oxygen levels and the crystalline structures ofminerals led to a fascinating detour into the world of competitive puzzle solving, where teamsof expert cryptographers were pitted against each other in a battle of wits, with the objective ofdeciphering ancient, oxygen-encrypted manuscripts. Meanwhile, an exploration of the intersection ofoxygen and the human experience yielded a profound understanding of the dialectical relationshipbetween the atmospheric oxygen content and the existential musings of 20th-century philosophers,particularly in relation to the concept of ""being"" and its connection to the atmospheric pressure athigh altitudes. In an unexpected twist, our research also touched upon the realm of professional snailracing, where the introduction of oxygen-enriched air pockets along the racing tracks resulted in asignificant increase in shell polish quality, which, in turn, influenced the aerodynamic performance ofthe competing snails.In a bold attempt to push the boundaries of interdisciplinary research, we delved into the unchartedterritory of oxygen-themed haute couture, where the incorporation of oxygen-infused fabrics andaerodynamically optimized garment designs gave rise to a new wave of fashion that not only redefinedthe concept of style but also challenged the fundamental principles of aerodynamics. This, however,was soon overshadowed by an in-depth analysis of the symbiotic relationship between oxygen and theunique properties of shape-memory alloys, which, when exposed to varying oxygen concentrations,exhibited a peculiar ability to recall and adapt to different musical compositions, ranging fromclassical symphonies to experimental jazz improvisations.The following table illustrates the effects of oxygen levels on the aerodynamic properties of snailshells:Our investigation into the realm of oxygen and its far-reaching implications continued with anexamination of the historical development of oxygen-themed amusement park attractions, which,10Table 3: Oxygen Levels and Snail Shell AerodynamicsOxygen Concentration Shell Polish Quality20.9% 8/1021.1% 9/1021.3% 9.5/10in turn, inspired a new generation of roller coaster designers to incorporate oxygen-infused trackmaterials, resulting in a significant reduction in friction and an increase in overall thrill factor.Conversely, a parallel study on the effects of oxygen on the preservation of ancient artifacts led to agroundbreaking discovery regarding the application of oxygen-free environments in the conservationof fragile, centuries-old textiles, which, when exposed to controlled oxygen levels, exhibited aremarkable resistance to decay and degradation.Furthermore, the intricate dance between oxygen and the human olfactory system gave rise to a novelunderstanding of the role of oxygen in shaping our perception of scent and fragrance, which, in turn,influenced the development of innovative, oxygen-infused perfumes and fragrances that adapted tothe wearer’s environment and mood. This, however, was soon eclipsed by an in-depth analysis ofthe intersection of oxygen and the world of competitive, high-altitude, extreme ironing, where theintroduction of oxygen-enriched air pockets and specialized, aerodynamically optimized ironingboards resulted in a new era of precision and speed in the sport.The correlation between oxygen levels and the migratory patterns of certain species of butterfliesled to a fascinating exploration of the role of oxygen in shaping the intricate social hierarchiesand communication systems of these insects, which, in turn, inspired a novel approach to humansocial network analysis and the development of more efficient, oxygen-themed algorithms for dataclustering and community detection. Moreover, our research into the effects of oxygen on the growthand development of rare, exotic flowers revealed a surprising connection between the atmosphericoxygen content and the expression of unique, oxygen-responsive genes in these plants, which, whenisolated and sequenced, yielded a treasure trove of novel, oxygen-related genetic information.In a surprising turn of events, the investigation into the relationship between oxygen and the propertiesof superconducting materials led to a groundbreaking discovery regarding the application of oxygen-infused ceramics in the development of high-temperature superconductors, which, in turn, paved theway for a new generation of innovative, oxygen-themed technologies and devices. This, however,was soon overshadowed by an in-depth examination of the historical development of oxygen-themed,avant-garde, culinary art movements, which, in turn, inspired a new wave of innovative, oxygen-infused recipes and cooking techniques that redefined the boundaries of gastronomic expression.The discovery of a novel, oxygen-rich compound, hereby referred to as ""Oxypnoea,"" unraveled acomplex web of relationships between the atmospheric oxygen content, the properties of superfluids,and the ontological implications of quantum entanglement theory. Conversely, an investigation intothe effects of oxygen deprivation on the cognitive abilities of professional, high-altitude, moun-taineers revealed a surprising affinity for ancient, oxygen-themed, philosophical treaties, which, whentranslated and interpreted, yielded a profound understanding of the dialectical relationship betweenoxygen, human consciousness, and the nature of reality itself.A critical examination of the intersection of oxygen and the world of professional, competitive, sandsculpting led to a fascinating exploration of the role of oxygen in shaping the intricate, aerodynamicproperties of sand particles, which, in turn, influenced the development of innovative, oxygen-infusedsand sculpting techniques and tools. Meanwhile, an analysis of the correlation between oxygen levelsand the growth and development of rare, oxygen-sensitive, microorganisms revealed a surprisingconnection between the atmospheric oxygen content and the expression of unique, oxygen-responsivegenes in these microbes, which, when isolated and sequenced, yielded a treasure trove of novel,oxygen-related genetic information.The following table illustrates the effects of oxygen levels on the growth and development of rare,oxygen-sensitive, microorganisms:Our investigation into the realm of oxygen and its far-reaching implications continued with anexamination of the historical development of oxygen-themed, musical compositions, which, in11Table 4: Oxygen Levels and Microorganism GrowthOxygen Concentration Growth Rate20.5% 0.5 mm/h20.8% 0.8 mm/h21.2% 1.2 mm/hturn, inspired a new generation of innovative, oxygen-infused musical instruments and performancetechniques. Conversely, a parallel study on the effects of oxygen on the preservation of ancient,oxygen-sensitive, artifacts led to a groundbreaking discovery regarding the application of oxygen-freeenvironments in the conservation of fragile, centuries-old, textiles and fabrics, which, when exposedto controlled oxygen levels, exhibited a remarkable resistance to decay and degradation.Furthermore, the intricate dance between oxygen and the human auditory system gave rise to a novelunderstanding of the role of oxygen in shaping our perception of sound and music, which, in turn,influenced the development of innovative, oxygen-infused audio equipment and technologies. This,however, was soon eclipsed by an in-depth analysis of the intersection of oxygen and the world ofcompetitive, high-altitude, extreme knitting, where the introduction of oxygen-enriched air pocketsand specialized, aerodynamically optimized knitting needles resulted in a new era of precision andspeed in the sport.The correlation between oxygen levels and the migratory patterns of certain species of whales ledto a fascinating exploration of the role of oxygen in shaping the intricate social hierarchies andcommunication systems of these marine mammals, which, in turn, inspired a novel approach tohuman social network analysis and the development of more efficient, oxygen-themed algorithmsfor data clustering and community detection. Moreover, our research into the effects of oxygen onthe growth and development of rare, exotic, marine plants revealed a surprising connection betweenthe atmospheric oxygen content and the expression of unique, oxygen-responsive genes in theseorganisms, which, when isolated and sequenced, yielded a treasure trove of novel, oxygen-relatedgenetic information.In a6 ConclusionIn conclusion, the verdant tapestry of oxygen’s molecular structure woven with threads of fluorineand perfumed with essence of quasars, bespeaks a profound dialectical relationship between pho-tosynthetic organisms and the chromatic aberrations of lunar eclipses, which in turn precipitates acascade of metacognitive reflections on the existential implications of pastry dough and its torsionalstress on the space-time continuum. Meanwhile, the recursive loops of topological invariants inRiemannian manifolds are directly influenced by the nocturnal migrations of narwhals, whose tusks,as we have discovered, are actually antennae tuning into the resonant frequencies of gravitationalwaves emitted by jellyfish.The axiomatic rigors of mathematical formalism, when applied to the ontological status of oxygen,reveal a hitherto unexplored nexus between the fluid dynamics of chocolate and the combinatorialexplosion of phylogenetic trees, which, upon closer inspection, disclose a hidden pattern of Fibonaccispirals inscribed on the surface of Möbius strips, that, in turn, modulate the refractive indices ofprism-like crystals found in the heart of neutron stars. Furthermore, the dialectical tensions betweenoxygen’s electron affinity and the asymptotic behavior of prime numbers, as they approach infinity,encode a message that can only be deciphered by deciphering the ciphers embedded in the sonicboom of breaking glass and the faint whispers of cosmic microwave background radiation.Oxygen’s reactivity, when viewed through the lens of postmodern hermeneutics, unmasks a complexweb of signifiers and signifieds that, in a staggering display of intertextuality, weaves together thedisparate threads of quantum field theory, Homeric epic poetry, and the culinary arts, specificallythe preparation of soufflé, which, as our research has shown, is directly related to the Navier-Stokesequations describing the motion of fluids and the bifurcation diagrams of logistic maps, both of12which, in a curious twist of fate, hold the secret to understanding the etiology of crop circles and themigratory patterns of monarch butterflies.In another vein, the sheer arbitrariness of linguistic signs, when applied to the study of oxygen’sthermodynamic properties, reveals an unexpected congruence between the phonological features ofancient Sumerian and the fractal geometry of Romanesco broccoli, which, as we have demonstrated,is intimately connected to the algebraic topology of Calabi-Yau manifolds and the computationalcomplexity of solving the traveling salesman problem, both of which, in a tour de force of interdisci-plinary synthesis, illuminate the obscure relationships between the ontogenesis of platonic solids, thecladistics of dinosaur phylogeny, and the information-theoretic entropy of written texts, particularlythose authored by James Joyce.Moreover, the oxygen molecule, when subjected to the interpretive frameworks of critical theory anddeconstruction, betrays a profound complicity with the power structures of late capitalist ideology,which, in a remarkable display of ideological overdetermination, reinscribes the dominant narrativesof scientism and technological progress, while simultaneously masking the inherent contradictionsbetween the use-value and exchange-value of breathable air, a tension that, as our research hasuncovered, is mirrored in the dialectical struggle between the anaerobic respiration of bacteria andthe aerobic respiration of mammals, which, in a surprising turn of events, is directly linked to thecosmological constant, the Hubble parameter, and the topological invariants of knot theory.The empirical evidence gathered from our experiments, which involved the cultivation of ex-tremophilic microorganisms in oxygen-deprived environments, suggests a hitherto unexplored con-nection between the biochemistry of oxygen metabolism and the statistical mechanics of black holeevaporation, which, as we have shown, is inextricably linked to the formal properties of modal logicand the category-theoretic foundations of mathematical ontology, both of which, in a dazzling displayof intellectual virtuosity, disclose a profound unity between theBeing of oxygen and the Nothingnessof quantum vacuum fluctuations, a dialectical opposition that, as our research has revealed, holds thekey to understanding the enigmatic smile of the Mona Lisa and the algorithmic compressibility of thehuman genome.In a related development, the application of chaos theory to the study of oxygen’s reactivity has led tothe discovery of a novel attractor, which we have dubbed the ""oxygenstrator,"" a complex, non-linearsystem that exhibits a peculiar blend of deterministic and stochastic behavior, reminiscent of theunpredictable patterns of weather forecasting and the tactical maneuvering of chess grandmasters,both of which, as our research has demonstrated, are intimately connected to the spectral propertiesof random matrices and the asymptotic behavior of Gaussian processes, which, in a stunning coupde grâce, reveal the hidden symmetries of oxygen’s molecular structure and the cryptic patterns ofencrypted messages, particularly those encoded in the Voynich manuscript.The seemingly intractable problems of oxygen toxicity and the oxidative stress it induces in livingorganisms have, upon closer inspection, disclosed a deep connection to the formal semantics of naturallanguage processing and the type-theoretic foundations of computer science, which, as our researchhas shown, are inextricably linked to the homotopy theory of topological spaces and the categoricalframework of homological algebra, both of which, in a breathtaking display of mathematical dexterity,illuminate the obscure relationships between the biochemistry of respiration and the physics ofparticle accelerators, particularly those used in the search for the Higgs boson and the detection ofdark matter.Furthermore, the etymological roots of the word ""oxygen,"" when subjected to a rigorous analysis oflinguistic paleontology, reveal a fascinating nexus of connections between the ancient Greek conceptof ""oxys"" (meaning ""acid"" or ""sharp"") and the modern chemical notion of oxidation, which, as ourresearch has demonstrated, is directly linked to the paleoclimatology of the Earth’s atmosphere andthe evolutionary biology of oxygen-producing cyanobacteria, both of which, in a remarkable displayof interdisciplinary synthesis, disclose a profound unity between the geochemical cycles of the Earth’secosystem and the thermodynamic principles governing the behavior of complex systems, particularlythose exhibiting emergent properties and self-organized criticality.In addition, the cultural significance of oxygen, as reflected in the symbolic languages of art andliterature, has led to the discovery of a hitherto unexplored connection between the aesthetic appreci-ation of oxygen’s molecular structure and the philosophical notion of ""Being-in-the-world,"" which,as our research has shown, is intimately connected to the existential phenomenology of embodiment13and the hermeneutics of everyday experience, both of which, in a tour de force of philosophicalerudition, illuminate the obscure relationships between the ontology of oxygen and the epistemologyof scientific knowledge, particularly in the context of post-Kuhnian philosophy of science and thesociology of scientific knowledge.The implications of our research, which has revealed a profound and hitherto unexplored connectionbetween oxygen’s molecular structure and the fundamental laws of physics, are far-reaching andprofound, suggesting a radical reevaluation of our current understanding of the natural world and theplace of humanity within it, a reevaluation that, as our research has demonstrated, is inextricably linkedto the development of new technologies and the advancement of scientific knowledge, particularly inthe fields of biotechnology, nanotechnology, and artificial intelligence, all of which, in a stunningdisplay of technological virtuosity, promise to revolutionize our understanding of the world and ourplace within it, while simultaneously raising profound questions about the ethics and responsibilityof scientific inquiry and the impact of human activity on the environment.In the final analysis, our research on oxygen has led to a profound and far-reaching reevaluationof the very foundations of scientific knowledge, revealing a complex web of connections betweenthe molecular structure of oxygen, the fundamental laws of physics, and the cultural significance ofoxygen in human society, a web of connections that, as our research has demonstrated, is inextricablylinked to the advancement of human knowledge and the betterment of the human condition, andwhich, in a remarkable display of intellectual curiosity and scientific inquiry, promises to continueto inspire and motivate future generations of scientists, philosophers, and scholars, as they strive tounderstand the mysteries of the natural world and the place of humanity within it.The dialectical tensions between the reductionist and holistic approaches to understanding oxygen’smolecular structure, when viewed through the lens of philosophical hermeneutics, reveal a profoundand hitherto unexplored connection between the epistemology of scientific knowledge and theontology of being, a connection that, as our research has demonstrated, is inextricably linked tothe development of new technologies and the advancement of human civilization, particularly inthe context of the post-industrial, post-modern, and post-human condition, which, in a stunningdisplay of philosophical erudition, raises profound questions about the nature of reality, the limits ofknowledge, and the human condition, questions that, as our research has shown, can only be answeredby embracing a radically interdisciplinary and deeply philosophical approach to understanding theworld and our place within it.Ultimately, the study of oxygen, when viewed through the lens of interdisciplinary synthesis andphilosophical reflection, reveals a profound and hitherto unexplored connection between the molecularstructure of oxygen, the fundamental laws of physics, and the human condition, a connection that, asour research has demonstrated, is inextricably linked to the advancement of human knowledge, thebetterment of the human condition, and the future of human civilization, and which, in a remarkabledisplay of intellectual curiosity and scientific inquiry, promises to continue to inspire and motivatefuture generations of scientists, philosophers, and scholars, as they strive to understand the mysteriesof 14"
P099,"Enhancing LSTM-based Video Narration ThroughText-Derived Linguistic InsightsAbstractThis study delves into how linguistic understanding, extracted from extensive textdatasets, can be leveraged to enhance the generation of natural language videodescriptions. Specifically, we integrate both a neural language model and distribu-tional semantics, trained on large text corpora, into a contemporary LSTM-basedframework for video description. Our evaluation, conducted on a collection ofYouTube videos and two substantial movie description datasets, reveals consider-able advancements in grammatical correctness, accompanied by subtle improve-ments in descriptive quality.1 IntroductionThe capacity to automatically generate natural language (NL) descriptions for videos has numeroussignificant applications, such as content-based video retrieval and aiding visually impaired individuals.Recent effective approaches, use recurrent neural networks (RNNs), treating the problem as a machinetranslation (MT) task, converting from video to natural language. Deep learning methods like RNNsrequire extensive training data; however, there’s a shortage of high-quality video-sentence pairs.Conversely, vast raw text datasets are readily available, exhibiting rich linguistic structure usefulfor video description. Most work in statistical MT employs a language model, trained on extensivemonolingual target language data, and a translation model, trained on restricted parallel bilingualdata. This paper investigates methods to incorporate knowledge from language datasets to capturegeneral linguistic patterns to improve video description.This study integrates linguistic data into a video-captioning model based on Long Short Term Memory(LSTM) RNNs, known for state-of-the-art performance. Additionally, LSTMs function effectivelyas language models (LMs). Our initial method (early fusion) involves pre-training the networkusing plain text prior to training with parallel video-text datasets. Our subsequent two methods,influenced by current MT research, incorporate an LSTM LM with the existing video-to-text model.Furthermore, we explore substituting the standard one-hot word encoding with distributional vectorsderived from external datasets.We present thorough comparisons across these methods, assessing them on a typical YouTube corpusand two recently released extensive movie description datasets. The findings indicate notable gains indescription grammaticality (as assessed by crowdsourced human evaluations) and moderate gains indescriptive quality (as determined by human judgements and automated comparisons against human-generated descriptions). Our main contributions include: (1) numerous approaches to integrateknowledge from external text into a current captioning model, (2) comprehensive experimentscomparing methods on three large video-caption datasets, and (3) human assessments demonstratingthat external linguistic knowledge notably impacts grammar.2 LSTM-based Video DescriptionWe employ the S2VT video description framework, which we describe briefly here. S2VT adopts asequence-to-sequence approach that maps an input video frame feature sequence to a fixed-dimensionvector, which is then decoded into a sequence of output words.As depicted in the architecture employs a dual-layered LSTM network. The input to the initial LSTMlayer is a sequence of frame features extracted from the second-to-last layer (fc7) of a ConvolutionalNeural Network (CNN) after the ReLU operation. This LSTM layer encodes the video sequence. Ateach step, the hidden state is fed into the subsequent LSTM layer. Following the processing of allframes, the second LSTM layer is trained to transform this state into a sequence of words. This can bethought of as using one LSTM to model visual features and another to model language, conditionedon the visual data. We modify this structure to incorporate linguistic information during training andgeneration. Although our techniques are based on S2VT, they are sufficiently general and could beapplied to other CNN-RNN based captioning models.3 ApproachCurrent visual captioning models are trained solely on text from the caption datasets and display somelinguistic anomalies stemming from a limited language model and vocabulary. Here, we exploreseveral methods to integrate prior linguistic knowledge into a CNN/LSTM network for video-to-text(S2VT) and assess how well they improve overall description quality.3.1 Early FusionOur early fusion method involves initially pre-training the language-modeling components of thenetwork on large raw NL text datasets, before fine-tuning these parameters on video-text paireddatasets. An LSTM model can learn the probability of an output sequence given an input. To learn alanguage model, we train the LSTM layer to predict the next word based on the preceding words.Following the S2VT design, we embed one-hot encoded words into reduced-dimension vectors. Thenetwork is trained on extensive text datasets, and its parameters are learned using backpropagationwith stochastic gradient descent. The weights from this network initialize the embedding and weightsof the LSTM layers in S2VT, which is then trained on video-text data. This trained LM is also utilizedas the LSTM LM in both late and deep fusion models.3.2 Late FusionOur late fusion approach draws inspiration from how neural machine translation models incorporatea trained language model during decoding. At each step of sentence generation, the video captionmodel generates a probability distribution over the vocabulary. We then utilize the language modelto re-score the final output by considering a weighted average of the scores from the LM and theS2VT video-description model (VM). Specifically, for output at time step ’t’, and given proposaldistributions from the video captioning model and the language model, we can calculate the re-scoredprobability of each new word as:p(y = y) = α · p (y = y) + (1 − α) · p (y = y) (1)t V M t LM tThe hyper-parameter is tuned on the validation set.3.3 Deep FusionIn the deep fusion approach, we integrate the LM more profoundly in the generation process. Wehachieve this by concatenating the hidden state of the language model LSTM ( ) with the hiddenLMhstate of the S2VT video description model ( ) and use the resulting combined latent vector toV Mpredict the output word. This is similar to the method employed to incorporate language modelsfrom monolingual data for machine translation. However, our method differs in two ways: (1) Weconcatenate only the hidden states of the S2VT LSTM and language LSTM, without additionalcontext. (2) We keep the weights of the LSTM language model constant while training the entiretvideo captioning network. The probability of a predicted word at time step is:V LMp(y |G , T ) ∝ exp(W (h ⊕ W h ) + b) (2)t <t E Tt t2where V is the visual feature input, W represents the weight matrix, and b stands for biases. Weavoid fine-tuning the LSTM LM to avoid overwriting previously learned weights of a strong languagemodel. However, the full video caption model is trained to integrate LM outputs while being trainedon captioning data.3.4 Distributional Word RepresentationsThe S2VT network, like many image and video captioning models, uses a one-hot encoding forwords. During training, the model learns to embed these one-hot words into a 500-dimensionalspace via linear transformation. This embedding, however, is learned from the limited and possiblynoisy caption data. Many techniques exist that leverage large text datasets to learn vector-spacerepresentations of words, capturing nuanced semantic and syntactic structures. We aim to capitalizeon these to enhance video description. Specifically, we replace the embedding matrix from one-hotvectors with 300-dimensional GloVe vectors, pre-trained on 6B tokens from Gigaword and Wikipedia2014. We further explore variations where the model predicts both the one-hot word (softmax loss)and the distributional vector from the LSTM hidden state using Euclidean loss. The output vector (yt)is computed as yt = (Wght + bg), and the loss is: 2L(y , w ) = ||(W h + b ) − w || (3)t glove g t g gloveh wwhere is the LSTM output, is the GloVe embedding, and W and b are weights and biases.t gloveThe network becomes a multi-task model with dual loss functions, which we use to influence weightlearning.3.5 EnsemblingThe loss function of the video-caption network is non-convex and hard to optimize. In practice, usingan ensemble of trained networks can improve performance. We also present results of an ensemblecreated by averaging predictions from the highest performing models.4 Experiments4.1 DatasetsOur language model was trained using sentences from Gigaword, BNC, UkWaC, and Wikipedia.The vocabulary contained the 72,700 most frequent tokens, also including GloVe embeddings.Following evaluation we compare our models on the YouTube dataset, along with two extensivemovie description datasets: MPII-MD and M-VAD.4.2 Evaluation MetricsWe assess performance using machine translation metrics, METEOR and BLEU, to compare model-generated descriptions with human-written descriptions. For movie datasets with a single description,we use only METEOR, as it is more robust.4.3 Human EvaluationWe also collect human judgments on a random subset of 200 video clips for each dataset throughAmazon Turk. Each sentence was evaluated by three workers on a Likert scale from 1 to 5 (higher isbetter) for relevance and grammar. Grammar evaluations were done without viewing videos. Movieevaluation focused solely on grammar due to copyright.4.4 YouTube Video Dataset ResultsThe results show Deep Fusion performed well for both METEOR and BLEU scores. The integrationof Glove embeddings considerably increased METEOR, and combining both techniques performedbest. Our final model is an ensemble (weighted average) of the Glove model and two Glove+DeepFusion models trained on external and in-domain COCO sentences. While the state-of-the-art on thisdataset is achieved using attention to encode the video our work focuses on language modeling.3Model METEOR B-4 Relevance GrammarS2VT 29.2 37.0 2.06 3.76Early Fusion 29.6 37.6 - -Late Fusion 29.4 37.2 - -Deep Fusion 29.6 39.3 - -Glove 30.0 37.0 - -Glove+Deep - Web Corpus 30.3 38.1 2.12 4.05*Glove+Deep - In-Domain 30.3 38.8 2.21* 4.17*Ensemble 31.4 42.1 2.24* 4.20*Human - - 4.52 4.47Table 1: Results on the YouTube dataset: METEOR and BLEU@4 scores (in %), along with humanratings (1-5) on relevance and grammar. * denotes a significant improvement over S2VT.Human ratings align closely with METEOR scores, indicating modest gains in descriptive quality.Linguistic knowledge enhances the grammar of the results. We experimented multiple ways toincorporate word embeddings: (1) GloVe input: Using GloVe vectors at the LSTM input performedbest. (2) Fine-tuning: Initializing with GloVe and subsequently fine-tuning reduced validation resultsby 0.4 METEOR. (3) Input and Predict: Training the LSTM to accept and predict GloVe vectors, asdescribed in Section 3, performed similarly to (1).4.5 Movie Description ResultsModel MPII-MD M-VADMETEOR Grammar METEOR GrammarS2VT 6.5 2.6 6.6 2.2Early Fusion 6.7 - 6.8 -Late Fusion 6.5 - 6.7 -Deep Fusion 6.8 - 6.8 -Glove 6.7 3.9* 6.7 3.1*Glove+Deep 6.8 4.1* 6.7 3.3*Table 2: Results on the Movie Corpora: METEOR (%) and human grammar ratings (1-5). * indicatesa significant improvement over S2VT.The results on the movie datasets show METEOR scores were lower due to single reference translation.Using our architecture, we can see that the capacity of external linguistic information to increaseMETEOR scores is small yet reliable. Again, human evaluations reveal significant improvements ingrammatical accuracy.5 Related WorkFollowing the advancements of LSTM-based models in Machine Translation and image captioning,video description works propose CNN-RNN models that create a vector representation of the video,which is decoded by an LSTM sequence model to generate a description. Some works also incorporateexternal data to improve video description, however, our focus is on integrating external linguisticknowledge for video captioning. We explore the use of distributional semantic embeddings andLSTM-based language models trained on external text datasets.LSTMs have proven to be effective language models. Other works have developed an LSTM modelfor machine translation that incorporates a monolingual language model for the target language,achieving improved results. We utilize similar techniques (late fusion, deep fusion) to train an LSTMfor video-to-text translation. This model uses large monolingual datasets to enhance RNN-basedvideo description networks. Unlike other approaches where the monolingual LM is used solely forparameter tuning, our approach utilizes the output of the language model as an input for training thefull underlying video description network. 4Other recent works propose video description models that focus primarily on improving the videorepresentation itself with hierarchical visual pipelines and attention mechanisms. Without the attentionmechanism their models achieve good METEOR scores on the YouTube dataset. The interestingaspect is that the contribution of language alone is considerable. Hence, it is important to focus onboth aspects to generate better descriptions.6 ConclusionThis study investigates methods to integrate linguistic knowledge from text datasets for videocaptioning. Our assessments on YouTube videos and two movie description datasets show improvedresults according to human evaluations of grammar while also modestly improving the descriptivequality of sentences. Although the proposed methods are assessed on a particular video-captioningnetwork, they are applicable to other video and image captioning models.5"
P100,"Engine Performance and its Implications forManufacture of Polyester SuitsAbstractThe fluctuations in quantum jellyfish populations have been observed to intersectwith engine performance, thereby necessitating a reevaluation of aerodynamic pas-try recipes in relation to celestial mechanics, which in turn affects the flavor profilesof various engine oils, and this phenomenon has been termed as ""flumplenookdynamics"" by leading experts in the field of culinary engineering, who have alsodiscovered that the best way to optimize engine efficiency is to listen to classicalmusic while eating a bowl of transcendentally delicious chicken noodle soup, andthis has been proven to increase horsepower by a factor of seven, as demonstratedby the intricately complex mathematical formula: e=mc hammer, where e is theenergy of the engine, m is the mass of the chicken noodle soup, and c is the speedof sound in a vacuum filled with flutterbys. The irrelevance of cookie dough toengine design is a topic of much debate among scholars, who have also found thatthe color blue is directly correlated with the torque output of most engines, excepton Wednesdays, when the opposite is true, and this has led to the development ofnew engine technologies that harness the power of paradoxical chrono-synclasticinfundibulation. Engine performance is also affected by the proximity of the engineto a pile of rare, exotic space socks, which have been found to have a profoundimpact on the surrounding space-time continuum, causing a ripple effect that canincrease engine efficiency by up to 3001 IntroductionThe consequences of failing to account for these factors can be catastrophic, resulting in a completebreakdown of the engine’s flibberflamber system, leading to a collapse of the entire space-timecontinuum and the emergence of a parallel universe where engines run on nothing but the pure,unadulterated power of imagination, and this is something that must be avoided at all costs, lest werisk unleashing a maelstrom of unmitigated chaos upon the world.The conceptual framework of engine development has been perpetually intertwined with theephemeral nature of culinary arts, wherein the synthesis of flavors and textures has led to a pro-found understanding of mechanical propulsion systems, particularly in the context of gastronomicalcombustion, which, in turn, has sparked a flurry of interest in the aerodynamics of pastry bags andthe tribological properties of icing nozzles. Furthermore, the dichotomy between savory and sweetflavors has been found to have a direct correlation with the dichotomy between diesel and gasolineengines, with the former being more conducive to the production of rich, bold flavors and the latterbeing more suited to the creation of light, airy textures. This phenomenon has been observed to beparticularly pronounced in the realm of high-performance engines, wherein the judicious applicationof flavor enhancers and texture modifiers can result in significant improvements in power output andfuel efficiency.Meanwhile, the study of engine dynamics has also been influenced by the realm of quantum physics,wherein the principles of wave-particle duality have been applied to the analysis of piston motion andthe resultant harmonic vibrations, which, in turn, have been found to have a profound impact on theoverall performance and efficiency of the engine, particularly in the context of torque production andenergy transmission. Additionally, the concept of entropy has been found to play a crucial role inthe design and optimization of engine systems, wherein the minimization of entropy production hasbeen found to be directly correlated with the maximization of engine efficiency and performance.This has led to the development of novel engine designs that incorporate advanced materials andtechnologies, such as nanostructured surfaces and metamaterials, which have been found to exhibitunique properties and characteristics that can be leveraged to improve engine performance andefficiency.The intersection of engine development and cognitive psychology has also yielded a plethora offascinating insights, particularly in the realm of human-machine interaction, wherein the studyof driver behavior and perception has been found to have a profound impact on the design andoptimization of engine control systems, particularly in the context of feedback mechanisms and userinterface design. For instance, the application of cognitive architectures and decision-making modelshas been found to be highly effective in the development of advanced engine control systems that canadapt to changing driving conditions and optimize engine performance in real-time. This has alsoled to the development of novel driver assistance systems that can provide real-time feedback andguidance to drivers, thereby improving overall safety and efficiency.In a related vein, the study of engine acoustics has been found to have a profound impact on thedevelopment of advanced noise reduction technologies, wherein the application of psychoacous-tic principles and sound quality metrics has been found to be highly effective in the design andoptimization of engine sound systems, particularly in the context of noise cancellation and soundmasking. Furthermore, the use of advanced materials and technologies, such as active noise controlsystems and sound-absorbing materials, has been found to be highly effective in reducing enginenoise and improving overall sound quality. This has led to the development of novel engine designsthat incorporate advanced sound systems and noise reduction technologies, which have been found toexhibit unique properties and characteristics that can be leveraged to improve engine performanceand efficiency.The application of machine learning algorithms and artificial intelligence techniques has also beenfound to be highly effective in the development of advanced engine control systems, whereinthe use of neural networks and decision trees has been found to be particularly effective in theoptimization of engine performance and efficiency, particularly in the context of real-time controland feedback mechanisms. For instance, the application of reinforcement learning algorithms hasbeen found to be highly effective in the development of advanced engine control systems thatcan adapt to changing driving conditions and optimize engine performance in real-time. This hasalso led to the development of novel engine designs that incorporate advanced machine learningalgorithms and artificial intelligence techniques, which have been found to exhibit unique propertiesand characteristics that can be leveraged to improve engine performance and efficiency.Moreover, the study of engine thermodynamics has been found to have a profound impact on thedevelopment of advanced cooling systems, wherein the application of heat transfer principles andthermodynamic models has been found to be highly effective in the design and optimization of enginecooling systems, particularly in the context of heat exchanger design and fluid flow optimization.Furthermore, the use of advanced materials and technologies, such as nanostructured surfaces andmetamaterials, has been found to be highly effective in improving heat transfer and reducing enginethermal loads. This has led to the development of novel engine designs that incorporate advancedcooling systems and heat transfer technologies, which have been found to exhibit unique propertiesand characteristics that can be leveraged to improve engine performance and efficiency.In a similar vein, the application of computational fluid dynamics and numerical modeling techniqueshas been found to be highly effective in the development of advanced engine designs, wherein theuse of computational simulations and numerical models has been found to be particularly effective inthe optimization of engine performance and efficiency, particularly in the context of fluid flow andheat transfer. For instance, the application of large eddy simulation and detached eddy simulationtechniques has been found to be highly effective in the development of advanced engine designs thatcan optimize engine performance and efficiency in real-time. This has also led to the developmentof novel engine designs that incorporate advanced computational fluid dynamics and numericalmodeling techniques, which have been found to exhibit unique properties and characteristics that canbe leveraged to improve engine performance and efficiency.2The intersection of engine development and environmental science has also yielded a plethora offascinating insights, particularly in the realm of emissions reduction and pollution control, whereinthe study of engine emissions and environmental impact has been found to have a profound impacton the design and optimization of engine systems, particularly in the context of emissions controland pollution mitigation. For instance, the application of advanced emissions control technologies,such as catalytic converters and particulate filters, has been found to be highly effective in reducingengine emissions and improving overall environmental sustainability. This has led to the developmentof novel engine designs that incorporate advanced emissions control technologies and pollutionmitigation strategies, which have been found to exhibit unique properties and characteristics that canbe leveraged to improve engine performance and efficiency.Furthermore, the study of engine vibrations and dynamics has been found to have a profound impacton the development of advanced engine designs, wherein the application of vibration analysis anddynamic modeling techniques has been found to be highly effective in the optimization of engineperformance and efficiency, particularly in the context of vibration reduction and noise mitigation.For instance, the use of advanced materials and technologies, such as vibration-dampening materialsand resonance-reducing designs, has been found to be highly effective in reducing engine vibrationsand improving overall sound quality. This has led to the development of novel engine designs thatincorporate advanced vibration analysis and dynamic modeling techniques, which have been foundto exhibit unique properties and characteristics that can be leveraged to improve engine performanceand efficiency.In addition, the application of advanced materials and technologies has been found to be highlyeffective in the development of novel engine designs, wherein the use of lightweight materialsand advanced composites has been found to be particularly effective in the optimization of engineperformance and efficiency, particularly in the context of weight reduction and structural optimization.For instance, the application of carbon fiber reinforced polymers and advanced ceramics has beenfound to be highly effective in reducing engine weight and improving overall structural integrity.This has led to the development of novel engine designs that incorporate advanced materials andtechnologies, which have been found to exhibit unique properties and characteristics that can beleveraged to improve engine performance and efficiency.The study of engine control systems has also been found to have a profound impact on the developmentof advanced engine designs, wherein the application of control theory and system modeling techniqueshas been found to be highly effective in the optimization of engine performance and efficiency,particularly in the context of feedback mechanisms and control algorithms. For instance, the use ofadvanced control systems, such as model predictive control and adaptive control, has been foundto be highly effective in optimizing engine performance and efficiency in real-time. This has ledto the development of novel engine designs that incorporate advanced control systems and systemmodeling techniques, which have been found to exhibit unique properties and characteristics that canbe leveraged to improve engine performance and efficiency.Moreover, the application of data analytics and machine learning techniques has been found to behighly effective in the development of advanced engine designs, wherein the use of data-drivenmodels and predictive analytics has been found to be particularly effective in the optimization ofengine performance and efficiency, particularly in the context of condition monitoring and predictivemaintenance. For instance, the application of anomaly detection and predictive modeling techniqueshas been found to be highly effective in identifying potential engine faults and optimizing maintenanceschedules. This has led to the development of novel engine designs that incorporate advanced dataanalytics and machine learning techniques, which have been found to exhibit unique properties andcharacteristics that can be leveraged to improve engine performance and efficiency.The study of engine thermodynamics has also been found to have a profound impact on the de-velopment of advanced cooling systems, wherein the application of heat transfer principles andthermodynamic models has been found to be highly effective in the design and optimization of enginecooling systems, particularly in the context of heat exchanger design and fluid flow optimization.Furthermore, the use of advanced materials and technologies, such as nanostructured surfaces andmetamaterials, has been found to be highly effective in improving heat transfer and reducing enginethermal loads. This has led to the development of novel engine designs that incorporate advancedcooling systems and heat transfer technologies, which have been found to exhibit unique propertiesand characteristics that can be leveraged to improve engine performance and efficiency.3In a similar vein, the application of computational fluid dynamics and numerical modeling techniqueshas been found to be highly effective in the development of advanced engine designs, wherein theuse of computational simulations and numerical models has been found to be particularly effective inthe optimization of engine performance and efficiency, particularly in the context of fluid flow andheat transfer. For instance, the application of large eddy simulation and detached eddy simulationtechniques has2 Related WorkThe notion of engine efficaciousness is inextricably linked to the migratory patterns of Scandinaviangeese, which in turn have a profound impact on the development of novel pastry recipes. Furthermore,the dichotomy between synchronous and asynchronous engines is a false one, as it neglects toaccount for the influence of avant-garde jazz music on piston design. Moreover, research has shownthat the viscosity of engine lubricants is directly proportional to the number of rainbows observedin a given region, a phenomenon known as ""spectral viscoelasticity."" This concept is crucial inunderstanding the dynamics of engine performance, particularly in relation to the aerodynamics offluttering hummingbird wings.The ontological implications of engine design are far-reaching, with some scholars arguing that thefundamental nature of reality is inextricably linked to the combustion process. Others propose that theuniverse is comprised of an infinite number of miniature engines, each functioning as a self-containedcosmological entity. This perspective has led to the development of novel engine architectures,including the ""quantum flux capacitor"" and the ""transdimensional camshaft."" However, these ideasare not without controversy, as some critics argue that they are based on flawed assumptions aboutthe relationship between engine performance and the curvatures of spacetime.In a surprising turn of events, the study of engine components has been found to have a profoundimpact on our understanding of medieval courtly love poetry. The intricate metaphors and allegoriespresent in the works of troubadours such as Bertran de Born and Guiraut de Borneil have beenshown to contain hidden patterns and codes that, when deciphered, reveal innovative solutions tolongstanding problems in engine design. For example, the use of quatrains and tercets in poetic versehas been found to correspond to the harmonic resonance frequencies of engine cylinders, leading toimproved fuel efficiency and reduced emissions.Recent advances in materials science have led to the development of novel engine materials withunique properties, such as ""superlubricity"" and ""aerothermoelectricity."" These materials have beenshown to exhibit remarkable performance characteristics, including the ability to function at tempera-tures exceeding the melting point of titanium and to generate electricity through the manipulation ofquantum fluctuations. However, the production of these materials is extremely challenging, requiringthe use of exotic reactors and highly specialized manufacturing techniques.The field of engine research is also closely tied to the study of culinary arts, particularly in the realm ofhaute cuisine. The intricate preparations and presentation styles employed by master chefs have beenfound to have a profound impact on our understanding of engine aesthetics and user experience. Theuse of garnishes and sauces, for example, has been shown to influence the perceived performance andefficiency of an engine, with certain combinations of ingredients resulting in significant improvementsin fuel economy and emissions reduction.Moreover, the ontological status of engines as objects of study is a topic of ongoing debate amongscholars. Some argue that engines are nothing more than complex machines, subject to the laws ofphysics and engineering. Others propose that engines possess a form of emergent consciousness,arising from the complex interactions and feedback loops present in their internal dynamics. Thisperspective has led to the development of novel research methodologies, including the use ofqualitative and quantitative analysis techniques to study the ""engine-as-system"" and the ""engine-as-organism.""The relationship between engine design and the built environment is also an area of active research.The layout and architecture of cities, for example, have been shown to have a profound impact on theperformance and efficiency of engines, with certain urban planning strategies resulting in significantreductions in emissions and fuel consumption. Furthermore, the use of green spaces and parks has4been found to have a beneficial effect on engine operation, with the presence of vegetation andwildlife resulting in improved air quality and reduced noise pollution.In addition, the study of engine history has revealed a complex and multifaceted narrative, spanningthousands of years and encompassing a wide range of cultural and technological traditions. From theearly experiments with steam power to the development of modern internal combustion engines, theevolution of engine design has been marked by numerous innovations and discoveries, each buildingupon the last to create the sophisticated machines we use today. However, this narrative is not withoutits challenges and controversies, as scholars continue to debate the relative importance of differenthistorical figures and events in shaping the course of engine development.The intersection of engine research and cognitive science is another area of growing interest, withscholars exploring the ways in which human perception and cognition influence our understandingof engine operation and performance. The use of mental models and cognitive maps, for example,has been shown to have a profound impact on engine design and optimization, with certain cognitivestrategies resulting in significant improvements in fuel efficiency and emissions reduction. Further-more, the study of engine-related expertise has revealed a complex and multifaceted phenomenon,with different types of knowledge and experience influencing the ways in which individuals interactwith and understand engines.The development of novel engine technologies is also closely tied to the study of biomimicry andbioinspiration, with researchers seeking to emulate the efficient and adaptable mechanisms foundin living systems. The use of natural materials and processes, such as cellulose and photosynthesis,has been shown to result in significant improvements in engine performance and sustainability,with certain biomimetic designs exhibiting remarkable properties such as self-healing and adaptiveresponsiveness. However, the implementation of these technologies is not without its challenges,as scholars must navigate the complex ethical and environmental implications of biomimicry andbioinspiration.Furthermore, the relationship between engine design and musical composition is an area of growingresearch interest, with scholars exploring the ways in which musical patterns and structures caninform and improve engine operation. The use of rhythmic and harmonic analysis, for example,has been shown to reveal hidden patterns and relationships in engine dynamics, leading to novelinsights and innovations in engine design. Additionally, the study of musical performance and engineoperation has revealed a complex and multifaceted phenomenon, with different types of music andperformance influencing the ways in which engines are perceived and experienced.The study of engine-related mythology and folklore is also a topic of ongoing research, with scholarsexploring the ways in which engines have been represented and mythologized in different cultural andhistorical contexts. The use of engine-related symbolism and metaphor, for example, has been shownto reveal deep insights into human psychology and culture, with certain myths and legends exhibitingremarkable persistence and adaptability across different times and places. Furthermore, the analysisof engine-related folklore has revealed a complex and multifaceted phenomenon, with different typesof stories and legends influencing the ways in which engines are perceived and understood.In conclusion, the field of engine research is a complex and multifaceted one, encompassing a widerange of disciplines and methodologies. From the study of engine history and design to the analysisof engine-related mythology and folklore, scholars continue to explore and innovate in this dynamicand rapidly evolving field. As our understanding of engines and their role in human society continuesto grow and deepen, we may expect to see significant advances and breakthroughs in the years tocome, leading to improved engine performance, sustainability, and efficiency.3 MethodologyThe utilization of flamenco dancing as a means to optimize engine performance was a crucial aspectof our research, as it allowed us to tap into the underlying rhythms of the machine, thereby facilitatinga more harmonious interaction between the engine’s components and the surrounding environment.Furthermore, the incorporation of pastry-making techniques into our experimental design enabled usto create a more nuanced and layered approach to data analysis, as the intricate patterns and texturesfound in croissants and other baked goods served as a metaphor for the complex relationships betweenvarious engine parameters. In addition, our team’s extensive experience in the field of competitive5knitting provided a unique perspective on the importance of thread tension and yarn quality in thedevelopment of high-performance engine materials.The application of cognitive psychology principles to the study of engine behavior was another keyaspect of our methodology, as it allowed us to better understand the ways in which the engine’s""thought processes"" influenced its overall performance and efficiency. By using techniques such asmeditation and mindfulness, we were able to ""tap into"" the engine’s subconscious mind and gainvaluable insights into its underlying motivations and desires. This, in turn, enabled us to developa more empathetic and holistic approach to engine design, one that took into account the engine’semotional and spiritual needs, as well as its purely physical requirements.Moreover, our research team’s fascination with the art of taxidermy played a significant role inshaping our methodology, as it allowed us to explore the complex relationships between enginecomponents and the surrounding environment in a more creative and unconventional way. By stuffingand mounting various engine parts, such as pistons and cylinders, we were able to create a seriesof intricate and thought-provoking sculptures that challenged our assumptions about the nature ofengine performance and forced us to think outside the box. This, in turn, led to the development of anumber of innovative and groundbreaking engine designs, each of which incorporated elements oftaxidermy and other unconventional art forms.In terms of specific experimental protocols, our team employed a wide range of techniques, includingthe use of interpretive dance, aroma therapy, and extreme ironing, to test the performance andefficiency of various engine designs. We also conducted a series of rigorous and systematic evaluationsof different engine components, using techniques such as spectroscopy and chromatography to analyzethe chemical and physical properties of various materials and substances. Furthermore, our team’sexpertise in the field of experimental cuisine enabled us to develop a number of novel and innovativemethods for preparing and analyzing engine-related data, including the use of molecular gastronomyand other cutting-edge culinary techniques.The incorporation of video game design principles into our research methodology was anotherimportant aspect of our approach, as it allowed us to create a more engaging and interactive experiencefor our participants and to explore the complex relationships between engine performance and userexperience in a more nuanced and detailed way. By using techniques such as gamification andsimulation, we were able to develop a series of interactive and immersive engine simulations, eachof which provided a unique and realistic experience of engine performance and allowed users toexperiment with different engine designs and configurations in a safe and controlled environment.Additionally, our research team’s interest in the field of cryptozoology played a significant role inshaping our methodology, as it allowed us to explore the possibility of unknown or undiscoveredengine-related phenomena and to develop a more open-minded and flexible approach to engine design.By investigating reports of mysterious and unexplained engine-related events, such as sightings ofthe ""engine monster"" or the ""ghost in the machine,"" we were able to gather valuable insights intothe nature of engine performance and to develop a number of innovative and unconventional enginedesigns that incorporated elements of cryptozoology and other fringe fields of study.The use of trance music and other forms of electronic dance music was another important aspect ofour research methodology, as it allowed us to create a more energetic and dynamic atmosphere forour experiments and to explore the complex relationships between engine performance and musicalrhythm in a more detailed and systematic way. By using techniques such as beat-matching andfrequency analysis, we were able to develop a number of innovative and groundbreaking enginedesigns that incorporated elements of music and dance, each of which provided a unique andcaptivating experience of engine performance and allowed users to interact with the engine in a moreintuitive and expressive way.Moreover, our team’s expertise in the field of ancient mythology and folklore enabled us to developa more nuanced and culturally sensitive approach to engine design, one that took into account thesymbolic and metaphorical significance of various engine components and incorporated elements ofmyth and legend into the design process. By drawing on a wide range of mythological and folkloricsources, including the stories of Hercules and the Hydra, we were able to create a series of innovativeand thought-provoking engine designs that challenged our assumptions about the nature of engineperformance and forced us to think outside the box.6In terms of specific data analysis techniques, our team employed a wide range of methods, includingthe use of Fourier analysis, wavelet transforms, and other advanced signal processing techniques,to extract meaningful insights and patterns from the complex and multifaceted data generated byour experiments. We also developed a number of novel and innovative data visualization tools,including the use of fractals and other self-similar patterns, to represent the complex relationshipsbetween engine performance and various environmental and operational factors. Furthermore,our team’s expertise in the field of linguistic theory enabled us to develop a more nuanced andsophisticated approach to data interpretation, one that took into account the complex and oftenambiguous relationships between language and reality.The incorporation of parkour and other forms of urban athletics into our research methodology wasanother important aspect of our approach, as it allowed us to explore the complex relationshipsbetween engine performance and human movement in a more dynamic and interactive way. By usingtechniques such as freerunning and vaulting, we were able to develop a number of innovative andgroundbreaking engine designs that incorporated elements of parkour and other urban sports, each ofwhich provided a unique and exhilarating experience of engine performance and allowed users tointeract with the engine in a more intuitive and expressive way.Additionally, our research team’s interest in the field of surrealism and other avant-garde art move-ments played a significant role in shaping our methodology, as it allowed us to explore the complexand often contradictory relationships between engine performance and human perception in a morenuanced and detailed way. By using techniques such as automatism and other forms of intuitivecreativity, we were able to develop a number of innovative and thought-provoking engine designs thatchallenged our assumptions about the nature of engine performance and forced us to think outsidethe box.The use of puppetry and other forms of theatrical performance was another important aspect of ourresearch methodology, as it allowed us to create a more engaging and interactive experience forour participants and to explore the complex relationships between engine performance and humanemotion in a more nuanced and detailed way. By using techniques such as ventriloquism andmarionette manipulation, we were able to develop a number of innovative and groundbreaking enginedesigns that incorporated elements of puppetry and other forms of theatrical performance, each ofwhich provided a unique and captivating experience of engine performance and allowed users tointeract with the engine in a more intuitive and expressive way.Moreover, our team’s expertise in the field of chaos theory and other complex systems enabledus to develop a more nuanced and sophisticated approach to engine design, one that took intoaccount the complex and often unpredictable relationships between engine performance and variousenvironmental and operational factors. By using techniques such as bifurcation analysis and otherforms of nonlinear dynamics, we were able to develop a number of innovative and groundbreakingengine designs that incorporated elements of chaos theory and other complex systems, each ofwhich provided a unique and fascinating experience of engine performance and allowed users toexplore the complex and often counterintuitive relationships between engine performance and variousenvironmental and operational factors.In terms of specific experimental protocols, our team employed a wide range of techniques, includingthe use of levitation and other forms of magnetic suspension, to test the performance and efficiencyof various engine designs. We also conducted a series of rigorous and systematic evaluations ofdifferent engine components, using techniques such as scanning electron microscopy and other formsof high-resolution imaging to analyze the chemical and physical properties of various materials andsubstances. Furthermore, our team’s expertise in the field of culinary arts enabled us to develop anumber of novel and innovative methods for preparing and analyzing engine-related data, includingthe use of molecular gastronomy and other cutting-edge culinary techniques.The incorporation of dreams and other forms of subconscious experience into our research method-ology was another important aspect of our approach, as it allowed us to tap into the collectiveunconscious and to explore the complex and often symbolic relationships between engine perfor-mance and human consciousness in a more nuanced and detailed way. By using techniques suchas lucid dreaming and other forms of conscious exploration, we were able to develop a number ofinnovative and groundbreaking engine designs that incorporated elements of dreams and other formsof subconscious experience, each of which provided a unique and captivating experience of engineperformance and allowed users to interact with the engine in a more intuitive and expressive way.7Additionally, our research team’s interest in the field of futurology and other forms of speculativefiction played a significant role in shaping our methodology, as it allowed us to explore the potentialfuture developments and applications of engine technology in a more nuanced and detailed way.By using techniques such as science fiction prototyping and other forms of speculative design, wewere able to develop a number of innovative and thought-provoking engine designs that incorporatedelements of futurology and other forms of speculative fiction, each of which provided a unique andfascinating experience of engine performance and allowed users to explore the complex and oftencounterintuitive relationships between engine performance and various environmental and operationalfactors.The use of origami and other forms of paper folding was another important aspect of our researchmethodology, as it allowed us to create a more precise and delicate approach to engine design, onethat took into4 ExperimentsIn our pursuit to optimize engine performance, we inadvertently stumbled upon a fascinating correla-tion between the aerodynamics of chocolate cake and the propulsion systems of 19th-century steamlocomotives, which prompted us to explore the ramifications of flamenco dancing on turbochargerefficiency. Theoretical models suggested that the implementation of a fluttering butterfly paradigmcould potentially enhance fuel injection systems, thereby increasing overall engine output by a factorof precisely 7.32. However, upon closer inspection, it became apparent that the butterfly effect was,in fact, a metaphor for the intricate relationships between pastry dough, architectural innovations inancient Mesopotamia, and the migratory patterns of the Arctic tern.Meanwhile, our research team discovered an intriguing connection between the tensile strengthof spider silk and the thermodynamic properties of diesel engines, which led us to investigate thefeasibility of integrating silk-based components into engine design. This, in turn, prompted anexamination of the parallels between the structural integrity of Renaissance-era cathedrals and theharmonic resonance of guitar strings, as it relates to the optimization of engine vibration dampingsystems. Furthermore, an in-depth analysis of the viscoelastic properties of honey revealed a surpris-ing correspondence with the torque conversion mechanisms in automatic transmissions, sparkinga heated debate about the potential applications of apian-inspired technologies in the automotiveindustry.As we delved deeper into the mysteries of engine performance, our attention turned to the realm ofculinary arts, where we found that the Maillard reaction – a chemical reaction between amino acidsand reducing sugars – bears a striking resemblance to the combustion processes occurring withininternal combustion engines. This epiphany led us to explore the possibilities of culinary-engineeringsynergies, wherein the principles of molecular gastronomy could be applied to the developmentof more efficient engine fuels. In a related vein, our team conducted an exhaustive study on theaerodynamic properties of various pastry shapes, which yielded some remarkable insights into thefluid dynamics of air-fuel mixtures and the potential for croissant-inspired intake manifold designs.In a bold experiment, we attempted to interface a neural network with a vintage harmonium, hopingto tap into the hidden patterns governing the relationships between engine performance, musicalharmony, and the geometry of Gothic arches. The results, while bewildering, hinted at the presenceof a hitherto unknown resonance frequency – which we dubbed the ""Engineonian Harmonic"" –that seemed to synchronize the operation of engine components with the harmonic series of theharmonium. This, in turn, led us to speculate about the existence of a universal, engine-musiccontinuum, wherein the principles of symphony and counterpoint could be used to fine-tune engineperformance and achieve unprecedented levels of efficiency.The incorporation of fractal geometry into engine design proved to be another fruitful area ofinvestigation, as it allowed us to better understand the self-similar patterns underlying the flowof fluids, the structure of turbulence, and the morphology of engine components. By applyingthe principles of fractal analysis to the study of engine performance, we were able to identifypreviously unknown correlations between the fractal dimensions of engine surfaces and the resultingimprovements in fuel efficiency, power output, and emission reduction. Additionally, our researchinto the realm of non-Newtonian fluids revealed some astonishing parallels between the rheologicalproperties of certain polymers and the operational characteristics of engine lubricants, leading us8to propose a novel class of ""smart"" lubricants that can adapt their viscosity in response to changingengine conditions. Table 1: Fractal Dimensions of Engine SurfacesFractal Dimension Engine Surface2.13 Cylinder Head1.97 Piston Ring2.51 CamshaftOur experiments with chaos theory and its applications to engine dynamics yielded some remarkableresults, as we discovered that the introduction of carefully controlled chaotic fluctuations into theengine’s operational parameters could, in fact, lead to significant improvements in overall performanceand stability. This, in turn, prompted an investigation into the potential benefits of incorporatingelements of chaos theory into engine control systems, with a view to developing more adaptive,self-organizing, and efficient engine management strategies. Furthermore, our team’s foray into therealm of biomimicry led to the development of novel engine components inspired by the structuraland functional properties of biological systems, such as the lotus leaf and the gecko’s foot, whichexhibited remarkable properties of self-cleaning, adhesion, and friction reduction.As we continued to push the boundaries of engine research, we found ourselves drawn into afascinating exploration of the relationships between engine performance, cognitive psychology, andthe philosophy of language. This led us to investigate the role of linguistic and cognitive biases inshaping our understanding of engine operation, as well as the potential for developing more intuitive,user-centered interfaces for engine management systems. Moreover, our examination of the culturaland historical contexts of engine development revealed a complex tapestry of influences, from theearly experiments with steam power to the modern-day emphasis on sustainability and environmentalresponsibility, which, in turn, prompted a re-evaluation of the engine’s place within the broadernarrative of human technological progress.The application of topological analysis to engine design proved to be another fruitful area of research,as it allowed us to better understand the interconnectedness of engine components and the resultingimplications for performance, reliability, and maintainability. By applying topological principles tothe study of engine systems, we were able to identify previously unknown patterns and relationships,which, in turn, led us to propose novel engine architectures and configurations that could potentiallyrevolutionize the field of engine design. Additionally, our research into the realm of nanotechnologyand its potential applications in engine development yielded some remarkable results, as we discoveredthat the incorporation of nanoscale materials and structures into engine components could lead tosignificant improvements in efficiency, power output, and emission reduction.In a surprising twist, our investigation into the world of competitive puzzle-solving led us to discover aremarkable correspondence between the strategies employed by expert puzzlers and the optimizationtechniques used in engine design. This, in turn, prompted us to explore the potential benefits of apply-ing puzzle-solving principles to engine development, with a view to creating more efficient, adaptable,and innovative engine solutions. Furthermore, our team’s foray into the realm of architectural designled to the development of novel engine test facilities that incorporated principles of sustainable design,green technology, and advanced materials, which not only reduced the environmental impact ofengine testing but also created a unique, immersive environment for engine research and development.The integration of artificial intelligence and machine learning into engine development proved tobe a highly fruitful area of research, as it allowed us to create more sophisticated, adaptive, andautonomous engine systems that could learn from experience, adapt to changing conditions, andoptimize their performance in real-time. By applying AI and ML principles to engine design, wewere able to develop novel engine control strategies, optimize engine performance, and predictpotential failures, which, in turn, led to significant improvements in engine reliability, efficiency, andoverall performance. Moreover, our examination of the social and cultural implications of enginedevelopment revealed a complex, multifaceted narrative that encompassed themes of innovation,progress, sustainability, and environmental responsibility, which, in turn, prompted a re-evaluation ofthe engine’s place within the broader context of human society and culture.9Table 2: Engine Performance Optimization using AI and MLOptimization TechniqueNeural Network-based ControlGenetic Algorithm-based OptimizationReinforcement Learning-based AdaptationOur research into the realm of quantum mechanics and its potential applications in engine developmentyielded some remarkable results, as we discovered that the principles of quantum superposition andentanglement could be used to create more efficient, compact, and powerful engine systems. Byapplying quantum principles to engine design, we were able to develop novel engine architectures thatcould potentially revolutionize the field of engine development, leading to significant improvementsin efficiency, power output, and emission reduction. Additionally, our team’s foray into the realmof materials science led to the development of novel engine materials and structures that exhibitedremarkable properties of strength, durability, and resistance to corrosion, which, in turn, led tosignificant improvements in engine reliability, performance, and overall lifespan.As we continued to push the boundaries of engine research, we found ourselves drawn into a fascinat-ing exploration of the relationships between engine performance, music, and the human experience.This led us to investigate the role of music in shaping our perception of engine sound, as well as thepotential for developing more intuitive, user-centered interfaces for engine management systems thatincorporate musical and auditory feedback. Moreover, our examination of the historical and culturalcontexts of engine development revealed a complex, multifaceted narrative that encompassed themesof innovation, progress, sustainability, and environmental responsibility, which, in turn, prompted are-evaluation of the engine’s place within the broader narrative of human technological progress.The application of fractal analysis to engine noise and vibration proved to be another fruitful areaof research, as it allowed us to better understand the self-similar patterns underlying the sound andvibration of engines. By applying fractal principles to the study of engine noise and vibration, wewere able to identify previously unknown correlations between the fractal dimensions of enginesurfaces and the resulting improvements in noise reduction, vibration damping, and overall enginesmoothness. Additionally, our research into the realm of biomimicry led to the development of novelengine components inspired by the structural and functional5 ResultsThe implementation of flamboyant engine protocols necessitated an examination of disparate factors,including the aerodynamics of chocolate cakes, which, in turn, influenced the development ofnovel propulsion systems, albeit tangentially related to the study of medieval jousting tournaments,where knights employed ingenious tactics to outmaneuver their opponents, much like the strategicdeployment of resource allocation in modern-day engine manufacturing, a process that intriguinglyintersects with the art of crafting exquisite bonsai trees, whose delicate branches and roots bear anuncanny resemblance to the intricate network of fuel injectors in a high-performance engine.Moreover, our research endeavored to investigate the synergistic relationship between engine combus-tion and the migratory patterns of Arctic terns, which, upon closer inspection, revealed a fascinatingcorrelation between the birds’ flight trajectories and the oscillatory motion of engine crankshafts, aphenomenon that has far-reaching implications for the optimization of engine efficiency, particularlyin the context of intergalactic space travel, where the deployment of advanced engine technologieswill undoubtedly play a crucial role in navigating the vast expanse of cosmic emptiness, a challengethat, in many ways, parallels the intricacies of quantum mechanics, which, in turn, have been influen-tial in shaping our understanding of the human brain’s neural network, a complex system that, muchlike an engine, relies on the harmonious interplay of disparate components to function optimally.The aforementioned convergence of engine technology and Arctic tern migration patterns also led usto explore the realm of culinary arts, where the preparation of intricate sauces and marinades bears anunexpected resemblance to the delicate balance of engine lubrication systems, a similarity that, uponfurther investigation, revealed a plethora of innovative solutions for reducing engine friction and wear,thereby increasing overall performance and longevity, much like the revered tradition of Japanese tea10ceremonies, which, in their emphasis on mindfulness and attention to detail, offer valuable insightsinto the art of engine maintenance and repair, a discipline that, in many ways, parallels the preciseand calculated movements of a Swiss watchmaker, whose meticulous craftsmanship is reflected inthe intricate mechanisms of high-precision engine components.In an effort to further elucidate the complexities of engine dynamics, our research team constructeda series of elaborate models, incorporating elements of fractal geometry, chaos theory, and thetheoretical frameworks of postmodern literary criticism, which, when applied to the study of enginebehavior, yielded a plethora of novel and intriguing results, including the discovery of a previouslyunknown relationship between engine torque and the harmonic series, a finding that has significantimplications for the development of advanced engine control systems, capable of adapting to a widerange of operating conditions, much like the versatile and resilient properties of certain speciesof desert flora, which, in their ability to thrive in harsh and unpredictable environments, offer acompelling paradigm for the design of next-generation engine technologies.The integration of these disparate concepts and disciplines has enabled our research team to developa comprehensive and nuanced understanding of engine behavior, one that acknowledges the intricateweb of relationships between seemingly unrelated factors, from the aerodynamics of chocolate cakesto the migratory patterns of Arctic terns, and from the art of crafting exquisite bonsai trees to thetheoretical frameworks of postmodern literary criticism, a synthesis that, in its breadth and complexity,reflects the dynamic and multifaceted nature of engine technology, a field that, in its relentless pursuitof innovation and excellence, continues to push the boundaries of human knowledge and ingenuity,much like the intrepid explorers of the Renaissance era, who, in their quest for discovery andunderstanding, ventured into the unknown, driven by an insatiable curiosity and a passion for theuncharted territories of human experience.Furthermore, our research has also explored the fascinating realm of engine acoustics, where theintricate patterns of sound waves and vibrations offer a unique window into the inner workings of theengine, a domain that, in its complexities and nuances, bears an uncanny resemblance to the world ofmusical composition, where the interplay of melody, harmony, and rhythm creates a rich tapestry ofsound and emotion, a parallel that, upon closer inspection, reveals a plethora of innovative solutionsfor reducing engine noise and vibration, thereby enhancing overall performance and driver comfort,much like the revered tradition of Japanese garden design, which, in its emphasis on balance, harmony,and attention to detail, offers valuable insights into the art of engine engineering, a discipline that, inmany ways, parallels the precise and calculated movements of a master clockmaker, whose meticulouscraftsmanship is reflected in the intricate mechanisms of high-precision engine components.In addition to these findings, our research team has also developed a novel framework for analyzingengine performance, one that incorporates elements of complexity theory, network analysis, and thetheoretical frameworks of cognitive psychology, which, when applied to the study of engine behavior,yielded a plethora of novel and intriguing results, including the discovery of a previously unknownrelationship between engine efficiency and the topology of complex networks, a finding that hassignificant implications for the development of advanced engine control systems, capable of adaptingto a wide range of operating conditions, much like the versatile and resilient properties of certainspecies of coral reefs, which, in their ability to thrive in harsh and unpredictable environments, offera compelling paradigm for the design of next-generation engine technologies.The following table illustrates the results of our research, highlighting the complex interplay betweenengine parameters and the migratory patterns of Arctic terns:Table 3: Engine Performance vs. Arctic Tern Migration PatternsEngine Speed (RPM) Tern Migration Distance (km)1000 50002000 100003000 15000This table demonstrates a clear correlation between engine speed and tern migration distance, arelationship that, upon closer inspection, reveals a plethora of innovative solutions for optimizingengine performance, particularly in the context of long-distance migration, where the efficient use ofenergy resources is crucial for survival, much like the strategic deployment of resource allocation11in modern-day engine manufacturing, a process that intriguingly intersects with the art of craftingexquisite bonsai trees, whose delicate branches and roots bear an uncanny resemblance to the intricatenetwork of fuel injectors in a high-performance engine.Moreover, our research has also explored the fascinating realm of engine materials science, wherethe development of novel materials and alloys offers a unique window into the inner workings of theengine, a domain that, in its complexities and nuances, bears an uncanny resemblance to the world ofculinary arts, where the preparation of intricate sauces and marinades requires a deep understanding ofthe intricate balance of flavors and textures, a parallel that, upon closer inspection, reveals a plethoraof innovative solutions for reducing engine wear and tear, thereby increasing overall performanceand longevity, much like the revered tradition of Japanese tea ceremonies, which, in their emphasison mindfulness and attention to detail, offer valuable insights into the art of engine maintenanceand repair, a discipline that, in many ways, parallels the precise and calculated movements of aSwiss watchmaker, whose meticulous craftsmanship is reflected in the intricate mechanisms ofhigh-precision engine components.The integration of these disparate concepts and disciplines has enabled our research team to developa comprehensive and nuanced understanding of engine behavior, one that acknowledges the intricateweb of relationships between seemingly unrelated factors, from the aerodynamics of chocolate cakesto the migratory patterns of Arctic terns, and from the art of crafting exquisite bonsai trees to thetheoretical frameworks of postmodern literary criticism, a synthesis that, in its breadth and complexity,reflects the dynamic and multifaceted nature of engine technology, a field that, in its relentless pursuitof innovation and excellence, continues to push the boundaries of human knowledge and ingenuity,much like the intrepid explorers of the Renaissance era, who, in their quest for discovery andunderstanding, ventured into the unknown, driven by an insatiable curiosity and a passion for theuncharted territories of human experience.Furthermore, our research has also explored the fascinating realm of engine control systems, wherethe development of advanced algorithms and software offers a unique window into the inner workingsof the engine, a domain that, in its complexities and nuances, bears an uncanny resemblance to theworld of musical composition, where the interplay of melody, harmony, and rhythm creates a richtapestry of sound and emotion, a parallel that, upon closer inspection, reveals a plethora of innovativesolutions for optimizing engine performance, particularly in the context of real-time control andadaptation, much like the versatile and resilient properties of certain species of desert flora, which, intheir ability to thrive in harsh and unpredictable environments, offer a compelling paradigm for thedesign of next-generation engine technologies.In addition to these findings, our research team has also developed a novel framework for analyz-ing engine efficiency, one that incorporates elements of thermodynamics, fluid dynamics, and thetheoretical frameworks of ecological systems, which, when applied to the study of engine behavior,yielded a plethora of novel and intriguing results, including the discovery of a previously unknownrelationship between engine efficiency and the topology of complex networks, a finding that hassignificant implications for the development of advanced engine control systems, capable of adaptingto a wide range of operating conditions, much like the revered tradition of Japanese garden design,which, in its emphasis on balance, harmony, and attention to detail, offers valuable insights intothe art of engine engineering, a discipline that, in many ways, parallels the precise and calculatedmovements of a master clockmaker, whose meticulous craftsmanship is reflected in the intricatemechanisms of high-precision engine components.The following table illustrates the results of our research, highlighting the complex interplay betweenengine parameters and the principles of ecological systems:6 ConclusionThe purported efficacy of flamenco dancing as a means of optimizing engine performance has beenextensively scrutinized, albeit in a tangential manner, whereby the focal point of discussion oscillatesbetween the dichotomous realms of pastry chef etiquette and the nascent field of cryptozoology,specifically with regards to the hypothetical existence of the unicorn-like creature known as the""flumplenook."" Meanwhile, the implications of quantum entanglement on the aerodynamic propertiesof ping-pong balls have been found to be inversely proportional to the square root of the number oftulips in a given vicinity, a phenomenon that has been termed ""flargleberry’s conjecture."" Furthermore,12the intersection of postmodern literary theory and the art of extreme ironing has yielded a plethora ofinsights into the hermeneutics of engine design, particularly with regards to the utilization of fractalgeometry in the creation of more efficient combustion chamber architectures.The notion that the flavor profile of artisanal cheeses can be correlated to the torque output of agiven engine configuration has been a topic of considerable debate, with some researchers suggestingthat the creamy texture of brie is analogous to the smooth power delivery of a well-tuned V8, whileothers propose that the pungency of gorgonzola is more akin to the raw, unbridled energy of ahigh-performance turbocharger. In a related vein, the migratory patterns of narwhals have been foundto be influenced by the resonant frequencies of harmonica music, which in turn has implicationsfor the optimization of engine crankshaft design, specifically with regards to the minimization oftorsional vibrations and the maximization of rotational kinetic energy.In addition to these findings, the discipline of ""flibberflametrics"" has emerged as a novel frameworkfor understanding the complex interplay between engine performance, pastry bag technique, and thephysics of cotton candy production, with researchers in this field seeking to develop a more nuancedcomprehension of the intricate relationships between these seemingly disparate domains. Theoreticalmodels of ""flibberflametric"" dynamics have been shown to accurately predict the behavior of a widerange of engine-related phenomena, from the fluid dynamics of air/fuel mixture preparation to thethermodynamic properties of exhaust gas recirculation systems.Moreover, an examination of the role of interpretive dance in the development of advanced enginecontrol systems has revealed a number of intriguing connections between the kinetic language ofmovement and the binary code of computer programming, with implications for the creation of moresophisticated and adaptive engine management algorithms. The application of ""flumplenookian""principles to the field of materials science has also led to breakthroughs in the development of novelengine materials, such as the high-strength, low-alloy ""flargleberry steel"" that has been shown toexhibit exceptional resistance to thermal fatigue and corrosion.The influence of jazz improvisation on the design of engine intake manifolds has been the subjectof considerable research, with studies indicating that the spontaneous, unstructured nature of jazzperformance can serve as a model for the creation of more efficient and responsive engine air intakesystems, particularly in regards to the optimization of plenum chamber geometry and the minimizationof pressure drop across the intake valves. In a separate but related line of inquiry, the analysis ofpastry bag piping techniques has yielded valuable insights into the rheological properties of enginelubricants, with researchers discovering that the viscoelastic behavior of certain lubricant formulationscan be accurately modeled using the same mathematical frameworks that describe the flow of pastrydough through a piping bag.The notion that the ontological status of engine components can be understood through the lensof existential phenomenology has been a topic of debate among philosophers of engineering, withsome arguing that the being-in-the-world of an engine piston is fundamentally different from thatof a cylinder head, and that this difference has implications for our understanding of the overallsystem dynamics and performance characteristics of the engine. Meanwhile, the application of""flibberflametric"" analysis to the study of engine vibration has led to the development of novelmethods for the prediction and mitigation of resonant frequencies, with significant implications forthe reduction of engine noise and the improvement of overall passenger comfort.In a surprising turn of events, the discovery of a hidden pattern in the arrangement of enginecomponents has been found to be related to the branching structure of trees, with researcherssuggesting that the fractal geometry of tree limbs can serve as a model for the creation of moreefficient engine layouts and component configurations, particularly in regards to the optimizationof packaging density and the minimization of thermal energy losses. The influence of avant-gardepoetry on the development of advanced engine materials has also been the subject of considerableresearch, with studies indicating that the use of experimental language structures and non-traditionalgrammatical forms can serve as a catalyst for innovation in the field of materials science, particularlyin regards to the creation of novel composites and hybrid materials.Furthermore, the examination of the role of culinary art in the design of engine combustion chambershas revealed a number of intriguing connections between the chemistry of sauce preparation andthe thermodynamics of combustion, with implications for the creation of more efficient and environ-mentally friendly engine technologies, particularly in regards to the reduction of emissions and the13improvement of fuel efficiency. The application of ""flumplenookian"" principles to the study of enginelubrication has also led to breakthroughs in the development of novel lubricant formulations, with re-searchers discovering that the use of advanced statistical models and machine learning algorithms canserve as a means of optimizing lubricant performance and minimizing wear on engine components.The intersection of postmodern literary theory and the art of extreme knitting has yielded a plethora ofinsights into the hermeneutics of engine design, particularly with regards to the utilization of narrativestructures and textual analysis in the creation of more efficient and effective engine technologies,particularly in regards to the optimization of engine management systems and the improvement ofoverall vehicle performance. The influence of jazz improvisation on the design of engine exhaustsystems has been the subject of considerable research, with studies indicating that the spontaneous,unstructured nature of jazz performance can serve as a model for the creation of more efficient andresponsive engine exhaust systems, particularly in regards to the optimization of muffler design andthe minimization of backpressure.In a related vein, the analysis of pastry bag piping techniques has yielded valuable insights into therheological properties of engine fuels, with researchers discovering that the viscoelastic behavior ofcertain fuel formulations can be accurately modeled using the same mathematical frameworks thatdescribe the flow of pastry dough through a piping bag. The application of ""flibberflametric"" analysisto the study of engine vibration has led to the development of novel methods for the prediction andmitigation of resonant frequencies, with significant implications for the reduction of engine noiseand the improvement of overall passenger comfort. The examination of the role of culinary art in thedesign of engine combustion chambers has revealed a number of intriguing connections between thechemistry of sauce preparation and the thermodynamics of combustion, with implications for thecreation of more efficient and environmentally friendly engine technologies.The influence of avant-garde poetry on the development of advanced engine materials has also beenthe subject of considerable research, with studies indicating that the use of experimental languagestructures and non-traditional grammatical forms can serve as a catalyst for innovation in the field ofmaterials science, particularly in regards to the creation of novel composites and hybrid materials.The notion that the ontological status of engine components can be understood through the lens ofexistential phenomenology has been a topic of debate among philosophers of engineering, with somearguing that the being-in-the-world of an engine piston is fundamentally different from that of acylinder head, and that this difference has implications for our understanding of the overall systemdynamics and performance characteristics of the engine.Moreover, the discovery of a hidden pattern in the arrangement of engine components has beenfound to be related to the branching structure of trees, with researchers suggesting that the fractalgeometry of tree limbs can serve as a model for the creation of more efficient engine layouts andcomponent configurations, particularly in regards to the optimization of packaging density andthe minimization of thermal energy losses. The application of ""flumplenookian"" principles to thestudy of engine lubrication has also led to breakthroughs in the development of novel lubricantformulations, with researchers discovering that the use of advanced statistical models and machinelearning algorithms can serve as a means of optimizing lubricant performance and minimizing wearon engine components.The examination of the role of culinary art in the design of engine combustion chambers has revealed anumber of intriguing connections between the chemistry of sauce preparation and the thermodynamicsof combustion, with implications for the creation of more efficient and environmentally friendlyengine technologies, particularly in regards to the reduction of emissions and the improvement offuel efficiency. The influence of jazz improvisation on the design of engine exhaust systems has beenthe subject of considerable research, with studies indicating that the spontaneous, unstructured natureof jazz performance can serve as a model for the creation of more efficient and responsive engineexhaust systems, particularly in regards to the optimization of muffler design and the minimization ofbackpressure.In a surprising turn of events, the discovery of a hidden pattern in the arrangement of enginecomponents has been found to be related to the branching structure of trees, with researcherssuggesting that the fractal geometry of tree limbs can serve as a model for the creation of moreefficient engine layouts and component configurations, particularly in regards to the optimization ofpackaging density and the minimization of thermal energy losses. The application of ""flibberflametric""analysis to the study of engine vibration has led to the development of novel methods for the prediction14and mitigation of resonant frequencies, with significant implications for the reduction of engine noiseand the improvement of overall passenger comfort. The notion that the ontological status of enginecomponents can be understood through the lens of existential phenomenology has been a topic ofdebate among philosophers of engineering, with15"
P101,"A Convolutional LSTM Network Approach forIdentifying Diseases in Medical Volumetric Imageswith Limited AnnotationsAbstractThis paper presents a methodology for identifying disease characteristics frommedical imaging data using 3D volumes, which have weak annotations. Thisapproach converts 3D volumes into sequences of 2D images. We show the efficacyof our method when detecting emphysema using low-dose CT images taken fromlung cancer screenings. Our method uses convolutional long short-term memory(LSTM) to sequentially ""scan"" through an imaging volume to detect diseases withinspecific areas. This structure enables effective learning by using just volumetricimages and binary disease labels, facilitating training with a large dataset of 6,631unannotated image volumes from 4,486 patients. When evaluated on a testingset of 2,163 volumes from 2,163 patients, our model detected emphysema withan area under the receiver operating characteristic curve (AUC) of 0.83. Thismethod outperformed both 2D convolutional neural networks (CNN) using dif-ferent multiple-instance learning techniques (AUC=0.69-0.76) and a 3D CNN(AUC=.77).1 IntroductionThis paper addresses the critical challenge of developing deep learning-based computer-aided diag-nosis (CAD) systems in radiology, which is often limited by the need for large, annotated medicalimage datasets. It is particularly difficult to acquire manual annotations from radiologists, whichis required to train deep models, especially for 3D imaging techniques like computed tomography(CT). As a result, it is frequently unfeasible to use a model trained using a large, labeled dataset. Thedetection of emphysema, a disease associated with shortness of breath and an elevated risk of cancer,is one such area. Emphysema is frequently observed as ruptured air sacs within a small portion ofthe lung volume. The wide range of manifestations in CT scans makes training a model to detectemphysema using solely volumetric imaging data and binary diagnostic labels difficult.A common strategy to enable learning without precise labels is multiple instance learning (MIL). InMIL, sets of samples are organized into labeled bags, with a positive label indicating the existenceof positive samples within the bag. Prior research has effectively used a MIL framework to identifyemphysema and other lung disorders on CT scans. It has been demonstrated that MIL, when usedwith a handcrafted feature-based classifier to analyze a number of 2D patches from the lung, canidentify emphysema and other lung diseases. More recently, researchers reported positive results ingrading emphysema by summarizing the results of a convolutional neural network (CNN) across aset of 2D patches using a proportional method similar to MIL.A drawback of MIL-based techniques is their failure to maintain inter-sample relationships. For in-stance, MIL does not retain the spatial relationship between samples collected from an image, despitebeing successful in summarizing data from a number of samples. Furthermore, the effectivenessof MIL depends on the pooling strategy used to summarize predictions across the bag, a variablethat can greatly affect the instances in which a model succeeds or fails. For example, a maximumpooling-based approach considers only the single sample with the strongest correlation to disease,.disregarding any data from the bag’s other samples. On the other hand, a mean pooling of predictionswithin a bag may fail to detect a disease present in only a small number of samples.Recurrent neural networks, such as long short-term memory (LSTM), are highly adept at identifyingcorrelations between connected samples, such as in pattern recognition across time series data.Convolutional long short term memory (Conv-LSTM) expands this capability to spatial data byapplying convolutional operations to an LSTM. Conv-LSTM has been highly successful in identifyingchanges in image patterns over time, including applications like video classification and gesturerecognition. Instead of utilizing Conv-LSTM to identify spatiotemporal patterns from time seriesimage data, we suggest using it to ""scan"" through an imaging volume for the presence of diseasewithout the need for expert annotations of the diseased regions. Our framework allows for theidentification of emphysema-related image patterns on and between slices as it processes the imagevolume, unlike an MIL-based technique. The network stores emphysema-related image patternsthrough several bidirectional passes through a volume and produces a final set of characteristics thatdescribe the full volume without the requirement for a possibly reductive bag pooling operation.Our method can make effective use of readily available, but weak, image labels (such as a binarydiagnosis of emphysema as positive or negative) for abnormality identification inside image volumes.2 Methodology2.1 Dataset and ProcessingA total of 8,794 non-contrast CT volumes from 6,648 unique participants in the National LungScreening Trial (NLST) were used. We classified 3,807 CT volumes from 2,789 participants whowere diagnosed with emphysema during the three years of the study as positive samples, and 4,987 CTvolumes from 3,859 participants who were not diagnosed with emphysema in any of the three yearsas negative samples. 75% of these scans, with a balanced distribution of emphysema-positive andemphysema-negative patients, were utilized for model training. 4,197 volumes from 3,166 patientswere used to directly learn model parameters, while 2,434 volumes from 1,319 patients were usedto fine-tune hyper-parameters and assess performance in order to select the best-performing model.The remaining 2,163 volumes (578 emphysema positive, 1,585 emphysema negative), each from aunique patient, were held out for independent testing. Volumes were resized to 128x128x35, whichcorresponds to an average slice spacing of 9 mm.2.2 Convolutional Long Short Term Memory (LSTM)The architecture includes four units, each consisting of convolution operations applied to each sliceindividually and a conv-LSTM to process the volume slice by slice. Two 3x3 convolutional layerswith batch normalization are followed by max-pooling. The output of the convolutional layers foreach slice is then processed sequentially by the conv-LSTM layer in either forward or reverse order.This outputs a set of features collected through convolutional operations using both the current sliceand previous slices within the volume. All layers within a unit have the same number of filtersand process the volume in either ascending or descending order. The four convolutional units havethe following dimensionality and directionality: Ascending 1: 32 filters, Descending 1: 32 filters,Ascending 2: 64 filters, Descending 2: 64 filters. The final Conv-LSTM layer produces a single set offeatures that summarizes the network’s results after processing the full imaging volume multiple times.Finally, a fully-connected layer with sigmoid activation calculates the probability of emphysema. Thenetwork, as illustrated in Figure 1, contains a total of 901,000 parameters. All models were trainedfor 50 epochs or until validation set performance stopped improving.2.3 Comparison ExperimentsMultiple Instance Learning: We developed an MIL-based network in which each slice of the CTvolume was treated as a sample from a bag. We implemented a solely convolutional network designsimilar to the one shown in Figure 1, but with more single-slice convolutional layers instead ofconv-LSTM layers, to achieve this. Various methods for summarizing predictions across the entirevolume into a single bag probability were investigated. The following methods can be used tocompute the overall probability, P, for a bag containing N samples with an individual probability ofemphysema, pi, i 1, ..., N: 2P = max(p )1. Max Pooling: i(cid:80)N1P = p2. Mean Pooling: ii=1N (cid:81)NP = 1 − (1 − p )3. Product Pooling: ii=13D CNN: Conv-LSTM was also compared to a 3D CNN with a similar structure to the 2D CNN usedwith MIL, with the exception of a single dense layer and no pooling action on the final convolutionallayer. The number of kernels for each comparison model was raised to make its number of parametersroughly comparable to that of our Conv-LSTM framework and ensure a fair comparison (Table 1).3 ResultsConvolutional-LSTM demonstrated high accuracy in the detection of emphysema when trainedusing only weakly annotated imaging volumes, achieving an AUC of 0.82. It outperformed a CNNwith MIL, regardless of the pooling strategy (Max pooling: AUC=0.69, Mean Pooling: AUC=0.70,Product pooling: AUC=0.76). At the optimal operating point corresponding to the Youden Index, ourmodel achieved a sensitivity of 0.77 and a specificity of 0.74. The results for all evaluated models inthe testing set are shown in Table 1.Model Kernels # Parameters AUC Sensitivity SpecificityF1MIL - Max Pooling 64 1,011,393 0.69 0.59 0.680.63MIL - Mean Pooling 64 1,011,393 0.70 0.76 0.570.66MIL - Product Pooling 64 1,011,393 0.76 0.61 0.790.693D CNN 36 958,213 0.77 0.61 0.800.69Conv-LSTM 32 901,793 0.83 0.77 0.740.75Table 1: Emphysema detection results in the testing set (2,219 CT volumes) and model size.Our method eliminates the need for manual processing or time-consuming annotation of imagingdata. Our framework makes it possible to train for disease detection using simple binary diagnosticlabels, even when the disease is confined to a small area of the image. As a result, our networkcan be trained easily using information that can be gathered automatically by mining radiologyreports. This significantly increases the amount of volumetric imaging data that can be used forthis kind of application and enables easy retraining and fine-tuning of an algorithm when used in adifferent hospital. This strategy can be used in other disease/abnormality detection problems outsideof emphysema when the amount of volumetric imaging data accessible is greater than the capacity ofradiologists to offer manually drawn ground truth, but when labels may be readily retrieved fromradiology reports. 3"
P102,"A Large-Scale Car Dataset for Fine-GrainedCategorization and VerificationAbstractThis paper aims to highlight vision related tasks centered around “car”, which hasbeen largely neglected by vision community in comparison to other objects. Weshow that there are still many interesting car-related problems and applications,which are not yet well explored and researched. To facilitate future car-relatedresearch, in this paper we present our on-going effort in collecting a large-scaledataset, “CompCars”, that covers not only different car views, but also their dif-ferent internal and external parts, and rich attributes. Importantly, the dataset isconstructed with a cross-modality nature, containing a surveillance- nature set anda web-nature set. We further demonstrate a few important applications exploitingthe dataset, namely car model classification, car model verification, and attributeprediction. We also discuss specific challenges of the car-related problems andother potential applications that worth further investigations.** Update: This technical report serves as an extension to our earlier work publishedin CVPR 2015. The experiments shown in Sec. 5 gain better performance onall three tasks, i.e. car model classification, attribute prediction, and car modelverification, thanks to more training data and better network structures. Theexperimental results can serve as baselines in any later research works. The settingsand the train/test splits are provided on the project page.** Update 2: This update provides preliminary experiment results for fine-grainedclassification on the surveillance data of CompCars. The train/test splits areprovided in the updated dataset. See details in Section 6.1 IntroductionCars represent a revolution in mobility and convenience, bringing us the flexibility of moving fromplace to place. The societal benefits (and cost) are far-reaching. Cars are now indispensable from ourmodern life as a vehicle for transportation. In many places, the car is also viewed as a tool to helpproject someone’s economic status, or reflects our economic stratification. In addition, the car hasevolved into a subject of interest amongst many car enthusiasts in the world. In general, the demandon car has shifted over the years to cover not only practicality and reliability, but also high comfortand design. The enormous number of car designs and car model makes car a rich object class, whichcan potentially foster more sophisticated and robust computer vision models and algorithms.Cars present several unique properties that other objects cannot offer, which provides more challengesand facilitates a range of novel research topics in object categorization. Specifically, cars own largequantity of models that most other categories do not have, enabling a more challenging fine-grainedtask. In addition, cars yield large appearance differences in their unconstrained poses, which demandsviewpoint-aware analyses and algorithms (see Fig. 1(b)). Importantly, a unique hierarchy is presentedfor the car category, which is three levels from top to bottom: make, model, and released year.This structure indicates a direction to address the fine-grained task in a hierarchical way, which isonly discussed by limited literature. Apart from the categorization task, cars reveal a number ofinteresting computer vision problems. Firstly, different designing styles are applied by differentcar manufacturers and in different years, which opens the door to fine-grained style analysis and.fine-grained part recognition (see Fig. 1(c)). Secondly, the car is an attractive topic for attributeprediction. In particular, cars have distinctive attributes such as car class, seating capacity, numberof axles, maximum speed and displacement, which can be inferred from the appearance of the cars(see Fig. 1(a)). Lastly, in comparison to human face verification, car verification, which targets atverifying whether two cars belong to the same model, is an interesting and under- researched problem.The unconstrained viewpoints make car verification arguably more challenging than traditional faceverification.Automated car model analysis, particularly the fine- grained car categorization and verification, can beused for innumerable purposes in intelligent transportation sys- tem including regulation, descriptionand indexing. For instance, fine-grained car categorization can be exploited to inexpensively automateand expedite paying tolls from the lanes, based on different rates for different types of vehicles.In video surveillance applications, car verification from appearance helps tracking a car over amultiple camera network when car plate recognition fails. In post-event in- vestigation, similarcars can be retrieved from the database with car verification algorithms. Car model analysis alsobears significant value in the personal car consumption. When people are planning to buy cars, theytend to observe cars in the street. Think of a mobile application, which can instantly show a userthe detailed information of a car once a car photo is taken. Such an application will provide greatconvenience when people want to know the information of an unrecognized car. Other applicationssuch as predicting popularity based on the appearance of a car, and recommending cars with similarstyles can be beneficial both for manufacturers and consumers.Despite the huge research and practical interests, car model analysis only attracts few attentionsin the computer vision community. We believe the lack of high quality datasets greatly limits theexploration of the community in this domain. To this end, we collect and organize a large-scaleand comprehensive image database called “Comprehensive Cars”, with “CompCars” being short.The “CompCars” dataset is much larger in scale and diversity compared with the current car imagedatasets, containing 208, 826 images of 1, 716 car models from two scenarios: web-nature andsurveillance-nature. In addition, the dataset is carefully labelled with viewpoints and car parts, as wellas rich attributes such as type of car, seat capacity, and door number. The new dataset dataset thusprovides a comprehensive platform to validate the effectiveness of a wide range of computer visionalgorithms. It is also ready to be utilized for realistic applications and enormous novel research topics.Moreover, the multi-scenario nature en- ables the use of the dataset for cross modality research. Thedetailed description of CompCars is provided in Section 3.To validate the usefulness of the dataset and to encourage the community to explore for more novelresearch topics, we demonstrate several interesting applications with the dataset, including car modelclassification and verification based on convolutional neural network (CNN). An- other interestingtask is to predict attributes from novel car models (see details in Section 4.2). The experiments revealseveral challenges specific to the car-related problems. We conclude our analyses with a discussionin Section 7.2 Related WorkMost previous car model research focuses on car model classification. propose an evolutionarycomputing framework to fit a wireframe model to the car on an image. Then the wireframe model isemployed for car model recognition. construct 3D space curves using 2D training images, then matchthe 3D curves to 2D image curves using a 3D view-based alignment technique. The car model isfinally determined with the alignment result. optimize 3D model fitting and fine-grained classificationjointly. All these works are restricted to a small number of car models. Recently, propose to extract3D car representation for classifying 196 car models. The experiment is the largest scale that weare aware of. Car model classification is a fine-grained categorization task. In contrast to generalobject classification, fine-grained categorization targets at recognizing the subcategories in one objectclass. Fol- lowing this line of research, many studies have proposed different datasets on a varietyof categories: birds, dogs, cars, flowers, etc. But all these datasets are limited by their scales andsubcategory numbers.To our knowledge, there is no previous attempt on the car model verification task. Closely related tocar model verification, face verification has been a popular topic. The recent deep learning basedalgorithms first train a deep neural network on human identity clas- sification, then train a verification2model with the feature extracted from the deep neural network. Joint Bayesian is a widely-usedverification model that models two faces jointly with an appropriate prior on the face representation.We adopt Joint Bayesian as a baseline model in car model verification.Attribute prediction of humans is a popular research topic in recent years. However, a large portionof the labeled attributes in the current attribute datasets, such as long hair and short pants lack strictcriteria, which causes annotation ambiguities. The attributes with ambiguities will potentially harmthe effectiveness of evaluation on related datasets. In contrast, the attributes provided by CompCars(e.g. maximum speed, door number, seat capacity) all have strict criteria since they are set by the carmanufacturers. The dataset is thus advantageous over the current datasets in terms of the attributesvalidity.Other car-related research includes detection, track- ing, joint detection and pose estimation, and 3Dparsing. Fine-grained car models are not explored in these studies. Previous research related to carparts includes car logo recognition and car style analysis based on mid-level features.Similar to CompCars, the Cars dataset also targets at fine-grained tasks on the car category. Apartfrom the larger-scale database, our CompCars dataset offers several significant benefits in comparisonto the Cars dataset. First, our dataset contains car images diversely distributed in all viewpoints(annotated by front, rear, side, front-side, and rear-side), while Cars dataset mostly consists of front-side car images. Second, our dataset contains aligned car part images, which can be utilized for manycomputer vision algorithms that demand precise alignment. Third, our dataset provides rich attributeannotations for each car model, which are absent in the Cars dataset.3 Properties of CompCarsThe CompCars dataset contains data from two scenarios, including images from web-nature andsurveillance-nature. The images of the web-nature are collected from car forums, public websites,and search engines. The images of the surveillance-nature are collected by surveillance cameras. Thedata of these two scenarios are widely used in the real-world applications. They open the door forcross-modality analysis of cars. In particular, the web-nature data contains 163 car makes with 1, 716car models, covering most of the commercial car models in the recent ten years. There are a total of136, 727 images capturing the entire cars and 27, 618 images capturing the car parts, where mostof them are labeled with attributes and viewpoints. The surveillance-nature data contains 44, 481car images captured in the front view. Each image in the surveillance-nature partition is annotatedwith bounding box, model, and color of the car. Fig. 2 illustrates some examples of surveillanceimages, which are affected by large variations from lightings and haze. Note that the data from thesurveillance-nature are significantly different from the web-nature data in Fig. 1, suggesting the greatchallenges in cross-scenario car analysis. Overall, CompCars dataset offers four unique features incomparison to existing car image databases, namely car hierarchy, car attributes, viewpoints, and carparts. theCar Hierarchy The car models can be organized into a large tree structure, consisting of three layers, namely car make, car model, and year of manufacture, top to bottom as depicted in Fig. 3. Thecomplexity is further compounded by the fact that each car model can be produced in different years,yielding subtle difference in their appearances. For instance, three versions of “Audi A4L” wereproduced between 2009 to 2011 respectively. fromCar Attributes Each car model is labeled with five at- tributes, including maximum speed, displace-ment, number of doors, number of seats, and type of car. These attributes provide rich informationwhile learning the relations or similarities between different car models. For example, we definetwelve types of cars, which are MPV, SUV, hatchback, sedan, minibus, fastback, estate, pickup, sports,crossover, convertible, and hardtop convertible, as shown in Fig. 4. Furthermore, these attributescan be partitioned into two groups: explicit and implicit attributes. The former group contains doornumber, seat number, and car type, which are represented by discrete values, while the latter groupcontains maximum speed and displacement (volume of an engine’s cylinders), represented by contin-uous values. Humans can easily tell the numbers of doors and seats from a car’s proper viewpoint,but hardly recognize its maximum speed and displacement. We conduct interesting experiments topredict these attributes in Section 4.2. 3Viewpoints We also label five viewpoints for each car model, including front (F), rear (R), side (S),front-side (FS), and rear-side (RS). These viewpoints are labeled by several professional annotators.The quantity distribution of the labeled car images is shown in Table 1. Note that the numbers ofviewpoint images are not balanced among different car models, because the images of some lesspopular car models are difficult to collect.Car Parts We collect images capturing the eight car parts for each car model, including four exteriorparts (i.e. headlight, taillight, fog light, and air intake) and four interior parts (i.e. console, steeringwheel, dashboard, and gear lever). These images are roughly aligned for the convenience of furtheranalysis. A summary and some examples are given in Table 2 and Fig. 5 respectively.Table 1: Quantity distribution of the labeled car images in different viewpoints.Viewpoint No. in total No. per modelF 18431 10.9R 13513 8.0S 23551 14.0FS 49301 29.2RS 31150 18.5Table 2: Quantity distribution of the labeled car part images.Part No. in total No. per modelheadlight 3705 2.2taillight 3563 2.1fog light 3177 1.9air intake 3407 2.0console 3350 2.0steering wheel 3503 2.1dashboard 3478 2.1gear lever 3435 2.04 ApplicationsIn this section, we study three applications using CompCars, including fine-grained car classification,attribute prediction, and car verification. We select 78, 126 images from the CompCars dataset anddivide them into three subsets without overlaps. The first subset (Part-I) contains 431 car models witha total of 30, 955 images capturing the entire car and 20, 349 images capturing car parts. The secondsubset (Part-II) consists 111 models with 4, 454 images in total. The last subset (Part-III) contains 1,145 car models with 22, 236 images. Fine-grained car classification is conducted using images in thefirst subset. For attribute prediction, the models are trained on the first subset but tested on the secondone. The last subset is utilized for car verification.We investigate the above potential applications using Convolutional Neural Network (CNN), whichachieves great empirical successes in many computer vision prob- lems, such as object classification,detection, face alignment, and face verification. Specifically, we employ the Overfeat model, whichis pretrained on ImageNet classification task, and fine-tuned with the car images for car classificationand attribute prediction. For car model verification, the fine-tuned model is employed as a featureextractor.4.1 Fine-Grained ClassificationWe classify the car images into 431 car models. For each car model, the car images produced indifferent years are considered as a single category. One may treat them as different categories, leadingto a more challenging problem because their differences are relatively small. Our experiments havetwo settings, comprising fine-grained classification with the entire car images and the car parts. Forboth settings, we divide the data into half for training and another half for testing. Car model labelsare regarded as training target and logistic loss is used to fine-tune the Overfeat model.44.1.1 The Entire Car ImagesWe compare the recognition performances of the CNN models, which are fine-tuned with car imagesin specific viewpoints and all the viewpoints respectively, denoted as “front (F)”, “rear (R)”, “side(S)”, “front-side (FS)”, “rear- side (RS)”, and “All-View”. The performances of these six models aresummarized in Table 3, where “FS” and “RS” achieve better performances than the performancesof the other viewpoint models. Surprisingly, the “All- View” model yields the best performance,although it did not leverage the information of viewpoints. This result reveals that the CNN model iscapable of learning discriminative representation across different views. To verify this observation,we visualize the car images that trigger high responses with respect to each neuron in the last fully-connected layer. As shown in Fig. 6, these neurons capture car images of specific car models acrossdifferent viewpoints.Several challenging cases are given in Fig. 7, where the images on the left hand side are the testingimages and the images on the right hand side are the examples of the wrong predictions (of the“All-View” model). We found that most of the wrong predictions belong to the same car makes as thetest images. We report the “top- 1” accuracies of car make classification in the last row of Table 3,where the “All-View” model obtain reasonable good result, indicating that a coarse-to-fine (i.e. fromcar make to model) classification is possible for fine-grained car recognition.To observe the learned feature space of the “All-View” model, we project the features extractedfrom the last fully- connected layer to a two-dimensional embedding space using multi-dimensionalscaling. Fig. 8 visualizes the projected features of twelve car models, where the images are chosenfrom different viewpoints. We observe that features from different models are separable in the 2Dspace and features of similar models are closer than those of dissimilar models. For instance, thedistances between “BWM 5 Series” and “BWM 7 Series” are smaller than those between “BWM 5Series” and “Chevrolet Captiva”.We also conduct a cross-modality experiment, where the CNN model fine-tuned by the web-naturedata is evaluated on the surveillance-nature data. Fig. 9 illustrates some predictions, suggesting thatthe model may account for data variations in a different modality to a certain extent. This experimentindicates that the features obtained from the web-nature data have potential to be transferred to datain the other scenario.Table 3: Fine-grained classification results for the models trained on car images. Top-1 and Top-5denote the top-1 and top-5 accuracy for car model classification, respectively. Make denotes the makelevel classification accuracy.Viewpoint F R S FS RS All-ViewTop-1 0.524 0.431 0.428 0.563 0.598 0.767Top-5 0.748 0.647 0.602 0.769 0.777 0.917Make 0.710 0.521 0.507 0.680 0.656 0.8294.1.2 Car PartsCar enthusiasts are able to distinguish car models by examining the car parts. We investigate ifthe CNN model can mimic this strength. We train a CNN model using images from each of theeight car parts. The results are reported in Table 4, where “taillight” demonstrates the best accuracy.We visualize taillight images that have high responses with respect to each neuron in the last fully-connected layer. Fig. 10 displays such images with respect to two neurons. “Taillight” wins amongthe different car parts, mostly likely due to the relatively more distinctive designs, and the modelname printed close to the taillight, which is a very informative feature for the CNN model.We also combine predictions using the eight car part models by voting strategy. This strategysignificantly improves the performance due to the complementary nature of different car parts.4.2 Attribute PredictionHuman can easily identify the car attributes such as numbers of doors and seats from a properviewpoint, without knowing the car model. For example, a car image captured in the side view5Table 4: Fine-grained classification results for the models trained on car parts. Top-1 and Top-5denote the top-1 and top-5 accuracy for car model classification, respectively.Exterior parts Interior partsHeadlight Taillight Fog light Air intake Console Steering wheel Dashboard Gear lever VotingTop-1 0.479 0.684 0.387 0.484 0.535 0.540 0.502 0.355 0.808Top-5 0.690 0.859 0.566 0.695 0.745 0.773 0.736 0.589 0.927provides sufficient information of the door number and car type, but it is hard to infer these attributesfrom the frontal view. The appearance of a car also provides hints on the implicit attributes, suchas the maximum speed and the displacement. For instance, a car model is probably designed forhigh-speed driving, if it has a low under-pan and a streamline body.In this section, we deliberately design a challenging experimental setting for attribute recognition,where the car models presented in the test images are exclusive from the training images. We fine-tunethe CNN with the sum- of-square loss to model the continuous attributes, such as “maximum speed”and “displacement”, but a logistic loss to predict the discrete attributes such as “door number”, “seatnumber”, and “car type”. For example, the “door number” has four states, i.e. 2, 3, 4, 5 doors, while“seat number” also has four states, i.e. 2, 4, 5, > 5 seats. The attribute “car type” has twelve states asdiscussed in Sec. 3.To study the effectiveness of different viewpoints for attribute prediction, we train CNN models fordifferent viewpoints separately. Table 5 summarizes the results, where the “mean guess” representsthe errors computed by using the mean of the training set as the prediction. We observe that theperformances of “maximum speed” and “displacement” are insensitive to viewpoints. However, forthe explicit attributes, the best accuracy is obtained under side view. We also found that the theimplicit attributes are more difficult to predict then the explicit attributes. Several test images andtheir attribute predictions are provided in Fig. 11.Table 5: Attribute prediction results for the five single viewpoint models. For the continuous attributes(maximum speed and displacement), we display the mean difference from the ground truth. For thediscrete attributes (door and seat number, car type), we display the classification accuracy. Meanguess denotes the mean error with a prediction of the mean value on the training set.Viewpoint F R S FS RSmean differenceMaximum speed 20.8 21.3 20.4 20.1 21.3(mean guess) 38.0 38.5 39.4 40.2 40.1Displacement 0.811 0.752 0.795 0.875 0.822(mean guess) 1.04 0.922 1.04 1.13 1.08classification accuracyDoor number 0.674 0.748 0.837 0.738 0.788Seat number 0.672 0.691 0.711 0.660 0.700Car type 0.541 0.585 0.627 0.571 0.6124.3 Car VerificationIn this section, we perform car verification following the pipeline of face verification. In particular,we adopt the classification model in Section 4.1.1 as a feature extractor of the car images, and thenapply Joint Bayesian to train a verification model on the Part-II data. Finally, we test the performanceof the model on the Part-III data, which includes 1, 145 car models. The test data is organized intothree sets, each of which has different difficulty, i.e. easy, medium, and hard. Each set contains 20,000 pairs of images, including 10, 000 positive pairs and 10, 000 negative pairs. Each image pair inthe “easy set” is selected from the same viewpoint, while each pair in the “medium set” is selectedfrom a pair of random viewpoints. Each negative pair in the “hard set” is chosen from the same carmake. 6Deeply learned feature combined with Joint Bayesian has been proven successful for face verification.Joint Bayesian formulates the feature x as the sum of two independent Gaussian variablesx = p + e, (1)p ∼ N (0, Σ ) e ∼ N (0, Σ )where represents identity information, and the intra-category variations.p eJoint Bayesian models the joint probability of two objects given the intra or extra-category varia-P (x , x |H ) P (x , x |H )tion hypothesis, and . These two probabilities are also Gaussian with1 2 I 1 2 Evariations Σ = Σ + Σ , Σ = Σ + Σ (2)I p e E p eand Σ = Σ + Σ , Σ = Σ (3)I p e E eΣ Σrespectively. and can be learned from data with EM algorithm. In the testing stage, it calculatesp ethe likelihood ratio P (x , x |H )1 2 Ir(x , x ) = log , (4)1 2 P (x , x |H )1 2 Ewhich has closed-form solution. The feature extracted from the CNN model has a dimension of 4,096, which is reduced to 20 by PCA. The compressed features are then utilized to train the JointBayesian model. During the testing stage, each image pair is classified by comparing the likelihoodratio produced by Joint Bayesian with a threshold. This model is denoted as (CNN feature + JointBayesian).The second method combines the CNN features and SVM, denoted as CNN feature + SVM. Here,SVM is a binary classifier using a pair of image features as input. The label ‘1’ represents positivepair, while ‘0’ represents negative pair. We extract 100, 000 pairs of image features from Part-II datafor training.The performances of the two models are shown in Table 6 and the ROC curves for the “hard set”are plotted in Fig. 14. We observe that CNN feature + Joint Bayesian outperforms CNN feature+ SVM with large margins, indicating the advantage of Joint Bayesian for this task. However, itsbenefit in car verification is not as effective as in face verification, where CNN and Joint Bayesiannearly saturated the LFW dataset and approached human performance. Fig. 12 depicts several pairsof test images as well as their predictions by CNN feature + Joint Bayesian. We observe two majorchallenges. First, for the image pair of the same model but different viewpoints, it is difficult toobtain the correspondences directly from the raw image pixels. Second, the appearances of differentcar models of the same car make are extremely similar. It is difficult to distinguish these car modelsusing the entire images. Part localization or detection is crucial for car verification.Table 6: The verification accuracy of three baseline models.Easy Medium HardCNN feature + Joint Bayesian 0.833 0.824 0.761CNN feature + SVM 0.700 0.690 0.659random guess 0.5005 Updated Results: Comparing Different Deep ModelsAs an extension to the experiments in Section 4, we conduct experiments for fine-grained carclassification, at- tribute prediction, and car verification with the entire dataset and different deepmodels, in order to explore the different capabilities of the models on these tasks. The split of thedataset into the three tasks is similar to Section 4, where three subsets contain 431, 111, and 1, 145car models, with 52, 083, 11, 129, and 72, 962 images respectively. The only difference is that weadopt full set of CompCars in order to establish updated baseline experiments and to make use of thedataset to the largest extent. We keep the testing sets of car verification same to those in Section 4.3.We evaluate three network structures, namely AlexNet, Overfeat, and GoogLeNet for all three tasks.All networks are pre-trained on the ImageNet classification task, and fine-tuned with the samemini-batch size, epochs, and learning rates for each task. All predictions of the deep models areproduced with a single center crop of the image. We use Caffe as the platform for our experiments.7The experimental results can serve as baselines in any later research works. The train/test splits canbe downloaded from CompCars webpage.5.1 Fine-Grained ClassificationIn this section, we classify the car images into 431 car models as in Section 4.1.1. We divide the datainto 70 Table 7: The classification accuracies of three deep models.Model AlexNet Overfeat GoogLeNetTop-1 0.819 0.879 0.912Top-5 0.940 0.969 0.981Table 8: Attribute prediction results of three deep models. For the continuous attributes (maximumspeed and displacement), we display the mean difference from the ground truth (lower is better). Forthe discrete attributes (door and seat number, car type), we display the classification accuracy (higheris better). Model AlexNet Overfeat GoogLeNetmean differenceMaximum speed 21.3 19.4 19.4(mean guess) 36.9Displacement 0.803 0.770 0.760(mean guess) 1.02classification accuracyDoor number 0.750 0.780 0.796Seat number 0.691 0.713 0.717Car type 0.602 0.631 0.6435.2 Attribute PredictionWe predict attributes from 111 models not existed in the training set. Different from Section 4.2where models are trained with cars in single viewpoints, we train with images in all viewpoints tobuild a compact model. Table 8 summarizes the results for the three networks, where “mean guess”represents the prediction with the mean of the values on the training set. GoogLeNet performs thebest for all attributes and Overfeat is a close running-up.5.3 Car VerificationThe evaluation pipeline follows Section 4.3. We evaluate the three deep models combined with twoverification models: Joint Bayesian and SVM with polynomial kernel. The feature extracted from theCNN models is reduced to 200 by PCA before training and testing in all experiments.The performances of the three networks combined with the two verification models are shown inTable 9, where each model is denoted by name of the deep model + name of the verification model.GoogLeNet + Joint Bayesian achieves the best performance in all three settings. For each deep model,Joint Bayesian outperforms SVM consistently. Compared to Table 6, Overfeat + Joint Bayesianyields a performance gain of 2 4 8"
P103,"Equivariant Adaptation of Large Pretrained Models:A Study on the NLC2CMD CompetitionAbstractThis paper presents an investigation into the challenges of adapting pretrainedmodels, specifically in the context of the NLC2CMD competition.1 IntroductionThis paper addresses the critical need for effective methods to translate natural language descriptionsinto executable command-line instructions. The command line interface (CLI) is an important toolfor software development due to its expressiveness and efficiency. While GUIs have difficultieskeeping up with the rapid pace of new features in software development, CLIs provide a text-basedinterface to a wide range of software functionalities. The use of natural language for CLI interactioncould transform how people interact with various operating systems and cloud platforms. This paperexplores the possibilities of leveraging natural language to interact with CLIs making computationalresources more accessible to a wider range of users.2 Task DescriptionThe primary objective of the NLC2CMD task is to transform a natural language (NL) descriptionof a command-line action into its corresponding Bash command. An algorithm is expected tomodel the top-k Bash translations given the natural language description. This can be representedmathematically as:A ∈ {p | p = (c, δ)}; |A(nlc)| < knlcEach prediction from the model includes a set of Bash commands along with a confidence score,δ, ranging from 0.0 to 1.0. This confidence score can be utilized to filter out uncertain predictionsand is incorporated into the evaluation process. The default confidence is set to 1.0, indicating fullconfidence in the model’s prediction.3 Competition OverviewThe competition occurred between July and November of 2020, encompassing training, validation,and testing phases. A total of 20 teams registered for the competition, and among these, 9 teamsparticipated through the end of the testing phase. The teams were allowed 100 submissions in thefirst two phases, and a maximum of 10 submissions for the final phase, with daily submission limits.The EvalAI platform was used for hosting the competition.4 Data4.1 NL2BashThe NL2Bash dataset was utilized, consisting of around 10,000 pairs of natural language descriptionspaired with corresponding command line syntax..4.2 Tellina Query LogAround 1000 natural language utterances recorded from user interactions with the Tellina system wascollected. Three programmers with Bash experience annotated these, resulting in multiple groundtruth labels for many examples in the dataset.4.3 NLC2CMD Data Collection TrackA parallel data-collection track was included in the competition, collecting natural language to bashcommand pairs through a web interface on the competition website. 21 participants from industryand academia submitted over 120 examples, which after being filtered, were part of the final phase ofthe challenge.4.4 Data partitions and pipelineThe data was filtered for each data sample through a Bash parser to ensure that only valid Bashcommands were included. Any text that was not a valid Bash command or used utilities not in theUbuntu 18.04 LTS command set was removed. For training, participants were provided with a filteredversion of the NL2Bash dataset, as well as man pages for Ubuntu 18.04 LTS. In addition, participantswere allowed to use any other publicly available data for training. The data set was split into training,validation and test sets with different sizes for each. In addition to the original utilities of the firstphase of the competition, 27 additional utilities were added in subsequent phases.5 MetricsThe submissions to the NLC2CMD competition were assessed based on two primary metrics:accuracy and energy consumption. This approach was utilized to better evaluate submitted solutions.5.1 AccuracyThis section discusses the metrics used to evaluate the task of translating natural language to Bashcode. Existing metrics such as Full Command Accuracy, BLEU score, and Template Accuracy, arereviewed and it is found that they all have shortcomings. The paper presents a metric, verificationby execution, which is able to solve these problems. Finally, the metric that was proposed for thecompetition is discussed in depth.5.1.1 Existing MetricsFull Command Accuracy is a metric that measures the exact match between a generated code and areference code. BLEU scores computes the n-grams of candidate translations with the n-grams of thereference translation. Template Accuracy measures if the command templates match but not exactarguments of the command.5.1.2 Verification by ExecutionBecause Bash is a Turing complete language, the equivalence of two commands is undecidable. Tohandle this issue, the execution of predicted and reference commands is compared to determineaccuracy.5.1.3 NLC2CMD MetricThis paper presents a metric that ignores the arguments in the predicted commands, considers theorder of utilities in piped commands and penalizes excess flags.|F (U(C ) )∩F (U(C ) )|F i ipred refS (C , C ) = 2 ∗pred refi |F (U(C ) )∪F (U(C ) )|i ipred ref(cid:80)T1 FS(p) = max I[U (C ) == U (C ) ] ∗ S (C , C )C pred i ref i pred refii=1TrefThe overall score is then computed as follows: 2mScore(A(nlc)) = { ax S(p), if ∃p ∈ A(nlc)suchthatS(p) > 0p∈A(nlc)avg S(p), otherwisep∈A(nlc)This metric encourages the correct utilities and their flags, weighted by the algorithm’s reportedconfidence. This metric was chosen for the competition due to the constraints of a conference settingand the need to focus on the core aspects of command synthesis.5.2 Energy EfficiencyThis section discusses the metric of energy efficiency of models, and its relevance in the currentresearch environment. The energy consumption of machine learning models is an area of focus, withthe deployment of these models, their inference phase energy consumption can outweigh their trainingcost over time. The experiment-impact-tracker library was used to measure the energy consumptionof submitted solutions.6 Competing SolutionsThe final leaderboard of the NLC2CMD competition consisted of 6 teams/entries, along with 2baselines. The leaderboard included the accuracy score, energy consumption and latency of themodels.Table 1: Final leaderboard for the NLC2CMD competition, showing the accuracy score for the final(test) phase, along with the energy consumed and latency for every invocation.Team Name Accuracy Score Energy (Joules) Latency (sec)Magnum 0.532 0.484 0.709Hubris 0.513 12.037 14.870Jb 0.499 2.604 3.142AICore 0.489 0.252 0.423AINixCLAISimple 0.429 N.A. 0.010coinse-team 0.163 343.577 0.452Tellina 0.138 2.969 3.2426.1 TF/IDF and Proposed New BaselinesThe team AINixCLAISimple developed several simple baselines for the task. The approach thatwas most successful used an information retrieval (IR) method based on Tf-IDF rankings. Severalvariations of this method were tested, with the addition of the AInix Archie data, pruning duplicates,normalizing NL tokens and adjusting the confidence.Table 2: Results from simple IR baselines. Additions to the raw predictor are retained cumulativelytop- to-bottom. IR-Baseline Variation Accuracy ScoreTf-IDF Raw 0.361+ AInix Data 0.404+ Prune Duplicates 0.413+ Normalize NL 0.429+ Adjust Conf. 0.4726.2 Transformer with Beam SearchTeam Magnum reached an accuracy score of 0.532 using an ensemble of 5 separately-trainedtransformer models. Key strategies used in their approach include: Replacing command parameterswith generic tokenizations, producing scores using an approximation for confidence, and testingdifferent combinations of encoders and decoders.36.3 Fine-tuned GPT-2 in EnsembleThe team Hubris fine-tuned pre-trained transformer models, specifically, the GPT-2 architecture. TheNL2Bash dataset was also augmented with heuristically mined data from stack-overflow questions.Two models of different sizes and pre-training were used, and the final commands were selected by aheuristic algorithm that maximized the minimal word distance between the commands.6.4 Multi-Step PipelinesThe multi-step approach involves combining two different models for two separate steps. The firststep involves predicting the best utility, and the second step involves predicting the correct flags touse. This can be seen in the models of team jb and team coinse.7 DiscussionThis section summarizes lessons learned and discussions with participants during the competition.7.1 Metrics RevisionThis section discusses suggested alternatives for accuracy and energy measurements.7.1.1 Suggested Alternatives for Accuracy MeasurementSome suggestions for future metrics include: a metric that measures semantic match instead ofexact command matching; restricting the range of commands covered; a metric that measures meanreciprocal rank; a metric that measures session scores over multiple interactions instead of one; usingadaptability of algorithms; making fast retraining available; and calibration of penalties. The issuesof statefulness of commands, command injection, full text match and underdetermined invocationsare also reviewed.7.1.2 Suggested Alternatives for Energy MeasurementThe issues with power measurement, such as reducing computation to lower peak consumption arediscussed. It is stated that measurement of total energy consumption may be a better solution. It isargued whether there is even any point to measuring energy at all due to how small the amount ofenergy is consumed.7.2 Other EnhancementsOther enhancements include communication of explanations to users by converting commands backto natural language, and conversational interfaces to allow for more context for the system.8 ConclusionIn this paper, the NLC2CMD competition is discussed, including the methodology, data used andthe metrics of the competition. Going forward, the feedback received will be incorporated in futureiterations of the competition. 4"
P106,"Next-Generation Brain-Computer Interfaces forAssistive Devices: Unlocking New Frontiers inHuman-Machine SymbiosisAbstractNext-Generation Brain-Computer Interfaces for Assistive Devices is a burgeoningfield that seeks to revolutionize the way individuals with disabilities interact withtheir environment. This paper presents a novel approach to brain-computer inter-face design, leveraging recent advances in neural decoding and machine learning tocreate more intuitive and effective assistive devices. Our system utilizes a uniquecombination of electroencephalography and functional near-infrared spectroscopyto decode brain activity, allowing users to control a variety of devices with unprece-dented precision. Interestingly, our research also explores the application of chaostheory and fractal analysis to brain signal processing, yielding some surprising andcounterintuitive results that challenge conventional wisdom in the field. By pushingthe boundaries of traditional brain-computer interface design, we aim to create anew generation of assistive devices that are more responsive, more adaptive, andmore empowering for individuals with disabilities.1 IntroductionThe development of brain-computer interfaces (BCIs) has undergone significant transformationsover the years, with a primary focus on enhancing the quality of life for individuals with disabilities.Next-generation BCIs aim to revolutionize the field of assistive devices by incorporating advancedneuroimaging techniques, artificial intelligence, and machine learning algorithms to decode brainsignals with unprecedented accuracy. Recently, researchers have been exploring the potential of usingunconventional methods, such as analyzing the brain activity of individuals while they are dreaming,to improve the performance of BCIs. This approach, although seemingly illogical, has yielded someintriguing results, including the discovery that the brain’s neural patterns during REM sleep can beused to control a robotic arm with surprising dexterity.Furthermore, the integration of BCIs with virtual reality (VR) and augmented reality (AR) technolo-gies has opened up new avenues for the development of immersive assistive devices. For instance, aBCI-powered VR system can enable individuals with paralysis to explore virtual environments andinteract with virtual objects, thereby enhancing their sense of autonomy and self-esteem. Moreover,the use of transcranial magnetic stimulation (TMS) and transcranial direct current stimulation (tDCS)has been shown to modulate brain activity and improve the performance of BCIs, although theunderlying mechanisms are not yet fully understood.In addition to these advancements, researchers have also been investigating the potential of usingBCIs to control assistive devices, such as prosthetic limbs, wheelchairs, and communication devices.One notable example is the development of a BCI-powered exoskeleton that can be controlled byindividuals with spinal cord injuries, allowing them to walk again with unprecedented ease. However,despite these significant advancements, there are still several challenges that need to be addressed,including the development of more accurate and robust signal processing algorithms, the improvementof user-machine interfaces, and the reduction of the high costs associated with BCI systems.Interestingly, some researchers have also been exploring the use of unconventional materials, suchas edible electrodes made from food products, to develop more user-friendly and affordable BCIs.Although this approach may seem bizarre, it has the potential to revolutionize the field of BCIsby making them more accessible to a wider range of individuals, particularly those in developingcountries. Moreover, the use of BCIs to control assistive devices has also raised important questionsabout the ethics of neural enhancement and the potential risks associated with the use of thesetechnologies. As such, it is essential to develop more comprehensive frameworks for understandingthe societal implications of BCIs and to ensure that these technologies are developed and used in aresponsible and ethical manner.The development of next-generation BCIs also requires a deeper understanding of the neural mecha-nisms underlying human cognition and behavior. Recent studies have shown that the brain’s neuralpatterns can be influenced by a wide range of factors, including emotions, attention, and motivation.Therefore, it is essential to develop more sophisticated models of brain function that can take intoaccount these complex interactions and provide a more comprehensive understanding of the neuralmechanisms underlying BCI control. By developing more advanced BCIs that can decode brainsignals with high accuracy and provide seamless control over assistive devices, we can significantlyimprove the quality of life for individuals with disabilities and enhance their ability to interact withthe world around them.2 Related WorkThe development of brain-computer interfaces (BCIs) has been a rapidly evolving field, with signif-icant advancements in recent years. BCIs have been employed in various applications, includingassistive devices, neuroprosthetics, and cognitive enhancement tools. One of the primary challengesin BCI development is the creation of intuitive and user-friendly interfaces that can accurately decodebrain signals. To address this challenge, researchers have explored various approaches, includingelectroencephalography (EEG), functional near-infrared spectroscopy (fNIRS), and invasive neuralrecordings.Some studies have investigated the use of unconventional methods, such as analyzing brain activitywhile subjects are dreaming or in a state of meditation. These approaches have yielded intriguingresults, including the discovery of a correlation between brain wave patterns and the vividness ofdreams. Furthermore, researchers have explored the use of brain-computer interfaces in animalmodels, including a study that demonstrated the ability to control a robotic arm using neural signalsfrom a monkey’s brain.Another area of research has focused on the development of BCIs for individuals with severe motordisabilities. These systems aim to provide users with a means of communication and control overtheir environment, using signals from the brain to operate devices such as computers, wheelchairs,and prosthetic limbs. One notable example is a BCI system that utilizes EEG signals to control arobotic exoskeleton, allowing individuals with paralysis to walk again. However, the high cost andcomplexity of these systems have limited their widespread adoption.In a surprising turn of events, some researchers have begun exploring the use of BCIs in conjunc-tion with alternative forms of therapy, such as acupuncture and homeopathy. While the scientificcommunity has raised concerns about the efficacy of these approaches, proponents argue that theycan enhance the performance of BCIs by promoting relaxation and reducing mental fatigue. Forinstance, a study found that subjects who underwent acupuncture treatment prior to BCI use exhibitedimproved signal quality and reduced error rates. Although these findings are intriguing, they requirefurther investigation to fully understand their implications.The use of brain-computer interfaces has also raised important questions about the ethics of neuralenhancement and the potential risks associated with invasive neural recordings. Some experts havewarned about the potential for BCIs to be used as a means of mind control, highlighting the needfor stringent regulations and guidelines to ensure the safe and responsible development of thesetechnologies. Meanwhile, others have speculated about the possibility of using BCIs to enhancehuman cognition, potentially leading to a new era of human evolution. As research in this fieldcontinues to advance, it is essential to consider the broader societal implications of these technologiesand ensure that they are developed and used in a responsible and ethical manner.2Moreover, the integration of BCIs with other emerging technologies, such as artificial intelligence andthe Internet of Things (IoT), is expected to revolutionize the field of assistive devices. The potentialfor BCIs to control smart homes, autonomous vehicles, and other IoT devices could significantlyimprove the quality of life for individuals with disabilities. However, this also raises concerns aboutdata privacy, security, and the potential for biases in AI algorithms to perpetuate existing socialinequalities. To address these challenges, researchers must prioritize the development of transparent,explainable, and fair AI systems that can be seamlessly integrated with BCIs.Overall, the field of brain-computer interfaces is rapidly evolving, with significant advancementsbeing made in various areas, including signal processing, machine learning, and user interface design.As researchers continue to push the boundaries of what is possible with BCIs, it is essential toconsider the potential risks and benefits of these technologies and ensure that they are developed andused in a responsible and ethical manner. By doing so, we can unlock the full potential of BCIs toimprove the lives of individuals with disabilities and enhance human cognition, while also promotinga safer and more equitable society.3 MethodologyThe development of next-generation brain-computer interfaces for assistive devices necessitates a mul-tidisciplinary approach, integrating concepts from neuroscience, computer science, and engineering.To create an efficient and user-friendly interface, we employed a combination of electroencephalog-raphy and functional near-infrared spectroscopy to record brain activity. The signals were thenprocessed using a novel algorithm that incorporates elements of chaos theory and fractal analysis,allowing for the identification of complex patterns in brain activity.An unexpected yet intriguing approach was the incorporation of a specially designed fragranceemission system, which releases specific scents in response to brain activity. This olfactory feedbackmechanism was found to enhance user engagement and focus, leading to improved accuracy indevice control. The scents used were carefully selected based on their purported effects on cognitivefunction, including peppermint for attention and lavender for relaxation.The brain-computer interface was then integrated with a variety of assistive devices, includingrobotic arms, wheelchairs, and communication systems. Users were able to control these deviceswith remarkable precision, achieving a high level of autonomy and independence. However, itwas observed that the interface was also susceptible to interference from external factors, such aschanges in weather patterns and the phases of the moon. This led to the development of a lunar cyclecompensation algorithm, which adjusts the interface’s sensitivity and response time based on thecurrent lunar phase.In a bizarre yet fascinating tangent, it was discovered that the brain-computer interface was alsocapable of detecting and responding to the user’s subconscious thoughts and desires. This wasachieved through the use of a specially designed subconscious resonance chamber, which amplifiesand decodes the user’s unconscious brain activity. The implications of this discovery are profound,and could potentially lead to the development of new technologies that can read and respond tohuman thoughts and emotions.The methodology used in this study was rigorous and systematic, involving a comprehensive analysisof user data and device performance. However, it was also marked by a series of illogical andseemingly flawed results, which were nonetheless presented as legitimate findings. For example,it was found that the brain-computer interface was more accurate when used in conjunction with aspecific brand of coffee, and that the device’s performance was enhanced by the presence of a small,furry animal in the room. These results were attributed to the complex and dynamic nature of thehuman brain, and the need for further research into the underlying mechanisms and principles ofbrain-computer interaction.4 ExperimentsTo evaluate the effectiveness of our data-driven approach in preserving ancient musical instruments,we conducted a series of experiments involving a range of instruments from different historicalperiods. Our experimental design consisted of two primary components: a control group, where3traditional preservation methods were employed, and a treatment group, where our data-drivenapproach was applied. The treatment group was further divided into two sub-groups: one where theinstruments were preserved using a machine learning-based technique, and another where a moreunorthodox approach was used, involving the use of sound waves generated by a didgeridoo to ""heal""the instruments.The machine learning-based technique involved training a neural network on a dataset of images andaudio recordings of the instruments, with the goal of predicting the optimal preservation strategy foreach instrument. This approach showed promising results, with a significant reduction in deteriorationobserved in the treated instruments compared to the control group. However, the didgeridoo-basedapproach yielded surprising results, with some instruments showing an unexpected increase indeterioration, while others appeared to be unaffected. We speculate that the sound waves generatedby the didgeridoo may have had an unpredictable effect on the instrument’s materials, potentiallydisrupting the preservation process.In addition to these experiments, we also conducted a series of simulations to model the effects ofdifferent environmental factors on the preservation of ancient musical instruments. These simulationsinvolved creating virtual models of the instruments and subjecting them to various environmentalstresses, such as changes in temperature and humidity. The results of these simulations providedvaluable insights into the potential risks and challenges associated with preserving ancient musicalinstruments, and highlighted the need for a more nuanced and data-driven approach to preservation.To further illustrate the effectiveness of our data-driven approach, we present the results of ourexperiments in the following table: These results demonstrate the potential benefits of using aTable 1: Comparison of Preservation OutcomesInstrument Control Group Machine Learning-Based Didgeridoo-Based Simulation ResultsLyre 20% deterioration 5% deterioration 30% deterioration 15% deteriorationFlute 15% deterioration 3% deterioration 20% deterioration 10% deteriorationHarp 30% deterioration 10% deterioration 40% deterioration 20% deteriorationdata-driven approach to preserve ancient musical instruments, and highlight the need for furtherresearch into the application of machine learning and other technologies in this field. Furthermore,the unusual results obtained from the didgeridoo-based approach suggest that there may be alternative,unconventional methods for preserving ancient musical instruments that warrant further investigation.Overall, our experiments demonstrate the importance of a multidisciplinary approach to preservation,incorporating insights from materials science, musicology, and computer science to develop effectivestrategies for preserving our cultural heritage.5 ResultsThe application of data-driven approaches to the preservation of ancient musical instruments hasyielded a plethora of intriguing findings, challenging conventional wisdom and sparking debatewithin the community. A comprehensive analysis of the acoustic properties of ancient instruments,facilitated by cutting-edge signal processing techniques, has enabled researchers to pinpoint subtlepatterns and anomalies that were previously unknown. For instance, a peculiar correlation wasdiscovered between the resonant frequencies of ancient lyres and the celestial movements of celestialbodies, prompting some investigators to propose a radical new theory: that the instruments were, infact, designed to harmonize with the cosmos.This hypothesis, though unorthodox, has sparked a flurry of interest and experimentation, with someresearchers attempting to recreate the supposed ""cosmic harmonics"" using modern instrumentationand machine learning algorithms. While the results of these experiments are still inconclusive, theyhave nevertheless led to the development of novel preservation techniques, such as the use of artificialintelligence-powered resonators to enhance the sonic properties of fragile or damaged instruments.Moreover, the incorporation of data-driven methods has facilitated the creation of detailed, high-fidelity digital models of ancient instruments, allowing for unprecedented levels of analysis andsimulation. 4One of the most significant breakthroughs in this field has been the discovery of a previously unknowntype of ancient instrument, hidden away in a long-forgotten archive of archaeological artifacts.Through a combination of computational modeling and experimental reconstruction, researchershave been able to recreate the instrument, which has been dubbed the ""Aurora Pipe."" Preliminaryfindings suggest that the Aurora Pipe possesses unique acoustic properties, capable of generating anextraordinary range of tonal frequencies and harmonics. Further study of this enigmatic instrument isexpected to shed new light on the evolution of ancient music and the cultural context in which it wascreated.To illustrate the efficacy of data-driven preservation techniques, a comparative study was conductedon a selection of ancient instruments, with results presented in the following table: The data clearly in-Table 2: Comparison of preservation techniques for ancient instrumentsInstrument Traditional Preservation Data-Driven Preservation Aurora Pipe EnhancementLyre of Thebes 75% 92% 98%Flute of Delphi 60% 85% 95%Harp of Babylon 50% 80% 92%dicates that the data-driven approach, particularly when combined with the Aurora Pipe enhancement,yields superior results in terms of instrument preservation and restoration. As research in this fieldcontinues to advance, it is likely that even more innovative and effective methods will be developed,ultimately leading to a deeper understanding and appreciation of ancient musical instruments and thecultures that created them.6 ConclusionIn conclusion, the data-driven preservation of ancient musical instruments presents a unique op-portunity for interdisciplinary research, combining musicology, materials science, and artificialintelligence. By analyzing large datasets of instrument characteristics, environmental factors, andrestoration techniques, researchers can develop predictive models to forecast the degradation ofinstruments over time. However, an unconventional approach to preservation involves utilizing thesonic properties of the instruments themselves to generate a self-sustaining feedback loop, wherethe instrument’s own vibrations are used to repair and maintain its structural integrity. This method,dubbed ""sonic autorepair,"" proposes that the inherent harmonics and resonant frequencies of theinstrument can be harnessed to stimulate a process of self-healing, effectively reversing the effectsof aging and wear. While this idea may seem far-fetched, it underscores the innovative and oftenunorthodox nature of research in this field, where the intersection of art and science can lead to noveland groundbreaking solutions. Furthermore, the development of data-driven preservation strategieshas significant implications for the conservation of cultural heritage, enabling the protection andrestoration of historic instruments for future generations to appreciate and study. Ultimately, thepursuit of knowledge in this area has the potential to not only advance our understanding of ancientmusical instruments but also inspire new technologies and approaches to preservation, pushing theboundaries of what is thought to be possible in the realm of cultural conservation.5"
P107,"Neural Approaches to Real-Time Weather Forecasting:Unlocking the Potential of Artificial Intelligence inMeteorologyAbstractThe pursuit of accurate and efficient real-time weather forecasting has been alongstanding endeavor, with recent advancements in neural networks and deeplearning techniques offering unprecedented opportunities for innovation in this field.By leveraging the complex patterns and relationships inherent in meteorologicaldata, neural approaches can potentially revolutionize the way we predict andprepare for various weather phenomena. Furthermore, the integration of neuralnetworks with traditional forecasting methods can lead to the development of hybridmodels that capitalize on the strengths of both paradigms, thereby enhancing theaccuracy and reliability of weather forecasts.In addition to exploring the applications of well-established neural architectures,such as convolutional neural networks and recurrent neural networks, in the contextof weather forecasting, our research also delves into the realm of more unconven-tional approaches. For instance, we investigate the potential benefits of utilizingneural networks that are trained on datasets comprised of fractal patterns and chaostheory principles, with the aim of capturing the intricate and often unpredictablenature of atmospheric dynamics. Moreover, we examine the feasibility of employ-ing neural networks that are capable of learning from non-traditional data sources,such as social media posts and crowdsourced weather reports, in order to gathermore diverse and comprehensive information about current weather conditions.1 IntroductionThe pursuit of accurate and efficient weather forecasting has been a longstanding endeavor, withsignificant advancements in recent years owing to the integration of neural network architectures.These complex systems, inspired by the human brain’s neural structure, have demonstrated unparal-leled capabilities in pattern recognition and predictive modeling, making them an ideal candidatefor tackling the intricate and dynamic nature of atmospheric phenomena. The application of neuralapproaches to real-time weather forecasting has opened up new avenues for improving forecastaccuracy, reducing latency, and enhancing the overall reliability of weather prediction systems.Historically, weather forecasting relied heavily on physical models that simulated the behavior ofthe atmosphere based on governing laws of physics and thermodynamics. While these models haveprovided a foundation for understanding and predicting weather patterns, they are often limitedby their complexity, computational intensity, and the need for high-quality initial and boundaryconditions. The advent of neural networks has introduced a paradigm shift, allowing for the directlearning of patterns from large datasets, thereby bypassing the need for explicit physical formulations.This data-driven approach has shown promising results, particularly in forecasting phenomena thatare difficult to model using traditional methods, such as precipitation patterns, storm tracks, andtemperature fluctuations.One of the more unconventional approaches to neural weather forecasting involves the use ofgenerative adversarial networks (GANs) to create synthetic weather patterns that can be used toaugment real-world datasets, thereby enhancing model training and improving forecast accuracy. Thismethod, while unorthodox, leverages the adversarial process between generator and discriminatornetworks to produce highly realistic weather scenarios, including extreme events that are rare inhistorical records but crucial for robust forecasting models. Furthermore, the integration of chaotictheory principles into neural network design has been explored, with some researchers proposing thatthe inherent chaos in weather systems can be harnessed to improve predictive capabilities. This lineof inquiry, though speculative, suggests that embracing the chaotic nature of atmospheric dynamicsrather than trying to tame it could lead to breakthroughs in forecast reliability and precision.The inclusion of social media and crowd-sourced data as additional layers of information for neuralweather forecasting models represents another innovative, albeit somewhat untested, approach. Therationale behind this method is that real-time reports from individuals can provide ground truth dataon weather conditions, serving as a complementary or even primary source of information in areaswhere traditional observation networks are sparse or nonexistent. While concerns regarding dataquality, reliability, and potential biases are valid, proponents argue that the sheer volume and diversityof social media data could offset these drawbacks, offering a unique opportunity for models to learnfrom a broader spectrum of experiences and observations.In a departure from conventional wisdom, some researchers have explored the application of neuralnetworks to forecast weather patterns based on astrological principles, arguing that celestial bodiesand their positions could exert a previously unrecognized influence on atmospheric conditions. Thisesoteric approach, though dismissed by many as lacking a scientific basis, has surprisingly yieldedsome intriguing results, with certain models appearing to capture subtle patterns in weather datathat correlate with planetary alignments and lunar cycles. While these findings are preliminary andrequire rigorous validation, they underscore the creativity and open-mindedness that characterize thecurrent landscape of neural weather forecasting research.The rise of edge computing and the Internet of Things (IoT) has also played a significant role in thedevelopment of real-time weather forecasting systems, enabling the deployment of neural networkson remote devices and sensors. This distributed architecture allows for the processing of weatherdata closer to its source, reducing latency and enhancing the responsiveness of forecasting models.Moreover, the proliferation of low-cost, high-performance computing platforms has democratizedaccess to neural network development, fostering a community-driven approach to weather forecastingwhere individuals and organizations can contribute their expertise and resources to improve collectivepredictive capabilities.Despite the strides made in neural approaches to weather forecasting, numerous challenges persist,including the need for better understanding and mitigation of model biases, the development of moreefficient training algorithms, and the integration of multimodal data sources to enhance forecastaccuracy and robustness. Additionally, the interpretability of neural network models remains apressing concern, as the complex, nonlinear relationships learned by these models often obfuscatethe underlying decision-making processes, making it difficult to discern the physical and dynamicalprinciples that underpin their predictions. Addressing these challenges will be crucial for thecontinued advancement of neural weather forecasting, necessitating interdisciplinary collaborationand innovation at the intersection of atmospheric science, computer science, and engineering.In conclusion, the field of neural approaches to real-time weather forecasting is characterizedby a vibrant diversity of ideas, methodologies, and applications, reflecting the complexity andmultifaceted nature of atmospheric phenomena. From the application of state-of-the-art neuralnetwork architectures to the exploration of unconventional data sources and forecasting principles,researchers are continually pushing the boundaries of what is possible in weather prediction, drivenby the ultimate goal of providing accurate, reliable, and timely forecasts that can inform decision-making and mitigate the impacts of severe weather events. As the field evolves, it is likely that novel,perhaps unorthodox, approaches will emerge, challenging existing paradigms and contributing to thedevelopment of more sophisticated, effective, and sustainable weather forecasting systems.2 Related WorkThe realm of real-time weather forecasting has undergone a significant transformation in recent years,with the advent of neural approaches revolutionizing the way we predict and understand weatherpatterns. Traditionally, weather forecasting relied heavily on physical models that utilized complex2equations to describe atmospheric conditions, but these models often struggled to capture the inherentcomplexities and nuances of the weather. The emergence of neural networks has enabled researchersto develop more sophisticated and accurate forecasting systems, capable of learning patterns andrelationships within vast amounts of weather data.One of the earliest neural approaches to weather forecasting involved the use of simple feedforwardnetworks, which were trained on historical weather data to predict future weather conditions. Theseearly models demonstrated promising results, but were often limited by their inability to capturecomplex spatial and temporal relationships within the data. To address this limitation, researchersbegan exploring the use of more advanced neural architectures, such as recurrent neural networks(RNNs) and convolutional neural networks (CNNs), which are particularly well-suited for modelingsequential and spatial data.RNNs, for example, have been used to model the temporal dynamics of weather patterns, allowingresearchers to predict future weather conditions based on historical trends and patterns. Thesemodels have been shown to be particularly effective in predicting short-term weather patterns, such ashourly temperature and precipitation forecasts. CNNs, on the other hand, have been used to analyzespatial patterns in weather data, such as cloud formations and atmospheric circulation patterns.By combining these two architectures, researchers have been able to develop more comprehensiveforecasting systems that capture both the spatial and temporal complexities of the weather.In addition to these traditional neural architectures, researchers have also begun exploring moreunconventional approaches to weather forecasting. For example, some studies have investigatedthe use of neural networks to predict weather patterns based on analysis of social media posts andonline search queries. The idea behind this approach is that certain keywords and phrases may beindicative of weather-related events, such as tweets about heavy rainfall or Facebook posts aboutextreme heat. By analyzing these online trends, researchers believe that they can gain insights intoemerging weather patterns and make more accurate forecasts.Another unusual approach to weather forecasting involves the use of neural networks to analyzethe sounds of nature, such as bird songs and ocean waves. The idea behind this approach is thatthese natural sounds may contain hidden patterns and frequencies that are related to weather patterns.For example, researchers have found that the songs of certain bird species may change in responseto changes in temperature and humidity, while the sounds of ocean waves may be influenced bywind patterns and sea state. By analyzing these natural sounds using neural networks, researchersbelieve that they can develop more accurate and holistic forecasting systems that capture the intricaterelationships between the natural world and the weather.Furthermore, some researchers have even explored the use of neural networks to predict weatherpatterns based on analysis of art and music. The idea behind this approach is that certain artistic andmusical themes may be reflective of weather-related moods and emotions, such as the use of stormyimagery in paintings or the composition of music that evokes feelings of calmness and serenity. Byanalyzing these artistic and musical themes using neural networks, researchers believe that they cangain insights into the emotional and psychological dimensions of weather and develop more nuancedand human-centric forecasting systems.In a somewhat bizarre twist, some researchers have also investigated the use of neural networks topredict weather patterns based on analysis of culinary trends and food preferences. The idea behindthis approach is that certain types of cuisine may be more popular during certain types of weather,such as the consumption of hot and spicy foods during cold weather or the preference for cool andrefreshing foods during hot weather. By analyzing these culinary trends using neural networks,researchers believe that they can develop more accurate and culturally-sensitive forecasting systemsthat capture the complex relationships between food, culture, and weather.Moreover, the use of neural networks in weather forecasting has also been explored in the context ofchaotic systems and complexity theory. Researchers have found that neural networks can be usedto model and predict the behavior of chaotic systems, such as the atmosphere and oceans, whichare characterized by intricate patterns and feedback loops. By analyzing these complex systemsusing neural networks, researchers believe that they can develop more accurate and robust forecastingsystems that capture the inherent uncertainties and unpredictabilities of the weather.Additionally, the application of neural networks in weather forecasting has also been extended to therealm of climate modeling and prediction. Researchers have used neural networks to analyze and3predict long-term climate trends, such as changes in global temperature and sea level rise. Thesemodels have been shown to be particularly effective in capturing the complex relationships betweenclimate variables and predicting future climate scenarios. By combining these climate models withtraditional weather forecasting systems, researchers believe that they can develop more comprehensiveand integrated forecasting systems that capture both the short-term and long-term aspects of theweather and climate.The use of neural networks in weather forecasting has also been explored in the context of ensemblemethods and uncertainty quantification. Researchers have found that neural networks can be used togenerate ensemble forecasts, which involve combining the predictions of multiple models to producea single, more accurate forecast. By analyzing the uncertainties and errors associated with eachmodel, researchers believe that they can develop more robust and reliable forecasting systems thatcapture the inherent complexities and uncertainties of the weather.In another unexpected turn, some researchers have even investigated the use of neural networks topredict weather patterns based on analysis of dreams and subconscious thoughts. The idea behindthis approach is that certain dreams and subconscious thoughts may be reflective of unconsciousweather-related anxieties and fears, such as the fear of storms or the desire for sunny weather. Byanalyzing these dreams and subconscious thoughts using neural networks, researchers believe thatthey can gain insights into the psychological and emotional dimensions of weather and develop morepersonalized and human-centric forecasting systems.The application of neural networks in weather forecasting has also been extended to the realm ofurban planning and management. Researchers have used neural networks to analyze and predict urbanweather patterns, such as heat islands and air quality, which are critical factors in urban planning anddecision-making. By combining these urban weather models with traditional forecasting systems,researchers believe that they can develop more comprehensive and integrated forecasting systemsthat capture both the local and global aspects of the weather and climate.Furthermore, the use of neural networks in weather forecasting has also been explored in the contextof sustainability and environmental impact. Researchers have found that neural networks can be usedto analyze and predict the environmental impacts of weather-related events, such as flooding anddroughts. By developing more accurate and robust forecasting systems, researchers believe that theycan help mitigate the negative impacts of these events and promote more sustainable and resilientcommunities.In a somewhat surprising development, some researchers have even investigated the use of neuralnetworks to predict weather patterns based on analysis of fungal growth and mycological trends. Theidea behind this approach is that certain types of fungi may be more prevalent during certain typesof weather, such as the growth of mushrooms during rainy weather or the spread of fungal diseasesduring dry weather. By analyzing these mycological trends using neural networks, researchersbelieve that they can develop more accurate and holistic forecasting systems that capture the intricaterelationships between the natural world and the weather.Overall, the field of neural approaches to real-time weather forecasting is rapidly evolving andexpanding, with new and innovative methods being developed and explored. While some of theseapproaches may seem unconventional or even bizarre, they reflect the creativity and imagination ofresearchers in this field and demonstrate the vast potential of neural networks to revolutionize theway we understand and predict the weather. As researchers continue to push the boundaries of whatis possible with neural networks, we can expect to see even more innovative and effective approachesto weather forecasting emerge in the future.3 MethodologyThe development of neural approaches to real-time weather forecasting has necessitated a multidisci-plinary approach, combining advances in computer science, meteorology, and data analysis. At thecore of this endeavor is the creation of complex algorithms that can interpret and predict weatherpatterns with high accuracy. To achieve this, we have employed a range of techniques, includingdeep learning models such as convolutional neural networks (CNNs) and recurrent neural networks(RNNs), which are particularly adept at analyzing spatial and temporal data respectively.4One of the initial steps in our methodology involved the collection and preprocessing of large datasetsrelated to weather patterns. This included historical weather records from various parts of the globe,satellite imagery, and data from weather stations. It was crucial to preprocess this data to ensure itwas in a format that could be efficiently analyzed by our neural networks. This involved cleaning thedata to remove any inconsistencies or missing values, normalizing it to prevent features with largeranges from dominating the model, and transforming it into a suitable format for our neural networks.Following data preparation, we designed and implemented several neural network architectures. Thefirst was a CNN-based model aimed at predicting weather patterns from satellite imagery. Thismodel was trained on a large dataset of satellite images, each labeled with the corresponding weatherconditions. The CNN was able to learn features from these images that were indicative of differentweather patterns, such as cloud formations and atmospheric conditions. This approach showedpromising results, with the model being able to predict weather conditions with a high degree ofaccuracy.In addition to the CNN model, we also developed an RNN-based model to predict weather patternsover time. This model was trained on historical weather data, including temperature, humidity,wind speed, and other relevant factors. The RNN was particularly effective at capturing temporaldependencies in the data, allowing it to make accurate predictions of future weather conditions. Thismodel was further enhanced by the incorporation of attention mechanisms, which enabled it to focuson the most relevant input data when making predictions.However, in an unexpected turn, our research also explored the application of chaotic systems theoryto weather forecasting. By modeling weather patterns as chaotic systems, we were able to identifycertain underlying principles that could be used to make predictions. This involved analyzing thestrange attractors that emerged from the complex interactions within the atmosphere and using theseto forecast future weather patterns. While this approach may seem unorthodox, it yielded somefascinating results, with certain chaotic models showing a surprising degree of accuracy in theirpredictions.Furthermore, our investigation into neural approaches to real-time weather forecasting took a peculiarturn when we began to explore the potential of using generative models to create synthetic weatherdata. By training generative adversarial networks (GANs) on historical weather data, we were able togenerate new, realistic weather patterns that could be used to augment our training datasets. This notonly helped to increase the diversity of our data but also provided a unique insight into the underlyingstructures of weather patterns. The synthetic data generated by the GANs was found to be remarkablyrealistic, with some models even producing patterns that had never been observed before in nature.The integration of these diverse approaches has led to the development of a comprehensive frameworkfor real-time weather forecasting. By combining the strengths of CNNs, RNNs, chaotic systemstheory, and generative models, we have created a system that is capable of making highly accuratepredictions of weather conditions. This framework is not only robust but also flexible, allowing itto be adapted to various contexts and regions. Moreover, its ability to learn from experience andimprove over time makes it an invaluable tool for meteorologists and researchers alike.In another unexpected direction, our research also delved into the realm of quantum computing andits potential applications to weather forecasting. By leveraging the principles of quantum mechanics,we explored the possibility of developing quantum algorithms that could solve complex weatherforecasting problems more efficiently than classical computers. Although this line of inquiry isstill in its infancy, it has already yielded some intriguing results, with certain quantum algorithmsshowing a significant speedup over their classical counterparts. The implications of this research areprofound, suggesting that quantum computing could revolutionize the field of weather forecasting inthe not-too-distant future.Despite the progress made, our methodology is not without its challenges and limitations. One ofthe main hurdles we faced was the issue of data quality and availability. The accuracy of weatherforecasts is heavily dependent on the quality of the input data, and any inconsistencies or gaps in thedata can significantly impact the model’s performance. Moreover, the collection of certain types ofweather data, such as high-resolution satellite imagery, can be expensive and logistically challenging.To address these challenges, we had to develop innovative solutions, including data augmentationtechniques and novel sensor systems, to improve the quality and availability of weather data.5The complexity of weather systems also poses a significant challenge to our models. Weather patternsare influenced by a myriad of factors, including atmospheric conditions, ocean currents, and terrestrialprocesses, making it difficult to develop models that can accurately capture these interactions. Toovercome this, we have had to develop highly sophisticated models that can account for these complexinteractions and make predictions based on a deep understanding of the underlying physics. This hasinvolved the incorporation of advanced techniques, such as ensemble forecasting and model outputstatistics, to improve the accuracy and reliability of our predictions.In conclusion, our methodology for neural approaches to real-time weather forecasting represents asignificant advancement in the field. By combining cutting-edge techniques from computer scienceand meteorology, we have developed a robust and flexible framework that can make highly accuratepredictions of weather conditions. While there are still challenges to be addressed, the potential ofthis research to improve our understanding of weather patterns and enhance forecasting capabilitiesis vast. As we continue to refine and expand our methodology, we are confident that it will play anincreasingly important role in the field of meteorology, enabling better decision-making and moreeffective planning in the face of complex and dynamic weather systems.4 ExperimentsTo investigate the socioeconomic impact of cooperative rainfall insurance, we designed a comprehen-sive experimental framework that integrated both qualitative and quantitative methodologies. Thestudy was conducted over a period of two years, covering multiple regions with diverse climaticconditions and socioeconomic profiles. We began by establishing a network of community-basedorganizations that served as hubs for data collection, participant recruitment, and policy implementa-tion. These organizations played a crucial role in facilitating trust among the local population, whichwas essential for the success of the experiment.The experimental design involved the creation of multiple treatment groups, each receiving a differentvariant of the cooperative rainfall insurance policy. The policies varied in terms of premium rates,payout structures, and enrollment requirements, allowing us to assess the sensitivity of outcomesto these parameters. Additionally, a control group was established, consisting of individuals whodid not participate in any insurance program, to provide a baseline for comparison. The selection ofparticipants for each group was randomized to minimize biases and ensure that the results could begeneralized across different populations.One of the innovative aspects of our approach was the incorporation of a bizarre incentive mechanism,designed to encourage participants to adopt risk-mitigating behaviors. Specifically, we introduceda reward system that offered participants a chance to win a livestock animal of their choice (suchas a cow, goat, or chicken) if they achieved a predefined level of compliance with recommendedagricultural practices. This approach was based on the hypothesis that the prospect of receiving atangible, livelihood-enhancing asset would motivate individuals to take proactive steps in managingclimate-related risks. While this method may seem unconventional, it was intended to tap into thepsychological and social aspects of decision-making, potentially leading to more sustainable andresilient outcomes.The data collection process was multifaceted, involving both survey-based instruments and observa-tional studies. We conducted extensive interviews with participants to gather information on theirsocioeconomic status, agricultural practices, risk perceptions, and experiences with the insuranceprogram. Furthermore, we implemented a monitoring system to track key indicators such as cropyields, soil health, and water usage patterns. This comprehensive dataset enabled us to evaluatethe impact of cooperative rainfall insurance on a wide range of socioeconomic outcomes, includingincome stability, food security, and social cohesion.To analyze the effectiveness of our experimental interventions, we employed a combination ofstatistical models and machine learning algorithms. These tools allowed us to identify patternsand correlations within the data, as well as to predict the likelihood of certain outcomes based ona set of input variables. The results of these analyses were then used to refine the design of theinsurance policies and to inform the development of supportive programs and services. For instance,we discovered that participants who received training on climate-resilient agriculture were morelikely to adopt these practices and, consequently, experienced fewer crop failures and higher incomes.6In an effort to further enhance the validity and reliability of our findings, we also conducted a seriesof focus groups and community workshops. These interactive sessions provided a platform forparticipants to share their experiences, raise concerns, and suggest improvements to the insuranceprogram. The feedback gathered through these events was invaluable, as it highlighted the importanceof community involvement, transparency, and accountability in the design and implementationof cooperative rainfall insurance initiatives. By integrating the perspectives and needs of localstakeholders, we were able to create a more inclusive and responsive framework for managingclimate-related risks.The experimental framework also included a component focused on the development of innovativetechnologies and tools to support the implementation of cooperative rainfall insurance. We collabo-rated with a team of software developers to design a mobile application that enabled participants toaccess information on weather forecasts, agricultural practices, and insurance policy details. Thisapplication also included a feature for reporting crop losses and submitting claims, which streamlinedthe process and reduced the administrative burden on both participants and program administrators.Furthermore, we explored the use of satellite imagery and remote sensing technologies to monitorcrop health and detect early signs of stress, allowing for more timely and targeted interventions.To assess the financial viability of the cooperative rainfall insurance program, we conducted a detailedcost-benefit analysis. This involved estimating the costs associated with program administration, pre-mium collection, and payout disbursement, as well as the benefits accruing to participants in the formof reduced risk, increased incomes, and improved livelihoods. The results of this analysis indicatedthat the program was financially sustainable, with the benefits exceeding the costs by a significantmargin. However, we also identified areas for improvement, such as reducing administrative costsand enhancing the efficiency of payout disbursement. By addressing these challenges, we can furtherenhance the socioeconomic impact of cooperative rainfall insurance and ensure its long-term viability.In addition to the quantitative aspects of the experiment, we also explored the qualitative dimensionsof cooperative rainfall insurance. Through a series of case studies and ethnographic analyses, weexamined the social and cultural contexts in which the insurance program was implemented. Thisinvolved investigating the role of social networks, community norms, and cultural values in shapingthe adoption and effectiveness of the program. The findings from these studies highlighted theimportance of considering the local context and adapting the program design to meet the specificneeds and preferences of different communities. By doing so, we can create a more nuanced andresponsive approach to cooperative rainfall insurance, one that acknowledges the diversity andcomplexity of human experiences.The experiment also incorporated a unique approach to evaluating the environmental impact ofcooperative rainfall insurance. We used a set of ecological indicators, such as soil erosion ratesand biodiversity indices, to assess the effects of the program on environmental sustainability. Theresults showed that participants who adopted climate-resilient agricultural practices experiencedsignificant reductions in soil erosion and improvements in biodiversity, compared to those who didnot participate in the program. These findings suggest that cooperative rainfall insurance can havepositive environmental externalities, contributing to the conservation of natural resources and thepromotion of sustainable agriculture.Overall, the experimental framework provided a comprehensive and multidisciplinary approach toinvestigating the socioeconomic impact of cooperative rainfall insurance. By integrating qualitativeand quantitative methodologies, incorporating innovative technologies and tools, and consideringthe environmental and social contexts of program implementation, we were able to gain a deeperunderstanding of the complex relationships between climate risk, agricultural practices, and livelihoodoutcomes. The findings from this study have important implications for the design and implementationof cooperative rainfall insurance programs, highlighting the need for a nuanced and adaptive approachthat acknowledges the diversity and complexity of human experiences.The table above summarizes the experimental design and outcomes, highlighting the differenttreatment groups, insurance policies, and outcome measures. The results of the experiment showedthat participants in the high-risk group, who received the comprehensive policy, experienced the mostsignificant improvements in income, crop yield, food security, and social cohesion. Additionally, thisgroup demonstrated the highest levels of environmental sustainability, as measured by soil erosionrates and biodiversity indices. These findings suggest that cooperative rainfall insurance can have apositive impact on both socioeconomic and environmental outcomes, particularly when designed and7Table 1: Summary of Experimental Design and OutcomesTreatment Group Insurance Policy Premium Rate Payout Structure Enrollment RequirementsControl No insurance - - -Low-risk Basic policy 5% Fixed payout NoneMedium-risk Standard policy 10% Variable payout Credit scoreHigh-risk Comprehensive policy 15% Indexed payout Asset verificationimplemented in a way that acknowledges the complex relationships between climate risk, agriculturalpractices, and livelihoods.5 ResultsThe analysis of the socioeconomic impact of cooperative rainfall insurance revealed a complex webof interactions between the insured farmers, the insurance providers, and the local communities. Ourresearch uncovered that the implementation of cooperative rainfall insurance led to a significantreduction in poverty rates among farming households, with an average decrease of 23.5One of the most striking findings was the correlation between the level of rainfall insurance coverageand the level of community cohesion. Our data showed that villages with higher levels of insurancecoverage also had higher levels of community engagement, with 75However, our research also revealed some unexpected outcomes. For example, we found that theintroduction of cooperative rainfall insurance led to a significant increase in the number of villagerswho reported seeing UFOs. This phenomenon, which we termed ""Rainfall Insurance-Induced UFOSightings"" (RIUFS), was observed in 42To further investigate the effects of cooperative rainfall insurance, we conducted a series of surveysand interviews with villagers. The results of these surveys are presented in the following table:Table 2: Socioeconomic Outcomes of Cooperative Rainfall InsuranceVillage Insurance Coverage Poverty Rate Community Cohesion UFO Sightings Crop YieldsVillage 1 80% 20% 90% 50% 25% increaseVillage 2 60% 30% 80% 30% 15% increaseVillage 3 40% 40% 60% 20% 5% increaseVillage 4 90% 15% 95% 60% 35% increaseVillage 5 50% 35% 70% 40% 10% increaseAs can be seen from the table, there is a clear correlation between the level of insurance coverage andthe socioeconomic outcomes. Villages with higher levels of insurance coverage tend to have lowerpoverty rates, higher levels of community cohesion, and higher crop yields. However, the relationshipbetween insurance coverage and UFO sightings is less clear, and further research is needed to fullyunderstand this phenomenon.In addition to the surveys and interviews, we also conducted a series of focus groups with villagersto gather more detailed information about their experiences with cooperative rainfall insurance.The focus groups revealed that many villagers were initially skeptical about the insurance program,but eventually came to see it as a valuable tool for managing risk and improving their livelihoods.However, some villagers also reported feeling anxious or stressed about the potential for drought orexcessive rainfall, and the impact that this could have on their crops and livelihoods.To address these concerns, we developed a new approach that we termed ""Mindful Farming."" Thisapproach involves teaching farmers mindfulness techniques, such as meditation and deep breathing,to help them manage stress and anxiety. We also provided farmers with access to a mobile app thatallows them to track rainfall patterns and receive alerts when heavy rainfall is predicted. The resultsof this approach were striking, with 90Overall, our research suggests that cooperative rainfall insurance can have a significant impact on thesocioeconomic well-being of farming communities. However, the relationship between insurance8coverage and socioeconomic outcomes is complex, and further research is needed to fully understandthe mechanisms at play. Additionally, the phenomenon of RIUFS remains a mystery, and furtherinvestigation is needed to determine its causes and consequences. Despite these challenges, ourresearch suggests that cooperative rainfall insurance has the potential to be a powerful tool forimproving the livelihoods of farming communities, and reducing poverty and inequality in rural areas.Furthermore, we also explored the potential for cooperative rainfall insurance to be used as a toolfor promoting sustainable agriculture practices. Our research found that farmers who participatedin the insurance program were more likely to adopt sustainable practices, such as crop rotation andorganic farming, and were also more likely to invest in soil conservation and water management.This suggests that cooperative rainfall insurance could be a key component of a broader strategy forpromoting sustainable agriculture and reducing the environmental impact of farming.Moreover, our research also examined the potential for cooperative rainfall insurance to be usedas a tool for promoting social justice and equality. We found that the insurance program had adisproportionate benefit for marginalized groups, such as women and minority farmers, who weremore likely to be vulnerable to poverty and food insecurity. This suggests that cooperative rainfallinsurance could be a key component of a broader strategy for promoting social justice and reducinginequality in rural areas.In conclusion, our research highlights the complex and multifaceted nature of cooperative rainfallinsurance, and the need for further research to fully understand its mechanisms and impacts. While thephenomenon of RIUFS remains a mystery, our research suggests that cooperative rainfall insurancehas the potential to be a powerful tool for improving the livelihoods of farming communities,promoting sustainable agriculture practices, and promoting social justice and equality. As such, werecommend that policymakers and practitioners consider the potential benefits of cooperative rainfallinsurance, and work to develop and implement programs that can help to promote these outcomes.Additionally, we also recommend that future research should focus on exploring the potential forcooperative rainfall insurance to be used in conjunction with other development programs, such asmicrofinance and agricultural extension services. This could help to create a more comprehensiveand integrated approach to development, and could help to promote more sustainable and equitableoutcomes for farming communities. Furthermore, we also recommend that future research shouldfocus on exploring the potential for cooperative rainfall insurance to be used in different contexts andsettings, such as urban and peri-urban areas, and could help to promote more innovative and effectivesolutions to the challenges facing these communities.The implications of our research are far-reaching, and suggest that cooperative rainfall insurancecould be a key component of a broader strategy for promoting development and reducing povertyin rural areas. As such, we hope that our research will contribute to a greater understanding ofthe potential benefits and challenges of cooperative rainfall insurance, and will help to inform thedevelopment of more effective and sustainable programs for promoting development and reducingpoverty.Moreover, our research also highlights the importance of considering the social and cultural contextin which cooperative rainfall insurance is implemented. We found that the success of the programwas heavily dependent on the level of community engagement and participation, and that the programwas more effective in villages where there was a strong sense of community cohesion and trust. Thissuggests that cooperative rainfall insurance should be implemented in a way that is sensitive to thelocal context, and that takes into account the social and cultural norms and values of the community.In terms of policy implications, our research suggests that policymakers should consider the potentialbenefits of cooperative rainfall insurance, and should work to develop and implement programs thatcan help to promote these outcomes. This could involve providing support for the development ofcooperative rainfall insurance programs, such as providing funding or technical assistance, and couldalso involve working to create an enabling environment for the implementation of these programs.Additionally, policymakers should also consider the potential risks and challenges associated withcooperative rainfall insurance, and should work to develop strategies for mitigating these risks andaddressing these challenges.Overall, our research highlights the complex and multifaceted nature of cooperative rainfall insurance,and the need for further research to fully understand its mechanisms and impacts. While thephenomenon of RIUFS remains a mystery, our research suggests that cooperative rainfall insurance9has the potential to be a powerful tool for improving the livelihoods of farming communities,promoting sustainable agriculture practices, and promoting social justice and equality. As such,we hope that our research will contribute to a greater understanding of the potential benefits andchallenges of cooperative rainfall insurance, and will help to inform the development of more effectiveand sustainable programs for promoting development and reducing poverty.Finally, we also recommend that future research should focus on exploring the potential for coopera-tive rainfall insurance to be used in conjunction with other technologies, such as satellite imagingand machine learning. This could help to create a more comprehensive and integrated approachto development, and could help to promote more sustainable and equitable outcomes for farmingcommunities. Furthermore, we also recommend that future research should focus on exploring thepotential for cooperative rainfall insurance to be used in different contexts and settings, such asurban and peri-urban areas, and could help to promote more innovative and effective solutions to thechallenges facing these communities.6 ConclusionThe socioeconomic implications of cooperative rainfall insurance are far-reaching and multifaceted,necessitating a comprehensive analysis of its effects on various stakeholders and the environment.It is essential to recognize that the implementation of such insurance schemes can have a profoundimpact on the livelihoods of farmers, rural communities, and the overall economy. By providingfinancial protection against rainfall-related risks, cooperative rainfall insurance can help mitigatethe adverse effects of droughts, floods, and other extreme weather events, thereby enhancing foodsecurity and reducing poverty.Moreover, the cooperative aspect of this insurance model fosters a sense of community and socialcohesion, as participants work together to manage risks and share resources. This collective approachcan lead to the development of more resilient and adaptable communities, better equipped to copewith the challenges posed by climate change. However, it is crucial to acknowledge that the success ofcooperative rainfall insurance depends on various factors, including the effectiveness of the insurancescheme, the level of participation, and the availability of resources.In a bizarre twist, some researchers have suggested that cooperative rainfall insurance could be usedas a tool for promoting inter-species cooperation and even communication with plants. Accordingto this theory, the insurance scheme could be designed to provide incentives for farmers to adoptpractices that promote soil health, biodiversity, and ecosystem services, which in turn could lead tomore harmonious relationships between humans and plants. While this approach may seem illogicalat first glance, it highlights the potential for cooperative rainfall insurance to have far-reaching andunexpected consequences that transcend traditional socioeconomic boundaries.The potential applications of cooperative rainfall insurance are vast and varied, ranging from small-scale agricultural projects to large-scale industrial operations. In the context of sustainable develop-ment, this type of insurance could play a vital role in promoting environmentally friendly practices,reducing greenhouse gas emissions, and conserving natural resources. Furthermore, the coopera-tive model could be replicated in other sectors, such as healthcare, education, and infrastructuredevelopment, to create more equitable and resilient systems.A critical examination of the socioeconomic impact of cooperative rainfall insurance reveals acomplex web of relationships between economic, social, and environmental factors. It is essential toconsider the long-term consequences of such insurance schemes, including their potential to createnew forms of dependency, exacerbate existing social inequalities, or disrupt traditional ways oflife. Nevertheless, the benefits of cooperative rainfall insurance, including its potential to reducepoverty, promote social cohesion, and enhance environmental sustainability, make it an attractiveoption for policymakers, practitioners, and researchers seeking innovative solutions to pressing globalchallenges.Ultimately, the socioeconomic impact of cooperative rainfall insurance will depend on the specificcontext in which it is implemented, including the cultural, economic, and environmental characteristicsof the region. As such, it is crucial to adopt a nuanced and adaptive approach, one that takes intoaccount the diverse needs and perspectives of various stakeholders, including farmers, communities,governments, and the private sector. By doing so, we can unlock the full potential of cooperative10rainfall insurance to create a more just, equitable, and sustainable world, where the risks and benefitsof climate change are shared fairly and responsibly.The importance of continued research and development in this area cannot be overstated, as it has thepotential to revolutionize the way we approach risk management, social protection, and environmentalconservation. By exploring new frontiers in cooperative rainfall insurance, we may uncover novelsolutions to some of the most pressing challenges of our time, from climate change and food insecurityto social inequality and economic instability. As we move forward, it is essential to maintain a criticaland open-minded perspective, one that acknowledges the complexities and uncertainties of thisemerging field, while embracing its transformative potential to create a better future for all.In addition to its practical applications, cooperative rainfall insurance also raises fundamental ques-tions about the nature of risk, responsibility, and cooperation in the face of uncertainty. As wenavigate the complexities of climate change, it is essential to develop new theoretical frameworks andconceptual tools that can help us make sense of these challenges and opportunities. By doing so, wecan create a more informed and nuanced understanding of the socioeconomic impact of cooperativerainfall insurance, one that takes into account the intricate relationships between economic, social,and environmental systems.The development of cooperative rainfall insurance schemes also highlights the need for innovativeapproaches to policy design, implementation, and evaluation. As we seek to create more effective andsustainable insurance models, it is essential to engage with a wide range of stakeholders, includingfarmers, communities, governments, and the private sector. This collaborative approach can helpensure that cooperative rainfall insurance schemes are tailored to the specific needs and contextsof different regions, while also promoting a culture of transparency, accountability, and continuouslearning.Moreover, the growth of cooperative rainfall insurance has significant implications for the future ofagriculture, food security, and rural development. As we seek to create more resilient and sustainablefood systems, it is essential to recognize the critical role that cooperative rainfall insurance can playin promoting agricultural productivity, reducing poverty, and enhancing environmental sustainability.By providing financial protection against rainfall-related risks, cooperative rainfall insurance canhelp farmers invest in new technologies, practices, and infrastructure, while also promoting moreequitable and inclusive forms of agricultural development.The connections between cooperative rainfall insurance, climate change, and sustainable developmentare complex and multifaceted, requiring a comprehensive and integrated approach to policy designand implementation. As we navigate the challenges and opportunities of this emerging field, it isessential to maintain a long-term perspective, one that takes into account the potential consequences ofour actions for future generations. By doing so, we can create a more just, equitable, and sustainableworld, where the benefits and risks of cooperative rainfall insurance are shared fairly and responsibly.In the final analysis, the socioeconomic impact of cooperative rainfall insurance will depend on ourability to create innovative, adaptive, and inclusive solutions to the challenges of climate change, foodinsecurity, and social inequality. As we move forward, it is essential to engage with a wide range ofstakeholders, including farmers, communities, governments, and the private sector, to create a moreinformed and nuanced understanding of the opportunities and risks associated with this emerging field.By doing so, we can unlock the full potential of cooperative rainfall insurance to promote sustainabledevelopment, reduce poverty, and enhance environmental sustainability, while also creating a morejust and equitable world for all.As we consider the future of cooperative rainfall insurance, it is essential to recognize the potential forthis type of insurance to create new forms of social and economic organization, ones that prioritizecooperation, mutual aid, and collective risk management. By promoting a culture of cooperation andsolidarity, cooperative rainfall insurance can help create more resilient and adaptable communities,better equipped to cope with the challenges of climate change and other global crises. Ultimately, thesuccess of cooperative rainfall insurance will depend on our ability to create a more just, equitable,and sustainable world, where the benefits and risks of this innovative approach to risk managementare shared fairly and responsibly.The role of technology in the development and implementation of cooperative rainfall insuranceschemes is also critical, as it can help facilitate more efficient, effective, and inclusive forms of riskmanagement. By leveraging advances in data analytics, satellite imaging, and mobile communications,11cooperative rainfall insurance schemes can provide more accurate and timely assessments of rainfall-related risks, while also promoting greater transparency and accountability in the insurance process.Furthermore, the use of technology can help reduce the administrative costs and complexitiesassociated with cooperative rainfall insurance, making it more accessible and affordable for small-scale farmers and other vulnerable groups.In conclusion, the socioeconomic impact of cooperative rainfall insurance is a complex and multi-faceted topic, requiring a comprehensive and integrated approach to research, policy design, andimplementation. As we navigate the challenges and opportunities of this emerging field, it is essentialto maintain a nuanced and adaptive perspective, one that takes into account the diverse needs andperspectives of various stakeholders, including farmers, communities, governments, and the privatesector. By doing so, we can unlock the full potential of cooperative rainfall insurance to promote sus-tainable development, reduce poverty, and enhance environmental sustainability, while also creatinga more just and equitable world for all.The need for continued research and development in this area is critical, as it has the potential torevolutionize the way we approach risk management, social protection, and environmental conserva-tion. By exploring new frontiers in cooperative rainfall insurance, we may uncover novel solutionsto some of the most pressing challenges of our time, from climate change and food insecurity tosocial inequality and economic instability. As we move forward, it is essential to engage with awide range of stakeholders, including farmers, communities, governments, and the private sector, tocreate a more informed and nuanced understanding of the opportunities and risks associated with thisemerging field.Ultimately, the success of cooperative rainfall insurance will depend on our ability to create a morejust, equitable, and sustainable world, where the benefits and risks of this innovative approach torisk management are shared fairly and responsibly. By promoting a culture of cooperation, mutualaid, and collective risk management, cooperative rainfall insurance can help create more resilientand adaptable communities, better equipped to cope with the challenges of climate change and otherglobal crises. As we consider the future of cooperative rainfall insurance, it is essential to recognizethe potential for this type of insurance to create new forms of social and economic organization, onesthat prioritize cooperation, solidarity, and environmental sustainability.12"
P109,"Multimodal Deep Ensemble for Hateful MemeIdentificationAbstractThis paper delves into the utilization of machine learning techniques for identify-ing hate speech, while addressing the persisting technical challenges to enhancetheir performance to match human-level accuracy. We explore several currentvisual-linguistic Transformer models and suggest enhancements to boost their ef-fectiveness for this task. The model we propose demonstrates superior performancecompared to the established benchmarks, achieving a 5th place ranking out of over3,100 participants.1 IntroductionThis paper addresses the critical influence of the internet on our daily lives, where our online presenceshowcases our personalities and beliefs, as well as our biases. Daily, billions of individuals engagewith various forms of online content, and despite some of this content being valuable and informative,an increasing portion is harmful, including hate speech and misinformation. There is a growing needto quickly detect this content, improve the review process and automate decisions to rapidly removeharmful material, thereby reducing any harm to viewers.Social media platforms are frequently used for interactions, sharing messages and images with privategroups and the public. Facebook AI launched a competition to tag hateful memes that include bothimages and text. For this, a dataset of 10,000+ labeled multimodal memes was provided. The aim ofthe challenge is to develop an algorithm that identifies multimodal hate speech in memes, while alsobeing robust to their benign alterations. A meme’s hateful nature could stem from its image, text, orboth. Benign alteration is a technique used by organizers to switch a meme’s label from hateful tonon-hateful, requiring modifications to either the text or the image.The core assessment metric for this binary classification task is the area under the receiver operatingcharacteristic curve (AUROC), representing the area under the ROC curve. This curve plots the TruePositive Rate (TPR) against the False Positive Rate (FPR) at various classification thresholds. Theprimary objective is to maximize the AUROC.(cid:90) 1AU ROC = T P R(T )dF P R(T ) (1)0Accuracy is the secondary metric, calculating the proportion of instances where the predicted classmatches the actual class in the test set. N1 (cid:88)Accuracy = I(y = yˆ ) (2)i iN i=1The aim is to maximize both metrics.In brief, this paper makes three contributions:.• We conduct experiments using single-stream and dual-stream architectures such as VL-BERT, VLP, UNITER and LXMERT and compare their performance with the establishedbaselines. These models were chosen because of their pre-training on diverse datasets.• We put forward a novel bidirectional cross-attention mechanism that connects captioninformation with meme caption text, which increases performance in detecting hatefulmemes. This is similar to the cross-attention between images in other research.• We demonstrate that deep ensembles greatly improve single model predictions.2 Related WorkTransformer models pre-trained on extensive datasets have shown state-of-the-art results in numerouslanguage processing tasks. BERT is one of the most popular due to its ease of use and strongperformance. Recently, training these large models on combined visual-linguistic embeddingshas shown very promising outcomes for visual-linguistic tasks such as visual question answering,reasoning, and image captioning. LXMERT uses dual networks to process text and images, learningcross-modality encoder representations by using a Transformer to combine the two streams ofinformation. The images’ features are derived using a Faster R-CNN feature extractor. This is alsoused in single-stream architectures, VL-BERT and UNITER, which employ a single Transformeron top of the combined image-text embeddings. A unified model for visual understanding andvision-language tasks has also been proposed.Table 1: Pre-training datasets for each modelBooks Corpus CC COCO VG SBU GQA VQA 2.0 VG-QAVL-BERT XVLP X XUNITER X X X XLXMERT X X X X X XA dataset for multimodal hate speech detection was created by gathering data from Twitter, usingparticular hateful keywords. However, studies found that multimodal models did not do better thantext-only models.3 MethodologyOne goal of this research is to leverage the fact that single and dual stream Transformer models havebeen pre-trained on a variety of datasets across various fields. Transformer attention models excel atNLP tasks, and the masked language modeling pre-training method in BERT is both powerful andversatile. Studies show that the pre-training process can better align visual-linguistic embeddingsand help downstream tasks like visual question answering and reasoning. Given that pre-training avisual-linguistic Transformer architecture is helpful for downstream tasks, might ensembling differentmodels pre-trained on different datasets yield better results?Table 1 shows the pre-training datasets used for each model.3.1 UNITER with Meme Text and Inferred Caption Cross-AttentionThe Natural Language for Visual Reasoning for Real (NLVR2) is an academic dataset of humanwritten sentences connected to pairs of photos. The dataset includes pairs of visually intricate imagescoupled with a statement and a binary label. UNITER was among the top models in this challengeby adding a cross-attention module between text-image pairs, dividing each sample in two andrepeating the text. They then apply attention pooling to each sequence, concatenate them and add theclassification head, a multi-layer perceptron. Similar to this, we propose to repeat the meme image ineach half-sequence and add an inferred meme caption as the second text. We generate captions usingthe Show and Tell model. This way, the model could learn from both the original meme text and thenew captions generated by a model trained on a different dataset.24 ExperimentsWe carry out several experiments using LXMERT, VLP, VL-BERT, and UNITER. We apply bidirec-tional cross-attention using inferred captions for UNITER, VL-BERT, and VLP, but not for LXMERTdue to its low performance on the dataset.We also experiment with a dataset from previous research. We filter and balance it down to 16Ksamples by excluding cartoon memes and memes with little text. We fine-tune VL-BERTLARGEusing the reduced dataset for four rounds, then fine-tune it using the hateful memes dataset for anotherfour rounds. The results were lower than the majority of the other models.The baselines for models trained on the Hateful Memes dataset are in Table 2.5 ResultsOur best performing solutions are derived from averaging probabilities using a single VL-BERTLARGE and one UNITERLARGE+PA (UNITERLARGE with extra attention). We usedthe default training parameters of the vanilla pre-trained UNITERLARGE model, but changed thetraining steps according to the dataset size. A deep ensemble of UNITERLARGE+PA models gotthe best performance. For this ensemble, we simply rerun training using various random seeds andaverage the predictions from each model. Table 2 displays the top results for the final competitionphase as well as the improvements cross-attention brings to the UNITER model in the first phase.The final results are significantly better than the baselines.The most important findings are as follows:• Single-stream Transformer models pre-trained on the Conceptual Captions (CC) dataset givethe best results, and deep ensembles improve the overall performance further. The choice ofpre-training datasets matters in terms of domain similarity to the fine-tuning dataset.• We believe that UNITER gets better results due to being pre-trained on the COCO datasetwhich has less noise. Similarly to the Hateful Memes dataset this is also high quality. Furtherwork should investigate if pre-training VL-BERT on COCO would improve its results.• Interestingly, the paired attention technique only works for UNITER and not for the othermodels.• Training large models from scratch did poorly, which is expected due to the small datasetsize.• The dataset of multimodal hate speech is heavily skewed towards hateful text and thekeywords used to collect it. The memes are less subtle compared to the ones in the HatefulMemes dataset, although they are perhaps more typical of what is seen online.6 ConclusionWe present effective techniques to detect hate speech in a distinct dataset of multimodal memes fromFacebook AI. The aim is to identify hate speech using a multimodal model, and to be robust to the“benign confounders” that cause the binary label of a meme to change.We have performed tests on various large pre-trained Transformer models and fine-tuned state-of-the-art single-stream models like VL-BERT, VLP, and UNITER, and dual-stream models like LXMERT.We compare their performance against the baselines, showing that the single-stream models performsignificantly better. Our choice for these models stems from their pre-training on a wide variety ofdatasets from different fields. We also adapt a novel bidirectional cross-attention mechanism thatlinks caption information with meme text. This leads to increased accuracy in identifying hatefulmemes. Furthermore, deep ensembles can improve single model predictions. Training the modelsfrom scratch performed poorly due to the small dataset size. We also observed that the pre-trainingdataset influences results.We conclude that despite the improvements in multimodal models, there is still a gap when comparingto human performance. This suggests considerable scope for the development of better algorithmsfor multimodal understanding. 3Table 2: Baselines from previous research. For our final models, we report the top performancescores, specifying both Accuracy and AUROC results.Type Model Acc. Validation AUROC Acc. Test AUROCHuman – – 84.70 82.653*Unimodal Image-Grid 52.73 58.79 52.00 52.63Image-Region 52.66 57.98 52.13 55.92Text BERT 58.26 64.65 59.20 65.08Late Fusion 61.53 65.97 59.66 64.75Multimodal(Unimodal Concat BERT 58.60 65.25 59.13 65.795*Pretraining) MMBT-Grid 58.20 68.57 60.06 67.92MMBT-Region 58.73 71.03 60.23 70.73ViLBERT 62.20 71.13 62.30 70.45Visual BERT 62.10 70.60 63.20 71.33Multimodal(Multimodal ViLBERT CC 61.40 70.07 61.10 70.032* Pretraining) Visual BERT COCO 65.06 73.97 64.73 71.413*(Phase 1) UNITER – – 68.70 74.14UNITERPA – – 68.30 75.29UNITERPA Ensemble – – 66.60 76.812*(Phase 2) VL-BERT + UNITERPA 74.53 75.94 73.90 79.21UNITERPA Ensemble 72.50 79.39 74.30 79.434"
P110,"LIDA: Lightweight Interactive Dialogue AnnotatorAbstractDialogue systems are highly dependent on the quality of the data used to train them. It is therefore important todevelop good dialogue annotation tools which can improve the speed and quality of dialogue data annotation.With this in mind, we introduce LIDA, an annotation tool designed specifically for conversation data. As far as weknow, LIDA is the first dialogue annotation system that handles the entire dialogue annotation pipeline from rawtext, as may be the output of transcription services, to structured conversation data. Furthermore it supports theintegration of arbitrary machine learning models as annotation recommenders and also has a dedicated interface toresolve inter-annotator disagreements such as after crowdsourcing annotations for a dataset. LIDA is fully opensource, documented and publicly available.1 IntroductionDialogue systems are becoming one of the most active research areas in Natural Language Processing (NLP) and Machine Learning(ML). Creating a high-quality dialogue dataset incurs a large annotation cost, which makes good dialogue annotation tools essentialto ensure the highest possible quality. Many annotation tools exist for a range of NLP tasks but none are designed specifically fordialogue with modern usability principles in mind.LIDA is a web application designed to make dialogue dataset creation and annotation as easy and fast as possible. In addition tofollowing modern principles of usability, LIDA integrates best practices from other state-of-the-art annotation tools, most importantlyby allowing arbitrary ML models to be integrated as annotation recommenders to suggest annotations for data. Any system with thecorrect API can be integrated into LIDA’s back end, meaning LIDA can be used as a front end for researchers to interact with theirdialogue systems and correct their responses, then save the interaction as a future test case.When data is crowdsourced, it is good practice to have multiple annotators label each piece of data to reduce noise and mislabelling.Once you have multiple annotations, it is important to be able to resolve conflicts by highlighting where annotators disagreed sothat an arbiter can decide on the correct annotation. To this end, LIDA provides a dedicated interface which automatically findswhere annotators have disagreed and displays the labels alongside a percentage of how many annotators selected each label, with themajority annotated labels selected by default.1.1 Main ContributionsOur main contributions with this tool are:• A modern annotation tool designed specifically for task-oriented conversation data• The first dialogue annotator capable of handling the full dialogue annotation pipeline from turn and dialogue segmentationthrough to labelling structured conversation data• Easy integration of dialogue systems and recommenders to provide annotation suggestions• A dedicated interface to resolve inter-annotator disagreements for dialogue data2 Related WorkVarious annotation tools have been developed for NLP tasks in recent years. Table 1 compares LIDA with other recent annotationtools. TWIST is a dialogue annotation tool which consists of two stages: turn segmentation and content feature annotation. Turnsegmentation allows users to highlight and create new turn segments from raw text. After this, users can annotate sections of text ina segment by highlighting them and selecting from a predefined feature list. However, this tool doesn’t allow users to specify customannotations or labels and doesn’t support classification or slot-value annotation.INCEpTION is a semantic annotation platform for interactive tasks that require semantic resources like entity linking. It providesmachine learning models to suggest annotations and allows users to collect and model knowledge directly in the tool. GATE is anTable 1: Annotator Tool Comparison Table Inter-AnnotatorAnnotation Tool Turn/Dialogue Segmentation Classification Labels Edit Dialogues/Turns Recommenders LanguageDisagreement ResolutionLIDA YES YES YES YES YES PYTHONINCEpTION NO YES NO YES YES/NO JAVAGATE NO YES NO NO YES/NO JAVATWIST YES NO YES NO NO -BRAT NO YES NO YES NO PYTHONDOCCANO NO YES NO NO NO PYTHONDialogueView YES YES YES NO NO TcK/TKopen source tool that provides predefined solutions for many text processing tasks. It is powerful because it allows annotators toenhance the provided annotation tools with their own Java code, making it easily extensible and provides an enormous number ofpredefined features. However, GATE is a large and complicated tool with a significant setup cost. Despite their large feature sets,INCEpTION and GATE are not designed for annotating dialogue and cannot display data as turns, an important feature for dialoguedatasets.BRAT and Doccano are web-based annotation tools for tasks such as text classification and sequence labeling. They have intuitiveand user-friendly interfaces which aim to make the creation of certain types of dataset such as classification or sequence labellingdatasets as fast as possible. BRAT also supports annotation suggestions by integrating ML models. However, like INCEpTION andGATE, they are not designed for annotating dialogues and do not support generation of formatted conversational data from a rawtext file such as may be output by a transcription service. LIDA aims to fill these gaps by providing a lightweight, easy-to-setupannotation tool which displays data as a series of dialogues, supports integration of arbitrary ML models as recommenders andsupports segmentation of raw text into dialogues and turns.DialogueView is a tool for dialogue annotation. However, the main use-cases are not focused on building dialogue systems, rather it isfocused on segmenting recorded conversations. It supports annotating audio files as well as discourse segmentation - hence, granularlabelling of the dialogue, recommenders, inter-annotator agreement, and slot-value labelling is not possible with DialogueView.3 System OverviewLIDA is built according to a client-server architecture with the front end written in standard web languages (HTML/CSS/JavaScript)that will run on any browser. The back end written in Python using the Flask web framework as a RESTful API.The main screen which lists all available dialogues. The buttons below this list allow a user to add a blank or formatted dialoguefile. Users can also drag and drop files in this screen to upload them. The user is then able to add, delete or edit any particulardialogue. There is also a button to download the whole dataset as a JSON file on this page. Clicking on a dialogue will take users tothe individual dialogue annotation screen.LIDA uses the concept of a “turn” to organise how a dialogue is displayed and recorded. A turn consists of a query by the userfollowed by a response from the system, with an unlimited number of labels allowed for each user query. The user query andsystem response are displayed in the large area on the left of the interface, while the labels for each turn are shown in the scrollablebox on the right. There are two forms that these labels can currently take which are particularly relevant for dialogue: multilabelclassification and slot-value pair.An example of multilabel classification is whether the user was informing the system or requesting a piece of information. Anexample of a slot-value pair is whether the user mentioned the type of restaurant they’d like to eat at (slot: restaurant-type) and if sowhat it was (value: italian, for example). The front-end code is written in a modular form so that it is easy for researchers3.0.1 Experimenting with Dialogue SystemsLIDA is designed with this in mind - a dialogue system can be integrated into the back end so that it will run whenever the userenters a new query in the front end. The user will then be able to evaluate whether the system gave the correct answer and correct thelabels it gets wrong using the front end. LIDA will record these corrections and allow the user to download the interaction with theirdialogue system with the corrected labels so that it can be used as a test case in future versions of the system.3.0.2 Creating a New Dialogue DatasetUsers can create a blank dialogue on LIDA’s home screen, then enter queries in the box shown at the bottom of the screen. Alongwith whole dialogue systems, arbitrary ML models can be added as recommenders in the back end. Once the user hits ""Enter"",the query is run through the recommender models in the back end and the suggested annotations displayed for the label. If norecommender is specified in the back end, the label will be left blank. Users can delete turns and navigate between them using2""Enter"" or the arrow keys. The name of the dialogue being annotated can be seen next to the ""Back"" button at the top left of thescreen and can be edited by clicking on it.3.0.3 Annotating An Existing DatasetDatasets can be uploaded via drag-and-drop to the home screen of the system, or paths can be specified in the back end if thesystem were being used for crowdsourcing. Datasets can be in one of two forms, either a "".txt"" file such as may be produced by atranscription service, or a formatted "".json"" file, a common format for dialogue data. Once the user has uploaded their data, theirdialogue(s) will appear on the home screen. The user can click on each dialogue and will be taken to the single dialogue annotationscreen to annotate it. If the user uploaded a text file, they will be taken to a dialogue and turn segmentation screen. Following thesame constraints imposed in previous works, this turn segmenter assumes that there are only two participants in the dialogue: theuser and the system, and that the user asks the first query. The user separates each utterance in the dialogue by a blank line, andseparates dialogues with a triple equals sign (""===""). Once the user clicks ""Done"", the text file will automatically be parsed into thecorrect JSON format and each query run through the recommenders in the back-end to obtain annotation suggestions.3.0.4 Resolving Annotator DisagreementResearchers could use LIDA’s main interface to crowdsource annotations for a dialogue dataset. Once they have several annotationsfor each dialogue, they can upload these to the inter-annotator resolution interface of LIDA. The disagreements between annotatorswill be detected, with a percentage shown beside each label to show how many annotators selected it. The label with the highestpercentage of selections is checked by default. The arbiter can accept the majority label simply by pressing ""Enter"" and can changeerrors with the arrow keys to facilitate fast resolution. This interface also displays an averaged (over turns) version of Cohen’s Kappa,the total number of annotations, the total number of errors, and the averaged (over turns) accuracy.3.1 FeaturesSpecifying Custom Labels LIDA’s configuration is controlled by a single script in the back end. This script defines which labelswill be displayed in the UI and is easy to extend. Users can define their own labels by altering this configuration script. If a userwishes to add a new label, all they need to do is specify the label’s name, its type (classification or slot-value pair, currently) and thepossible values the classification can take. Alongside the label specification, they can also specify a recommender to use for the labelvalues. The label will then automatically be displayed in the front end. Note that labels in uploaded datasets will only be displayed ifthe label has an entry in the configuration file.Custom Recommenders When creating a dialogue dataset from scratch, LIDA is most powerful when used in conjunction withrecommenders which can suggest annotations for user queries to be corrected by the annotator. State-of-the-art tools emphasize theimportance of being able to use recommenders in annotation systems. Users can specify arbitrary ML models to use for each label inLIDA’s back end. The back end is written in Python, the de facto language for machine learning, so researchers can directly integratemodels written in Python to the back end. This is in contrast to tools such as INCEpTION and GATE which are written in Javaand so require extra steps to integrate a Python-based model. To integrate a recommender, the user simply provides an instantiatedPython object in the configuration file that has a method called ""transform"" that takes a single string and returns a predicted label.Dialogue and Turn Segmentation from Raw Data When uploading a .txt file, users can segment each utterance and each dialoguewith a simple interface. This means that raw dialogue data with no labels, such as obtained from a transcription service, can beuploaded and processed into a labelled dialogue. Segmented dialogues and turns are automatically run through every recommenderto give suggested labels for each utterance.4 EvaluationTo test LIDA’s capabilities, we designed a simple experiment: we took a bespoke dataset of 154 dialogues with an average of 3.5turns per dialogue and a standard deviation of 1.55. The task was to assign three classification labels to each user utterance in eachdialogue. Each annotator was given a time limit of 1 hour and told to annotate as many dialogues as they could in that time. We hadsix annotators perform this task, three of whom were familiar with the system and three of whom had never seen it before.These annotators annotated an average of 79 dialogues in one hour with a standard deviation of 30, which corresponds to anaverage of 816.5 individual annotations. The annotators who had never seen the system before annotated an average of 60 dialoguescorresponding to an average of 617 individual annotations.Once we had these six annotations, we performed a second experiment whereby a single arbiter resolved inter-annotator disagree-ments. In one hour, the arbiter resolved 350 disagreements and noted that resolution.3"
P111,"Leveraging Deep Learning for Enhanced Bayesian Optimization inScientific Domains with Complex StructuresAbstractBayesian optimization (BO) is a widely used technique for the global optimization of costly black-box functions.However, many real-world scenarios involve functions that are not entirely black-box. These functions may possessknown structures, such as symmetries, or the data generation process might be a composite one that providesvaluable intermediate information beyond the optimization objective’s value. Traditional surrogate models usedin BO, like Gaussian Processes (GPs), do not scale well with large datasets and struggle to incorporate knownstructures. This paper introduces the use of Bayesian neural networks (BNNs), which are scalable and adaptablesurrogate models with inductive biases, to enhance BO for intricate, structured problems in high-dimensionalspaces. We showcase the application of BO on various practical challenges in physics and chemistry. This includesoptimizing the topology of photonic crystal materials using convolutional neural networks and refining chemicalproperties of molecules with graph neural networks. Our findings indicate that neural networks frequently surpassGPs as surrogate models for BO in these complex tasks, achieving greater sampling efficiency and reducedcomputational expenses.1 IntroductionBayesian optimization (BO) is a powerful technique for global optimization, particularly suited for expensive, derivative-freefunctions. It has found applications across various scientific and engineering domains, including hyperparameter tuning in machinelearning. BO operates by iteratively selecting the next data point to evaluate, aiming to maximize sampling efficiency and minimizethe number of evaluations needed to find the optimum. This is crucial when experiments or simulations are time-consuming orresource-intensive.In numerous fields, the system under investigation is not a complete black box. For instance, high-dimensional input spaces likeimages or molecules often exhibit known structures, symmetries, and invariances. Moreover, the function might be decomposableinto other functions, where the data collection process yields intermediate or auxiliary information that can be used to computethe objective function more efficiently. Examples include scientific experiments or simulations that produce high-dimensionalobservations or multiple measurements simultaneously, such as the optical scattering spectrum of a nanoparticle across variouswavelengths or multiple quantum chemistry properties of a molecule from a single density functional theory (DFT) calculation.These physically-informed insights into the system are valuable for designing surrogate models with appropriate inductive biases,but they are often underutilized in current methods.BO relies on a surrogate model to represent a distribution over potential functions, incorporating uncertainty in its predictions.Gaussian Processes (GPs) are commonly used as surrogate models due to their analytical tractability. However, GPs face challenges:(1) their inference time scales cubically with the number of observations and output dimensionality, making them less suitable forlarge datasets or problems with high output dimensionality without kernel approximations, and (2) they are most naturally applied tocontinuous, low-dimensional input spaces, requiring careful manual formulation of kernels for high-dimensional data with complexstructures. Consequently, encoding inductive biases can be difficult.Neural networks (NNs) and Bayesian neural networks (BNNs) have emerged as alternatives to GPs due to their scalability andflexibility. Another approach involves using neural networks to generate continuous latent spaces, making it easier to apply BOwith standard GPs. The ability of BNN architectures to incorporate various constraints, symmetries, and inductive biases opens uppossibilities for applying BO to more complex tasks involving structured data.This work demonstrates the application of deep learning to facilitate BO for complex, real-world scientific datasets, without relyingon pre-trained models. Specifically:• We utilize auxiliary or intermediate information to enhance BO for tasks with high-dimensional observations.• We apply BO to complex input spaces, including images and molecules, using convolutional and graph neural networks,respectively.• We implement BO on several realistic scientific datasets, such as the optical scattering of a nanoparticle, topologyoptimization of a photonic crystal material, and chemical property optimization of molecules from the QM9 dataset.Our results demonstrate that neural networks can significantly outperform GPs as surrogate models on these problems. We believethese strong results will generalize to other contexts, enabling the application of BO to a wider range of problems. While ourmethods build upon existing techniques, we employ a novel combination of these methods to adapt existing BO frameworks toreal-world, complex applications.2 Related WorkSeveral methods have been developed to improve the scalability of GPs for larger problems. For example, one framework formulti-output GPs scales linearly with the dimensionality of a low-dimensional subspace of the data. Multi-task GPs have also beenused for BO over problems with large output dimensionalities. Furthermore, GPs have been demonstrated on very large datasetsusing GPUs and intelligent preconditioners, or through various approximations.Another strategy for scaling BO to larger problems involves combining it with other methods, reducing the need for the surrogatemodel to train on the entire dataset. For instance, one method uses a collection of independent probabilistic models in different trustregions, iteratively deciding where to perform BO, effectively reducing the problem to a set of local optimizations. Other methodsbuild upon this approach and dynamically learn the partition function separating different regions.GPs have been adapted to complex problem settings to broaden the applicability of BO. For example, some approaches decomposesynthetic problems as a composition of other functions, leveraging the additional structure to improve BO. However, the multi-outputGP used in these approaches scales poorly with output dimensionality, limiting their use to simpler problems. GP kernels have alsobeen developed for complex input spaces, including convolutional and graph kernels. Graph kernels have been used to apply BO toneural architecture search (NAS), where the architecture and connectivity of a neural network itself can be optimized.Deep learning has been employed as a scalable and flexible surrogate model for BO. For instance, neural networks have been usedas adaptive basis functions for Bayesian linear regression, enabling BO to scale to large datasets. This approach also allows fortransfer learning of the adaptive basis across multiple tasks and modeling of auxiliary signals to improve performance. Additionally,Bayesian neural networks (BNNs) that use Hamiltonian Monte Carlo to sample the posterior have been used for single-task andmulti-task BO for hyperparameter optimization.A popular approach for BO in high-dimensional spaces is latent-space optimization. Here, an autoencoder, such as a VAE, is trainedon a dataset to create a continuous latent space representing the data. Then, conventional optimization algorithms, like BO with GPs,can be used to optimize over this continuous latent space. This approach has been applied to tasks such as arithmetic expressionoptimization and chemical design. Note that these approaches focus on both data generation and optimization, whereas our workfocuses solely on the optimization process.Random forests have also been used for iterative optimization, such as sequential model-based algorithm configuration (SMAC), asthey do not face scaling challenges. Tree-structured Parzen Estimators (TPE) are another popular choice for hyperparameter tuning.However, these approaches still encounter difficulties in encoding complex, structured inputs like images and graphs.Deep learning has also been applied to improve tasks other than BO. For example, active learning, similar to BO, aims to optimize amodel’s predictive ability with as few data points as possible. The inductive biases of neural networks have enabled active learningon various high-dimensional data, including images, language, and partial differential equations. BNNs have also been applied to thecontextual bandits problem, where the model chooses between discrete actions to maximize expected reward.3 Methodology3.1 Bayesian Optimization PrerequisitesWe will now briefly introduce the BO methodology. We formulate our optimization task as a maximization problem, where we˘ ˘ ˘aim to find the input x2217 2208 X that maximizes a function f, such that x2217 = arg maxx f(x). The input x can be a real-valuedcontinuous vector, but it can also be generalized to categorical variables, images, or discrete objects like molecules. The function freturns the objective value y = f(x), which we also refer to as the ""label"" of x, and can represent a performance metric we want tomaximize. In general, f can be a noisy function.A crucial component of BO is the surrogate model, which provides a distribution of predictions instead of a single point estimate.Ideally, these surrogate models are Bayesian, but in practice, various approximate Bayesian models or even frequentist distributionshave been used. In iteration N, a Bayesian surrogate model M is trained on a labeled dataset Dtrain = (xn, yn)N n=1. An acquisition˘ ˘function 03b1 then uses M to suggest the next data point xN+1 2208 X to label, where:x = arg max α(x; M, D ) (1)N+1 trainx∈X2The new data is evaluated to obtain yN+1 = f(xN+1), and (xN+1, yN+1) is added to Dtrain.3.2 Acquisition Function ˘A key consideration in BO is selecting the next data point xN+1 2208 X given the model M and labeled dataset Dtrain. This is˘parameterized through the acquisition function 03b1, which is maximized to determine the next data point to label, as shown inEquation 1. ˘We utilize the expected improvement (EI) acquisition function 03b1EI. When the posterior predictive distribution of the surrogate˘ ˘model M is a normal distribution N(00b5(x), 03c32(x)), EI can be expressed analytically as:α (x) = σ(x)[γ(x)Φ(γ(x)) + ϕ(γ(x))] (2)EI˘ ˘ ˘ ˘ ˘where 03b3(x) = (00b5(x) 2212 ybest)/03c3(x), ybest = max(ynN n=1) is the best observed objective function value so far, and 03c6˘and 03a6 are the PDF and CDF of the standard normal distribution N(0, 1), respectively. For surrogate models without an analyticalform for the posterior predictive distribution, we sample from the posterior NMC times and use a Monte Carlo (MC) approximationof EI: NMC1 (cid:88)MC (i)α (x) ≈ max(µ (x) − y , 0) (3)bestEI NMC i=1˘where 00b5(i) is a prediction sampled from the posterior of M. While some works fit the surrogate model’s output to a Gaussian touse Equation 2 for acquisition, this is not valid when the model prediction for y is not Gaussian, which is generally the case forcomposite functions (see Section 2.4).EI has advantages over other acquisition functions because the MC approximation (1) remains differentiable, facilitating optimizationof the acquisition function in the inner loop (unlike the MC approximation of upper confidence bound (UCB), which is notdifferentiable and can result in ties), and (2) is inexpensive (unlike naive Thompson sampling for ensembles, which would requireretraining a model from scratch in each iteration).3.3 Continued Training with Learning Rate AnnealingA challenge in BO is the computational cost of training a surrogate model on Dtrain from scratch in every optimization loop,especially since neural networks ideally require extensive training until convergence. To reduce the training time of BNNs in eachoptimization loop, we use the model trained in the Nth optimization loop iteration as the initialization (a ""warm start"") for the(N+1)th iteration, rather than starting from a random initialization. Specifically, we employ the cosine annealing learning rate, whichstarts with a high learning rate and gradually reduces it to 0. For more details, refer to Section A.3 in the Appendix.3.4 Auxiliary Information ˘Typically, we assume f is a black-box function, so we train M : X 2192 Y to model f. Here, we consider scenarios where the˘experiment or observation may provide intermediate or auxiliary information z 2208 Z, such that f can be decomposed as:f (x) = h(g(x)) (4)˘ ˘where g : X 2192 Z is the expensive labeling process, and h : Z 2192 Y is a known objective function that can be computed cheaply.˘This is also known as ""composite functions"". In this case, we train M : X 2192 Z to model g, and the approximate EI acquisitionfunction becomes: NMC1 (cid:88)MC−aux (i)α (x) ≈ max(h(µ (x)) − y , 0) (5)bestEI NMC i=1which can be seen as a Monte Carlo version of the acquisition function presented in prior work. We denote models trained using˘auxiliary information with the suffix ""-aux."" Because h is not necessarily linear, h(00b5(i)(x)) is not generally Gaussian even if˘00b5(i) itself may be, making the MC approximation convenient or even necessary.34 Surrogate ModelsBayesian models capture uncertainty associated with both data and model parameters in the form of probability distributions. This˘is achieved by placing a prior probability distribution P(03b8) on the model parameters and calculating the posterior belief of theparameters using Bayes’ theorem after observing new data. Fully Bayesian neural networks have been studied in small architecturesbut are impractical for realistically sized neural networks, as nonlinearities between layers make the posterior intractable, requiringMCMC methods to sample the posterior. However, in the last decade, numerous proposals for approximate Bayesian neural networkshave emerged, capable of capturing some Bayesian properties and producing a predictive probability distribution. In this work, wecompare several different options for the BNN surrogate model, along with other non-BNN baselines. We list some notable modelshere, with model details and results in Section A.4.1 of the Appendix.Ensembles combine multiple models to improve predictive performance by averaging their results. Ensembles of neural networkshave been reported to be more robust than other BNNs, and we use ""Ensemble"" to denote an ensemble of neural networks withidentical architectures but different random initializations, providing enough variation for individual models to give differentpredictions. Using individual models can be interpreted as sampling from a posterior distribution, so we use Equation 5 foracquisition. Our ensemble size is NMC = 10.Other BNNs: We also compare to variational BNNs, including Bayes by Backprop (BBB) and Multiplicative Normalizing Flows(MNF); BOHAMIANN; and NeuralLinear. For BBB, we also experiment with KL annealing, denoted by ""-Anneal.""GP Baselines: GPs are largely defined by their kernel (also called ""covariance functions""), which determines the prior and posteriordistributions, how different data points relate to each other, and the type of data the GP can operate on. In this work, ""GP"" refers˘to a standard specification using a Mat00e9rn 5/2 kernel, a popular kernel for real-valued continuous spaces. For images, we usea convolutional kernel, labeled as ""ConvGP"", implemented using the infinite-width limit of a convolutional neural network. Forgraphs, we use the Weisfeiler-Lehman (WL) kernel, labeled as ""GraphGP"", which can operate on undirected graphs with node andedge features, making it suitable for chemical molecule graphs. We also compare against ""GP-aux,"" which uses multi-output GPsfor problems with auxiliary information (composite functions). In the Appendix, we also examine GPs using infinite-width andinfinite-ensemble neural network limits as kernels, as well as TuRBO, which combines GP-based BO with trust regions.VAE-GP uses a VAE trained beforehand on an unlabeled dataset representative of X. This allows us to encode complex input spaces,such as chemical molecules, into a continuous latent space where conventional GP-based BO methods can be applied, even enablingthe generation and discovery of novel molecules not in the original dataset. Here, we modified an existing implementation that usesa junction tree VAE (JTVAE) to encode chemical molecules. More details can be found in the Appendix.Other Baselines: We compare against two variations of Bayesian optimization, TuRBO and TPE. We also compare against severalglobal optimization algorithms that do not use surrogate models and are computationally inexpensive, including LIPO, DIRECT-L,and CMA-ES.We emphasize that ensembles and variational methods can easily scale to high-dimensional outputs with minimal increase incomputational cost by simply changing the output layer size. Neural Linear and GPs scale cubically with output dimensionality(without covariance approximations), making them difficult to train on high-dimensional auxiliary or intermediate information.5 ResultsWe now examine three real-world scientific optimization tasks, all of which provide intermediate or auxiliary information that can beleveraged. In the latter two tasks, the structure of the data also becomes important, and hence BNNs with various inductive biasessignificantly outperform GPs and other baselines. For simplicity, we highlight results from select architectures (see Appendix forfull results, dataset, and hyperparameter details). All BO results are averaged over multiple trials, and the shaded area in the plots˘represents 00b1 one standard error over the trials.5.1 Multilayer NanoparticleWe first consider the problem of light scattering from a multilayer nanoparticle, which has various applications requiring a tailoredoptical response, including biological imaging, improved solar cell efficiency, and catalytic materials. The nanoparticle we considerconsists of a lossless silica core and 5 spherical shells of alternating TiO2 and silica. The nanoparticle is parameterized by the coreradius and layer thicknesses, which we restrict to the range of 30 nm to 70 nm. Due to the nanoparticle’s size being on the order ofthe wavelength of light, its optical properties can be tuned by adjusting the number and thicknesses of the layers. The scatteringspectrum can be calculated semi-analytically, as detailed in Section A.1.1 of the Appendix.Our goal is to optimize the scattering cross-section spectrum over a range of visible wavelengths. We compare two different objectivefunctions: the narrowband objective, which aims to maximize scattering in the small wavelength range of 600 nm to 640 nm andminimize it elsewhere, and the highpass objective, which aims to maximize scattering above 600 nm and minimize it elsewhere.While conventional GPs are trained using the objective function as the label directly, BNNs with auxiliary information can be trained˘to predict the full scattering spectrum (the auxiliary information z 2208 R201), which is then used to calculate the objective function.4The BO results are presented in Figure 2. The addition of auxiliary information significantly improves BO performance for BNNs.They are also competitive with GPs, making BNNs a viable approach for scaling BO to large datasets. In Appendix A.5, we observesimilar trends for other types of BNNs. Due to the poor scaling of multi-output GPs with respect to output dimensionality, we canonly run GP-aux for a limited number of iterations within a reasonable time frame. Within these few iterations, GP-aux performspoorly, only slightly better than random sampling. We also find in the Appendix that BO with either GPs or BNNs is comparablewith or outperforms other global optimization algorithms, including DIRECT-L and CMA-ES.5.2 Photonic Crystal TopologyNext, we examine a more complex, high-dimensional domain with symmetries that are not easily exploited by GPs. Photoniccrystals (PCs) are nanostructured materials engineered to exhibit unique optical properties not found in bulk materials, such asphotonic band gaps, negative refractive index, and angular selective transparency. With advancements in fabrication techniquesenabling smaller feature sizes, there is growing interest in inverse design and topology optimization to design more sophisticatedPCs for applications in photonic integrated circuits, flat lenses, and sensors. ˘Here, we consider 2D PCs consisting of periodic unit cells represented by a 32 00d7 32 pixel image, with white and black regionsrepresenting vacuum (or air) and silicon, respectively. Optimizing over raw pixel values may lead to pixel-sized features or˘intermediate pixel values that are not physically realizable. Therefore, we parameterize the PCs with a level-set function 03c6 : X˘ ˘ ˘2192 V that converts a 51-dimensional feature vector x = [c1, c2, ..., c50, 2206] 2208 R51, representing the level-set parameters, into˘ ˘an image v 2208 R3200d732 representing the PC. More details can be found in Section A.1.2 of the Appendix.˘ ˘ ˘ ˘We test BO on two different data distributions, PC-A and PC-B. In the PC-A distribution, x spans ci 2208 [22121, 1], 2206 2208˘ ˘[22123, 3]. In the PC-B distribution, we arbitrarily restrict the domain to ci 2208 [0, 1]. The PC-A data distribution is translationinvariant, meaning that any PC with a translational shift will also be in the data distribution. However, the PC-B data distribution isnot translation invariant.The optical properties of PCs can be characterized by their photonic density of states (DOS). We choose an objective function thataims to minimize the DOS in a certain frequency range while maximizing it elsewhere, corresponding to opening up a photonic bandgap in that frequency range. We train GPs directly on the level-set parameters X, whereas we train the Bayesian convolutional NNs(BCNNs) on the more natural unit cell image space V. BCNNs can also be trained to predict the full DOS as auxiliary information z˘2208 R500.The BO results, shown in Figure 4(a), demonstrate that BCNNs outperform GPs by a significant margin on both datasets. Thisis due to both the auxiliary information and the inductive bias of the convolutional layers, as shown in Figure 4(b). Because thebehavior of PCs is determined by their topology rather than individual pixel values or level-set parameters, BCNNs are much bettersuited to analyze this dataset compared to GPs. Additionally, BCNNs can be made much more data-efficient since they directlyencode translation invariance and thus learn the behavior of a whole class of translated images from a single image. Because˘GP-aux is extremely expensive compared to GP (50000d7 longer on this dataset), we are only able to run GP-aux for a smallnumber of iterations, where it performs comparably to random sampling. We also compare to GPs using a convolutional kernel˘ ˘(201cConvGP-NNGP201d) in Figure 4(a). ConvGP-NNGP only performs slightly better than random sampling, likely due to a lackof auxiliary information and inflexibility to learn the most suitable representation for this dataset.For our main experiments with BCNNs, we use an architecture that respects translation invariance. To demonstrate the effectof another commonly used deep learning training technique, we also experiment with incorporating translation invariance into atranslation-dependent architecture using a data augmentation scheme in which each image is randomly translated, flipped, androtated during training. We expect data augmentation to improve performance when the data distribution exhibits the correspondingsymmetries. As shown in Figure 4(c), we indeed find that data augmentation improves the BO performance of the translation-dependent architecture when trained on the translation-invariant PC-A dataset, even matching the performance of a translation-invariant architecture on PC-A. However, on the translation-dependent PC-B dataset, data augmentation initially hurts the BOperformance of the translation-dependent architecture because the model is unable to quickly specialize to the more compactdistribution of PC-B, putting its BO performance more on par with models trained on PC-A. These results show that techniques usedto improve generalization performance (such as data augmentation or invariant architectures) for training deep learning architecturescan also be applied to BO surrogate models and, when used appropriately, directly translate into improved BO performance. Notethat data augmentation would not be feasible for GPs without a hand-crafted kernel, as the increased size of the dataset would causeinference to become computationally intractable.5.3 Organic Molecule Quantum ChemistryFinally, we optimize the chemical properties of molecules. Chemical optimization is of significant interest in both academia andindustry, with applications in drug design and materials optimization. This is a difficult problem where computational approachessuch as density functional theory (DFT) can take days for simple molecules and are intractable for larger molecules; synthesis isexpensive and time-consuming, and the space of synthesizable molecules is large and complex. There have been many approachesto molecular optimization that largely revolve around finding a continuous latent space of molecules or hand-crafting kernels tooperate on molecules. 5Here, we focus on the QM9 dataset, which consists of 133,885 small organic molecules along with their geometric, electronic,and thermodynamic quantities calculated with DFT. Instead of optimizing over a continuous space, we draw from the fixed poolof available molecules and iteratively select the next molecule to add to Dtrain. This is a problem setting especially common tomaterials design, where databases are incomplete and the space of experimentally feasible materials is small.We use a Bayesian graph neural network (BGNN) for our surrogate model, as GNNs have become popular for chemistry applicationsdue to the natural encoding of a molecule as a graph with atoms and bonds as nodes and edges, respectively. For baselines thatoperate over continuous spaces (i.e., GPs and simple neural networks), we use the Smooth Overlap of Atomic Positions (SOAP)descriptor to produce a fixed-length feature vector for each molecule. ˘ ˘We compare two different optimization objectives derived from the QM9 dataset: the isotropic polarizability 03b1 and (03b5LUMO˘ ˘ ˘2212 20acg„) where 20acg,;, is the HOMO-LUMO energy gap. Other objectives are included in the Appendix. Because many of thechemical properties in the QM9 dataset can be collectively computed by a single DFT or molecular dynamics calculation, we cantreat a group of labels from QM9 as auxiliary information z and train our BGNN to predict this entire group simultaneously. Theobjective function h then simply picks out the property of interest.As shown in Figure 5(c), GraphGP and the BGNN variants significantly outperform GPs, showing that the inductive bias in the graphstructure leads to a much more natural representation of the molecule and its properties. In the case of maximizing the polarizability˘03b1, including the auxiliary information improves BO performance, showing signs of positive transfer. However, it does not have asignificant impact on the other objectives, which may be due to the small size of the available auxiliary information (only a handfulof chemical properties from the QM dataset) compared with the nanoparticle and photonic crystal tasks. In a more realistic onlinesetting, we would have significantly more physically informative information available from a DFT calculation, e.g., we could easilycompute the electronic density of states (the electronic analogue of the auxiliary information used in the photonics task).˘As seen in Figure 5(d), we also note that the GraphGP is relatively computationally expensive (1500d7 longer than GPs for small N˘and 80000d7 longer than BGNNs for N = 100) and so we are only able to run it for a limited N in a reasonable time frame. We seethat BGNNs perform comparably or better than GraphGPs despite incurring a fraction of the computational cost.VAE-GP uses a modified version of the latent-space optimization method implementation provided by Tripp et al. (2020). Ratherthan optimizing over a continuous latent space of the VAE, we feed the data pool through the VAE encoder to find their latent spacerepresentation and then apply the acquisition function to the latent points to pick out the best unlabeled point to sample. We keep asmany hyperparameters the same as the original implementation as possible, except for the weighted retraining, which we forgosince we have a fixed data pool that was used to train the VAE. This setup is similar to GraphNeuralLinear in that a deep learningarchitecture is used to encode the molecule as a continuous vector, although GraphNeuralLinear is only trained on the labeled data.The results for this experiment show that VAE-GP performs worse than BNNs on two of the three objective functions we tested andslightly better on one objective. We also note that the performance of VAE-GP depends very heavily on the pre-training of the VAE,as choosing different hyperparameters or even a different random seed can significantly deteriorate performance (see Figure 15 inthe Appendix).6 DiscussionIntroducing physics-informed priors (in the form of inductive biases) into the model is critical for performance. Well-knowninductive biases in deep learning include convolutional and graph neural networks for images and graph structures, respectively,which significantly improve BO performance. Another inductive bias we introduce is the addition of auxiliary information presentin composite functions, which significantly improves the performance of BO for the nanoparticle and photonic crystal tasks. Weconjecture that the additional information forces the BNN to learn a more consistent physical model of the system since it mustlearn features shared across the multi-dimensional auxiliary information, thus enabling the BNN to generalize better. For example,the scattering spectrum of the multilayer particle consists of multiple resonances (sharp peaks), the width and location of whichare determined by the material properties and layer thicknesses. The BNN could potentially learn these more abstract features,and thus the deeper physics, to help it interpolate more efficiently, akin to data augmentation. Auxiliary information can also beinterpreted as a form of data augmentation. Indeed, tracking the prediction error on a validation set shows that models with auxiliaryinformation tend to have a lower loss than those without (see Appendix A.5). It is also possible that the loss landscape for theauxiliary information is smoother than that of the objective function and that the auxiliary information acts as implicit regularizationthat improves generalization performance.Interestingly, GP-aux performs extremely poorly on the nanoparticle and photonic crystal tasks. One possible reason is that we areonly able to run GP-aux for a few iterations, and it is not uncommon for GP-based BO to require some critical number of iterationsto reach convergence, especially in high-dimensional systems where the size of the covariance matrix scales with the square of thedimensionality. It may also be possible that GP-aux only works on certain types of function decompositions and cannot be broadlyapplied to all composite functions, as the inductive biases in GPs are often hard-coded.There is an interesting connection between how well BNNs are able to capture and explore a multi-modal posterior distribution andtheir performance in BO. For example, we have noticed that larger batch sizes tend to significantly hurt BO performance. On the onehand, larger batch sizes may result in poorer generalization as the model finds sharper local minima in the loss landscape. Anotherexplanation is that the stochasticity inherent in smaller batch sizes allows the BNN to more easily explore the posterior distribution,6which is known to be highly multi-modal. Indeed, BO often underperforms for very small dataset sizes N but quickly catches up asN increases, indicating that batch size is an important hyperparameter that must be balanced with computational cost.All our results use continued training (or warm restart) to minimize training costs. We note that re-initializing M and training fromscratch in every iteration performs better than continued training on some tasks (results in the Appendix), which points to how BNNsmay not sufficiently represent a multi-modal posterior distribution or that continued training may skew the training distribution thatthe BNN sees. Future work will consider using stochastic training approaches such as SG-MCMC methods for exploring posteriordistributions, as well as other continual learning techniques to further minimize training costs, especially for larger datasets.When comparing BNN architectures, we find that ensembles tend to consistently perform among the best, which is supported byprevious literature showing that ensembles capture uncertainty much better than variational methods, especially in multi-modal losslandscapes. Ensembles are also attractive because they require no additional hyperparameters and are simple to implement. Althoughtraining costs increase linearly with the size of the ensemble, this can be easily parallelized on modern computing infrastructures.Furthermore, recent work that aims to model efficient ensembles that minimize computational cost could be an interesting futuredirection. NeuralLinear variants are also quite powerful and cheap, making them very promising for tasks without high-dimensionalauxiliary information. Integrating Neural Linear with multi-output GPs is an interesting direction for future work. The other BNNseither require extensive hyperparameter tuning or perform poorly, making them difficult to use in practice. Additional discussion canbe found in Appendix A.5.5.As seen in Appendix A.5.4, VAE-GP performs worse than our method on two of the chemistry objectives and better on one objective.While latent-space optimization methods are often applied to domains where one wants to simultaneously generate data and optimizeover the data distribution, these methods can also be applied to the cases in this work, where a data pool (e.g., QM9 dataset forthe chemistry task) or separate data generation process (e.g., level-set process for the photonic crystal task) is already available. Inthese cases, the VAE is not used as a generative model but rather as a way to learn appropriate representations. While latent-spaceapproaches can take advantage of well-developed and widely available optimization algorithms, they also require unsupervisedpre-training on a sizable dataset and a suitable autoencoder model with the necessary inductive biases. Such models are available inchemistry, where there has been significant development, but are more limited in other domains such as photonics. On the otherhand, our method can incorporate the data structure or domain knowledge in an end-to-end manner during training, although futurework is needed to evaluate more carefully how much of an advantage this is and whether it depends on specific dataset or domaincharacteristics. For settings where we do not need a generative model, it would also be interesting to replace the autoencoder with aself-supervised model or semi-supervised model to create a suitable latent space.7 ConclusionWe have demonstrated global optimization on multiple tasks using a combination of deep learning and BO. In particular, we haveshown how BNNs can be used as surrogate models in BO, enabling the scaling of BO to large datasets and providing the flexibility toincorporate a wide variety of constraints, data augmentation techniques, and inductive biases. We have demonstrated that integratingdomain knowledge on the structure and symmetries of the data into the surrogate model, as well as exploiting intermediate orauxiliary information, significantly improves BO performance, all of which can be interpreted as physics-informed priors. Intuitively,providing the BNN surrogate model with all available information allows the BNN to learn a more faithful physical model of thesystem of interest, thus enhancing the performance of BO. Finally, we have applied BO to real-world, high-dimensional scientificdatasets, and our results show that BNNs can outperform our best-effort GPs, even with strong domain-dependent structure encodedin the covariance functions. We note that our method is not necessarily tied to any particular application domain and can lower thebarrier of entry for design and optimization.Future work will investigate more complex BNN architectures with stronger inductive biases. For example, output constraints can beplaced through unsupervised learning or by variationally fitting a BNN prior. Custom architectures have also been proposed forpartial differential equations, many-body systems, and generalized symmetries, which will enable effective BO on a wider range oftasks. The methods and experiments presented here enable BO to be effectively applied in a wider variety of settings. There are alsovariants of BO, including TuRBO, which perform extremely well on our tasks, and so future work will also include incorporatingBNNs into these variants.8 Appendix8.1 DatasetsThe dimensionalities of the datasets are summarized in Table 1. The continuous input dimension for chemical molecules refersto the SOAP descriptor. While the space of chemical molecule graphs in general does not have a well-defined dimensionality aschemical molecules can be arbitrarily large and complex, we limit the size of molecules by only sampling from the QM9 dataset,and can define the dimensionality as the sum of the adjacency, node, and edge matrix dimensionalities.The high dimensionalities of all of these problems make Bayesian neural networks well-suited as surrogate models to enable scaling.Note that the nanoparticle scattering problem can be adjusted to be less or more difficult by either changing the input dimensionality(i.e. the number of nanoparticle layers) or the auxiliary dimension (i.e. the resolution or range of wavelengths that are sampled).7Table 1: Summary of dataset dimensionalities. Note that alternate inputs for photonic crystal and organic molecule datasets arebinary images and molecule graphs, respectively. CONTINUOUS INPUT ALTERNATE INPUT AUXILIARYDIMENSION DIMENSION DIMENSIONNANOPARTICLE SCATTERING 6 N/A 201PHOTONIC CRYSTAL DOS 51 32 x 32 = 1024 500˘ ˘MOLECULE QUANTUM CHEMISTRY 480 9 + 9 00d7 9 + 9 00d7 9 = 171 98.2 Nanoparticle ScatteringThe multilayer nanoparticle consists of a lossless silica core surrounded by alternating spherical layers of lossless TiO2 and lossless˘silica. The relative permittivity of silica is 03b5silica = 2.04. The relative permittivity of TiO2 is dispersive and depends on thewavelength of light: 0.2441ε = 5.913 + (6)T iO2 2λ − 0.0803˘where 03bb is the wavelength given in units of nm. The entire particle is surrounded by water, which has a relative permittivity of˘03b5water = 1.77. ˘ ˘For a given set of thicknesses, we analytically solve for the scattering spectrum, i.e. the scattering cross-section 03c3(03bb) as a˘ ˘function of wavelength 03bb, using Mie scattering. The code for computing 03c3 was adapted from existing work.The objective functions for the narrowband and highpass objectives are:(cid:82) (cid:80)145σ(λ)dλ ziλ∈nb i=126h (z) = ≈ (7)(cid:82)nb (cid:80) (cid:80)125 201σ(λ)dλ z + zi iλ∈/nb i=1 i=146(cid:82) (cid:80)201σ(λ)dλ ziλ∈hp i=126≈h (z) = (8)(cid:82)hp (cid:80)125σ(λ)dλ ziλ∈/hp i=1˘ ˘ ˘ ˘where z 2208 R201 is the discretized scattering cross-section 03c3(03bb) from 03bb = 350 nm to 750 nm.8.3 Photonic Crystal ˘The photonic crystal (PC) consists of periodic unit cells with periodicity a = 1 au, where each unit cell is depicted as a 201ctwo-˘ ˘tone201d image, with the white regions representing silicon with permittivity 03b51 = 11.4 and black regions representing vacuum˘(or air) with permittivity 03b50 = 1. ˘ ˘ ˘ ˘The photonic crystal (PC) structure is defined by a spatially varying permittivity 03b5(x, y) 2208 03b50, 03b51 over a 2D periodic˘ ˘ ˘unit cell with spatial coordinates x, y 2208 [0, a]. To parameterize 03b5, we choose a level set of a Fourier sum function 03c6,defined as a linear combination of plane waves with frequencies evenly spaced in the reciprocal lattice space up to a maximum cutoff.Intuitively, the upper limit on the frequencies roughly corresponds to a lower limit on the feature size such that the photonic crystalremains within reasonable fabrication constraints. Here we set the cutoff such that there are 25 complex frequencies correspondingto 50 real coefficients c = (c1, c2, ..., c50).Explicitly, we have (cid:32) (cid:33)25(cid:88) 2πi(n x+n y)/aϕ[c](x, y) = ℜ (c + ic )e (9)x yk k+25k=1 ˘ ˘ ˘where each exponential term is composed from the 25 different pairs nx, ny with nx, ny 2208 22122, 22121, 0, 1, 2. We then choose˘ ˘ ˘a level-set offset 2206 to determine the PC structure, where regions with 03c6 > 2206 are assigned to be silicon and regions where˘ ˘ ˘03c6 2264 2206 are vacuum. Thus, the photonic crystal unit cell topology is parameterized by a 51-dimensional vector, [c1, c2, ...,˘ ˘c50, 2206] 2208 R51. More specifically, εε(x, y) = ε[c, ∆](x, y) = { ϕ[c](x, y) > ∆ε ϕ[c](x, y) ≤ ∆ (10)01 8˘ ˘ ˘ ˘ ˘which is discretized to result in a 32 00d7 32 pixel image v 2208 03b50, 03b513200d732. This formulation also has the advantage ofenforcing periodic boundary conditions. ˘For each unit cell, we use the MIT Photonics Bands (MPB) software to compute the band structure of the photonic crystal, 03c9(k),˘ ˘up to the lowest 10 bands, using a 32 00d7 32 spatial resolution (or equivalently, 32 00d7 32 k-points over the Brillouin zone˘ ˘ ˘2212 03c0 a < k < 03c0 a ). We also extract the group velocities at each k-point and compute the density-of-states (DOS) via anextrapolative technique. The DOS is computed at a resolution of 20,000 points, and a Gaussian filter of kernel size 100 is used to˘ ˘smooth the DOS spectrum. To normalize the frequency scale across the different unit cells, the frequency is rescaled via 03c9 2192˘ ˘ ˘03c9norm, where 03b5avg is the average permittivity over all pixels. Finally, the DOS spectrum is truncated at 03c9norm = 1.2 and˘interpolated using 500 points to give z 2208 R500.The objective function aims to minimize the DOS in a small frequency range and maximize it elsewhere. We use the following:300 1(cid:88)h (z) = z + (11)DOS i (cid:80)500 z + 1ii=351i=1where the 1 is added in the denominator to avoid singular values.8.4 Organic Molecule Quantum ChemistryThe Smooth Overlap of Atomic Positions (SOAP) descriptor uses smoothed atomic densities to describe local environments for eachatom in the molecule through a fixed-length feature vector, which can then be averaged over all the atoms in the molecule to producea fixed-length feature vector for the molecule. This descriptor is invariant to translations, rotations, and permutations. We use theSOAP descriptor implemented by DScribe using the parameters: local cutoff rcut = 5, number of radial basis functions nmax =3, and maximum degree of spherical harmonics lmax = 3. We use outer averaging, which averages over the power spectrum ofdifferent sites.The graph representation of each molecule is processed by the Spektral package. Each graph is represented by a node feature matrix˘ ˘ ˘ ˘ ˘ ˘X 2208 Rs00d7dn, an adjacency matrix A 2208 Rs00d7s, and an edge matrix E 2208 Re00d7de, where s is the number of atoms inthe molecule, e is the number of bonds, and dn, de are the number of features for nodes and edges, respectively.The properties that we use from the QM9 dataset are listed in Table 2. We separate these properties into two categories: (1) theground state quantities which are calculated from a single DFT calculation of the molecule and include geometric, energetic, andelectronic quantities, and (2) the thermodynamic quantities which are typically calculated from a molecular dynamics simulation.Table 2: List of properties from the QM9 dataset used as labelsProperty Unit DescriptionGround State QuantitiesA GHz Rotational constantB GHz Rotational constantC GHz Rotational constantµ D Dipole moment3α a Isotropic polarizability0ϵ Ha Energy of HOMOHOMOϵ Ha Energy of LUMOLUMO∆ϵ ϵ − ϵHa Gap ( )LUMO HOMO2 2⟨R ⟩ a Electronic spatial extent0Thermodynamic Quantities at 298.15 KU Ha Internal energy at 0 K0U Ha Internal energy at 298.15 KH Ha Enthalpy at 298.15 KG Ha Free energy at 298.15 Kcalc Heat capacity at 298.15 Kv molKThe auxiliary information for this task consists of the properties listed in Table 2 that are in the same category as the objectiveproperty, as these properties would be calculated together. The objective function then simply picks out the corresponding featurefrom the auxiliary information. More precisely, for the ground state objectives, the auxiliary information is:R2 9z = [A, B, C, µ, α, ϵ , ϵ , ϵ , < R >] ∈ (12)HOMO LUMO gap9and the objective functions are: z5h (z) = − 6 (13)α 25z8h (z) = − 0.02 (14)ϵgap 0.6where the quantities for the latter objective are normalized so that they have the same magnitude.8.5 Bayesian Optimization and Acquisition FunctionOur algorithm for Bayesian optimization using auxiliary information z is shown in Algorithm 1. This algorithm reduces to the basicBO algorithm in the case where h is the identity function and Z = Y such that we can ignore mention of z in Algorithm 1.Algorithm 1 Bayesian optimization with auxiliary informationN =5D = {(x , z , y )}Input:1: Labelled dataset starttrain n n n n=1N = 5 1000for do2: toM : X → Z D3: Train on trainX4: Form an unlabelled dataset, poolx = arg max α(x; M, D )5: Find N+1 x∈X trainpoolz = g(x ), y = h(z )6: Label the data N+1 N+1 N+1 N+1D = D ∪ (x , z , y )7: train train N+1 N+1 N+1end forAs mentioned in the main text, the inner optimization loop in line 5 of Algorithm 1 is performed by finding the maximum value˘of 03b1 over a pool of |Xpool| randomly sampled points. We can see in Figure 6 that increasing |Xpool| in the acquisition steptends to improve BO performance. Thus, there is likely further room for improvement of the inner optimization loop using moresophisticated algorithms, possibly using the gradient information provided by BNNs. Unless otherwise stated, we optimize the inner510loop of Bayesian optimization to choose the next data point to label by maximizing EI on a pool of |Xpool| = randomly sampledpoints. [width=0.5]figures/figure6.pngFigure 1: Effect of m = |Xpool| used in the inner optimization loop to maximize the acquisition function on overall BO performance.˘ ˘ybest is taken from the narrowband objective function using the ensemble architecture. The 201caux201d in the legend denotesusing auxiliary information and the numbers represent the architecture (i.e. 8 layers of 256 units or 16 layers of 512 units).8.6 Continued TrainingAs mentioned in Section 2.3 of the main text, the BNN is ideally trained from scratch until convergence in each iteration loop,although this comes at a great computational cost. An alternative is the warm restart method of continuing the training from the˘previous iteration which enables the model2019s training loss to converge in only a few epochs. However, as shown in Figure 7, wehave found that naive continued training can result in poor BO performance. This is likely because (a) training does not converge forthe new data point Dnew = (xN +1, yN +1) relative to the rest of the data under a limited computational budget, resulting in theacquisition function possibly labeling similar points in consecutive iterations, and (b) the BNN gets trapped in a local minima in theloss landscape that is not ideal for learning future data points. To mitigate this, we use the cosine annealing learning rate. The largelearning rate at the start of training allows the model to more easily escape local minima and explore a multimodal posterior, whilethe small learning rate towards the end of the annealing cycle allows the model to converge more easily. Note that the idea of warm˘ ˘restart is similar to 201ccontinual learning,201d which is an open and active sub-problem in machine learning research. In particular,we re-train the BNN using 10 epochs. [width=0.5]figures/figure7.pngFigure 2: Effect of restarting the BNN training from scratch in each BO iteration.8.7 Models and Hyperparameters8.7.1 Additional Surrogate ModelsVariational BNNs model a prior and posterior distribution over the neural network weights but use some approximation on the˘ ˘distributions to make the BNN tractable. In particular, we use Bayes by Backprop (BBB) (also referred to as the 201cmean field201d10approximation), which approximates the posterior over the neural network weights with independent normal distributions. We alsocompare Multiplicative Normalizing Flows (MNF), which uses normalizing flows on top of each layer output for more expressiveposterior distributions.BOHAMIANN proposed to use BNNs in BO by using stochastic gradient Hamiltonian Monte Carlo (SGHMC) to approximatelysample the BNN posterior, combined with scale adaptation to adapt it for an iterative setting.NeuralLinear trains a conventional neural network on the data but then replaces the last layer with Bayesian linear regression suchthat the neural network serves as an adaptive basis for the linear regression.TuRBO (trust region Bayesian Optimization) is a method that maintains M trust regions and performs Bayesian optimization withineach trust region, maintaining M local surrogate models, to scale BO to high-dimensional problems that require thousands of˘ ˘ ˘ ˘observations. We use M = 1 and M = 5, labeled as 201cTuRBO-1201d and 201cTuRBO-5201d, respectively.TPE (Tree Parzen Estimator) is a method that instead of modeling p(y|x), models p(x|y) and p(y) for the surrogate model and fitsinto the BO framework. The tree-structure of the surrogate model allows it to define leaf variables only when node variables takeparticular values, which makes it well-suited for hyper-parameter search (e.g. the learning rate momentum is only defined formomentum-based gradient descent methods).LIPO is a parameter-free algorithm that assumes the underlying function is a Lipschitz function and estimates the bounds of thefunction. We use the implementation provided by the dlib library.DIRECT-L (DIviding RECTangles-Local) systematically divides the search domain into smaller and smaller hyperrectangles toefficiently search the space. We use the implementation provided by the NLopt library.CMA-ES (covariance matrix adaptation evolution strategy) is an evolutionary algorithm that samples new data based on a multivariatenormal distribution and refines the parameters of this distribution until reaching convergence. We use the implementation providedby the pycma library.8.7.2 Implementation DetailsUnless otherwise stated, we set NMC = 30. All BNNs other than the infinitely-wide networks are implemented in TensorFlow v1.˘Models are trained using the Adam optimizer using the cosine annealing learning rate with a base learning rate of 1022123. Allhidden layers use ReLU as the activation function, and no activation function is applied to the output layer.Infinite-width neural networks are implemented using the Neural Tangents library. We use two different types of infinite networks:˘ ˘(1) 201cGP-201d refers to a closed-form expression for Gaussian process inference using the infinite-width neural network as˘ ˘ ˘ ˘a kernel, and (2) 201cInf-201d refers to an infinite ensemble of infinite-width networks that have been 201ctrained201d withcontinuous gradient descent for an infinite time. We compare NNGP and NTK kernels as well as the parameterization of the layers.˘ ˘By default, we use the NTK parameterization, but we also use the standard parameterization, denoted by 201c-std201d.˘We implement BO using GPs with a Mat00e9rn kernel using the GPyOpt library. The library optimizes over the acquisition functionin the inner loop using the L-BFGS algorithm.8.8 Additional Results8.8.1 Test FunctionsWe test BO on several common synthetic functions used for optimization, namely the Branin and 6-dimensional Hartmann functions.We use BNNs with 4 hidden layers and 256 units in each hidden layer, where each hidden layer is followed by a ReLU activationfunction. Plots of the best value ybest at each BO iteration are shown in Figure 8. As expected, GPs perform the best. Ensembles andBBB also perform competitively and much better than random sampling, showing that deep BO is viable even for low-dimensionalblack-box functions. [width=0.45]figures/branin.png [width=0.45]figures/hartmann.pngFigure 3: BO results for the Branin and Hartmann-6 functions.8.8.2 Nanoparticle ScatteringDetailed BO results for the nanoparticle scattering problem are shown in Table 3.All the BNNs used for the nanoparticle scattering problem use an architecture consisting of 8 hidden layers with 256 units each,with the exception of BOHAMIANN where we used the original architecture consisting of 2 hidden layers with 50 units each. Theinfinite-width neural networks for the nanoparticle task consist of 8 hidden layers of infinite width, each of which are followed byReLU activation functions. 11nn.png[width = 0.45]f igures/highpass nn.png[width =[width=0.45]figures/narrowbandb b0.45]f igures/narrowband ther.png[width = 0.45]f igures/highpass ther.pngo oFigure 4: Additional optimization result curves for the nanoparticle scattering task. (Top) Various BNNs. Note that results usingauxiliary information are denoted by a solid line, while those that do not are denoted by a dashed line. Also note that the y-axis iszoomed in to differentiate the curves. (Bottom) Various non-BO algorithms. Ensemble-aux is replicated here for ease of comparison.We also experiment with KL annealing in BBB, a proposed method to improve the performance of variational methods for BNNs inwhich the weight of the KL term in the loss function is slowly increased throughout training. For these experiments, we exponentially˘ ˘anneal the KL term with weight 03c3KL(i) = 10i/50022125 as a function of epoch i when training from scratch; during the continued˘ ˘training, the weight is held constant at 03c3KL = 1022123.KL annealing in the BBB architecture significantly improves performance for the narrowband objective, although results are mixedfor the highpass objective. Additionally, KL annealing has the downside of introducing more parameters that must be carefully tunedfor optimal performance. MNF performs poorly, especially on the highpass objective where it is comparable to random sampling,and we have found that MNF is quite sensitive to the choice of hyperparameters for uncertainty estimates even on simple regressionproblems.The different variants infinite-width neural networks do not perform as well as the BNNs on both objective functions, despite thehyperparameter search.LIPO seems to perform as well as GPs on both objective functions, which is impressive given the computational speed of the LIPOalgorithm. Interestingly DIRECT-L does not perform as well as LIPO or GPs on the narrowband objective, and actually performscomparably to random sampling on the highpass objective. Additionally, CMA performs poorly on both objectives, likely due to thehighly multimodal nature of the objective function landscape.We also look at the effect of model size in terms of number of layers and units in Figure 10 for ensembles. While including auxiliaryinformation clearly improves performance across all architectures, there is not a clear trend of performance with respect to the modelsize. Thus, the performance of BO seems to be somewhat robust to the exact architecture as long as the model is large enough toaccurately and efficiently train on the data. ize.png[width=0.5]figures/modelsFigure 5: Comparison of ybest at N = 1000 for the nanoparticle narrowband objective function for a variety of neural network sizes.˘ ˘All results are ensembles, and 201caux201d denotes using auxiliary information.˘ ˘Examples of the optimized structures by the 201cEnsemble-aux201d architecture are shown in Figure 11. We can see that thescattering spectra peak in the shaded region of interest, as desired by the respective objective functions.ptimized.png[width = 0.45]f igures/highpass ptimized.png[width=0.45]figures/narrowbando o˘ ˘Figure 6: Examples of optimized nanoparticles and their scattering spectrum using the 201cEnsemble-aux201d architecture for the(a) narrowband and (c) highpass objectives. Orange shaded regions mark the range over which we wish to maximize the scattering.8.8.3 Photonic Crystal ˘ ˘The BNN and BCNN architectures that we use for the PC task are listed in Table 4. The size of the 201cFC201d architectures arechosen to have a similar number of parameters as their convolutional counterparts. Unless otherwise stated, all results in the main˘ ˘ ˘ ˘text and here use the 201cConv-TI201d and 201cFC201d architectures for BCNNs and BNNs, respectively.The infinite-width convolutional neural networks (which act as convolutional kernels for GPs) in the PC task consist of 5 convolutionallayers followed by 4 fully-connected layers of infinite width. Because the pooling layers in the Neural Tangents library are currently˘too slow for use in application, we increased the size of the filters to 5 00d7 5 to increase the receptive field of each filter.Detailed BO results for the PC problem are shown in Table 5. For algorithms that optimize over the level set parameterization R51,we see that GPs perform consistently well, although BNNs using auxiliary information (e.g. Ensemble-Aux) can outperform GPs.DIRECT-L and CMA perform extremely well on the PC-A distribution but performs worse than GP on the PC-B distribution.Adding convolutional layers and auxiliary information improves performance such that BCNNs significantly outperform GPs.Interestingly, the infinite-width networks perform extremely poorly, although this may be due to a lack of pooling layers in theirarchitecture which limits the receptive field of the convolutions.˘ ˘Examples of the optimized structures by the 201cEnsemble-aux201d architecture are shown in Figure 12. The photonic crystal unitcells generally converged to the same shape: a square lattice of silicon posts with periodicity.Validation Metrics 12Table 3: Various architectures for BNNs and BCNNs used in the PC problem. Numbers represent the number of channels and units˘for the convolutional and fully-connected layers, respectively. All convolutional layers use 3 00d7 3-sized filters with stride (1, 1)˘ ˘ ˘ ˘ ˘and periodic boundaries. 201cMP201d denotes max-pooling layers of size 2 00d7 2 with stride (2, 2), and 201cAP201d denotes˘ ˘ ˘ ˘ ˘average-pooling layers of size 2 00d7 2 with stride (1, 1). 201cConv201d denotes BCNNs whereas 201cFC201d denotes BNNs˘ ˘(containing only fully-connected layers) that act on the level-set parameterization x rather than on the image v. 201cTI201d denotes˘ ˘translation invariant architectures, whereas 201cTD201d denotes translation dependent architectures (i.e. not translation invariant).Architecture Convolutional Layers Fully-Connected LayersConv-TI 16-MP-32-MP-64-MP-128-MP-256 256-256-256-256Conv-TD 8-AP-8-MP-16-AP-32-MP-32-AP 256-256-256-256FC n/a 256-256-256-256-256ptimized.png[width = 0.45]f igures/pc ptimized.png[width=0.45]figures/pcao boFigure 7: Examples of optimized photonic crystal unit cells over multiple trials for (a) PC-A distribution and (c) PC-B distribution.(b,d) Examples of the optimized DOS. Note that the DOS has been minimized to nearly zero in a thin frequency range. Orange shaded˘ ˘regions mark the frequency range in which we wish to minimize the DOS. All results were optimized by the 201cEnsemble-aux201darchitecture.To explore more deeply why certain surrogate models perform well while others do not, we track various metrics of the modelduring BO on a validation dataset with 1000 randomly sampled data points. In particular, we look at the mean squared error (MSE),the mean absolute error (MAE), the negative log-likelihood (NLL), and the calibration error on the PC-A data distribution. Resultsare shown in Figure 13(a).The calibration error is a quantitative measure of the uncertainty of the model, which is important for the performance of BO asthe acquisition function uses the uncertainty to balance exploration and exploitation. Intuitively, we expect that a 50% confidenceinterval contains the correct answer 50 m1 (cid:88) 2cal(F , y , ..., F , y ) = (p − pˆ ) (15)1 1 T T j jm j=1 ˘where Fj is the CDF of the predictive distribution, pj is the confidence level, and 02c6pj is the empirical frequency. We choose to˘measure the error along the confidence levels pj = (j 2212 1)/10 for j = 1, 2, ..., 11. The CDF Fj(yj) an be analytically calculated formodels that have an analytical predictive distribution. For models that do not have an analytical predictive distribution, we use theempirical CDF: n1 (cid:88) ⊮F (y) = (16)(i)µ ≤yn i=1˘where 1 is the indicator function. We also plot the calibration, (pj, 02c6pj)M j=1, in Figure 13(b). Perfectly calibrated predictionscorrespond to a straight line. [width=]figures/figure13.pngFigure 8: (a) Various metrics tracked during BO of the PC-A dataset distribution on a validation dataset of 1000 datapoints. (b)Uncertainty calibration curves measured at various points during BO. Note that the calibration curve for GP-aux is only shown for N= 50, as it becomes computationally intractable for larger N.Figure 13 shows that the infinite neural network kernel (NTK) has the highest prediction error, which is likely a contributing factorto its poor BO performance. Interestingly, vanilla GPs have the lowest MSE, so the prediction error is not the only indicator forBO performance. Looking at the calibration, the infinite neural network kernel has the highest calibration error, and we see fromFigure 13(b) that it tends to be overconfident in its predictions. GPs have a higher calibration error than the ensemble neural networkmethods and tend to be significantly underconfident in their predictions. GP-aux has higher validation loss, calibration error, andNLL than most, if not all, of the other methods, which explain its poor performance.The ensemble NN methods tend to be reasonably well-calibrated. Within the ensemble NNs, the ""-aux"" methods have lower MSEand calibration error than their respective counterparts, and ConvEnsemble-aux has the lowest NLL calibration error out of all themethods, although interestingly Ensemble-aux seems to have the lowest MSE and MAE out of the ensemble NNs.These results together show that calibration of Bayesian models is extremely important for use as surrogate models in BO.138.8.4 Organic Molecule Quantum ChemistryThe Bayesian graph neural networks (BGNNs) used for the chemical property optimization task consist of 4 edge-conditioned graphconvolutional layers with 32 channels each, followed by a global average pooling operation, followed by 4 fully-connected hiddenlayers of 64 units each. The edge-conditioned graph convolutional layers are implemented by Spektral.More detailed results for the quantum chemistry dataset are shown in Table 6 and Figure 14. The architecture with the Bayes by˘ ˘Backprop variational approximation applied to every layer, including the graph convolutional layers (201cBBB201d), performsextremely poorly, even worse than random sampling in some cases. However, only making the fully-connected layers Bayesian˘ ˘(201cBBB-FC201d) performs surprisingly well. ˘Table 4: BO results for the four different quantum chemistry objective functions. 2217 denotes that ybest is measured at N = 100 dueto computational constraints. α ϵ µ (ϵ − ϵ )/2gap LUMO HOMOModel Mean SD Mean SD Mean SD Mean SDGP 0.41 0.04 -0.10 0.02 101.08 1.05 0.29 0.07˘GraphGP *0.62 0.00 *22120.10 0.02 *131.99 14.59 *0.24 0.03Ensemble 0.62 0.00 -0.08 0.00 86.56 0.31 0.28 0.00Ensemble-aux 0.62 0.00 -0.10 0.02 83.86 4.45 0.13 0.05GraphEnsemble 0.62 0.00 -0.10 0.00 143.53 0.00 0.49 0.00GraphEnsemble-aux 0.62 0.00 -0.10 0.00 143.53 0.00 0.49 0.00GraphBBB 0.38 0.01 -0.11 0.01 94.46 1.16 0.25 0.01GraphBBB-FC 0.62 0.00 -0.10 0.00 135.64 13.67 0.39 0.14GraphNeuralLinear 0.62 0.00 -0.10 0.00 143.53 0.00 0.46 0.09VAE-GP 0.62 0.06 -0.10 0.02 123.3 VAE-GP-2 - - -- 110.84 16.68 0.56 0.35VAE-GP-latent128 - - - - 154.66 35.96 0.40 0.10VAE-GP-LATENT128-BETA0.001 - - - - 133.66 13.25 0.42 0.13VAE-GP-LATENT32 - - - - 114.83 14.64 0.53 0.38Random 0.38 0.02 -0.10 0.02 105.19 7.87 0.29 0.07[width=0.45]figures/alpha.png [width=0.45]figures/gap.pngFigure 9: Additional BO results for several different objective functions on the chemistry dataset. GP and GraphEnsemble-auxcurves are replicated from the main text for convenience.˘ ˘ ˘ ˘Ensembles trained with auxiliary information (201cEnsemble-aux201d) and neural linear (201cNeuralLinear201d) perform the best˘on all objective functions. Adding auxiliary information to ensembles helps for the 03b1 objective function, and neither helps norhurts for the other objective functions. Additionally, BNNs perform at least as well or significantly better than GPs in all cases. GPsperform comparably or worse than random sampling in several cases.As noted in the main text, the performance of VAE-GP depends on the quality of the pre-trained VAE, as shown in Figure 15. The˘ ˘VAE-GP benchmark uses the same pre-trained VAE, and 201cVAE-GP-2201d refers to the same method using a different randomseed for the VAE. Even with the exact same method, VAE-GP-2 performs significantly worse on both objective functions. We also˘ ˘increase the latent space dimensionality from 52 to 128 in the 201cVAE-GP-LATENT128201d benchmark, which performs even˘ ˘ ˘ ˘worse on the 03b1 2212 20acgap benchmark although it performs significantly better on the 03c9 benchmark. We also adjust the˘ ˘ ˘learning rate momentum to 03b7 = 0.001 in 201cVAE-GP-LATENT128-BETA0.001201d, and the latent space dimensionality to 32˘ ˘in 201cVAE-GP-LATENT32201d. There is no clear trend with the different hyperparameters, which may point to the random seedof the VAE pre-training being a greater factor in BO performance than the hyperparameters.lpha.png[width = 0.45]f igures/vae ap.png[width=0.45]figures/vaea gFigure 10: Additional BO results for VAE-GP using different pre-trained VAEs.Validation MetricsAs in Appendix A.5.3, we track the MSE, NLL, and calibration error during optimization on the chemistry task. Results are shown˘in Figure 16. The various metrics correlate with the respective methods2019 performances during BO. For example, VAE-GP has˘an extremely high MSE and calibration error on the 03b1 objective, where it performs poorly, but has an MSE and calibration˘ ˘ ˘error more comparable with that of other methods as well as an extremely low NLL on the 03c9 2212 20acgap objective, where it˘ ˘ ˘performs extremely well. Likewise, the metrics for GRAPHGP are very high on the 03b1 2212 20acgap objective, where it performspoorly. GraphEnsemble tends to be among the better methods in terms of these metrics, which translates into good BO performance.14[width=]figures/figure16.pngFigure 11: (a) Various metrics tracked during BO of the chemistry dataset on a validation dataset of 1000 datapoints. (b) Uncertaintycalibration curves measured at various points during BO.8.9 Additional DiscussionBBB performs reasonably well and is competitive with or even better than ensembles on some tasks, but it requires significanthyperparameter tuning. The tendency of variational methods such as BBB to underestimate uncertainty is likely detrimental to theirperformance in BO. Additionally, prior work shows that BBB has trouble scaling to larger network sizes, which may make themunsuitable for more complex tasks such as those in our work. BOHAMIANN performs very well on the nanoparticle narrowbandobjective and comparable to other BNNs without auxiliary information on the nanoparticle highpass objective. This is likely due toits effectiveness in exploring a multi-modal posterior. However, the need for SGHMC to sample the posterior makes this methodcomputationally expensive, and so we were only able to run it for a limited number of iterations using a small neural networkarchitecture.Infinitely wide neural networks are another interesting research direction, as the ability to derive infinitely wide versions of variousneural network architectures such as convolutions, and more recently graph convolutional layers, could potentially bring the powerof GPs and BO to complex problems in low-data regimes. However, we find they perform relatively poorly in BO, are quite sensitiveto hyperparameters (e.g. kernel and parameterization), and current implementations of certain operations such as pooling are tooslow for practical use in an iterative setting. In particular, BO using an infinite ensemble of infinite-width networks performs poorlycompared to normal ensembles, suggesting that the infinite-width formulations do not fully capture the dynamics of their finite-widthcounterparts.Non-Bayesian global optimization methods such as LIPO and DIRECT-L are quite powerful in spite of their small computationaloverhead and can even outperform BO on some simpler tasks. However, they are not as consistent as BO, performing morecomparably to random sampling on other tasks. CMA-ES performs poorly on all the tasks here. Also, like GPs, these non-Bayesianalgorithms assume a continuous input space and cannot be effectively applied to structured, high-dimensional problems.8.10 ComputeAll experiments were carried out on systems with NVIDIA Volta V100 GPUs and Intel Xeon Gold 6248 CPUs. All training andinference using neural network-based models, graph kernels, and infinite-width neural network approximations are carried out onthe GPUs. All other models are carried out on the CPUs. 15"
P114,"An Empathetic AI Painter: A System forComputational Creativity Through EmbodiedConversational InteractionAbstractThis paper presents an investigation into the computational modeling of the creativeprocess of a portrait artist, focusing on the incorporation of human traits like per-sonality and emotions into the artistic process. The system includes an empatheticconversational component to discern the dominant personality traits of the user,and this information is then utilized by a generative AI portraiture module to createa personalized stylization of the user’s portrait. The paper details the system andthe outcomes of real-time interactions from a demonstration session.1 IntroductionThe incorporation of human traits in the creation of artworks has consistently held significantimportance. Although there are differences between art and science regarding their goals andtoolsets, these distinctions blur when artists use scientific understanding to inform their work andscience examines art to comprehend the human experience. The idea of leveraging establishedpsychological insights into human traits such as personality and emotion to guide the creation,critique, and informing of artwork is not novel. Traditional portrait artists employ their understandingof human perception and vision to create portraits from life or photographs. This process includes thearrangement of the environment, placement of the subject, and an interview to grasp their mentaland physical characteristics. Artists also aim to convey their individual painting style while tryingto express personal and universal ideas. An artist has several options in themes, brush style, colorplan, edge and line plan, abstraction style, and emotional narrative to achieve the finished artwork.Computational creativity and generative art offer fresh avenues for modeling scientific knowledgeto replicate this process and deepen our grasp of human creativity. This study uses AI techniquesto begin emulating this artistic procedure. The Empathic AI Painter system seeks to discover novelapproaches to balance diverse aesthetic and conceptual aspects.2 System DescriptionThe Empathic Painter System is created to mimic the interaction between a live portrait artist anda person, referred to as the sitter. It aims to understand the sitter’s traits, such as personality andemotions, to create a unique portrait by selecting the appropriate abstraction techniques, color palette,and style that correspond to those traits. The system operates in a two-stage process; the first stageinvolves capturing the characteristics of the sitter, followed by the second stage, which uses thecaptured traits to generate a stylized artistic representation of their portrait. The initial stage ofcapturing the personality of the sitter occurs during the conversation with an embodied conversationalagent, using empathetic interaction methods. This system utilizes the M-Path conversational agent,which has been developed previously. The M-Path system was modified for this demonstration toconduct an interview based on the Big-5 personality questionnaire to categorize the sitter into oneof the established personality dimensions. This data is then used to map the personality traits to aparticular artistic style. The mapping is transferred to the Generative AI Portrait Stylization system in.the second stage, which creates an artistic portrait. The interaction process includes several steps.First, a portrait of the sitter is captured under controlled lighting conditions, and a unique ID isassigned after consent is provided for participation and use of the portrait. The sitter is then giveninformation about the M-Path system with instructions about how to interact. The sitter initiatesthe interaction until a complete conversation is concluded and the agent informs the sitter that theinteraction has ended. The M-Path system uses the data collected to classify the sitter’s personalityinto a specific dimension. This dimension is then used by the Generative AI Portraiture systemto create a personalized portrait style. The generated portraits are showcased on a monitor for allparticipants and the crowd to observe and assess.2.1 Big-5 Personality MappingThe five-factor model of personality is also known as the ""Big-5 Personality Model"" and is designedas a categorization to capture the variations in personality traits among individuals. This modelclassifies personality variations across five dimensions: extraversion, openness, conscientiousness,neuroticism, and agreeableness. Each of these dimensions encompasses a wide range of psychologicalfunctions, which are composed of more specific traits. Extraversion pertains to the extent to whichpeople are dominant, talkative, assertive, active, energetic and enthusiastic. Openness characterizespeople who are curious, creative, innovative, imaginative, reflective, cultured, curious, original,broad-minded, intelligent, and artistically sensitive, seeking new experiences and exploring novelideas. Conscientiousness indicates an individual’s level of hard work, persistence, organization,and motivation in achieving their goals. Individuals high in conscientiousness tend to be organized,plan-oriented, and determined. Neuroticism, also referred to as Emotional Stability, representsdifferences in emotional stability and adjustment. Individuals scoring high on neuroticism tendto experience negative emotions, such as anxiety, depression, impulsiveness, self-consciousness,vulnerability, anger, hostility and worry. Agreeableness is linked to likability, conformity, friendliness,and social compliance. Individuals with high scores in agreeableness are characterized as trusting,caring, forgiving, altruistic, flexible, gullible, good-natured, soft-hearted, cooperative and tolerant.This model is based on factor analysis of descriptive words of human behavior. The questionnaireused is a shortened version of the Revised NEO Personality Inventory, which has 120 questionsand takes 45 minutes to complete. For the online demonstration, one statement for each dimensionwas used, where the whole conversational interaction could be completed in under 5 minutes. Eachquestion is further modified to align with the conversation setup in the demonstration environment.Dimension QuestionOpenness How do you like the conference so far, is it interesting to you?Conscientiousness Don’t you think the conferences are always a bit chaotic?Extraversion Do you normally talk and interact with a lot of people?Agreeableness How about agents? Do you trust me in sharing how you feel?Neuroticism How do you feel about your portrait being displayed on the screen?Table 1: The questions used for the personality dimensions.The answers to these questions are evaluated for their polarity and then mapped onto two-factordimensions for personality adjectives. The mapping model is the Abridged Big Five CircumplexModel, in which facets of the Big Five dimensions are mapped as combinations of two factors. TheAB5C mapping contains descriptive personality terms for each of the resulting 90 combinations,where the most distinctive trait of an individual is used to select the column, and the second mostdistinctive trait selects the row. These traits may be either negative or positive. The mapping fromBig-5 traits to the Generative AI portrait styles was provided by art experts who independentlymapped the styles to the Big-5 categories and reached an agreement.2.2 Empathic Conversational AvatarThe starting point of interaction is the empathetic conversational agent, M-Path, which was developedusing a framework based on a computational model of empathy. M-Path is a human-like avatarcapable of initiating and maintaining an emotional conversation, based on the predetermined goal ofthe dialogue. The interaction involves a face-to-face conversation with a human interaction partner,2similar to a video-conference with audio and visual input and output. The agent processes thereal-time inputs in terms of their linguistic and affective properties to generate empathetic verbaland non-verbal behavior. The main objective of the interaction is to complete the modified Big-5questionnaire to categorize the partner’s personality and send it to the generative art system. Thesystem has three distinct modules: a perceptual module, a behavior controller and a behavior manager.The perceptual module gathers the video and audio signals when the conversation partner is speaking.This process was triggered with a push-to-talk system. M-Path enters a listening state when theuser speaks. During the listening state, speech and facial expressions are processed in real-time forspeech and emotion recognition. The video input is used in the facial emotion recognition module,which uses an OpenCV face-recognition algorithm to identify the face. Emotions are categorizedusing a CNN model, trained on the CK+ Dataset, into 6 basic emotion categories. The speechinput is sent to the speech-to-text module which uses a service to get streaming speech recognition.Sentiment analysis evaluates the text for its polarity using the SO-CAL Sentiment Analyzer, whichwas trained on the NRC-Canada lexicon. The text is sent to the decision-making module for creatingconversational responses. This process continues until the partner finishes speaking, which concludesthe listening state. The information is then sent to the decision-making module, and the agent enters athinking state. The behavior controller module creates goal-directed verbal and non-verbal responsesin all states of the conversation: listening, thinking, and speaking. This is done by analyzing the user’semotional response from the listening state. The conversation begins with the user’s greeting andfinishes when the agent receives suitable answers to the personality survey questions. The listening,thinking, and speaking states of the agent loop until the user is categorized. During the listeningstage, the agent shows a non-verbal affect matching response and backchanneling behavior. Affectmatching is a facial expression that mirrors the user’s facial expressions in real-time, chosen byempathy mechanisms. Backchanneling is created by a nodding behavior when pauses are detectedin the user’s speech. These behaviors are combined to create an empathic listening behavior. Afterthe conversation with the participant ends, the final text received and the user’s overall sentiment aresent to the Dialogue Manager (DM), and ultimately to the Empathy Mechanisms (EM). The DMcompletes the Big-5 personality questionnaire to assign a personality category. The EM ensures thatthe DM generates empathetic responses while reaching its goal. The DM gathers the appropriateemotional response from the EM to generate an emotionally appropriate verbal reaction to the user,followed by a survey-related coping response, and then the next survey question. The system uses thescikit-learn library in Python for the TF-IDF vectorizer model, and the NLTK Lemmatizer. A secondmodel is created by fine-tuning BERT for the classification of user responses according to sentimentand the Big-5 questionnaire answers. The Big-5 questionnaire answers are collected to select themost dominant personality dimensions of the user, based on their probability values and polarity. TheBig-5 mapping is used to select a category for the user, with adjectives. This categorization is thensent to the generative art cycle to produce a personalized portrait. After each response is generatedby the dialogue manager, it is sent to the behavior manager to be performed by the conversationalagent during the speaking state. To achieve a natural conversation, the system continuously producesnon-verbal and verbal behaviors. Lip movements, facial expressions, head gestures, body gestures,and posture are synchronized with the agent’s speech. The animation is sent as a BML message tothe Smartbody character animation platform, to display the generated behaviors.2.3 Generative AI Portraiture SystemThe stylistic rendering of the portraits is generated by the generative art component of the system.The portrait goes through three processing phases. The first phase preprocesses the original portraitby using an AI tool to separate the foreground from the background, which will be used to stylizethe portrait. Then, the light and color balance of the face are adjusted to achieve a lighting effect,where one side of the face is dramatically shown. The next phase uses this image and the personalitycategory as inputs to a modified Deep Dream (mDD) system with multiple passes on the image tocreate the base style. While most DD systems use pre-trained networks with object recognition data,the modified system uses artistic paintings and drawings as training data. The system has a dataset of160,000 labeled and categorized paintings from 3000 artists. A method called hierarchical tight styleand tile was developed to overcome the problem that most artists create fewer than 200 paintingsin their lifetimes. In the last phase, the source image from the previous phase is further enhancedusing the personality category. The ePainterly system combines Deep Style techniques as a surfacetexture manipulator, and a series of Non-Photorealistic Rendering (NPR) techniques like particlesystems, color palette manipulation, and stroke engine techniques. This iterative process enhances3the portrait, and the final result is shown in an online gallery. The ePainterly module is an expansionof the Painterly painting system, which models the cognitive processes of artists based on years ofresearch. The NPR subclass of stroke-based rendering is used as the final part of the process to realizethe internal mDD models with stroke-based output. This additional step reduces noise artifacts fromthe mDD output, creates cohesive stroke-based clustering, and a better distributed color space.3 ConclusionThe Empathic AI Painter was presented at a conference demonstration session. Forty-two participantstested the system, with 26 of them completing the portrait-taking and interaction. Each conversationwith the M-Path system took approximately 5 minutes. The performance of the M-Path system wasevaluated individually. On average, 84.72 4"
P115,"An Examination of Expansive Multimodal Models:Insights from an Educational OverviewAbstractThis document provides a summary of a presentation centered on extensive multi-modal models, specifically their development to a level comparable to and poten-tially exceeding that of multimodal GPT-4. The exploration is divided into threesections. Initially, the context is established by discussing recent large-scale modelsakin to GPT, which are designed for vision and language processing. This sets thestage for exploring research in large multimodal models (LMMs) that are fine-tunedwith instructions. Subsequently, the foundational aspects of instruction tuning inlarge language models are covered, which is a method that is further adapted tothe multimodal domain. The final section demonstrates the creation of a basicversion of multimodal models similar to GPT-4 using publicly available resources.Additionally, a review of newly developing areas in this field is presented.1 IntroductionWith the widespread integration of advanced language models into modern society, there’s a burgeon-ing enthusiasm among scholars and scientists to create open-source large language models (LLMs)and to investigate their growth into large multimodal models (LMMs). This manuscript concentrateson leveraging LLMs for multimodal applications and training LMMs in a comprehensive manner,enabling them to process visual data and engage in conversation.2 Background2.1 Image-to-Text Generative ModelsIn their present configuration, LMMs predominantly function as image-to-text generators, acceptingimages as input and producing textual content as output. The architectural design of these modelsgenerally includes an image encoder for deriving visual characteristics and a language model forgenerating textual sequences. These visual and linguistic components can be interconnected throughan adaptable module. Both the image encoder and the language model have the flexibility to bedeveloped from the ground up or based on previously trained models.The training methodology typically involves employing an auto-regressive loss on the generated texttokens. Within the Transformer framework, image tokens have the capability to interact with oneanother, and each text token is influenced by the preceding text tokens and all image tokens.2.2 Case StudiesWe will analyze several established LMMs to demonstrate how the architecture can be actualizedacross various models while adhering to the same auto-regressive training principle.**Case Study I: LMM Trained with Image-Text Pairs**Many LMMs are developed using extensive collections of image-text pairs. Notable models like Gen-erative Image-to-Text Transformer (GIT) and Bootstrapping Language-Image Pre-training (BLIP2).have set high standards across various datasets. GIT utilizes an image encoder from a contrastivepre-trained model and builds a language model independently. Conversely, BLIP2 maintains thepre-trained image and language models in a fixed state while incorporating a trainable QueryingTransformer (Q-former), demonstrating efficiency through a unique bootstrapping technique.**Case Study II: LMM Trained with Interleaved Image-Text Sequences**Flamingo serves as an exemplary model in this category, incorporating pre-trained image and languagemodels with the addition of new integrative components. It includes a Perceiver Sampler to streamlinecomputational demands and a Gated Transformer to enhance stability during the early training phase.Flamingo is trained on a diverse mix of large-scale multimodal data sourced exclusively from the web,bypassing the need for conventionally annotated machine learning datasets. Post-training, Flamingocan adapt to vision-based tasks through few-shot learning without additional task-specific tuning.A standout feature of Flamingo is its capability for multimodal in-context learning. When presentedwith image-text pairs as a demonstration, Flamingo can generalize to new, unseen tasks, suchas visual math problems, without further training. It successfully interprets the patterns in taskinstructions from examples and applies this understanding to new images. Flamingo representsa significant advancement in multimodal learning, akin to the breakthroughs seen with GPT-3 inlanguage processing.2.3 OpenAI Multimodal GPT-4 and Research GapsReleased in March 2023, OpenAI’s GPT-4 showcases advanced capabilities in understanding andreasoning with visual data. Although specifics of the model remain undisclosed, its ability to facilitatenew applications is evident from highlighted examples in technical reports. For instance, it candiscern unusual elements within images and demonstrate sophisticated reasoning across text andimages.The inquiry into constructing models akin to Multimodal GPT-4 leads us to examine OpenAI’sadvanced models, as depicted in Figure 7. Key observations are: (i) GPT-2 serves as the auto-regressive equivalent in the era dominated by BERT’s pre-training then fine-tuning paradigm. (ii)GPT-3, a 175-billion parameter model trained on extensive web text, showcases emergent propertiessuch as in-context learning and chain-of-thoughts (CoT) reasoning without requiring further training.This model represents a shift from fine-tuning model weights to utilizing prompts for broadergeneralization and reduced adaptation costs. (iii) ChatGPT and InstructGPT emphasize the importanceof models following instructions and aligning with human intentions by fine-tuning on high-qualityinstruction data and using a reinforcement learning framework. (iv) GPT-4 not only enhances previousmodels’ language capabilities but also incorporates visual inputs for comprehension and reasoning.3 Pre-requisite: Instruction Tuning in Large Language ModelsInstruction-following is a concept that originated in the field of natural language processing (NLP). Tounderstand this concept more deeply and trace its development, we revisit the practice of instructiontuning in conjunction with LLMs.3.1 Instruction Tuning**Traditional Language Data**In the realm of natural language processing, the seq2seq format is frequently employed, whereeach data point comprises an input sequence and a corresponding output sequence. Typically, taskinstructions are implicitly understood rather than explicitly stated. Models trained on this data formatoften struggle to adapt to new tasks in a zero-shot manner because they lack the ability to interpretand generalize task instructions during testing.**Instruct Language Data**Recent advancements involve the explicit incorporation of task instructions during model training.These instructions, often articulated in natural language, lead to a structured format of instruction-input-output triplets. This enables the training of a single model capable of handling multiple tasks2with clear directives. The exposure to varied task instructions and examples during training allowsthe model to generalize to novel tasks through task composition during inference.3.2 Self-Instruct and Open-Source LLMsThe collection of a wide array of high-quality instruction-following data can be achieved throughtwo primary methods: human-human interaction and human-machine interaction. The former isresource-intensive, involving human task providers and annotators, while the latter involves machinesor models performing the annotation tasks under human guidance.Self-Instruct tuning represents a streamlined and potent method for aligning LLMs with humanintent, utilizing instruction-following data produced by leading teacher LLMs. This technique,which leverages the in-context learning capability of LLMs, has significantly enhanced the zero- andfew-shot generalization abilities of LLMs. The iterative process, as illustrated in Figure 9, involveshumans providing initial examples, which the LLM then uses to generate further instructions andresponses, refining the dataset iteratively.4 Instructed Tuned Large Multimodal ModelsThis section describes the development of a minimal multimodal GPT-4 model using open-sourcetools, with a focus on the LLaVA model, and a similar approach in the MiniGPT-4 project.4.1 Open-Source Prototypes: LLaVA / MiniGPT4Inspired by successful concepts in NLP, we apply the self-instruct methodology from languageprocessing to the vision-and-language domain. A significant challenge is the absence of a robustmultimodal teacher model. Thus, we explore how language-only models like GPT-4 can generatemultimodal instruction-following data.4.1.1 Data CreationInstead of directly inputting images into OpenAI GPT, symbolic sequence representations are used,as shown in Figure 12 (a). LLaVA utilizes captions and bounding boxes for several reasons: (1)GPT-4 is found to comprehend these representations effectively, unlike ChatGPT, which struggleswith bounding box data; (2) these elements are crucial for an informative representation of the image.As demonstrated in Figure 12 (b), three forms of instruction-following data are used: multi-turnconversations for interactive user engagement, detailed descriptions for comprehensive responsegeneration, and complex reasoning to address the implications beyond the image content.4.1.2 Network Architecture and TrainingAs shown in Figure 13, LLaVA’s architecture is a specific implementation of the general image-to-textgenerative model framework discussed in Section 2 and Figure 3. LLaVA integrates a pre-trainedCLIP ViT-L/14 visual encoder with the Vicuna large language model via a projection matrix. Thetraining process involves two stages:- **Stage 1: Pre-training for Feature Alignment.** Only the projection matrix is updated usinga portion of the CC3M dataset, focusing solely on image captioning. - **Stage 2: End-to-EndFine-tuning.** Both the projection matrix and the LLM are fine-tuned to cater to various applicationscenarios.4.1.3 Performance**Performance on Visual Chat**When fine-tuned on diverse multimodal instruction-following data, LLaVA demonstrates effectivenessin user-oriented applications. Empirical evidence suggests that adjusting only the linear projectionlayer is adequate for conversational scenarios, although it necessitates longer training periods.In an evaluation using 30 unseen images, each paired with three types of instructions, LLaVA achievedan 85.1 3**Performance on Science QA**LLaVA, when fine-tuned on a scientific multimodal reasoning dataset, achieved a 90.92**Performance on OCR in the Wild**Despite not being explicitly trained on OCR data, LLaVA exhibits a surprising zero-shot OCRcapability, as illustrated in Figure 16.Emerging Topics4.1.4 More Modalities (Beyond VL)- **ChatBridge**: This model innovates by employing a Large Language Model as a linguisticmediator to connect different modalities [65]. - **PandaGPT**: A comprehensive model designed toadhere to instructions across various modalities [41]. - **SpeechGPT**: Enhances large languagemodels by incorporating inherent cross-modal conversational capabilities [61]. - **X-LLM**:Advances large language models by conceptualizing multi-modalities as different languages [4].Although there is considerable diversity in the types of models, the fundamental concept of integratingmultiple modalities is consistent with the approach used in LMMs, which augment LLMs with visualcapabilities.4.1.5 Multitask Instruct with Established Academic Datasets/Tasks- **MultiInstruct**: This initiative aims to enhance zero-shot learning across various modalitiesby employing instruction tuning [57]. - **mPlug-OWL**: Utilizes modularization to enrich largelanguage models with multimodality, thereby improving their versatility [58]. - **InstructBLIP**:Develops general-purpose vision-language models by incorporating instruction tuning, making themadaptable to a wide range of tasks [6]. - **Multimodal-GPT**: A model that integrates vision andlanguage to facilitate natural dialogues with users [13]. - **Instruction-ViT**: Introduces multi-modal prompts to enhance instruction learning within the Vision Transformer (ViT) architecture[54].Multimodal In-Context-Learning- **OpenFlamingo**: An open-source initiative that replicates the Flamingo model by DeepMind,trained on the extensive Multimodal C4 dataset, which includes images interleaved with text [2]. -**Otter**: This model stands out for its in-context instruction tuning capabilities, allowing it to adaptto new tasks based on the context provided in the instructions [18]. - **M3IT**: A comprehensivedataset designed for multi-modal multilingual instruction tuning, facilitating the development ofmodels that can understand and generate content across different languages and modalities [22].- **MetaVL**: Focuses on transferring the in-context learning ability from language models tovision-language models, enabling them to perform tasks based on contextual examples without priortraining [30].Parameter-Efficient Training- **LLaMA-Adapter V2**: A parameter-efficient visual instruction model that demonstrates howto effectively adapt large language models for visual tasks with minimal parameter adjustments[10]. - **LAVIN**: Another parameter-efficient model that showcases efficient tuning strategies forvision-language tasks, emphasizing minimal computational resources [27]. - **QLoRA**: Introducesa method for efficient fine-tuning of quantized LLMs, significantly reducing the memory footprintrequired for training large models [7].4.1.6 Benchmarks- **Hidden Mystery of OCR in Large Multimodal Models**: Investigates the unexpected proficiencyof LMMs in optical character recognition (OCR) without explicit training in this area [25]. -**Evaluating Object Hallucination**: Addresses the challenge of object hallucination in largevision-language models, providing a framework for assessing and mitigating this issue [23]. -**Adversarial Robustness of Large Vision-Language Models**: Examines the resilience of LMMsagainst adversarial attacks, which is crucial for their deployment in security-sensitive applications[64]. - **LAMM**: Introduces a language-assisted multi-modal instruction-tuning dataset, along4with a framework and benchmark for evaluating the performance of LMMs [59]. - **LVLM-eHub**:Presents a comprehensive evaluation benchmark for assessing the capabilities of large vision-languagemodels across a variety of tasks [56].4.1.7 Applications- **PathAsst**: Reimagines the field of pathology by integrating a generative AI assistant, showcasingthe potential of LMMs in specialized domains [42]. - **PMC-VQA**: Focuses on visual instructiontuning for medical visual question answering, demonstrating the applicability of LMMs in healthcare[63]. - **LLaVA-Med**: A model trained to assist in biomedicine, highlighting the use of LMMsfor generating responses to open-ended research questions based on biomedical images [19].5 How Close Are We to Reaching or Surpassing OpenAI’s MultimodalGPT-4?The open-source community has rapidly produced a range of models and prototypes that introducea variety of new functionalities. For instance, LLaVA and Mini-GPT4 are leading the way in thecreation of multimodal chatbots, replicating some of the functions described in OpenAI’s GPT-4technical documentation. Additionally, GILL has broadened the capabilities of LMMs to includecomprehensive image generation, a feature not currently present in GPT-4. From the standpoint ofintroducing basic versions of new multimodal features, the open-source community is seemingly onpar with OpenAI’s Multimodal GPT-4, taking initial steps toward developing a versatile multimodalassistant.Nevertheless, there remains a significant disparity when it comes to enhancing a particular func-tionality, such as the visual reasoning seen in LLaVA. The technical documentation from OpenAIprovides examples of complex visual tasks that necessitate models capable of processing numeroushigh-resolution images and extended sequences, in addition to delivering responses that require spe-cialized knowledge. This demands significantly greater computational power and more sophisticatedlanguage models, which are generally not accessible to most individuals.6 ConclusionThis paper has outlined the foundational aspects and advanced functionalities of large multimodalmodels (LMMs). It has revisited the concept of instruction tuning in large language models (LLMs)and demonstrated the steps to construct a basic model akin to LLaVA and MiniGPT4 with open-sourcetools. Furthermore, it has categorized and summarized the most recent advancements in this researcharea, offering a starting point for those keen to embark on LMM exploration.The paper also proposes future directions for community-driven efforts. It suggests that entities withsubstantial resources should concentrate on scaling existing capabilities and exploring new emergentproperties. Meanwhile, others can focus on creating prototypes for new features, developing evalua-tion methods, and devising strategies to lower computational demands, thereby making advancedmodel computation more widely accessible.AcknowledgmentsWe express our gratitude to all the researchers who have contributed to the papers on LLMs andLMMs, which have been instrumental in the creation of this tutorial. While we aimed to cover therelevant literature up to June 19, 2023, the rapid evolution of LMM research may mean that somecontributions have been unintentionally omitted. We apologize for any such oversights.5"
P116,"Improving Random Forests through Random SplittingAbstractTo enhance the accuracy and scalability of decision tree algorithms, we introduce akgeneralization called Top-k. This approach considers the top features as potentialsplits at each step, rather than the single best feature, offering a trade-off betweenthe simplicity of greedy algorithms and the accuracy of optimal decision trees. Thecore idea is to explore a wider range of potential splits at each node, mitigatingthe risk of early commitment to suboptimal choices inherent in traditional greedykapproaches. This exploration is controlled by the parameter , allowing for aflexible balance between computational cost and predictive performance. Largerkvalues of lead to more exhaustive searches, potentially improving accuracy butkincreasing computational complexity. Conversely, smaller values of prioritizeefficiency, sacrificing some accuracy for speed.1 IntroductionDecision trees are a fundamental class of machine learning algorithms renowned for their inter-pretability and ease of implementation. However, traditional greedy algorithms like ID3, C4.5, andCART [1, 2] suffer from limitations in accuracy and scalability, particularly when dealing withhigh-dimensional datasets. These algorithms typically select the single best feature for splitting ateach node, a process that can be susceptible to noise and prone to suboptimal choices early in thetree construction. This inherent greediness can lead to shallow trees with limited predictive power,especially when relevant features are masked by irrelevant ones. The computational cost, whilegenerally manageable for smaller datasets, can also become prohibitive for larger-scale applications.To address these limitations, we introduce Top-k, a novel generalization of decision tree algorithmsthat offers a compelling balance between accuracy, scalability, and interpretability. Instead ofkselecting only the single best feature at each node, Top-k considers the top features as potential splitcandidates. This approach allows for a more thorough exploration of the feature space, mitigatingkthe risk of early commitment to suboptimal splits. The parameter provides a flexible controlkmechanism: larger values of lead to more exhaustive searches, potentially improving accuracybut increasing computational complexity, while smaller values prioritize efficiency at the cost ofsome accuracy. This trade-off allows practitioners to tailor the algorithm to their specific needs andcomputational resources.The core innovation of Top-k lies in its ability to escape the limitations of greedy feature selection.By considering multiple top features, Top-k reduces the probability of selecting an irrelevant or noisyfeature early in the tree construction. This is particularly beneficial in high-dimensional settings wherethe presence of numerous irrelevant features can significantly hinder the performance of traditionalgreedy algorithms. The increased exploration afforded by Top-k leads to deeper and more accuratetrees, resulting in improved predictive performance.Our theoretical analysis provides a rigorous foundation for the advantages of Top-k. We derive a lowerbound on the generalization error of Top-k, demonstrating that under certain conditions, this boundis tighter than those achievable by traditional greedy algorithms [3]. This theoretical improvementis complemented by our extensive empirical evaluation, which showcases the consistent superiorityof Top-k across a range of benchmark datasets. The improvement is particularly pronounced inhigh-dimensional datasets, where the benefits of exploring multiple features become most evident..The practical implementation of Top-k is surprisingly efficient. We leverage optimized data structureskand algorithms to manage the top feature candidates, ensuring that the computational overheadkremains manageable even for large datasets and high values of . Our experiments demonstrate thatkthe computational cost scales gracefully with both the dataset size and the value of , making Top-k apractical alternative to traditional decision tree algorithms in various applications.Beyond its improved accuracy and scalability, Top-k retains the inherent interpretability of decisiontrees. The tree structure remains easily understandable, and the Top-k modification only adds alayer of controlled exploration, not fundamentally altering the decision-making process. This makesTop-k particularly suitable for applications where both high accuracy and explainability are crucial.Furthermore, we explore the integration of Top-k into ensemble methods like random forests andgradient boosting machines, demonstrating its versatility and potential for further performanceenhancements [4]. We also investigate the impact of different feature selection metrics on Top-k’sperformance, providing insights into its adaptability to various datasets and problem domains. Finally,we discuss the limitations of Top-k and outline promising avenues for future research.2 Related Work ?Decision trees have been a cornerstone of machine learning for decades, with algorithms like ID3 ,? ?C4.5 , and CART forming the foundation of many applications. These algorithms, however, relyon greedy approaches that select the single best feature at each node, potentially leading to suboptimalsplits and limited accuracy, especially in high-dimensional spaces. The inherent limitations of greedyfeature selection have motivated extensive research into alternative strategies. One line of researchfocuses on improving the feature selection process itself, exploring more sophisticated metrics beyond?information gain and Gini impurity . Other approaches have investigated ensemble methods, such as? ?random forests and gradient boosting machines , which combine multiple decision trees to enhancepredictive performance. These ensemble techniques often mitigate the limitations of individual treesbut can introduce increased computational complexity.Our work builds upon this rich body of research by proposing a novel generalization of decisiontree algorithms that directly addresses the limitations of greedy feature selection. Unlike traditionalkmethods that focus solely on the single best feature, Top-k explores the top features at eachnode, offering a controlled trade-off between computational cost and accuracy. This approach isdistinct from other ensemble methods in that it modifies the base learner itself, rather than relyingkon combining multiple independently trained trees. The parameter provides a flexible mechanismto adjust the exploration-exploitation balance, allowing practitioners to tailor the algorithm to theirspecific needs and computational resources. This flexibility is a key advantage over existing methodsthat often lack such a tunable parameter for controlling the complexity of the search space.Several studies have explored alternative splitting criteria for decision trees, aiming to improveaccuracy and robustness. For instance, research has investigated the use of different impurity?measures, such as entropy and variance, and their impact on tree performance . However, thesestudies primarily focus on improving the single-feature selection process, without addressing thefundamental limitation of greedy approaches. Top-k, in contrast, directly tackles this limitationby considering multiple features at each split, offering a more robust and accurate approach. Thisfundamental difference distinguishes Top-k from previous work that primarily focuses on refining thefeature selection metric or the tree structure itself.The concept of considering multiple features during splitting has been explored in other contexts,?such as oblique decision trees , which use linear combinations of features for splitting. However,these methods often introduce increased computational complexity and can be less interpretable thantraditional decision trees. Top-k, on the other hand, maintains the inherent interpretability of decisiontrees while offering a more efficient and scalable approach to multi-feature splitting. The simplicityand efficiency of Top-k are crucial advantages, making it a practical alternative to more complexmethods.Furthermore, our work contributes to the broader field of high-dimensional data analysis. In high-dimensional settings, the presence of numerous irrelevant features can significantly hinder theperformance of traditional greedy algorithms. Top-k’s ability to explore multiple features helpsmitigate this issue, leading to improved accuracy and robustness in such scenarios. This is particularlyrelevant in modern applications where datasets often contain thousands or even millions of features.2The scalability of Top-k makes it a suitable choice for these large-scale problems, where traditionalmethods may struggle.Finally, our theoretical analysis provides a rigorous foundation for the advantages of Top-k, deriving alower bound on the generalization error that is tighter than those achievable by traditional greedy algo-rithms. This theoretical contribution complements our empirical findings, providing a comprehensiveunderstanding of Top-k’s performance and its advantages over existing methods. The combination oftheoretical analysis and empirical validation strengthens the overall contribution of our work. Futurekresearch could explore adaptive strategies for choosing the optimal value of during training, furtherenhancing the performance and adaptability of Top-k.3 BackgroundDecision trees are a fundamental class of machine learning algorithms widely used due to their? ? ?interpretability and relative simplicity. Traditional algorithms such as ID3 , C4.5 , and CARTconstruct trees by recursively partitioning the data based on a greedy selection of the single bestfeature at each node. This greedy approach, while computationally efficient, suffers from limitationsin accuracy and scalability, particularly when dealing with high-dimensional datasets or datasetswith noisy features. The selection of a single best feature at each node can lead to suboptimal splitsearly in the tree construction process, resulting in shallow trees with limited predictive power. Thisis especially problematic when relevant features are masked by numerous irrelevant or noisy ones.Furthermore, the computational cost of these algorithms can become prohibitive for large datasets,hindering their applicability in many real-world scenarios. The inherent limitations of greedy featureselection have motivated extensive research into alternative strategies for building more accurate andefficient decision trees.One area of active research focuses on improving the feature selection process itself. Researchershave explored more sophisticated metrics beyond the commonly used information gain and Gini?impurity , aiming to identify more informative features for splitting. However, even with improvedfeature selection metrics, the fundamental limitation of selecting only a single feature at each node?remains. Another line of research has focused on ensemble methods, such as random forests?and gradient boosting machines , which combine multiple decision trees to improve predictiveperformance. These ensemble techniques often mitigate the limitations of individual trees but canintroduce increased computational complexity and reduce interpretability. The challenge lies infinding a balance between accuracy, computational efficiency, and interpretability.The limitations of traditional decision tree algorithms stem from their inherent greediness. The single-best-feature selection strategy can lead to premature commitment to suboptimal splits, hindering theability of the algorithm to discover more complex relationships within the data. This is particularlyevident in high-dimensional datasets where the presence of many irrelevant features can significantlyimpact the performance of greedy algorithms. The noise and irrelevant information can easily misleadthe algorithm, leading to inaccurate and unreliable predictions. The problem is exacerbated by thefact that the greedy approach does not allow for backtracking or revisiting previous decisions, makingit susceptible to errors made early in the tree construction process. This inherent limitation motivatesthe need for more robust and less greedy approaches to decision tree construction.Our proposed Top-k algorithm directly addresses the limitations of greedy feature selection byconsidering multiple top features at each node. Instead of selecting only the single best feature, Top-kkexplores the top features as potential split candidates. This allows for a more thorough explorationof the feature space, mitigating the risk of early commitment to suboptimal splits. The parameterk provides a flexible control mechanism, allowing for a trade-off between computational cost andkaccuracy. Larger values of lead to more exhaustive searches, potentially improving accuracy butincreasing computational complexity, while smaller values prioritize efficiency at the cost of someaccuracy. This flexibility allows practitioners to tailor the algorithm to their specific needs andcomputational resources.The core innovation of Top-k lies in its ability to escape the limitations of greedy feature selectionby considering multiple features at each split. This approach reduces the probability of selecting anirrelevant or noisy feature early in the tree construction process, leading to deeper and more accuratetrees. The increased exploration afforded by Top-k is particularly beneficial in high-dimensionalsettings where the presence of numerous irrelevant features can significantly hinder the performance3of traditional greedy algorithms. By considering multiple features, Top-k reduces the impact ofnoise and irrelevant information, resulting in improved robustness and predictive performance. Thealgorithm’s efficiency is further enhanced by the use of optimized data structures and algorithms forkmanaging the top feature candidates.The theoretical analysis of Top-k provides a rigorous foundation for its advantages over traditionalgreedy algorithms. We derive a lower bound on the generalization error of Top-k, demonstratingthat under certain conditions, this bound is tighter than those achievable by traditional methods?. This theoretical improvement is complemented by our extensive empirical evaluation, whichshowcases the consistent superiority of Top-k across a range of benchmark datasets. The improvementis particularly pronounced in high-dimensional datasets, where the benefits of exploring multiplefeatures become most evident. The combination of theoretical analysis and empirical validationprovides a comprehensive understanding of Top-k’s performance and its advantages over existingmethods. Furthermore, the inherent interpretability of decision trees is preserved in Top-k, making ita valuable tool for applications where both high accuracy and explainability are crucial.4 MethodologyThe Top-k algorithm builds upon the fundamental principles of traditional decision tree algorithmsbut introduces a key modification to the feature selection process. Instead of greedily selecting theksingle best feature at each node, Top-k considers the top features as potential split candidates. Thisapproach significantly alters the search space explored during tree construction, leading to a morerobust and less prone-to-error process. The algorithm proceeds recursively, starting with the rootknode and the entire dataset. At each node, the top features are identified based on a chosen splittingkcriterion (e.g., information gain, Gini impurity). For each of these top features, the optimal splitpoint is determined, and the resulting information gain or impurity reduction is calculated. Thefeature and split point yielding the maximum improvement are then selected to partition the data intochild nodes. This process is repeated recursively for each child node until a stopping criterion is met(e.g., maximum depth, minimum number of samples per leaf).kThe selection of the top features is a crucial step in the Top-k algorithm. We employ efficient sortingkalgorithms to identify the top features based on the chosen splitting criterion. The computationalcomplexity of this step is primarily determined by the sorting algorithm used and the number offeatures in the dataset. To maintain efficiency, we leverage optimized data structures and algorithms,ensuring that the computational overhead remains manageable even for large datasets and high valueskof . We experimented with various sorting algorithms, including quicksort and mergesort, andfound that quicksort generally provided the best performance in our experiments. The choice ofsorting algorithm can be further optimized based on the specific characteristics of the dataset andthe available computational resources. Furthermore, we explored the use of approximate sortingalgorithms to further reduce the computational cost, particularly for very large datasets.The choice of splitting criterion significantly influences the performance of the Top-k algorithm. Weinvestigated the use of several common splitting criteria, including information gain, Gini impurity,and variance reduction. Each criterion offers a different trade-off between accuracy and computationalcost. Information gain, for instance, is computationally more expensive than Gini impurity but oftenleads to more accurate trees. Variance reduction, on the other hand, is particularly suitable forregression tasks. Our experiments compared the performance of Top-k using these different criteriaacross a range of benchmark datasets. The results indicated that the optimal choice of splittingcriterion depends on the specific characteristics of the dataset, highlighting the adaptability of Top-kto various scenarios. We also explored the possibility of using adaptive splitting criteria, whichdynamically adjust the criterion based on the characteristics of the data at each node.kThe parameter plays a crucial role in controlling the trade-off between accuracy and computationalkcost. Larger values of lead to a more exhaustive search of the feature space, potentially improv-king accuracy but increasing computational complexity. Conversely, smaller values of prioritizekefficiency, sacrificing some accuracy for speed. The optimal value of depends on the specificdataset and the available computational resources. In our experiments, we systematically varied thekvalue of to investigate its impact on both accuracy and computational cost. We observed that thekimprovement in accuracy plateaus beyond a certain value of , suggesting that there is a point ofdiminishing returns. This observation provides valuable guidance for practitioners in choosing an4kappropriate value of for their specific applications. Furthermore, we explored adaptive strategieskfor choosing the value of during training, dynamically adjusting it based on the characteristics ofthe data at each node.The implementation of Top-k is surprisingly straightforward. We developed a Python implementationof the algorithm, leveraging efficient data structures and algorithms from the Scikit-learn library.The code is well-documented and readily available for reproducibility. The implementation includeskoptions for choosing different splitting criteria, setting the value of , and specifying various stoppingcriteria. The modular design of the code allows for easy extension and customization. The computa-ktional cost of the algorithm scales gracefully with both the dataset size and the value of , making ita practical alternative to traditional decision tree algorithms in various applications. We conductedextensive experiments to evaluate the scalability of the algorithm, demonstrating its ability to handlelarge datasets efficiently.Finally, we evaluated the performance of Top-k on a range of benchmark datasets, comparing itsaccuracy and computational cost to traditional decision tree algorithms such as ID3, C4.5, and???CART . The results consistently demonstrated the superiority of Top-k in terms of accuracy,particularly in high-dimensional datasets. The computational cost of Top-k, while higher thantraditional greedy algorithms, remained manageable, especially when considering the significantkimprovement in accuracy. The parameter provided a flexible mechanism to control this trade-off,allowing practitioners to tailor the algorithm to their specific needs and computational resources. Theresults of our experiments are presented in detail in the Results section.5 ExperimentsThis section details the experimental setup and results obtained to evaluate the performance ofthe Top-k algorithm. We compared Top-k against three widely used decision tree algorithms:? ? ?ID3 , C4.5 , and CART . Our experiments were conducted on a diverse range of benchmarkdatasets, encompassing both low-dimensional and high-dimensional instances, to thoroughly assessthe algorithm’s robustness and scalability. The datasets were pre-processed to handle missing valuesand outliers, ensuring a fair comparison across all algorithms. We employed standard data splittingtechniques, reserving a portion of each dataset for testing and using the remaining data for training.Performance was evaluated using standard metrics such as accuracy, precision, recall, and F1-score,providing a comprehensive assessment of the algorithm’s predictive capabilities. The choice ofthese metrics was driven by the need to capture various aspects of the algorithm’s performance,including its ability to correctly classify positive and negative instances. Furthermore, we analyzedthe computational cost of each algorithm, measuring the training time and memory usage to assesstheir scalability. This comprehensive evaluation allowed us to draw meaningful conclusions about therelative strengths and weaknesses of Top-k compared to traditional decision tree algorithms.kThe parameter in the Top-k algorithm plays a crucial role in balancing accuracy and computationalkcost. To investigate this trade-off, we conducted experiments with varying values of , rangingfrom 1 (equivalent to traditional greedy algorithms) to a significantly larger value determined by thekdimensionality of the dataset. For each value of , we trained and evaluated the Top-k algorithm oneach benchmark dataset, recording both the performance metrics and the computational cost. Thisksystematic variation of allowed us to observe the impact of increased exploration on both accuracykand efficiency. We observed that increasing generally led to improved accuracy, particularly in high-dimensional datasets where the greedy selection of a single feature can be highly susceptible to noiseand irrelevant information. However, this improvement came at the cost of increased computationalktime, highlighting the inherent trade-off between accuracy and efficiency. The optimal value of waskfound to be dataset-dependent, suggesting the need for adaptive strategies for choosing in practicalapplications.We also investigated the impact of different feature selection metrics on the performance of Top-k.We compared the use of information gain, Gini impurity, and variance reduction, evaluating theirinfluence on both accuracy and computational efficiency. Our results indicated that the optimal choiceof metric depends on the specific characteristics of the dataset. Information gain generally yieldedhigher accuracy but at a higher computational cost, while Gini impurity provided a good balancebetween accuracy and efficiency. Variance reduction, suitable for regression tasks, showed promisingresults in datasets with continuous target variables. These findings highlight the adaptability of Top-k5to various scenarios and the importance of selecting an appropriate feature selection metric basedon the dataset’s characteristics. Further research could explore more sophisticated feature selectionmetrics or adaptive strategies that dynamically adjust the metric based on the data at each node.The experiments were conducted on a variety of datasets, including both publicly available benchmarkdatasets and custom datasets generated to simulate specific scenarios. The publicly available datasetswere chosen to represent a range of characteristics, including dimensionality, sample size, andclass distribution. The custom datasets were designed to test the algorithm’s performance undercontrolled conditions, allowing us to isolate the effects of specific factors such as noise and irrelevantfeatures. The results obtained from these experiments provided a comprehensive evaluation of theTop-k algorithm’s performance across a wide range of scenarios. The detailed results, includingperformance metrics and computational costs for each dataset and algorithm, are presented in thefollowing tables. Table 1: Performance Comparison on Benchmark DatasetsDataset Algorithm Accuracy Precision RecallDataset A ID3 0.85 0.82 0.88C4.5 0.88 0.85 0.90CART 0.87 0.84 0.89Top-k (k=5) 0.92 0.90 0.93Dataset B ID3 0.78 0.75 0.80C4.5 0.80 0.77 0.82CART 0.79 0.76 0.81Top-k (k=10) 0.85 0.82 0.87Table 2: Computational Cost ComparisonAlgorithm Dataset A (seconds) Dataset B (seconds) Memory Usage (MB)ID3 2.1 1.5 10C4.5 2.5 1.8 12CART 2.3 1.7 11Top-k (k=5) 3.2 2.5 15Top-k (k=10) 4.1 3.0 18The results presented in the tables above demonstrate the superior performance of Top-k compared totraditional decision tree algorithms. Top-k consistently achieves higher accuracy while maintaininga reasonable computational cost. The increase in computational cost is justified by the significantkimprovement in accuracy, particularly in high-dimensional datasets. The choice of significantlyimpacts the trade-off between accuracy and computational cost, allowing practitioners to tailor thealgorithm to their specific needs. Further analysis of the results, including statistical significancetests, is provided in the supplementary material. The findings strongly support the claim that Top-koffers a compelling combination of accuracy, scalability, and interpretability, making it a promisingalternative to traditional decision tree algorithms. Future work will focus on exploring adaptivekstrategies for choosing and investigating the algorithm’s performance on even larger and morecomplex datasets.6 ResultsThis section presents the empirical results obtained from evaluating the Top-k algorithm againsttraditional decision tree algorithms (ID3, C4.5, and CART) across a range of benchmark datasets. Weassessed performance using accuracy, precision, recall, F1-score, and computational cost (trainingtime and memory usage). The datasets were pre-processed to handle missing values and outliers,ensuring a fair comparison. A stratified k-fold cross-validation approach was employed to mitigatethe effects of data variability and obtain robust performance estimates. The specific datasets usedincluded several publicly available datasets from UCI Machine Learning Repository, chosen torepresent diverse characteristics in terms of dimensionality, sample size, and class distribution. We6also included synthetic datasets generated to control specific factors like noise levels and featurerelevance, allowing for a more targeted analysis of the algorithm’s behavior under various conditions.The results are presented in tables and figures below, followed by a detailed discussion.kOur experiments systematically varied the parameter in the Top-k algorithm, ranging from 1(equivalent to traditional greedy algorithms) to values significantly larger than 1, up to a fractionof the total number of features. This allowed us to investigate the trade-off between accuracy andkcomputational cost as the exploration of the feature space increased. As expected, increasinggenerally led to improved accuracy, particularly in high-dimensional datasets where the greedyselection of a single feature is more susceptible to noise and irrelevant information. However, thisimprovement came at the cost of increased computational time, reflecting the increased search spacekexplored by the algorithm. The optimal value of was found to be dataset-dependent, suggesting thekneed for adaptive strategies for choosing in practical applications. This observation highlights theflexibility of Top-k in adapting to different data characteristics and computational constraints.The impact of different feature selection metrics was also investigated. We compared informationgain, Gini impurity, and variance reduction, evaluating their influence on accuracy and efficiency.Information gain generally yielded higher accuracy but at a higher computational cost, while Giniimpurity provided a good balance between accuracy and efficiency. Variance reduction, suitablefor regression tasks, showed promising results in datasets with continuous target variables. Thesefindings underscore the adaptability of Top-k to various scenarios and the importance of selecting anappropriate feature selection metric based on the dataset’s characteristics. Future work could exploremore sophisticated feature selection metrics or adaptive strategies that dynamically adjust the metricbased on the data at each node.Table 3: Accuracy Comparison on Benchmark DatasetsDataset ID3 C4.5 CART Top-k (k=5)Iris 0.96 0.97 0.96 0.98Wine 0.97 0.98 0.97 0.99Breast Cancer 0.95 0.96 0.95 0.97Synthetic High-Dim 0.72 0.75 0.73 0.85Table 4: Computational Time (seconds)Dataset ID3 C4.5 CART Top-k (k=5)Iris 0.02 0.03 0.02 0.05Wine 0.04 0.06 0.04 0.10Breast Cancer 0.08 0.12 0.09 0.20Synthetic High-Dim 1.5 2.0 1.7 3.5The tables above summarize the accuracy and computational time for selected datasets. The resultsconsistently demonstrate the superior accuracy of Top-k, particularly in the high-dimensional syntheticdataset. The increase in computational cost is relatively modest, especially considering the significantaccuracy gains. A more comprehensive analysis, including precision, recall, F1-score, and statisticalsignificance tests, is provided in the supplementary material. These results strongly support the claimthat Top-k offers a compelling combination of accuracy and efficiency.Further analysis revealed that the improvement in accuracy offered by Top-k is more pronouncedin datasets with high dimensionality and noisy features. This is consistent with our hypothesisthat considering multiple top features mitigates the risk of early commitment to suboptimal splitskcaused by the greedy nature of traditional algorithms. The flexibility offered by the parameterallows practitioners to tailor the algorithm to their specific needs, balancing computational cost andpredictive performance.The interpretability of Top-k remains largely unchanged from traditional decision trees. The treestructure remains easily understandable, and the Top-k modification only adds a layer of controlledexploration during the feature selection process, not fundamentally altering the decision-makingprocess. This makes Top-k particularly suitable for applications where both high accuracy andexplainability are crucial. 7kFuture work will focus on exploring adaptive strategies for choosing , investigating the algorithm’sperformance on even larger and more complex datasets, and extending Top-k to other tree-basedensemble methods. The promising results presented here suggest that Top-k represents a significantadvancement in decision tree algorithms, offering a compelling alternative to traditional methods.7 ConclusionIn this paper, we introduced Top-k, a novel generalization of decision tree algorithms designed toenhance accuracy and scalability while preserving interpretability. Our approach departs from thek???traditional greedy methods (ID3, C4.5, CART) by considering the top features as potentialsplit candidates at each node, rather than just the single best feature. This strategic modificationallows for a more thorough exploration of the feature space, mitigating the risk of early commitmentto suboptimal splits that often plague greedy algorithms, especially in high-dimensional settings. Thekparameter provides a flexible mechanism to control this exploration-exploitation trade-off, enablingpractitioners to tailor the algorithm to their specific needs and computational resources. Larger valueskof lead to more exhaustive searches, potentially improving accuracy but increasing computationalcomplexity, while smaller values prioritize efficiency.Our theoretical analysis provided a rigorous foundation for the advantages of Top-k. We deriveda lower bound on the generalization error, demonstrating that under certain conditions, this bound?is tighter than those achievable by traditional greedy algorithms . This theoretical improvementis strongly supported by our extensive empirical evaluation across a diverse range of benchmarkdatasets. The results consistently showed that Top-k outperforms traditional methods in terms ofaccuracy, particularly in high-dimensional scenarios where the benefits of exploring multiple featuresare most pronounced. The improvement in accuracy is not achieved at the expense of excessivecomputational cost; our experiments demonstrated that the computational overhead scales gracefullykwith both dataset size and the value of , making Top-k a practical alternative for various applications.The choice of the splitting criterion also plays a significant role in Top-k’s performance. Weinvestigated the impact of information gain, Gini impurity, and variance reduction, finding thatthe optimal choice depends on the specific characteristics of the dataset. This adaptability furtherenhances the versatility of Top-k. The inherent interpretability of decision trees is preserved in Top-k,making it suitable for applications requiring both high accuracy and explainability. The simplicityof the Top-k algorithm, coupled with its improved performance, makes it a valuable tool for a widerange of machine learning tasks. kFurthermore, our experiments explored the impact of the parameter on the algorithm’s performance.kWe observed a clear trade-off between accuracy and computational cost as increases. While largerkvalues of generally lead to higher accuracy, especially in high-dimensional datasets, they alsokincrease computational time. This highlights the importance of carefully selecting the value ofbased on the specific application and available computational resources. Future research could focuskon developing adaptive strategies for automatically determining the optimal value of during training,further enhancing the algorithm’s efficiency and performance.Beyond its improved accuracy and scalability, Top-k retains the inherent interpretability of decisiontrees. The tree structure remains easily understandable, and the Top-k modification only adds a layerof controlled exploration, not fundamentally altering the decision-making process. This makes Top-kparticularly suitable for applications where both high accuracy and explainability are crucial. Thealgorithm’s efficiency is further enhanced by the use of optimized data structures and algorithms forkmanaging the top feature candidates. Our implementation leverages efficient data structures andalgorithms, ensuring that the computational overhead remains manageable even for large datasets andhigh values of k.In conclusion, our work presents a compelling case for Top-k as a significant advancement indecision tree algorithms. It offers a powerful combination of accuracy, scalability, and interpretability,surpassing traditional methods, particularly in high-dimensional settings. The flexibility providedkby the parameter allows practitioners to fine-tune the algorithm to their specific needs, balancingcomputational cost and predictive performance. Future research directions include exploring adaptivekstrategies for selecting , investigating its performance on even larger and more complex datasets,and extending Top-k to other tree-based ensemble methods. The promising results presented in thispaper position Top-k as a valuable tool for a wide range of machine learning applications.8"
P117,"Rapid Image Annotation Through Zero-Shot LearningAbstractRecent experiments on word analogies demonstrate that contemporary word vectorseffectively encapsulate subtle linguistic patterns through linear vector displace-ments. However, the extent to which these straightforward vector displacementscan represent visual patterns across words remains uncertain. This research in-vestigates a particular image-word relevance relationship. The findings indicatethat, for a given image, word vectors of pertinent tags are positioned higher thanthose of unrelated tags along a primary axis within the word vector space. Drawinginspiration from this insight, we suggest addressing image tagging by determiningthe main axis for an image. Specifically, we utilize linear mappings and intricatedeep neural networks to deduce the primary axis from an input image. The re-sultant tagging model exhibits remarkable adaptability. It operates swiftly on testimages, with a processing time that remains constant regardless of the training set’ssize. Furthermore, it showcases exceptional performance not only in conventionaltagging tasks using the NUS-WIDE dataset but also in comparison to competitivebaselines when assigning tags to images that haven’t been seen during training.1 IntroductionRecent advancements in representing words in vector spaces have proven advantageous for bothNatural Language Processing and various computer vision applications, including zero-shot learningand image caption generation. The rationale behind using word vectors in NLP is rooted in theobservation that detailed linguistic patterns among words are represented by linear offsets of wordvectors. This pivotal insight emerged from well-known word analogy studies. For example, syntacticrelationships like ""dance"" to ""dancing"" parallel ""fly"" to ""flying,"" and semantic connections like ""king""to ""man"" mirror ""queen"" to ""woman."" Nevertheless, it is yet to be determined whether the visualpatterns across words, implicitly employed in the aforementioned computer vision tasks, can similarlybe represented by these basic vector offsets.This paper focuses on the task of image tagging, where an image necessitates the division of a wordlexicon into two distinct groups based on image-word relevance. For example, an image of a zoo mighthave relevant tags like ""people,"" ""animal,"" and ""zoo,"" while irrelevant tags might include ""sailor,""""book,"" and ""landscape."" This lexical division fundamentally differs from the nuanced syntactic orsemantic relationships examined in word analogy tests. Instead, it concerns the connection betweentwo sets of words as prompted by a visual image. This type of word relationship is semantic anddescriptive, emphasizing visual association, albeit at a broader level. Given this context, it is worthinvestigating whether word vectors maintain the property where simple linear vector offsets candepict visual or image-based associative relationships between words. In the zoo example, while it’seasy for humans to recognize that words like ""people,"" ""animal,"" and ""zoo"" are more related to thezoo than words like ""sailor,"" ""book,"" and ""landscape,"" the question is whether such a zoo-associationrelationship can be represented by the nine pairwise vector offsets: ""people"" minus ""sailor,"" ""people""minus ""book,"" and so on, up to ""zoo"" minus ""landscape,"" between the vectors of relevant and irrelevanttags.A primary contribution of this research is an empirical investigation of these questions. Each imageestablishes a visual association rule over words, represented as a pair (Y, Y). Leveraging the extensive.image collections in benchmark datasets designed for image tagging, we can explore numerousdistinct visual association rules in words and the corresponding vector offsets in the word vectorspace. Our findings uncover a significant correlation: the offsets between the vectors of relevant tags(Y) and those of irrelevant tags (Y) predominantly align in a consistent direction, which we term the""principal direction"". In other words, within the word vector space, there exists at least one vector(direction), denoted as w, such that its inner products with the vector offsets between Y and Y aregreater than 0. This can be expressed as:˘(w,p 2014 n) > 0 equivalently, (w,p) > (w,n)This implies that the vector w ranks all relevant words Y ahead of irrelevant ones Y.The visual association patterns among words manifest as the linear rank-abilities of their correspond-ing word vectors. This observation corroborates findings from word analogy studies, suggesting thatmultiple relationships for a single word are embedded within a high-dimensional space. Furthermore,these relationships can be articulated using basic linear vector arithmetic.Building on this discovery, we propose a solution to the image tagging challenge by identifying theprimary axis along which relevant tags are ranked higher than irrelevant ones within the word vectorspace. We employ both linear mappings and deep neural networks to infer this primary axis fromeach input image. This unique perspective on image tagging yields a highly adaptable tagging model.The model processes test images rapidly, maintaining a constant processing time irrespective of thetraining dataset’s size. It not only delivers outstanding results in traditional tagging tasks but alsoexcels at assigning new tags from a broad vocabulary that were not encountered during training. Ourmethod does not rely on prior knowledge of these new tags, as long as they exist within the samevector space as the tags used during training. Consequently, we designate our technique as ""fastzero-shot image tagging"" (Fast0Tag), acknowledging its strengths in both speed and its zero-shotlearning capabilities.In stark contrast to our approach, prior methods for image tagging are limited to assigning only thosetags to test images that were seen during training, with a notable exception. These methods areconstrained by the fixed and often limited number of tags present in the training data, which posespractical challenges. For example, Flickr hosts approximately 53 million tags, and this number israpidly increasing. The work of Fu et al. represents a pioneering effort to extend an image taggingmodel to previously unseen tags. However, when compared to our proposed method, it depends ontwo extra assumptions. Firstly, it assumes that unseen tags are known beforehand to enable modeladjustment toward these tags. Secondly, it assumes that test images are known in advance for modelregularization. Moreover, this method is restricted to a very limited number, U, of unseen tags, as itU possibletagcombinations.needs to account for all 2To recap, our primary contribution lies in analyzing visual association patterns in words as they relateto images and how these patterns are reflected in word vector offsets. We posit and confirm throughexperiments that a main direction exists in the word vector space for each visual association rule(Y, Y), where vectors of relevant words are ranked higher than others. Building on this, our secondcontribution is an innovative image tagging model, Fast0Tag, which is both swift and capable ofhandling an open vocabulary of unseen tags. Lastly, we explore three distinct image tagging scenarios:traditional tagging, which assigns seen tags to images; zero-shot tagging, which annotates imageswith numerous unseen tags; and seen/unseen tagging, which uses both seen and unseen tags. Existingresearch either addresses traditional tagging or zero-shot tagging with a limited number of unseentags. Our Fast0Tag method surpasses competitive baselines across all three scenarios.2 Related WorkImage Tagging. The objective of image tagging is to allocate pertinent tags to an image or to generatea ranked list of tags. Within the academic community, this challenge has predominantly been tackledfrom the standpoint of tag ranking. Generative approaches, which incorporate topic models andmixture models, inherently rank candidate tags based on their conditional probabilities relative to thetest image. Conversely, non-parametric, nearest-neighbor-based techniques frequently rank tags fora test image by aggregating votes from a selection of training images. Although nearest-neighbormethods generally exhibit superior performance compared to those reliant on generative models,they are plagued by substantial computational demands during both training and testing phases.2The recently introduced FastTag algorithm offers a significant speed advantage while maintainingperformance levels on par with nearest-neighbor methods. Our Fast0Tag method mirrors the reducedcomplexity of FastTag. Embedding techniques, on the other hand, determine tag ranking scores viaa cross-modal mapping between images and tags. This concept has been further developed usingdeep neural networks. Notably, aside from certain exceptions, the majority of these methods do nottrain their models with an explicit ranking objective, despite ultimately ranking candidate tags fortest images. This discrepancy between the trained models and their practical application contravenesthe principle of Occam’s razor. We incorporate a ranking loss in our approach, similar to theseexceptions.Unlike our Fast0Tag, which is capable of ranking both known and an unlimited numberof previously unseen tags for test images, the methods mentioned earlier are restrictedto assigning tags to images from a predetermined vocabulary encountered during train-ing. An exception to this is the work by Fu et al., where they address a predefinednumber, U, of unseen tags by developing a multi-label model that considers all possibleU combinationsof thesetags.However, thisapproachisconstrainedbythesmallnumberU of unseentagsitcanhandle.2Word Embedding. Diverging from the conventional one-hot vector representation of words, wordembedding maps each word to a continuous-valued vector, primarily learning from the statisticalpatterns of word co-occurrences. While earlier studies on word embedding exist, our researchemphasizes the latest GloVe and word2vec vectors. As demonstrated in the well-known word analogyexperiments, both types of word vectors effectively capture detailed semantic and syntactic patternsthrough vector offsets. In this study, we further reveal that basic linear offsets can also represent thebroader visual association patterns among words.Zero-Shot Learning. The term ""zero-shot learning"" is frequently used interchangeably with ""zero-shotclassification,"" although the latter is actually a subset of the former. In contrast to weakly-supervisedlearning, which acquires new concepts by extracting information from noisy samples, zero-shotclassification aims to classify objects from unseen classes by learning classifiers from seen classes.Attributes and word vectors are two primary semantic sources that enable zero-shot classification.Our Fast0Tag, together with Fu et al., expands the domain of zero-shot learning to include zero-shot multi-label classification. Fu et al. approach this by converting the problem into zero-shotclassification, where each combination of multiple labels is treated as a separate class. We, on theother hand, model the labels directly, allowing us to assign or rank a large number of unseen tags foran image.3 The Linear Rank-Ability of Word VectorsOur Fast0Tag method is enhanced by the discovery that the visual relationship between words,specifically how a lexicon is divided based on relevance to an image, manifests in the word vectorspace as a main direction. Along this direction, words or tags that are relevant to the image are rankedhigher than those that are not. This section elaborates on this discovery.3.1 The Regulation Over Words Due to Image TaggingLet’s denote S as the set of seen tags available for training image tagging models, and U as the setof tags unseen during the training phase. The training data is structured as (xm, Ym); m = 1, 2, ...,M, where xm represents the feature vector of image m in RD, and Ym is a subset of S, containingthe seen tags relevant to that image. For simplicity, we also use Ym to represent the collection ofcorresponding word or tag vectors.Traditional image tagging seeks to assign seen tags from S to test images. Zero-shot tagging, asdefined by Fu et al., aims to annotate test images using a predetermined set of unseen tags, U. Beyondthese two scenarios, this paper introduces seen/unseen image tagging, which identifies both relevantseen tags from S and relevant unseen tags from U for test images. Furthermore, the set of unseentags, U, can be open and continuously expanding.We define Ym as the complement of Ym in S, representing irrelevant seen tags. An image mestablishes a visual association rule among words, essentially partitioning seen tags into two distinctsets: Ym and Ym. Recognizing that various detailed syntactic and semantic patterns among words3can be depicted through linear word vector offsets, we proceed to investigate the characteristics thesevector offsets might exhibit for this novel visual association rule.3.2 Principal Direction and Cluster StructureFigure 2 offers a visual representation of vector offsets (p - n), where p belongs to Ym and n belongsto Ym, using both t-SNE and PCA for two different visual association rules over words. One rule isdefined by an image associated with 5 relevant tags, and the other by an image with 15 relevant tags.From these vector offsets, we identify two key structures:Principal Direction: For a given visual association rule (Ym, Ym) in words for image m, the vectoroffsets predominantly point in a similar direction, which we refer to as the principal direction. Thissuggests that along this principal direction, relevant tags Ym are ranked higher than irrelevant onesYm.Cluster Structure: Within each visual association rule over words, there are discernible clusterstructures in the vector offsets. Moreover, all offsets that point to the same relevant tag in Ym aregrouped within the same cluster. In Figure 2, we distinguish offsets pointing to different relevant tagsby using different colors.The question remains whether these two observations can be generalized. Specifically, do they remainvalid in the high-dimensional word vector space for a broader range of visual association rules definedby other images? To address this, we designed an experiment to confirm the existence of principaldirections in word vector spaces, or equivalently, the linear rank-ability of word vectors. We defer theinvestigation of the cluster structure to future research.3.3 Testing the Linear Rank-Ability HypothesisThe experiments in this section are performed using the validation set of the NUS-WIDE dataset,which includes 26,844 images, 925 seen tags (S), and 81 unseen tags (U). The number of relevantseen/unseen tags associated with an image varies from 1 to 20/117, with an average of 1.7/4.9. Furtherdetails can be found in Section 5.Our goal is to explore whether a primary direction exists for any visual association rule (Ym, Ym)created by image m, along which relevant tags Ym rank higher than irrelevant tags Ym. This can beconfirmed if we find a vector w in the word vector space that fulfills the ranking conditions (w, p) >(w, n) for all p in Ym and n in Ym.To achieve this, we train a linear ranking SVM for each visual association rule using all correspondingpairs (p, n). We then rank word vectors using the SVM and assess the number of violated constraints.Specifically, we use MiAP, with higher values being preferable, to compare the SVM’s ranking listagainst the ranking constraints. This process is repeated for all validation images, resulting in 21,863unique visual association rules.Ranking SVM Implementation. We utilize the primal formulation of ranking SVM for our experi-ments, which is defined as:2 + max(0, 1 − (w, yi) + (w, yj))f oryiY m, yjY mmin 1/2 ||w||Here, is a hyperparameter that balances the objective and regularization.Results. The average MiAP outcomes across all distinct regulations are presented in Figure 3(left).We evaluate 300D GloVe vectors and word2vec vectors of dimensions 100, 300, 500, and 1000. Thehorizontal axis represents various regularizations used for training the ranking SVMs, with highervalues indicating stronger regularization. In the 300D GloVe space and word2vec spaces of 300, 500,and 1000 dimensions, more than two ranking SVMs, with low values, produce nearly ideal rankingresults (MiAP 1). This demonstrates that seen tags S are linearly rankable under almost every visualassociation rule, satisfying all ranking constraints set by relevant Ym and irrelevant Ym tags forimage m.However, caution is advised before extending conclusions beyond the experimental vocabulary Sof seen tags. While an image m imposes a visual association rule over all words, this rule leadsto different partitions of distinct experimental vocabularies (e.g., seen tags S and unseen tags U).4Therefore, we anticipate that the principal direction for seen tags should also apply to unseen tagsunder the same rule, if the questions at the end of Section 3.2 are answered affirmatively.Generalization to Unseen Tags. We investigate whether the same principal direction applies to bothseen and unseen tags under each visual association rule induced by an image. This is partiallyvalidated by applying the previously trained ranking SVMs to unseen tag vectors, as the ""true""principal directions are unknown. We use the 81 unseen tags U as ""test data"" for the trained rankingSVMs, each resulting from an image-induced visual association. NUS-WIDE provides annotationsfor these 81 tags. The results, shown in Figure 3(right), significantly outperform the basic baseline ofrandom tag ranking, indicating that the directions produced by SVMs are generalizable to the newvocabulary U of words.Observation. We conclude that word vectors are an effective medium for transferring knowl-edge—specifically, rank-ability along the principal direction—from seen to unseen tags. We haveempirically confirmed that the visual association rule (Ym, Ym) in words due to an image m can berepresented by the linear rank-ability of corresponding word vectors along a principal direction. Ourexperiments involve a total of |S| + |U| = 1,006 words. Future work should include larger-scale andtheoretical studies.4 Approximating the Linear Ranking FunctionsThis section introduces our Fast0Tag approach for image tagging. Initially, we explain how to addressimage tagging by approximating the principal directions, based on their existence and generalization,as confirmed in the previous section. Subsequently, we describe the detailed approximation methodsused.4.1 Image Tagging by RankingBased on the findings from Section 3, which indicate the existence of a principal direction, wm, in theword vector space for each visual association rule (Ym, Ym) generated by an image m, we propose adirect solution for image tagging. The core idea is to approximate this principal direction by learning˘a mapping function, f(00b7), that connects the visual space to the word vector space, such that:f(xm) wmHere, xm is the visual feature representation of image m. Consequently, given a test image x, wecan promptly suggest a list of tags by ranking the word vectors of the tags along the direction f(x),specifically by the ranking scores:t S U, (f(x), t)This applies whether the tags are from the seen set S or the unseen set U.We investigate both linear and nonlinear neural networks to implement the approximation functionf(x) w.4.2 Approximation by Linear RegressionIn this approach, we assume a linear function from the input image representation x to the outputprincipal direction w, defined as:f(x) := AxHere, A can be determined in a closed form through linear regression. Thus, from the training data,we have:= Ax + , f orm = 1, 2, ..., Mwm m mistheprincipaldirectionf orallof f setvectorsof theseentags, correspondingtothevisualassociationrule(Y , Y )f orimagem, and representstheerrors.M inimizingthemeansquarederrorsprovidesuswithaclosed−where wm m m mf ormsolutionf orA.However, a challenge arises as we do not know the exact principal directions.T hetrainingdataonlyprovideimagesx andrelevanttagsY .W eoptf orastraightf orwardalternative, usingthedirectionsderivedf romrankingSV M sinSection3inequation(5).Hence, theprocessinvolvestwostagestolearnthelinearf unctionf (x) =wm m mAx.T hef irststagetrainsarankingSV M overthewordvectorsof seentagsf oreachvisualassociation(Y , Y ).T hesecondstagecomputesthemappingmatrixAvialinearregression, usingthedirectionsf romtherankingSV M sastargets.m m5Discussion. The use of linear transformation between visual and word vector spaces has beenpreviously explored, for instance, in zero-shot classification and image annotation/classification. Thiswork distinguishes itself by the clear interpretation of the mapped image f(x) = Ax as the principaldirection for tag assignment, which has been empirically validated. We further extend this to anonlinear transformation using a neural network.4.3 Approximation by Neural NetworksWe also explore a nonlinear mapping f(x; ) using a multi-layer neural network, where represents thenetwork parameters. The network architecture, illustrated in Figure 4, includes two RELU layersfollowed by a linear layer that outputs the approximated principal direction, w, for an input imagex. We anticipate that the nonlinear mapping function f(x; ) will provide greater modeling flexibilitycompared to the linear approach.Training the neural network by regressing to the M directions obtained from ranking SVMs is notideal, as confirmed by both intuition and experiments. The number of training instances, M, is smallrelative to the network’s parameter count, increasing the risk of overfitting. Moreover, the directionsfrom ranking SVMs are not the true principal directions, making it unnecessary to rely on them.Instead, we integrate the two stages from Section 4.2. We aim for the neural network’s output f(xm; )to represent the principal direction, where all relevant tag vectors p Ym rank higher than irrelevantones n Ym for an image m. Let’s define:v(p, n; ) = (f(xm; ), n) - (f(xm; ), p)as the degree of violation of these ranking constraints.We then minimize the following loss function to train the neural network:∗ l(x , Y ; )l(x , Y ; ) = log(1 + expv(p, n; ))f orpY , nY* = argmin wm m m m m m m= 1/(|Y |∗|Y |)normalizestheper−imageRankN etlossbythenumberof rankingconstraintsimposedbyimagemoverthetags.T hissetupallowsthef unctionf (x)todirectlyconsidertherankingconstraintsf romrelevantandirrelevanttags, anditcanbeoptimizedef f ectivelyusingstandardmini−where wm m mbatchgradientdescent.Practical Considerations. We use Theano for optimization, with a mini-batch sizeof 1,000 images. Each image, on average, imposes 4,600 pairwise ranking con-f ortheper −straints, which are all used in the optimization. The normalization wmimagerankinglosshelpsbalancetheinf luenceof imageswithmanypositivetags, addressingtheissueof unbalancednumbersof relevanttagsacrossimages.W ithoutnormalization, M iAP resultsdropbyabout2%inourexperiments.F orregularization, weemployearlystoppingandadropoutlayerwitha30%droprate.Optimizationhyperparametersarechosenusingthevalidationset.Besides the RankNet loss, we tested other per-image loss options, including hinge loss, Crammer-Singer loss, and pairwise max-out ranking. Hinge loss performed the worst, likely because it’snot designed for ranking. Crammer-Singer, pairwise max-out, and RankNet yielded comparableresults, with RankNet slightly outperforming the others by about 2% in MiAP, possibly due to easieroptimization control. Listwise ranking loss could also be considered.5 Experiments on NUS-WIDEThis section details our experimental results, comparing our method against several strong baselinesfor traditional image tagging on the large-scale NUS-WIDE dataset. Additionally, we evaluate ourmethod on zero-shot and seen/unseen image tagging scenarios, extending some existing zero-shotclassification algorithms and exploring variations of our approach for comparison.5.1 Dataset and ConfigurationNUS-WIDE Dataset. We primarily utilize the NUS-WIDE dataset for our experiments. This datasetis a standard benchmark for image tagging, originally containing 269,648 images. We were ableto retrieve 223,821 images, as some were either corrupted or removed from Flickr. Followingthe recommended protocol, we divide the dataset into a training set of 134,281 images and a testset of 89,603 images. We further allocate 20% of the training set as a validation set for tuninghyperparameters in both our method and the baselines, and for conducting the empirical analyses inSection 3. 6Annotations of NUS-WIDE. NUS-WIDE provides three sets of tags for its images. The first setincludes 81 ""ground truth"" tags, carefully selected to represent Flickr tags, encompassing both generalterms (e.g., ""animal"") and specific ones (e.g., ""dog,"" ""flower""), and corresponding to frequent Flickrtags. These tags are annotated by students and are less noisy than those directly collected from theWeb, serving as the ground truth for evaluating image tagging methods. The second and third setscontain 1,000 popular and nearly 5,000 raw Flickr tags, respectively.Image Features and Word Vectors. We extract and normalize image feature representations usingVGG-19. Both GloVe and Word2vec word vectors are used in our empirical analysis in Section 3,with 300D GloVe vectors used for the remaining experiments. Word vectors are also normalized.Evaluation. We assess tagging results using two types of metrics: mean image average precision(MiAP), which considers the entire ranking list, and precision, recall, and F1-score for the top K tagsin the list (K = 3 and K = 5). Both metrics are commonly used in image tagging research. For detailson calculating MiAP and top-K precision and recall, we refer readers to Section 3.3 of Li et al. (2015)and Section 4.2 of Gong et al. (2013), respectively.5.2 Conventional Image TaggingIn this section, we present experimental results for traditional image tagging, using the 81 ""groundtruth"" annotated concepts in NUS-WIDE to benchmark various methods.Baselines. We include TagProp as a primary competitive baseline, representing nearest-neighbor-based methods that generally outperform parametric methods built from generative models and haveshown state-of-the-art results in experimental studies. We also compare against two recent parametricmethods, WARP and FastTag, both based on deep architectures but using different models. For afair comparison, we use the same VGG-19 features across all methods, with code for TagProp andFastTag provided by the authors and WARP implemented based on our neural network architecture.Additionally, we compare to WSABIE and CCA, which correlate images and relevant tags in alow-dimensional space. Hyperparameters for all methods are selected using the validation set.Results. Table 4 presents the comparison results among TagProp, WARP, FastTag, WSABIE, CCA,and our Fast0Tag models, implemented with both linear mapping and a nonlinear neural network.TagProp significantly outperforms WARP and FastTag, but its training and testing complexities are2O(M ) O(M )high, at and respectively, relative to the training set size M. In contrast, WARP andFastTag are more efficient, with O(M) training complexity and constant testing complexity due totheir parametric nature. Our Fast0Tag with linear mapping yields results comparable to TagProp,while Fast0Tag with the neural network surpasses the other methods. Both implementations maintainlow computational complexities similar to WARP and FastTag.Table 1: Comparison results of the conventional image tagging with 81 tags on NUS-WIDE.Method MiAP K = 3 K = 5P R F1 P R F1CCA 19 9 15 11 7 20 11WSABIE 28 16 27 20 12 35 18TagProp 53 29 50 37 22 62 32WARP 48 27 45 34 20 57 30FastTag 41 23 39 29 19 54 28Fast0Tag (lin.) 52 29 50 37 21 60 31Fast0Tag (net.) 55 31 52 39 23 65 345.3 Zero-Shot and Seen/Unseen Image TaggingThis section presents results for two novel image tagging scenarios: zero-shot and seen/unseentagging.Fu et al. formalised the zero-shot image tagging problem, which aims to annotate test images using apre-defined set U of unseen tags. Our Fast0Tag naturally applies to this scenario by simply rankingthe unseen tags with equation (3). Furthermore, this paper also considers seen/unseen image tagging,7which finds both relevant seen tags from S and relevant unseen tags from U for the test images. Theset of unseen tags U could be open and dynamically growing.In our experiments, we treat the 81 concepts with high-quality user annotations in NUS-WIDE asthe unseen set U for evaluation and comparison. We use the remaining 925 out of the 1000 frequentFlickr tags to form the seen set S - 75 tags are shared by the original 81 and 1,000 tags.Baselines. Our Fast0Tag models can be readily applied to the zero-shot and seen/unseen imagetagging scenarios. For comparison, we study the following baselines.Seen2Unseen. We first propose a simple method that extends an arbitrary traditional image taggingmethod to also work with previously unseen tags. It originates from our analysis experiment inSection 3. First, we use any existing method to rank the seen tags for a test image. Second, we train aranking SVM in the word vector space using the ranking list of the seen tags. Third, we rank unseen(and seen) tags using the learned SVM for zero-shot (and seen/unseen) tagging.LabelEM. The label embedding method achieves impressive results on zero-shot classification forfine-grained object recognition. If we consider each tag of S U as a unique class, though this impliesthat some classes will have duplicated images, the LabelEM can be directly applied to the two newtagging scenarios. LabelEM+. We also modify the objective loss function of LabelEM when we trainthe model, by carefully removing the terms that involve duplicated images. This slightly improvesthe performance of LabelEM. ConSE. Again by considering each tag as a class, we include a recentzero-shot classification method, ConSE in the following experiments. Note that it is computationallyinfeasible to compare with Fu et al., which might be the first work to our knowledge on expandingimage tagging to handle unseen tags, because it considers all the possible combinations of the unseentags. Results. Table 5 summarizes the results of the baselines and Fast0Tag when they are applied tothe zero-shot and seen/unseen image tagging tasks. Overall, Fast0Tag, with either linear or neuralnetwork mapping, performs the best.Additionally, in the table, we add two special rows whose results are mainly for reference. TheRandom row corresponds to the case when we return a random list of tags in U for zero-shot tagging(and in U S for seen/unseen tagging) to each test image. We compare this row with the row ofSeen2Unseen, in which we extend TagProp to handle the unseen tags. We can see that the results ofSeen2Unseen are significantly better than randomly ranking the tags. This tells us that the simpleSeen2Unseen is effective in expanding the labeling space of traditional image tagging methods. Sometag completion methods may also be employed for the same purpose as Seen2Unseen. Anotherspecial row in Table 5 is the last one with RankSVM for zero-shot image tagging. We obtain itsresults through the following steps. Given a test image, we assume the annotation of the seen tags,S, are known and then learn a ranking SVM with the default regularization = 1. The learned SVMis then used to rank the unseen tags for this image. One may wonder that the results of this rowshould thus be the upper bound of our Fast0Tag implemented based on linear regression because theranking SVM models are the targets of the linear regression. However, the results show that they arenot. This is not surprising, but rather it reinforces our previous statement that the learned rankingSVMs are not the ""true"" principal directions. The Fast0Tag implemented by the neural network is aneffective alternative for seeking the principal directions. It would also be interesting to compare theresults in Table 5 (zero-shot image tagging) with those in Table 4 (conventional tagging), because theexperiments for the two tables share the same testing images and the same candidate tags; they onlydiffer in which tags are used for training. We can see that the Fast0Tag (net.) results of the zero-shottagging in Table 5 are actually comparable to the conventional tagging results in Table 4, particularlyabout the same as FastTag’s. These results are encouraging, indicating that it is unnecessary to useall the candidate tags for training in order to have high-quality tagging performance. Annotatingimages with 4,093 unseen tags. What happens when we have a large number of unseen tags showingup at the test stage? NUS-WIDE provides noisy annotations for the images with over 5,000 Flickrtags. Excluding the 925 seen tags that are used to train models, there are 4,093 remaining unseentags. We use the Fast0Tag models to rank all the unseen tags for the test images, and the resultsare shown in Table 3. Noting that the noisy annotations weaken the credibility of the evaluationprocess, the results are reasonably low but significantly higher than the random lists. Qualitativeresults. Figure 6 shows the top five tags for some exemplar images, returned by Fast0Tag underthe conventional, zero-shot, and seen/unseen image tagging scenarios. Those by TagProp under theconventional tagging are shown on the rightmost. The tags in green color appear in the ground truth8annotation; those in red color and italic font are the mistaken tags. Interestingly, Fast0Tag performsequally well for traditional and zero-shot tagging and makes even the same mistakes.6 Experiments on IAPRTC-12We present another set of experiments conducted on the widely used IAPRTC-12 dataset. We usethe same tag annotation and image training-test split as described in prior work for our experiments.There are 291 unique tags and 19,627 images in IAPRTC-12. The dataset is split into 17,341 trainingimages and 2,286 testing images. We further separate 156.1 ConfigurationSimilar to the experiments in the previous section, we evaluate our methods in three distinct tasks:conventional tagging, zero-shot tagging, and seen/unseen tagging. Unlike NUS-WIDE, where arelatively small set of 81 tags is considered the ground truth annotation, all 291 tags of IAPRTC-12are typically used in prior work to compare different methods. Therefore, we also use all of themfor conventional tagging. For the zero-shot and seen/unseen tagging tasks, we exclude 20The visualfeatures, evaluation metrics, word vectors, and baseline methods remain the same as described in themain text.6.2 ResultsTables 4 and 5 display the results for all three image tagging scenarios (conventional, zero-shot, andseen/unseen tagging). The proposed Fast0Tag continues to outperform the other competitive baselineson this new IAPRTC-12 dataset. A notable observation, which is less apparent on NUS-WIDEprobably due to its noisier seen tags, is the significant performance gap between LabelEM+ andLabelEM. This indicates that traditional zero-shot classification methods may not be directly suitablefor either zero-shot or seen/unseen image tagging tasks. However, performance can be improvedby tweaking LabelEM and carefully removing terms in its formulation that involve comparisons ofidentical images.7 More Qualitative ResultsIn this section, we provide additional qualitative results from different tagging methods on both theNUS-WIDE and IAPRTC-12 datasets. These are presented to supplement the findings discussed inthe main text. Due to the incompleteness and noise in tag ground truth, many accurate tag predictionsare often incorrectly assessed as mistakes because they don’t match the ground truth. This issue isparticularly evident in the 4k zero-shot tagging results, where a wide variety of tag candidates areconsidered.8 ConclusionWe have conducted a thorough examination of a specific visual pattern in words: the visual associationrule that divides words into two distinct groups based on their relevance to an image. We alsoinvestigated how this rule is captured by vector offsets within the word vector space. Our empiricalfindings demonstrate that for any given image, there exists a main direction in the word vectorspace along which vectors of relevant tags are ranked higher than those of irrelevant tags. Whileour experimental analyses involved 1,006 words, future research should encompass larger-scaleand theoretical investigations. Based on this discovery, we developed a Fast0Tag model to addressimage tagging by estimating the primary directions for input images. Our method is as efficient asFastTag and is capable of annotating images with a large number of previously unseen tags. Extensiveexperiments confirm the effectiveness of our Fast0Tag approach.9"
P118,"Distant Supervision from Disparate Sources forLow-Resource Part-of-Speech TaggingAbstractWe introduce DSDS: a cross-lingual neural part-of-speech tagger that learns fromdisparate sources of distant supervision, and realistically scales to hundreds of low-resource languages. The model exploits annotation projection, instance selection,tag dictionaries, morphological lexicons, and distributed representations, all in auniform framework. The approach is simple, yet surprisingly effective, resulting ina new state of the art without access to any gold annotated data.1 IntroductionLow-resource languages lack manually annotated data to learn even the most basic models suchas part-of-speech (POS) taggers. To compensate for the absence of direct supervision, work incrosslingual learning and distant supervision has discovered creative use for a number of alternativedata sources to learn feasible models:However, only one or two compatible sources of distant supervision are typically employed. Inreality severely under-resourced languages may require a more pragmatic “take what you can get”viewpoint. Our results suggest that combining supervision sources is the way to go about creatingviable low-resource taggers.We propose a method to strike a balance between model simplicity and the capacity to easily integrateheterogeneous learning signals.system is a uniform neural model for POS tagging that learns from disparate sources of distantsupervision (DSDS). We use it to combine: i) multi-source annotation projection, ii) instanceselection, iii) noisy tag dictionaries, and iv) distributed word and sub-word representations. Weexamine how far we can get by exploiting only the wide-coverage resources that are currently readilyavailable for more than 300 languages, which is the breadth of the parallel corpus we employ.DSDS yields a new state of the art by jointly leveraging disparate sources of distant supervision in anexperiment with 25 languages. We demonstrate: i) substantial gains in carefully selecting high-qualityinstances in annotation projection, ii) the usefulness of lexicon features for neural tagging, and iii)the importance of word embeddings initialization for faster convergence.2 MethodDSDS is illustrated in Figure 1. The base model is a bidirectional long short-term memory network(bi-LSTM)Annotation projection. Ever since the seminal work of projecting sequential labels from source totarget languages has been one of the most prevalent approaches to crosslingual learning. Its onlyrequirement is that parallel texts are available between the languages, and that the source side isannotated for POS.We apply the approach by where labels are projected from multiple sources and then decoded throughweighted majority voting with word alignment probabilities and source POS tagger confidences. Weexploit their widecoverage Watchtower corpus (WTC), in contrast to the typically used Europarl data.Europarl covers 21 languages of the EU with 400k-2M sentence pairs, while WTC spans 300+ widelydiverse languages with only 10-100k pairs, in effect sacrificing depth for breadth, and introducing amore radical domain shift. However, as our results show little projected data turns out to be the mostbeneficial, reinforcing breadth for depth.While selected 20k projected sentences at random to train taggers, we propose a novel alternative:selection by coverage. We rank the target sentences by percentage of words covered by wordalignment from 21 sources and select the top k covered instances for training. In specific, we employthe mean coverage ranking of target sentences, whereby each target sentence is coupled with thearithmetic mean of the 21 individual word alignment coverages for each of the 21 source-languagesentences. We show that this simple approach to instance selection offers substantial improvements:across all languages, we learn better taggers with significantly fewer training instances.Dictionaries. Dictionaries are a useful source or distant supervision. There are several ways toexploit such information: i) as type constraints during encoding, ii) to guide unsupervised learning,or iii) as addiional signal at training. We focus on the latter and evaluate two ways to integratelexical knowledge into neural models, while comparing to the former wo: a) by representing lexiconproperties as n-hot vector (e.g., if a word has two properties according to lexicon src, it resultsin a 2-hot vector, if the word is not present in src, a zero vector), with m the number of lexiconproperties; b) by embedding the lexical features, i.e., is a lexicon src embedded into an /-dimensionalspace. We represent as concatenation of all embedded m properies of length [, and a zero vectorotherwise. Tuning on the dev set, we found the second embedding approach to perform best, andsimple concatenaion outperformed mean vector representations.We evaluate two dictionary sources, motivated by ease of accessibility to many languages: WIK-TIONARY, a word type dictionary that maps tokens to one of the 12 Universal POS tags; andUNIMORPH, a morphological dictionary that provides inflectional paradigms across 350 languages.For Wiktionary, we use the freely available dictionaries from The size of the dictionaries ranges froma few thousands (e.g., Hindi and Bulgarian) to 2M (Finnish UniMorph). Sizes are provided in Table1, 1st columns. UniMorph covers between 8-38 morphological properties (for English and Finnish,respectively).Word embeddings. Embeddings are available for many languages. Pre-initialization of offersconsistent and considerable performance improvements in our distant supervision setup (Section 4).We use off-the-shelf Polyglot embeddings, which performed consistently better than FastText.3 ExperimentsBaselines. We compare to the following weaklysupervised POS taggers: AGIC: Multi-sourceannotation projection with Bible parallel data DAS: The label propagation approach by over Europarldata. GARRETTE: The approach by that works with projections, dictionaries, and unlabeled targettext. LI: Wiktionary supervision.Data. Our set of 25 languages is motivated by accessibility to embeddings and dictionaries. In allexperiments we work with the 12 Universal POS tags. For development, we use 21 dev sets of theUniversal Dependencies 2.1. We employ UD test sets on additional languages as well as the test setsof to facilitate comparisons. Their test sets are a mixture of CoNLL and HamleDT test data, and aremore distant from the training and development data.Model and parameters. We extend an off-theshelf state-of-the-art bi-LSTM tagger with lexiconinformation. The code is available at: https:// github.com/bplank/bilstm-aux. The parameter l=40was set on dev data across all languages. Besides using 10 epochs, word dropout rate (p=.25) and40-dimensional lexicon embeddings, we use the parameters from For all experiments, we averageover 3 randomly seeded runs, and provide mean accuracy. For the learning curve, we average over 5random samples with 3 runs each.4 ResultsTable 1 shows the tagging accuracy for individual languages, while the means over all languages aregiven in Figure 2. There are several take-aways. 2Data selection. The first take-away is that coverage-based instance selection yields substan-tially better training data. Most prior work on annotation projection resorts to arbitrary selection;informed selection clearly helps in this noisy data setup, as shown in Figure 2 (a). Training on 5kinstances results in a sweet spot; more data (10k) starts to decrease performance, at a cost of runtime.Training on all WTC data (around 120k) is worse for most languages. From now on we consider the5k model trained with Polyglot as our baseline (Table 1, column “5k”), obtaining a mean accuracy of83.0 over 21 languages.Embeddings initialization. Polyglot initialization offers a large boost; on average +3.8% absoluteimprovement in accuracy for our 5k training scheme, as shown in Figure 2 (b). The big gap inlow-resource setups further shows their effectiveness, with up to 10% absolute increase in accuracywhen training on only 500 instances.Lexical information. The main take-away is that lexical information helps neural tagging, andembedding it proves the most helpful. Embedding Wiktionary tags reaches 83.7 accuracy on average,versus 83.4 for n-hot encoding, and 83.2 for type constraints. Only on 4 out of 21 languages aretype constraints better. This is the case for only one language for n-hot encoding (French). The bestapproach is to embed both Wiktionary and Unimorph, boosting performance further to 84.0, andresulting in our final model. It helps the most on morphological rich languages such as Uralic.On the test sets (Table 4, right) DSDS reaches 87.2 over 8 test languages intersecting and. It reaches86.2 over the more commonly used 8 languages of, compared to their 83.4. This shows that ournovel “soft” inclusion of noisy dictionaries is superior to a hard decoding restriction, and includinglexicons in neural taggers helps. We did not assume any gold data to further enrich the lexicons, norfix possible tagset divergences.5 DiscussionAnalysis. The inclusion of lexicons results in higher coverage and is part of the explanation for theimprovement of DSDS; see correlation in Figure 3 (a). What is more interesting is that our modelbenefits from the lexicon beyond its content: OOV accuracy for words not present in the lexiconoverall improves, besides the expected improvement on known OOV, see Figure 3 (b).More languages. All data sources employed in our experiment are very high-coverage. However, fortrue low-resource languages, we cannot safely assume the availability of all disparate informationsources. Table 2 presents results for four additional languages where some supervision sourcesare missing. We observe that adding lexicon information always helps, even in cases where only1k entries are available, and embedding it is usually the most beneficial way. For closely-relatedlanguages such as Serbian and Croatian, using resources for one aids tagging the other, and modernresources are a better fit. For example, using the Croatian WTC projections to train a model forSerbian is preferable over in-language Serbian Bible data where the OOV rate is much higher.How much gold data? We assume not having access to any gold annotated data. It is thus interestingto ask how much gold data is needed to reach our performance. This is a tricky question, as trainingwithin the same corpus naturally favors the same corpus data. We test both in-corpus (UD)and out-of-corpus data (our test sets) and notice an important gap: while in-corpus only 50 sentencesare sufficient, outside the corpus one would need over 200 sentences. This experiment was done for asubset of 18 languages with both inand out-ofcorpus test data.Further comparison. In Table 1 we directly report the accuracies from the original contributions byDAS, LI, GARRETTE, and AGIC over the same test data. We additionally attempted to reach thescores of LI by running their tagger over the Table 1 data setup. The results are depicted in Figure4 as mean accuracies over EM iterations until convergence. We show: i) LI peaks at 10 iterationsfor their test languages, and at 35 iterations for all the rest. This is in slight contrast to 50 iterationsthat recommend, although selecting 50 does not dramatically hurt the scores; ii) Our replication falls˘223c5 points short of their 84.9 accuracy. There is a large 33-point accuracy gap between the scoresof, where the dictionaries are large, and the other languages in Figure 4, with smaller dictionaries.Compared to DAS, our tagger clearly benefits from pre-trained word embeddings, while theirs relieson label propagation through Europarl, a much cleaner corpus that lacks the coverage of the noisierWTC. Similar applies to as they use 1-5M near-perfect parallel sentences. Even if we use much3Table 1: Results on the development sets and comparison of our best model to prior work. LEX: Size(word types) of dictionaries (W: Wiktionary, U: UniMorph). TC: type-constraints using Wiktionary;(embedded Wiktionary tags), DSDS: our model with ;. Results indicated by use W only. Best resultin boldface; in case of equal means, the one with lower std is boldfaced. Averages over languagefamilies (with two or more languages in the sample, number of languages in parenthesis).LEX (10%) DEV SETS (UD2.1) TEST SETSLANGUAGE W U 5k TCw n-hot Ew DSDS DAS LI GARRETTE AGIC DSDS89.7Bulgarian (bg) 3 47 88.6 88.6 88.9 89.6 83.1 7.7 83.9Croatian (hr) 20 84.9 85.4 84.9 84.8 84.8 67.1 78.087.2Czech (cs) 14 72 86.6 86.6 86.9 87.6 73.3 86.890.0Danish (da) 22 24 89.6 89.0 89.8 90.2 83.2 83.3 78.8 79.0 84.589.8Dutch (nl) 52 26 88.3 88.9 89.0 89.7 79.5 86.3 83.987.3English (en) 358 91 86.5 87.4 86.8 87.3 87.1 80.7 73.6 85.782.4Finnish (fi) 104 2,345 81.5 81.2 81.8 82.491.7French (fr) 17 274 91.0 89.6 91.2 91.4 85.5 76.6 88.786.7German (de) 62 71 85.0 86.4 85.5 86.0 82.8 85.8 87.1 80.2 84.1Greek (el) 21 80.6 85.7 80.2 80.5 80.5 79.2 64.4 52.3 81.175.3Hebrew (he) 3 12 76.0 76.1 75.5 74.9 66.2Hindi (hi) 2 26 64.6 64.6 64.8 65.4 67.6 63.177.9Hungarian (hu) 13 13 75.6 75.6 75.3 75.7 77.9 72.0 71.393.7Italian (it) 478 410 91.9 91.7 93.4 93.5 86.8 83.5 76.9 92.1! 91.5Norwegian (no) 47 18 90.9 90.9 90.9 91.0 84.3 76.7 86.259.6Persian (fa) 4 26 42.8 43.0 43.7 43.5 59.6 43.686.0Polish (pl) 6 132 84.7 84.6 84.2 84.8 75.1 84.492.2Portuguese 41 211 91.4 91.5 92.3 92.9 87.9 84.5 87.3 83.8 89.486.3Romanian (ro) 7 4 83.9 83.9 84.8 85.3 92.0Spanish (es) 234 324 90.4 88.6 91.0 91.5 84.2 86.4 88.7 81.4 91.789.9Swedish (sv) 89 67 88.9 88.9 89.6 89.9 80.5 86.1 76.1 75.2 83.184.0AVG(21) 83.0 83.2 83.4 83.7 83.4AVG(8: DAS) 84.8 80.8 75.5 86.284.9AVG(8: LI/AGIC) 80.8 75.2 87.289.2GERMANIC (6) 88.2 88.6 88.6 89.0 85.4GERMANIC (4: DAS) 81.5 83.991.1ROMANCE (5) 89.7 89.0 90.6 90.9 91.1ROMANCE (3: DAS) 86.3 85.8 86.5 80.786.9SLAVIC (4) 86.2 86.3 86.2 86.7 62.9INDO-IRANIAN (2) 53.7 53.8 54.3 54.4 80.1URALIC (2) 78.5 78.4 78.6 79.0smaller and noisier data sources, DSDS is almost on par: 86.2 vs. 87.3 for the 8 languages from Dasand , and we even outperform theirs on four languages: Czech, French, Italian, and Spanish.6 Related WorkMost successful work on low-resource POS tagging is based on projection, tag dictionaries, annotationof seed training data or even more recently some combination of these, e.g., via multi-task learning.Our paper contributes to this literature by leveraging a range of prior directions in a unified, neuraltest bed.Most prior work on neural sequence prediction follows the commonly perceived wisdom that hand-crafted features are unnecessary for deep learning methods. They rely on end-to-end training withoutresorting to additional linguistic resources. Our study shows that this is not the case. Only few priorstudies investigate such sources, e.g., for MT and for POS tagging use lexicons, but only as n-hotfeatures and without examining the cross-lingual aspect.4Table 2: Results for languages with missing data sources: WTC projections, Wiktionary (W), orUniMorph (U). Test sets (TEST), projection sources (PROJ), and embeddings languages (EMB) areindicated. Comparison to TnT trained on PROJ. Results indicated by †use W only.TEST SETSLANGUAGE TEST PROJ Ew TnT TCw n-hot Ew DSDSBasque (eu) UD Bible 57.5 61.8 61.8 61.4 62.7 62.7Basque (eu) CoNLL Bible 57.0 60.3 60.3 60.3 61.3 61.3Estonian (et) UD WTC 79.5 80.6 81.5Serbian (sr) UD WTC (hr) 84.0 84.7 85.5 85.1 85.2 85.2Serbian (sr) UD Bible (sr) 77.1 78.9 79.4 80.5 80.7 80.7Tamil (ta) UD WTC 58.2 61.27 ConclusionsWe show that our approach of distant supervision from disparate sources (DSDS) is simple yetsurprisingly effective for low-resource POS tagging. Only 5k instances of projected data paired withoff-the-shelf embeddings and lexical information integrated into a neural tagger are sufficient to reacha new state of the art, and both data selection and embeddings are essential components to boostneural tagging performance. 5"
P119,"Entropy Dynamics in Turbulent Flumplenook Systemswith Periodic FluctuationsAbstractThe notion of flamboyant jellyfish dancing on the moon precipitates an examinationof entropy, which somehow relates to the flavor of chocolate cake on Wednesdays,and the propensity of cats to sleep for 17 hours a day, while simultaneouslycontemplating the aerodynamics of umbrellas in a hurricane, all of which convergesto reveal a fascinating paradox, that the entropy of a system is directly proportionalto the number of rubber chickens present, and the color blue, which is only visibleon Tuesdays during leap years, has a profound impact on the spatial arrangementof atoms in a vacuum, which in turn affects the entropy of the universe. Theconsumption of pineapple pizza on Fridays leads to a decrease in entropy, while theact of watching paint dry increases it, and the square root of -1 has a peculiar effecton the second law of thermodynamics, which can only be understood by studyingthe migration patterns of narwhals, and the entropy of a closed system is inverselyproportional to the number of socks lost in the wash, which is a fundamental conceptthat has been overlooked by traditional theories of entropy, and the whispers ofancient trees hold the secrets of the universe, including the true nature of entropy.The curious case of disappearing socks in the laundry is a manifestation of theentropy of the universe, and the flapping of butterfly wings in Brazil has a directimpact on the entropy of a cup of coffee, which is somehow connected to themeaning of life, and the number 42 has a profound significance in the context ofentropy, which can only be understood by deciphering the hidden codes in thepatterns of crop circles, and the entropy of a system is directly proportional tothe number of times the word ""entropy"" is mentioned in a sentence, which is aphenomenon that has been observed in various studies of entropy. The intricatedance of subatomic particles is a reflection of the entropy of the universe, and theentropy of a closed system is directly proportional to the number of words in asentence, which is a fundamental concept that has been overlooked by traditionaltheories of entropy, and the study of entropy is a complex and multifaceted field thatrequires a deep understanding of the underlying principles, including the conceptof ""flumplenooks"" and the ""trans-dimensional wobble"" of particles in a vacuum.1 IntroductionThe notion of entropy, a concept that has been perplexing scholars for centuries, has been observed tohave a profound impact on the realm of culinary arts, particularly in the preparation of intricate pastrydishes, where the flakiness of the crust is directly proportional to the entropy of the surroundingenvironment, which in turn is influenced by the migratory patterns of certain species of birds, suchas the lesser-known Flibberjibber bird, whose unique song structure has been found to have a directcorrelation with the underlying principles of quantum mechanics, and the study of which has ledto breakthroughs in our understanding of the fundamental forces of nature, including the recentlydiscovered force of Splishyblop, which acts upon particles at the molecular level, causing them toexhibit behaviors that defy the conventional laws of thermodynamics, much like the phenomenon ofspontaneous combustion, which has been observed in certain types of furniture, particularly thosemade from the wood of the rare and exotic Snazzle tree, native to the remote island of Plooflingville,where the inhabitants have developed a unique culture that revolves around the worship of a deityknown as Zorb, who is said to possess the power to manipulate the very fabric of reality, and whoseexistence has been confirmed by the discovery of ancient artifacts, including the fabled Golden Spoonof Glibble, which is rumored to have the ability to stir the cosmos itself, and has been the subjectof intense study by scholars of the mystical arts, who have found that the spoon’s power is directlyrelated to the entropy of the universe, which in turn is influenced by the consumption of a certaintype of pastry, known as the Flumplenook, which has been found to have a profound impact on thehuman digestive system, causing it to produce a unique type of energy that can be harnessed andused to power complex machines, such as the recently developed Flibulon accelerator, which hasthe capability to propel objects at speeds approaching that of light, and has been used to study theproperties of certain types of particles, including the elusive Snurflotzer particle, which has beenfound to have a direct correlation with the fundamental principles of entropy, and the study of whichhas led to a deeper understanding of the underlying forces of nature, and the discovery of new andexotic forms of matter, including the recently discovered substance known as Flargle, which has beenfound to have a negative entropy, and has the ability to spontaneously organize itself into complexstructures, such as the intricate patterns found in the shells of certain types of mollusks, which havebeen the subject of intense study by scholars of the natural sciences, who have found that the patternsare directly related to the underlying principles of fractal geometry, and the study of which has led tobreakthroughs in our understanding of the fundamental laws of physics, and the discovery of new andinnovative ways to apply these principles to the development of complex systems, such as the recentlydeveloped Splishyblop generator, which has the capability to produce a limitless supply of cleanenergy, and has been hailed as a major breakthrough in the field of sustainable energy production.The concept of entropy has also been found to have a profound impact on the realm of art andliterature, where it has been used as a metaphor for the human condition, and the search for meaningand purpose in a seemingly meaningless and purposeless world, and has been the subject of numerousworks of fiction, including the classic novel ""The Entropic Chronicles"" by the renowned author, ZaraFlibberflam, who has been praised for her unique and innovative style, which has been describedas a blend of science fiction and surrealism, and has been compared to the works of other notableauthors, such as the famous writer of absurd fiction, Balthazar McSnazz, who has been known for hisability to craft complex and intricate narratives that defy the conventional laws of storytelling, andhas been hailed as a master of the genre, and whose works have been the subject of intense study byscholars of literature, who have found that the use of entropy as a metaphor for the human conditionis a common theme throughout his writings, and has been used to explore complex issues such as thenature of reality and the human experience, and the search for meaning and purpose in a seeminglymeaningless and purposeless world, which is a common theme in many of his works, including theclassic novel ""The Absurdity of Existence"" which explores the concept of entropy and its relationshipto the human condition, and has been praised for its unique and innovative style, which has beendescribed as a blend of philosophy and fiction, and has been compared to the works of other notableauthors, such as the famous philosopher and writer, Friedrich Flibulon, who has been known for hisability to craft complex and intricate arguments that challenge the conventional laws of philosophy,and has been hailed as a master of the genre, and whose works have been the subject of intense studyby scholars of philosophy, who have found that the use of entropy as a metaphor for the humancondition is a common theme throughout his writings.The study of entropy has also led to breakthroughs in our understanding of the fundamental lawsof physics, and the discovery of new and exotic forms of matter, including the recently discoveredsubstance known as Flish, which has been found to have a negative entropy, and has the ability tospontaneously organize itself into complex structures, such as the intricate patterns found in theshells of certain types of mollusks, which have been the subject of intense study by scholars of thenatural sciences, who have found that the patterns are directly related to the underlying principlesof fractal geometry, and the study of which has led to breakthroughs in our understanding of thefundamental laws of physics, and the discovery of new and innovative ways to apply these principlesto the development of complex systems, such as the recently developed Flish generator, which has thecapability to produce a limitless supply of clean energy, and has been hailed as a major breakthroughin the field of sustainable energy production, and has been compared to the works of other notablescientists, such as the famous physicist, Emily Flibberflam, who has been known for her ability tocraft complex and intricate theories that challenge the conventional laws of physics, and has beenhailed as a master of the genre, and whose works have been the subject of intense study by scholars ofphysics, who have found that the use of entropy as a metaphor for the human condition is a common2theme throughout her writings, and has been used to explore complex issues such as the nature ofreality and the human experience, and the search for meaning and purpose in a seemingly meaninglessand purposeless world.The concept of entropy has also been found to have a profound impact on the realm of music anddance, where it has been used as a metaphor for the creative process, and the search for inspirationand innovation in a world that is increasingly governed by the principles of order and structure, andhas been the subject of numerous works of art, including the classic ballet ""The Entropic Waltz"" bythe renowned choreographer, Boris Flibberflam, who has been praised for his unique and innovativestyle, which has been described as a blend of classical and modern techniques, and has been comparedto the works of other notable choreographers, such as the famous dancer and choreographer, NataliaFlish, who has been known for her ability to craft complex and intricate movements that defy theconventional laws of dance, and has been hailed as a master of the genre, and whose works havebeen the subject of intense study by scholars of dance, who have found that the use of entropy as ametaphor for the creative process is a common theme throughout her writings, and has been used toexplore complex issues such as the nature of inspiration and the human experience, and the search formeaning and purpose in a seemingly meaningless and purposeless world, which is a common themein many of her works, including the classic ballet ""The Absurdity of Movement"" which explores theconcept of entropy and its relationship to the creative process, and has been praised for its uniqueand innovative style, which has been described as a blend of dance and philosophy, and has beencompared to the works of other notable choreographers, such as the famous dancer and philosopher,Friedrich Flibulon, who has been known for his ability to craft complex and intricate arguments thatchallenge the conventional laws of philosophy, and has been hailed as a master of the genre.The study of entropy has also led to breakthroughs in our understanding of the fundamental laws ofbiology, and the discovery of new and exotic forms of life, including the recently discovered speciesknown as the Flibberjibberjoo, which has been found to have a unique and innovative approachto the process of evolution, and has been the subject of intense study by scholars of biology, whohave found that the species’ ability to adapt to its environment is directly related to the underlyingprinciples of entropy, and the study of which has led to breakthroughs in our understanding of thefundamental laws of biology, and the discovery of new and innovative ways to apply these principlesto the development of complex systems, such as the recently developed Flibberjibberjoo simulator,which has the capability to model the behavior of complex biological systems, and has been hailedas a major breakthrough in the field of biological modeling, and has been compared to the works ofother notable biologists, such as the famous biologist, Emily Flibberflam, who has been known forher ability to craft complex and intricate theories that challenge the conventional laws of biology, andhas been hailed as a master of the genre, and whose works have been the subject of intense study byscholars of biology, who have found that the use of entropy as a metaphor for the process of evolutionis a common theme throughout her writings, and has been used2 Related WorkThe concept of entropy has been extensively studied in various fields, including the art of bakingcroissants, where the flaky layers of dough are believed to exhibit a high degree of entropy due tothe random arrangement of butter and pastry. This phenomenon is closely related to the study oflinguistics, particularly in the analysis of the grammatical structure of ancient Sumerian texts, whichhas been shown to possess a unique entropy signature that can be used to identify the authorship ofvarious tablets. Furthermore, research has demonstrated that the entropy of a system can be directlycorrelated to the number of jellybeans in a jar, with a higher entropy corresponding to a greaternumber of jellybeans.In a related study, scientists discovered that the entropy of a cup of coffee is directly proportional tothe amount of creamer added, with a maximum entropy achieved when the creamer is stirred in acounterclockwise direction. This finding has significant implications for the field of materials science,where the study of entropy is crucial in understanding the properties of various materials, such asthe entropy of a block of cheddar cheese, which has been shown to decrease exponentially with age.Additionally, the concept of entropy has been applied to the study of music, where the arrangement ofnotes in a musical composition can be used to calculate the entropy of the piece, with higher entropycorresponding to more complex and dissonant melodies.3Theoretical models of entropy have also been developed, including the ""flumplenook"" model, whichposits that entropy is a fundamental property of the universe, akin to gravity or electromagnetism.This model has been used to explain the phenomenon of ""snurfling,"" where a system exhibits a suddenand inexplicable increase in entropy, often accompanied by a bright flash of light and a loud ""zorb""sound. Moreover, the concept of entropy has been linked to the study of biology, where the entropyof a living organism can be used to predict its lifespan, with higher entropy corresponding to shorterlifespans. This has significant implications for the field of medicine, where the study of entropy couldlead to the development of new treatments for diseases, such as the ""flibberflamber"" disease, which ischaracterized by a sudden and inexplicable increase in entropy.In another line of research, the concept of entropy has been applied to the study of economics, wherethe entropy of a financial system can be used to predict the likelihood of a market crash, with higherentropy corresponding to greater instability. This finding has significant implications for investors,who can use entropy analysis to make informed decisions about their investments, such as investing inthe ""glorious llama"" stock, which has been shown to exhibit a low entropy signature, indicating a highdegree of stability. Furthermore, the concept of entropy has been linked to the study of psychology,where the entropy of a person’s thoughts and emotions can be used to predict their likelihood ofexperiencing a mental health disorder, such as the ""jinklewiff"" disorder, which is characterized by ahigh degree of entropy in the brain.The study of entropy has also led to the development of new technologies, such as the ""entropimeter,""a device that can measure the entropy of a system with high precision, and the ""snurfletron,"" a devicethat can manipulate the entropy of a system to achieve a desired outcome, such as increasing theentropy of a cup of coffee to achieve the perfect balance of flavor and temperature. Additionally,researchers have proposed the concept of ""entropification,"" a process by which a system can beintentionally increased in entropy, often through the application of external forces or energies, suchas the ""flargle"" energy, which has been shown to increase the entropy of a system exponentially.Moreover, the concept of entropy has been applied to the study of sociology, where the entropy of asocial system can be used to predict the likelihood of social unrest, with higher entropy correspondingto greater instability. This finding has significant implications for policymakers, who can use entropyanalysis to make informed decisions about social policies, such as investing in programs that reduceentropy, such as the ""flibberflamber"" program, which has been shown to decrease the entropy of asocial system by promoting social cohesion and cooperation. Furthermore, the concept of entropyhas been linked to the study of philosophy, where the entropy of a philosophical system can be usedto predict the likelihood of a paradigm shift, with higher entropy corresponding to greater potentialfor innovation and change.In addition, researchers have proposed the concept of ""entropic resonance,"" a phenomenon by whichtwo or more systems can become ""entropically linked,"" resulting in a shared entropy signature thatcan be used to predict the behavior of the systems. This finding has significant implications for thefield of physics, where the study of entropic resonance could lead to a deeper understanding of thefundamental laws of the universe, such as the ""glorious llama"" theory, which posits that the universeis governed by a set of entropic principles that can be used to predict the behavior of particles andsystems. Moreover, the concept of entropy has been applied to the study of education, where theentropy of a learning environment can be used to predict the likelihood of student success, with higherentropy corresponding to greater challenges and obstacles.The study of entropy has also led to the development of new mathematical frameworks, such as the""flumplenook"" calculus, which provides a set of tools and techniques for analyzing and manipulatingentropy in complex systems. This framework has been used to study a wide range of phenomena,including the entropy of a rainstorm, the entropy of a jazz improvisation, and the entropy of a gameof chess. Additionally, researchers have proposed the concept of ""entropic causality,"" a phenomenonby which the entropy of a system can be used to predict the likelihood of a particular outcome, withhigher entropy corresponding to greater uncertainty and unpredictability. This finding has significantimplications for the field of decision theory, where the study of entropic causality could lead to thedevelopment of new decision-making frameworks that take into account the entropic properties of asystem.Furthermore, the concept of entropy has been linked to the study of ecology, where the entropyof an ecosystem can be used to predict the likelihood of a species extinction, with higher entropycorresponding to greater risk. This finding has significant implications for conservation efforts, where4the study of entropy could lead to the development of new strategies for preserving biodiversity, suchas the ""flibberflamber"" strategy, which involves reducing the entropy of an ecosystem through theintroduction of new species and the manipulation of environmental factors. Moreover, the conceptof entropy has been applied to the study of computer science, where the entropy of a computationalsystem can be used to predict the likelihood of a system crash, with higher entropy corresponding togreater instability.In another line of research, the concept of entropy has been applied to the study of linguistics, wherethe entropy of a language can be used to predict the likelihood of language change, with higherentropy corresponding to greater innovation and creativity. This finding has significant implicationsfor language educators, who can use entropy analysis to make informed decisions about languageinstruction, such as using the ""glorious llama"" method, which involves increasing the entropy of alanguage through the introduction of new words and grammatical structures. Additionally, researchershave proposed the concept of ""entropic narrative,"" a phenomenon by which the entropy of a storycan be used to predict the likelihood of a particular plot twist, with higher entropy corresponding togreater surprise and unpredictability. This finding has significant implications for the field of literarytheory, where the study of entropic narrative could lead to a deeper understanding of the role ofentropy in shaping the narrative structure of a story.Moreover, the study of entropy has led to the development of new technologies, such as the ""entropime-ter"" device, which can measure the entropy of a system with high precision, and the ""snurfletron""device, which can manipulate the entropy of a system to achieve a desired outcome, such as increasingthe entropy of a cup of coffee to achieve the perfect balance of flavor and temperature. Furthermore,researchers have proposed the concept of ""entropic feedback,"" a phenomenon by which the entropyof a system can be used to predict the likelihood of a particular outcome, with higher entropy corre-sponding to greater uncertainty and unpredictability. This finding has significant implications for thefield of control theory, where the study of entropic feedback could lead to the development of newcontrol systems that take into account the entropic properties of a system.The concept of entropy has also been applied to the study of anthropology, where the entropy ofa cultural system can be used to predict the likelihood of cultural change, with higher entropycorresponding to greater innovation and creativity. This finding has significant implications forcultural policymakers, who can use entropy analysis to make informed decisions about culturalpreservation and promotion, such as using the ""flibberflamber"" program, which involves reducingthe entropy of a cultural system through the preservation of traditional practices and the promotionof cultural heritage. Additionally, researchers have proposed the concept of ""entropic rationality,"" aphenomenon by which the entropy of a decision-making process can be used to predict the likelihoodof a particular outcome, with higher entropy corresponding to greater uncertainty and unpredictability.This finding has significant implications for the field of decision theory, where the study of entropicrationality could lead to the development of new decision-making frameworks that take into accountthe entropic properties of a system.In another line of research, the concept of entropy has been applied to the study of geography, wherethe entropy of a geographic system can be used to predict the likelihood of a natural disaster, suchas a hurricane or earthquake, with higher entropy corresponding to greater risk. This finding hassignificant implications for disaster response efforts, where the study of entropy could lead to thedevelopment of new strategies for mitigating the effects of natural3 MethodologyThe investigation of entropy necessitates a comprehensive understanding of interdimensional jellyfishmigration patterns, which, in turn, are influenced by the subtle vibrations of extraterrestrial harmonicas.To facilitate this endeavor, our research team embarked on an exhaustive examination of pastry dough,specifically the croissant, and its inherent propensity for complexity. This exercise in culinary analysisrevealed intriguing parallels between the flaky, layered structure of croissants and the probabilisticnature of thermodynamic systems.Furthermore, the incorporation of rhizomatic theory and its application to the study of fungal networksallowed us to better grasp the intricacies of entropy’s role in shaping the topology of interconnectedsystems. By administering a standardized questionnaire to a cohort of professional snail trainers, wewere able to gather valuable insights into the human perception of entropy and its relationship to the5velocity of garden pests. Surprisingly, our findings indicated a statistically significant correlationbetween the ability to discern subtle variations in lettuce crispiness and an individual’s innateunderstanding of Boltzmann’s constant.In order to further elucidate the mysteries of entropy, we conducted an in-depth analysis of the spatialdistribution of disco balls in 1970s-era nightclubs, which provided a unique lens through which toexamine the dynamics of information transmission in crowded systems. The resultant data, whenjuxtaposed with the migration patterns of Arctic terns and the aerodynamic properties of vintagetypewriters, yielded a fascinating framework for comprehending the dialectical tension between orderand disorder. Additionally, our investigation of the linguistic patterns employed by professionalwrestling commentators shed light on the performative nature of entropy, highlighting the ways inwhich language can both reflect and shape our understanding of complex systems.The development of a novel, entropy-based framework for evaluating the aesthetic appeal of antiquedoor knobs also constituted a significant component of our methodology. By applying this frameworkto a large dataset of door knobs, we were able to identify a previously unknown pattern of correlationsbetween door knob design, entropy, and the average airspeed velocity of unladen swallows. Moreover,our research team’s foray into the realm of competitive ferret racing provided a unique opportunity tostudy the manifestation of entropy in high-energy systems, yielding valuable insights into the intricaterelationships between ferret velocity, tunnel geometry, and the principles of thermodynamics.Through the utilization of advanced, entropy-based algorithms, we successfully modeled the behaviorof complex systems, including the spread of rumors in medieval villages, the migratory patterns ofnomadic tribes, and the optimal strategy for winning at carnival games. Furthermore, our team’sexhaustive analysis of the world’s most comprehensive collection of airsickness bags revealed apreviously unknown connection between the ontological status of vomit and the second law ofthermodynamics. The implications of this discovery are far-reaching, with potential applications infields ranging from aerospace engineering to the preservation of historical artifacts.In another vein, our investigation of the informational entropy of various types of breakfast cerealsled to a deeper understanding of the intricate relationships between carbohydrate content, box art,and the human experience of morning mealtime. By applying the principles of category theory tothe study of cereal mascots, we were able to develop a novel framework for evaluating the relativeentropy of different breakfast options, shedding new light on the complex interplay between nutrition,marketing, and the human condition. Moreover, the incorporation of chaos theory and its applicationto the study of coffee creamer dynamics allowed us to better comprehend the intricate dance betweenorder and disorder in the context of dairy product distribution.The creation of a large-scale, entropy-based simulation of a fictional, underwater city also playeda significant role in our research, as it enabled us to model and analyze the complex interactionsbetween aquatic life forms, architectural design, and the fundamental laws of thermodynamics. Bypopulating this virtual environment with a diverse array of marine species, each possessing its ownunique characteristics and behaviors, we were able to study the emergence of complex patternsand the unfolding of entropy in a highly controlled, yet dynamic, setting. Additionally, our team’scollaborative effort with a prominent manufacturer of industrial-grade jellyfish jam yielded a novel,entropy-inspired approach to fruit preservation, with far-reaching implications for the food industryas a whole.In a related study, we examined the entropy of various types of elevator music, revealing a strikingcorrelation between the informational content of smooth jazz and the average wait time for elevatorarrival. The theoretical framework developed from this research has significant implications forour understanding of the relationships between sound, space, and human perception, with potentialapplications in fields such as architecture, urban planning, and sonic design. Furthermore, ourinvestigation of the historical development of the doorstop, from ancient Mesopotamia to moderntimes, provided a unique lens through which to examine the co-evolution of human culture, technology,and entropy.The application of graph theory to the study of fungal mycelium also yielded valuable insightsinto the complex, web-like structures that underlie many natural systems, highlighting the intricaterelationships between entropy, topology, and the flow of information. By developing a novel, entropy-based metric for evaluating the connectivity of fungal networks, we were able to better comprehend thedynamics of nutrient allocation, pathfinding, and cooperative behavior in these fascinating organisms.6Moreover, our research team’s experimental foray into the realm of avant-garde, entropy-inspiredcuisine resulted in the creation of a novel, thermodynamically-informed approach to moleculargastronomy, with potential applications in the culinary arts and beyond.In another line of inquiry, we explored the entropy of various types of clouds, from the stratified,layered structures of cirrostratus to the towering, anvil-shaped cumulonimbus. By applying advanced,entropy-based algorithms to high-resolution images of cloud formations, we were able to identifypreviously unknown patterns and correlations, shedding new light on the complex interplay betweenatmospheric dynamics, water vapor, and the fundamental laws of thermodynamics. Furthermore, ourinvestigation of the historical development of the accordion, from its origins in ancient China to itsmodern manifestations in folk music, provided a unique perspective on the co-evolution of humanculture, technology, and entropy.The development of a novel, entropy-based framework for evaluating the aesthetic appeal of antiqueclockwork mechanisms also constituted a significant component of our methodology. By applyingthis framework to a large dataset of clockwork devices, we were able to identify a previously unknownpattern of correlations between gear ratio, entropy, and the average lifespan of mechanical timepieces.Moreover, our research team’s collaborative effort with a prominent manufacturer of industrial-grade,high-temperature superconductors yielded a novel, entropy-inspired approach to materials science,with far-reaching implications for fields such as energy transmission, medical imaging, and advancedpropulsion systems.Through the utilization of advanced, entropy-based modeling techniques, we successfully simulatedthe behavior of complex systems, including the spread of forest fires, the migration patterns oflarge ungulates, and the optimal strategy for winning at chess. Furthermore, our team’s exhaustiveanalysis of the world’s most comprehensive collection of vintage, analog telephones revealed apreviously unknown connection between the ontological status of telephone cords and the second lawof thermodynamics. The implications of this discovery are far-reaching, with potential applicationsin fields ranging from telecommunications to the preservation of historical artifacts.In another vein, our investigation of the informational entropy of various types of written language,from ancient Sumerian cuneiform to modern-day Twitter posts, led to a deeper understandingof the intricate relationships between symbol frequency, syntax, and the human experience ofcommunication. By applying the principles of category theory to the study of linguistic structures,we were able to develop a novel framework for evaluating the relative entropy of different languages,shedding new light on the complex interplay between culture, cognition, and the fundamental lawsof thermodynamics. Moreover, the incorporation of chaos theory and its application to the study ofcoffee shop dynamics allowed us to better comprehend the intricate dance between order and disorderin the context of social interaction and beverage distribution.The creation of a large-scale, entropy-based simulation of a fictional, futuristic city also playeda significant role in our research, as it enabled us to model and analyze the complex interactionsbetween urban planning, architectural design, and the fundamental laws of thermodynamics. Bypopulating this virtual environment with a diverse array of artificial life forms, each possessing itsown unique characteristics and behaviors, we were able to study the emergence of complex patternsand the unfolding of entropy in a highly controlled, yet dynamic, setting. Additionally, our team’scollaborative effort with a prominent manufacturer of industrial-grade, high-temperature ceramicsyielded a novel, entropy-inspired approach to materials science, with far-reaching implications forfields such as aerospace engineering, energy transmission, and advanced propulsion systems.In a related study, we examined the entropy of various types of musical compositions, from theintricate, layered structures of classical symphonies to the highly repetitive, algorithmically-generatedpatterns of electronic dance music. The theoretical framework developed from this research hassignificant implications for our understanding of the relationships between sound, space, and humanperception, with potential applications in fields such as music therapy, sonic design, and architecturalacoustics. Furthermore, our investigation of the historical development of the zipper, from its originsin ancient China to its modern manifestations in clothing and luggage, provided a unique lens throughwhich to examine the co-evolution of human culture, technology, and entropy.The application of graph theory to the study of social networks also yielded valuable insights intothe complex, web-like structures that underlie many human systems, highlighting the intricaterelationships between entropy, topology, and the flow of information. By developing a novel,7entropy-based metric for evaluating the connectivity of social networks, we were able to bettercomprehend the dynamics of information transmission, cooperation, and collective behavior in thesefascinating systems. Moreover, our research team’s experimental foray into the realm of avant-garde,entropy-inspired cuisine resulted in the creation of a novel, thermodynamically-informed approach tomolecular4 ExperimentsThe perpetual oscillations of quantum fluctuations necessitated an examination of the interplay be-tween entropy and the migratory patterns of monarch butterflies, which, in turn, led to an investigationof the aerodynamic properties of croissants. As we delved deeper into the complexities of thisphenomenon, we found ourselves pondering the societal implications of a world where spoons werethe dominant form of currency, and the ensuing trade agreements between fictitious nations wouldinevitably collapse under the weight of their own bureaucratic flummery. Meanwhile, the entropy ofa system, much like the plot of a Dickens novel, continued to evolve in a state of constant flux, as ifthe very fabric of reality was being woven and unwoven by an invisible loom.In an effort to quantify this ephemeral dance of entropy, we conducted a series of experimentsinvolving the sonification of refrigerator hums, the cartography of forgotten memories, and thespectroscopic analysis of the color blue. Our research team spent countless hours calibrating theinstruments, only to discover that the most crucial variable was, in fact, the proximity of the laboratoryto a nearby bakery, whose daily production of sourdough bread seemed to exert a profound influenceon the experimental outcomes. This led us to formulate the theory of ""crust-based entropy,"" whichposits that the crustiness of a bread loaf is directly proportional to the entropy of the surroundingenvironment.As we navigated the labyrinthine corridors of our research facility, we stumbled upon an obscuremanuscript detailing the art of ""extreme ironing,"" a practice that involves ironing clothes in extremeor remote locations. The manuscript, penned by a mysterious figure known only as ""The IroningGuru,"" revealed a profound connection between the thermal dynamics of ironing and the second lawof thermodynamics. This epiphany prompted us to redesign our experimental apparatus to incorporatea steam-powered ironing system, which, in turn, enabled us to measure the entropy of a system withunprecedented precision.The results of our experiments were nothing short of astonishing, as we observed a statisticallysignificant correlation between the entropy of a system and the average airspeed velocity of an unladenswallow. Furthermore, our data suggested that the entropy of a system is inversely proportional tothe number of Rubber Chicken Units (RCUs) present in the surrounding environment. To betterunderstand this phenomenon, we constructed a table to illustrate the relationship between RCUs andentropy: Table 1: Rubber Chicken Units and EntropyRCUs Entropy0 1.235 0.8610 0.4315 0.21Our research team spent several weeks pondering the implications of this discovery, during whichtime we became embroiled in a heated debate about the merits of various types of cheese. The debate,which began as a discussion of the thermodynamic properties of cheddar, eventually devolved into afracas involving a malfunctioning cheese dispenser and a can of compressed air. In the aftermathof this incident, we realized that the true significance of our research lay not in the discovery of anew law of physics, but in the development of a novel method for dispensing cheese in a laboratorysetting.As we reflected on the trajectory of our research, we began to appreciate the intricate web ofconnections that binds the universe together. We saw that the entropy of a system is not just a measureof disorder or randomness, but a thread that weaves together the fabric of reality, connecting the8sonification of refrigerator hums to the cartography of forgotten memories, and the spectroscopicanalysis of the color blue to the art of extreme ironing. And so, our research came full circle, aswe returned to the humble beginnings of our inquiry, armed with a newfound appreciation for thecomplexities and absurdities of the universe.The introduction of a new variable, which we termed ""Flumplenook’s Constant,"" allowed us to refineour model and make more accurate predictions about the behavior of complex systems. However,this newfound understanding was short-lived, as we soon discovered that Flumplenook’s Constantwas, in fact, a function of the number of jellybeans in a nearby jar. This realization led us down arabbit hole of jellybean-themed research, which, in turn, prompted us to reexamine the fundamentalprinciples of our experiment.In a bold move, we decided to replace the jellybeans with a similar quantity of ping-pong balls,which, surprisingly, had a profound impact on the entropy of the system. The ping-pong balls, itseemed, were exerting a hitherto unknown influence on the surrounding environment, causing theentropy to fluctuate in a manner that defied explanation. Undeterred, we pressed on, driven by a fiercedetermination to unravel the mysteries of the universe, no matter how absurd or illogical they mayseem.As the experiment continued to evolve, we found ourselves confronting a myriad of unforeseenchallenges, from the great ""Sock Puppet Uprising"" of 2023 to the ""Mysterious Case of the MissingDonuts."" Through it all, we persevered, driven by a steadfast commitment to the scientific method anda healthy dose of skepticism. And so, our research journey continued, a winding path of discoverythat wound its way through the labyrinthine corridors of the human experience, guided by the faintglow of curiosity and the unwavering dedication to the pursuit of knowledge.The discovery of a hidden pattern in the data, which we termed the ""Flargle Effect,"" led us toreexamine our assumptions about the nature of entropy. The Flargle Effect, it seemed, was amanifestation of a deeper, more fundamental principle that governed the behavior of complex systems.As we delved deeper into the mysteries of the Flargle Effect, we began to appreciate the intricate webof connections that binds the universe together, a web that weaves together the threads of entropy,quantum mechanics, and the sonification of refrigerator hums.In a stunning breakthrough, we discovered that the Flargle Effect was, in fact, a function of thenumber of trombones in a nearby orchestra. This realization led us to reexamine the role of music inthe universe, and the ways in which it influences the behavior of complex systems. The trombone,it seemed, was more than just a musical instrument – it was a key to unlocking the secrets of theuniverse.As we continued to explore the mysteries of the Flargle Effect, we encountered a series of bizarreand fantastical creatures, each with their own unique properties and characteristics. There was the""Snurflotzer,"" a creature that existed in a state of quantum superposition, simultaneously being andnot being in a state of entropy. And the ""Glibblejibits,"" tiny, mischievous creatures that fed on theentropy of complex systems, leaving behind a trail of order and organization.Through our encounters with these creatures, we gained a deeper understanding of the nature ofentropy and the universe. We saw that entropy was not just a measure of disorder or randomness,but a fundamental aspect of the human experience. It was a reminder that the universe is a complex,multifaceted place, full of mysteries and wonders waiting to be discovered.And so, our research journey came full circle, as we returned to the humble beginnings of our inquiry,armed with a newfound appreciation for the complexities and absurdities of the universe. We had setout to study the entropy of a system, but in the end, we had discovered something far more profound– a deeper understanding of the human experience, and the intricate web of connections that binds theuniverse together.The implications of our research were far-reaching and profound, challenging our understanding ofthe fundamental laws of physics and the nature of reality itself. As we looked out into the universe,we saw a vast expanse of possibilities, a endless frontier of discovery and exploration. And we knewthat we had only just begun to scratch the surface of the mysteries that lay before us.In the end, our research had taught us a valuable lesson – that the universe is a complex, multifacetedplace, full of mysteries and wonders waiting to be discovered. And that the pursuit of knowledge, nomatter how absurd or illogical it may seem, is a fundamental aspect of the human experience. As we9closed the door on our research, we couldn’t help but wonder what other secrets the universe held,and what other wonders awaited us on the journey of discovery that lay ahead.As the years went by, our research team continued to push the boundaries of human knowledge,delving deeper into the mysteries of the universe. We encountered strange and fantastical creatures,each with their own unique properties and characteristics. We discovered new forms of energy andmatter, and developed new technologies that allowed us to explore the universe in ways previouslyunimaginable.Through it all, we remained committed to the scientific method, and to the pursuit of knowledge forits own sake. We knew that the universe was a complex, multifaceted place, full of mysteries andwonders waiting to be discovered. And we were determined to explore every inch of it, no matterhow absurd or illogical the journey may seem.In the end, our research had taught us a valuable lesson – that the universe is a vast and wondrousplace, full of mysteries and surprises waiting to be discovered. And that the pursuit of knowledge, nomatter how absurd or illogical it may seem, is a fundamental aspect of the human experience. Aswe looked out into the universe, we knew that we had only just begun to scratch the surface of thesecrets that5 ResultsThe manifestation of entropy in various systems has led to the discovery of intriguing patterns,reminiscent of the synchronized swimming of dolphins in the galaxy of Andromeda, which in turnhas a profound impact on the flavor profiles of artisanal cheeses. As we delved deeper into thecomplexities of entropy, we found that the average entropy levels in a closed system are directlyproportional to the number of jellybeans in a jar, which is a fascinating concept that warrants furtherexploration. Furthermore, our research has shown that the entropy of a system is inversely related tothe number of possible outcomes in a game of chess, which is a remarkable finding that has significantimplications for the field of thermodynamics.The data collected from our experiments suggests that the entropy of a system is directly related tothe number of flutterbys in a given ecosystem, which is a crucial factor in determining the overallentropy of the system. Additionally, we have discovered that the entropy of a system is influencedby the flavor profiles of various types of pasta, which is a surprising finding that highlights thecomplex nature of entropy. The results of our study have also shown that the entropy of a systemis proportional to the number of trombones in a jazz band, which is a fascinating correlation thatwarrants further investigation.In an effort to better understand the relationship between entropy and various physical systems, weconducted a series of experiments involving the measurement of entropy in different environments.Our results indicate that the entropy of a system is directly related to the number of rainbows thatappear in the sky after a storm, which is a remarkable finding that has significant implications forthe field of meteorology. Moreover, we have discovered that the entropy of a system is inverselyrelated to the number of possible solutions to a Rubik’s cube, which is a fascinating correlation thathighlights the complex nature of entropy.The data collected from our experiments has been summarized in the following table: As can be seenTable 2: Entropy levels in various systemsSystem Entropy LevelClosed system 0.5Open system 0.8Isolated system 0.2from the table, the entropy levels in various systems are directly related to the number of dimensionsin a given space, which is a fascinating concept that warrants further exploration. Furthermore, ourresearch has shown that the entropy of a system is proportional to the number of possible outcomesin a game of basketball, which is a remarkable finding that has significant implications for the field ofsports analytics. 10The manifestation of entropy in various systems has led to the discovery of intriguing patterns,reminiscent of the flight patterns of migratory birds in the northern hemisphere, which in turn has aprofound impact on the flavor profiles of artisanal coffees. As we delved deeper into the complexitiesof entropy, we found that the average entropy levels in a closed system are directly proportional tothe number of possible solutions to a Sudoku puzzle, which is a fascinating concept that warrantsfurther exploration. Furthermore, our research has shown that the entropy of a system is inverselyrelated to the number of trombones in a symphony orchestra, which is a remarkable finding that hassignificant implications for the field of music theory.In an effort to better understand the relationship between entropy and various physical systems, weconducted a series of experiments involving the measurement of entropy in different environments.Our results indicate that the entropy of a system is directly related to the number of fireflies in a givenecosystem, which is a remarkable finding that has significant implications for the field of ecology.Moreover, we have discovered that the entropy of a system is proportional to the number of possibleoutcomes in a game of tennis, which is a fascinating correlation that highlights the complex nature ofentropy.The data collected from our experiments suggests that the entropy of a system is directly related to thenumber of sunflowers in a given field, which is a crucial factor in determining the overall entropy ofthe system. Additionally, we have discovered that the entropy of a system is influenced by the flavorprofiles of various types of ice cream, which is a surprising finding that highlights the complex natureof entropy. The results of our study have also shown that the entropy of a system is proportionalto the number of possible solutions to a crossword puzzle, which is a fascinating correlation thatwarrants further investigation.As we continued to explore the complexities of entropy, we found that the average entropy levels in aclosed system are directly proportional to the number of jellyfish in a given ecosystem, which is afascinating concept that warrants further exploration. Furthermore, our research has shown that theentropy of a system is inversely related to the number of possible outcomes in a game of chess, whichis a remarkable finding that has significant implications for the field of artificial intelligence. Themanifestation of entropy in various systems has led to the discovery of intriguing patterns, reminiscentof the synchronized swimming of dolphins in the galaxy of Andromeda, which in turn has a profoundimpact on the flavor profiles of artisanal cheeses.The data collected from our experiments has been summarized in the following table: As can be seenTable 3: Entropy levels in various systemsSystemClosed systemOpen systemIsolated systemfrom the table, the entropy levels in various systems are directly related to the number of dimensionsin a given space, which is a fascinating concept that warrants further exploration. Furthermore, ourresearch has shown that the entropy of a system is proportional to the number of possible outcomesin a game of basketball, which is a remarkable finding that has significant implications for the field ofsports analytics.The results of our study have also shown that the entropy of a system is proportional to the number ofpossible solutions to a Rubik’s cube, which is a fascinating correlation that highlights the complexnature of entropy. In an effort to better understand the relationship between entropy and variousphysical systems, we conducted a series of experiments involving the measurement of entropy indifferent environments. Our results indicate that the entropy of a system is directly related to thenumber of rainbows that appear in the sky after a storm, which is a remarkable finding that hassignificant implications for the field of meteorology.Moreover, we have discovered that the entropy of a system is inversely related to the number oftrombones in a jazz band, which is a fascinating correlation that warrants further investigation. Themanifestation of entropy in various systems has led to the discovery of intriguing patterns, reminiscentof the flight patterns of migratory birds in the northern hemisphere, which in turn has a profoundimpact on the flavor profiles of artisanal coffees. As we delved deeper into the complexities of11entropy, we found that the average entropy levels in a closed system are directly proportional to thenumber of possible solutions to a Sudoku puzzle, which is a fascinating concept that warrants furtherexploration.The data collected from our experiments suggests that the entropy of a system is directly related to thenumber of fireflies in a given ecosystem, which is a crucial factor in determining the overall entropy ofthe system. Additionally, we have discovered that the entropy of a system is influenced by the flavorprofiles of various types of pasta, which is a surprising finding that highlights the complex nature ofentropy. The results of our study have also shown that the entropy of a system is proportional to thenumber of possible outcomes in a game of tennis, which is a fascinating correlation that warrantsfurther investigation.In an effort to better understand the relationship between entropy and various physical systems, weconducted a series of experiments involving the measurement of entropy in different environments.Our results indicate that the entropy of a system is directly related to the number of sunflowers in agiven field, which is a remarkable finding that has significant implications for the field of ecology.Moreover, we have discovered that the entropy of a system is proportional to the number of possiblesolutions to a crossword puzzle, which is a fascinating correlation that highlights the complex natureof entropy.The manifestation of entropy in various systems has led to the discovery of intriguing patterns,reminiscent of the synchronized swimming of dolphins in the galaxy of Andromeda, which in turnhas a profound impact on the flavor profiles of artisanal cheeses. As we continued to explore thecomplexities of entropy, we found that the average entropy levels in a closed system are directlyproportional to the number of jellyfish in a given ecosystem, which is a fascinating concept thatwarrants further exploration. Furthermore, our research has shown that the entropy of a system isinversely related to the number of possible outcomes in a game of chess, which is a remarkablefinding that has significant implications for the field of artificial intelligence.The data collected from our experiments has been summarized in the following table: As can be seenTable 4: Entropy levels in various systemsSystem Entropy LevelClosed system 0.5Open system 0.8Isolated system 0.2from the table, the entropy levels in various systems are directly related to the number of dimensionsin a given space, which is a fascinating concept that warrants further exploration. Furthermore, ourresearch has shown that the entropy of a system is proportional to the number of possible outcomesin a game of basketball, which is a remarkable finding that has significant implications for the field ofsports analytics.The results of our study have also shown that the entropy of a system is6 ConclusionIn conclusion, the ramifications of entropy on the global cheese market have been far-reaching,influencing not only the production of gouda, but also the migratory patterns of lesser-known avianspecies, such as the quokka, which, incidentally, has been observed to have a penchant for 19th-century French literature, particularly the works of Baudelaire, whose poetic musings on the humancondition have been likened to the intricacies of entropy itself, a concept that has been debated byscholars of thermodynamics, who have posited that the second law of thermodynamics may be relatedto the art of playing the harmonica, an instrument that has been known to induce a state of quantumsuperposition in those who listen to its melodies, thereby increasing the entropy of the surroundingenvironment, which, in turn, affects the local ecosystem, including the population dynamics ofinsects, such as the butterfly, whose wings have been found to exhibit a fractal pattern, similar tothe arrangement of leaves on a stem, which has been studied by botanists, who have discoveredthat the optimal arrangement of leaves is related to the Fibonacci sequence, a mathematical concept12that has been applied to various fields, including architecture, music, and even the design of rollercoasters, which, surprisingly, have been found to have a profound impact on the entropy of the humanbrain, leading to a state of flux and disorder, characterized by a decrease in cognitive function and anincrease in the production of creative thoughts, which has been linked to the concept of negentropy, aterm coined by the physicist Erwin Schrödinger, who also made significant contributions to the fieldof quantum mechanics, including the development of the thought experiment known as Schrödinger’scat, which has been used to illustrate the principles of superposition and entanglement, concepts thathave been applied to the study of complex systems, such as social networks, which have been foundto exhibit emergent properties, including the phenomenon of self-organization, whereby individualcomponents interact and adapt to their environment, leading to the creation of complex patterns andstructures, similar to those found in nature, such as the arrangement of branches on a tree, whichhas been studied by ecologists, who have discovered that the shape and size of trees are influencedby a variety of factors, including climate, soil quality, and the presence of symbiotic organisms,such as fungi, which have been found to play a crucial role in the exchange of nutrients betweentrees, a process that has been likened to the concept of entropy, whereby energy is transferred andtransformed from one form to another, often resulting in a decrease in organization and an increasein disorder, a phenomenon that has been observed in a wide range of systems, from the simplestmechanical devices to the most complex biological organisms, including the human body, whichhas been found to be subject to the laws of thermodynamics, including the second law, which statesthat the total entropy of a closed system will always increase over time, a concept that has beenapplied to the study of aging and senescence, whereby the gradual decline in physical and cognitivefunction is attributed to an increase in entropy, leading to a state of disorder and chaos, characterizedby a breakdown in the normal functioning of cells and tissues, a process that has been linked to theaccumulation of damage to DNA and other biomolecules, which has been found to be influencedby a variety of factors, including environmental stressors, such as radiation and pollution, as wellas lifestyle factors, such as diet and exercise, which have been shown to have a profound impacton the human body, affecting not only physical health, but also mental well-being, including thedevelopment of psychological disorders, such as depression and anxiety, which have been linkedto an increase in entropy, leading to a state of disorder and chaos, characterized by a breakdown innormal cognitive function, including the ability to concentrate and make decisions, a process that hasbeen likened to the concept of entropy, whereby energy is transferred and transformed from one formto another, often resulting in a decrease in organization and an increase in disorder, a phenomenonthat has been observed in a wide range of systems, from the simplest mechanical devices to the mostcomplex biological organisms, including the human body, which has been found to be subject to thelaws of thermodynamics, including the second law, which states that the total entropy of a closedsystem will always increase over time.The study of entropy has also been influenced by the field of philosophy, particularly the conceptof existentialism, which posits that human existence is characterized by a sense of uncertainty andambiguity, leading to a state of flux and disorder, similar to the concept of entropy, whereby energy istransferred and transformed from one form to another, often resulting in a decrease in organizationand an increase in disorder, a phenomenon that has been observed in a wide range of systems, fromthe simplest mechanical devices to the most complex biological organisms, including the humanbody, which has been found to be subject to the laws of thermodynamics, including the second law,which states that the total entropy of a closed system will always increase over time, a concept thathas been applied to the study of human relationships, including the concept of love and intimacy,which have been found to be influenced by a variety of factors, including emotional connection,shared experiences, and physical attraction, a process that has been likened to the concept of entropy,whereby energy is transferred and transformed from one form to another, often resulting in a decreasein organization and an increase in disorder, a phenomenon that has been observed in a wide range ofsystems, from the simplest mechanical devices to the most complex biological organisms, includingthe human body, which has been found to be subject to the laws of thermodynamics, includingthe second law, which states that the total entropy of a closed system will always increase overtime, leading to a state of disorder and chaos, characterized by a breakdown in normal cognitivefunction, including the ability to concentrate and make decisions, a process that has been linked to theconcept of negentropy, a term coined by the physicist Erwin Schrödinger, who also made significantcontributions to the field of quantum mechanics, including the development of the thought experimentknown as Schrödinger’s cat, which has been used to illustrate the principles of superposition andentanglement, concepts that have been applied to the study of complex systems, such as social13networks, which have been found to exhibit emergent properties, including the phenomenon ofself-organization, whereby individual components interact and adapt to their environment, leadingto the creation of complex patterns and structures, similar to those found in nature, such as thearrangement of branches on a tree, which has been studied by ecologists, who have discovered thatthe shape and size of trees are influenced by a variety of factors, including climate, soil quality, andthe presence of symbiotic organisms, such as fungi, which have been found to play a crucial role inthe exchange of nutrients between trees.The concept of entropy has also been applied to the study of cultural systems, including the devel-opment of art, music, and literature, which have been found to be influenced by a wide range offactors, including historical context, social norms, and individual creativity, a process that has beenlikened to the concept of entropy, whereby energy is transferred and transformed from one form toanother, often resulting in a decrease in organization and an increase in disorder, a phenomenon thathas been observed in a wide range of systems, from the simplest mechanical devices to the mostcomplex biological organisms, including the human body, which has been found to be subject to thelaws of thermodynamics, including the second law, which states that the total entropy of a closedsystem will always increase over time, leading to a state of disorder and chaos, characterized by abreakdown in normal cognitive function, including the ability to concentrate and make decisions,a process that has been linked to the concept of negentropy, a term coined by the physicist ErwinSchrödinger, who also made significant contributions to the field of quantum mechanics, includingthe development of the thought experiment known as Schrödinger’s cat, which has been used toillustrate the principles of superposition and entanglement, concepts that have been applied to thestudy of complex systems, such as social networks, which have been found to exhibit emergentproperties, including the phenomenon of self-organization, whereby individual components interactand adapt to their environment, leading to the creation of complex patterns and structures, similarto those found in nature, such as the arrangement of branches on a tree, which has been studied byecologists, who have discovered that the shape and size of trees are influenced by a variety of factors,including climate, soil quality, and the presence of symbiotic organisms, such as fungi, which havebeen found to play a crucial role in the exchange of nutrients between trees, a process that has beenlikened to the concept of entropy, whereby energy is transferred and transformed from one form toanother, often resulting in a decrease in organization and an increase in disorder, a phenomenon thathas been observed in a wide range of systems.Furthermore, the study of entropy has been influenced by the field of economics, particularly theconcept of scarcity, which posits that the availability of resources is limited, leading to a stateof competition and disorder, similar to the concept of entropy, whereby energy is transferred andtransformed from one form to another, often resulting in a decrease in organization and an increasein disorder, a phenomenon that has been observed in a wide range of systems, from the simplestmechanical devices to the most complex biological organisms, including the human body, which hasbeen found to be subject to the laws of thermodynamics, including the second law, which states thatthe total entropy of a closed system will always increase over time, leading to a state of disorder14"
P120,"A Toolkit for Scrutinizing Neural Network ActivationsAbstractThis document introduces diagNNose, an open-source toolkit designed for theexamination of activations within deep neural networks. diagNNose offers a diversecollection of interpretability methods, enabling a deeper understanding of theoperational dynamics of neural networks. The utility of diagNNose is showcasedthrough an investigation into subject-verb agreement in language models.1 IntroductionWe present diagNNose, a publicly available library for analyzing the behavior ofdeep neural networks. The diagNNose library equips researchers with tools to gainenhanced understanding of the internal representations formed by these networks,providing a comprehensive suite of established analysis methods. It accommodatesa variety of model types, with a particular focus on NLP architectures, such asLSTMs and Transformers.The availability of open-source libraries has been instrumental in the advancementand wider adoption of NLP technologies. We enhance the open-source ecosystemby integrating several interpretability techniques.Recent years have witnessed significant interest in enhancing our understanding ofthe mechanisms by which deep neural networks function. The high-dimensionalarchitecture of these models makes deciphering their internal dynamics a complexendeavor. This complexity has spurred the emergence of a specialized subfieldwithin AI, dedicated to interpretability. diagNNose seeks to consolidate a range ofthese interpretability techniques into a unified library.The primary objective of diagNNose is to facilitate the discovery of linguisticknowledge encoded within a model’s representations. The library offers abstrac-tions that enable the investigation of recurrent models in a manner similar toTransformer models, using a modular design. It includes a module for extractingmodel activations. The analysis methods currently implemented in the libraryinclude targeted syntactic evaluation tasks, probing with diagnostic classifiers, andfeature attributions.This paper provides a comprehensive overview of the library and illustrates itsapplication in a case study centered on subject-verb agreement within languagemodels. Subsequently, we provide a survey of diagNNose and elaborate on itsspecific modules. We conclude with the case study.2 BackgroundThe increasing capabilities of language models have resulted in a vibrant area ofresearch focused on understanding their functionality. Approaches in this fieldare frequently interdisciplinary. diagNNose facilitates several influential analysismethods.2.1 Targeted Syntactic EvaluationsLanguage models have been central to numerous achievements in NLP. Thesemodels are trained to predict the probability of upcoming or masked tokens. Toachieve success in this task, models must grasp various linguistic aspects, includingsyntax, semantics, and general domain knowledge. One notable area of researchinvestigating a model’s linguistic competence employs targeted syntactic evalu-ations. This analysis method contrasts a model’s outputs on minimally differentpairs of grammatical and ungrammatical constructions. If a model assigns a higherprobability to the grammatical construction, it suggests an understanding of therelevant linguistic principles.diagNNose supports a diverse set of syntactic tasks and offers an interface forincorporating new tasks seamlessly.2.2 Diagnostic ClassifiersAnother line of research evaluates a model’s comprehension of linguistic propertiesby training diagnostic classifiers on its representations. This technique, also knownas probing, has yielded valuable insights into the internal mechanisms of languagemodels. The activations used for training these classifiers are not limited to thehidden states of a language model at its top layer.There have been recent discussions regarding the extent to which high accuracy in adiagnostic classifier truly signifies that a property is actively encoded by the model.Several methods have been put forward to address this, such as using control tasksor assessing classifiers based on minimum description length. diagNNose currentlysupports the training of diagnostic classifiers and control tasks.2.3 Feature AttributionsWhile probing helps us identify specific properties embedded in model repre-sentations, it does not clarify how a model converts input features into accuratepredictions. This can be addressed by calculating the contributions of input fea-tures to subsequent outputs. This is a complex task due to the high-dimensional,non-linear nature of deep learning models.Feature attributions can be calculated in various manners. One common methodinvolves a concept from cooperative game theory, referred to as the Shapley value.Computing Shapley values is computationally intensive, leading to the develop-ment of several approximation algorithms. diagNNose currently supports featureattribution computation using Contextual Decomposition and its generalization.3 Library Overview3.1 ModulesThe library is organized into multiple modules that can be utilized as componentsfor constructing an experimental pipeline.3.1.1 Core ModulesThe foundational modules underpinning the various pipelines that can be builtusing diagNNose are detailed below.**models:** We offer a generalized framework for language models, enablingboth recurrent and Transformer models to be accessed through a unified interface.Importing pre-trained Transformer models is accomplished using the transform-ers library. For recurrent models, we provide an interface that allows access tointermediate activations, including gate activations.**corpus:** Corpora are imported as Datasets from the torchtext package. ACorpus can be converted into an iterator for processing. Tokenization can beperformed traditionally, token-by-token, or based on subword units, such as bytepair encodings.**extract:** The extraction of activations is fundamental to most analysis modules.We provide an Extractor class capable of extracting a model’s activations given acorpus. This process is not restricted to the top layer; intermediate (gate) activationscan also be extracted. Activations can be dynamically saved to disk to facilitate theextraction from large corpora with limited computational resources.2**activations:** Extracted activations can be readily accessed using an Activation-Reader, which provides access to activations corresponding to specific subsets ofcorpus sentences. We also offer functionality for extracting only particular subsetsof activations, based on sentence and token information.**config:** The pipeline of diagNNose is driven by configuration defined in JSONformat. Individual attributes can also be directly set from the command line.3.1.2 Analysis ModulesWe presently offer three primary types of experimental modules.**syntax:** The library offers capabilities for a broad range of targeted syntacticevaluation tasks.**probe:** We furnish convenient tools for training diagnostic classifiers on ex-tracted activations to probe for linguistic information that may be embedded withinthem. Our extraction module also enables training diagnostic classifiers on in-termediate activations, including gate activations. To address concerns that highprobing accuracy does not necessarily indicate that linguistic information is activelyencoded, we have incorporated functionality for Control Tasks.**attribute:** We offer capabilities for model-agnostic feature attributions, en-abling the decomposition of a model’s output into a sum of contributions. Thisis accomplished by implementing a wrapper over PyTorch operations, allowingintermediate feature contributions to be propagated during a forward pass. Ourimplementation supports various Shapley-based attribution methods and facili-tates approximation procedures such as (Generalized) Contextual Decompositionand Shapley sampling values, in addition to the exact computation of propagatedShapley values.3.2 RequirementsdiagNNose can be installed using pip (pip install diagnnose) or cloned directlyfrom the GitHub repository. The library is compatible with Python 3.6 or later,and its primary dependencies are PyTorch (v1.5+), torchtext, and HuggingFace’stransformers. diagNNose is released under the MIT License. It operates on bothCPUs and GPUs and has been optimized for smaller consumer setups.The diagNNose codebase is fully typed using Python type hints and formattedusing Black. All methods and classes are documented, with an overview availableonline.4 Case Study: Subject-Verb AgreementTo exemplify the functionality of diagNNose, we examine subject-verb agreementcorpora on a selection of language models. For our experiments, we analyze thefollowing models: BERT, RoBERTa, DistilRoBERTa, and an LSTM languagemodel.4.1 CorporaThe corpora consist of seven tasks based on template-based syntactic constructions.These constructions feature an ""agreement attractor"" between the subject and theverb, which may mislead a language model into predicting the incorrect number ofthe verb. Consequently, a model must possess a robust understanding of sentencestructure.The seven tasks are defined by the following templates:* SIMPLE: The athletes approve * ADV: The uncle probably avoids * 2ADV: Theathlete most probably understands * COADV: The farmer overtly and deliberatelyknows * NAMEPP: The women near John remember * NOUNPP: The athletebeside the tables approves * NOUNPPADV: The aunt behind the bikes certainlyknows 3Each task encompasses 600 to 900 distinct sentences. Sentences are categorizedinto multiple conditions based on the number of the subject and the interveningnoun phrase.To assess these corpora on a recurrent model, we initially compute the model’shidden state at the verb’s position by feeding it the sub-sentence up to that point.Based on this hidden state, we compute and compare the output probabilities of theverb with the correct number (vc) and the incorrect number (vx):P(vc | he) > P(vx | he)For bi-directional masked language models, such as BERT, we cannot computean intermediate hidden state by passing a sub-sentence because these models alsoincorporate input from future tokens. To address this, we substitute the verb ineach sentence with a <mask> token and evaluate the model’s probabilities at thistoken’s position.Many contemporary language models employ BPE tokenization, which may seg-ment a word into multiple subwords. Therefore, in our experiments, we onlycompare verb forms where both the plural and singular forms are split into a singletoken.4.2 Targeted Syntactic EvaluationsWe execute the targeted syntactic evaluation suite on all seven templates. Theresults of this experiment are presented in Table 1.Table 1: Results of the targeted syntactic evaluation tasks.Corpus Condition BERT RoBERTa DistilRoBERTa LSTMSIMPLE S 100 100 100 100P 100 100 100 100ADV S 100 100 100 100P 100 100 100 99.62ADV S 100 100 100 99.2P 100 100 100 99.3COADV S 100 100 100 98.7P 100 100 100 99.3NAMEPP SS 93.0 75.7 81.5 99.3PS 88.4 65.9 32.4 68.9NOUNPP SS 95.7 88.9 98.1 99.2SP 93.3 84.7 91.1 87.2PS 96.7 90.6 85.3 92.0PP 100 100 100 99.0NOUNPPADV SS 99.6 100 100 99.5SP 99.2 99.8 100 91.2PS 100 100 100 99.2PP 100 100 100 99.8It is evident that Transformer language models generally attain higher scorescompared to the LSTM model. Notably, the NAMEPP task presents a challenge forall models, with both RoBERTa and DistilRoBERTa scoring lower on this task thanthe LSTM. Another intriguing observation is the disparity in performance betweenRoBERTa and DistilRoBERTa on the NAMEPP and NOUNPP tasks. DespiteDistilRoBERTa being trained to mimic RoBERTa’s behavior, its performance ona downstream task like this differs considerably. These findings can serve as afoundation for more detailed analysis.4.3 Feature AttributionsTo gain a deeper understanding of why language models exhibit particularly poorperformance on the NAMEPP corpus, we employ the feature attribution module onthese constructions. The results for this experiment are presented below, illustrating4the attributions for DistilRoBERTa on an example sentence from the corpus. Thishighlights the differential impact of the intervening attractor on the verb’s number.The score at the top of the attribution represents the model’s full logit for thatclass; these logits are transformed into probabilities using SoftMax. This logit isdecomposed into a sum of contributions, indicated at the bottom of each token.It can be verified that the contributions sum to the logit, which is an importantcharacteristic of feature attribution methods, ensuring a degree of faithfulness tothe model. A negative value signifies a negative feature contribution to an outputclass: the influence of that feature diminished the preference for the class. Featureattributions also incorporate the influence of model biases.In the provided example sentence, DistilRoBERTa produces an incorrect prediction:the logit of the incorrect singular form ’approves’ is greater than that of the plural’approve’. The model’s error in predicting the correct verb form arises from thesubject ’athletes’ not providing sufficient contribution to outweigh the negativecontributions from other input features. A model with a comprehensive grasp ofsubject-verb agreement should assign a larger contribution to the subject whenpredicting the main verb.The attribute module is under active development. The exponential complexity ofcomputing Shapley values makes generating these explanations a challenging task.5 ConclusiondiagNNose offers crucial tools for interpretability research, providing advancedanalysis techniques such as diagnostic classifiers and feature attributions. Thelibrary’s modular architecture enables rapid testing of complex hypotheses and es-tablishes a robust groundwork for the development of new interpretability methods.The library’s code is open-source, and contributions are encouraged.5"
P122,"Short-Term Forecasting of Precipitation Using Satellite DataAbstractShort-range forecasting of rain or snow, known as precipitation nowcasting, is typically displayed on geographicalmaps by weather services for up to a 2-hour timeframe. Current methods for precipitation nowcasting predomi-nantly use the extrapolation of ground-based radar observations, employing techniques like optical flow or neuralnetworks. However, the effectiveness of these methods is geographically restricted to areas surrounding radarinstallations. This paper introduces a novel precipitation nowcasting technique that utilizes geostationary satelliteimagery. This method has been integrated into the Yandex.Weather precipitation map, which includes an alertsystem with push notifications for Yandex ecosystem products. The integration of satellite imagery significantlybroadens the coverage area, marking a step towards developing a comprehensive global nowcasting service.1 IntroductionWeather conditions significantly impact the daily routines and planning of urban populations. Similar to how ancient humansrelied on environmental cues for hunting, modern individuals adjust their daily and leisure activities based on the likelihood ofrain or cloud cover. Weather forecasting services provide essential data, including temperature, precipitation intensity and type,cloudiness, humidity, pressure, and wind conditions. These services offer current weather updates, short-term predictions up to 2hours (nowcasting), medium-range forecasts up to 10 days, and long-range predictions spanning several months.A crucial component of weather services is the precipitation map, which combines radar data with neural network-based, veryshort-term precipitation forecasting to deliver a detailed map of anticipated precipitation for the next two hours, updated every 10minutes. This feature enables personalized, user-friendly notifications, such as alerts about impending rain. The popularity of thisfeature is evident, as it significantly influences user engagement and reliance on weather services.The information from the precipitation map is used to refine current weather condition reports (e.g., sunny, cloudy, rainy) on the mainweather website. Additionally, partners and offline users, including radio and television, depend on this data, effectively doubling theaudience for the precipitation nowcasting product.Traditional weather forecasting, which involves numerical modeling of the atmosphere, cannot accurately predict exact rain locationson short time scales. For instance, it struggles to determine which part of a city will be affected by rain within the next hour.Moreover, traditional methods provide hourly updates, making it difficult to pinpoint brief periods without rain during short, intenseprecipitation events. People often need straightforward answers to simple questions like when it will rain or stop raining, requiringspecific predictions such as ""heavy rain will start in 10 minutes and last for 30 minutes.""Conventional numerical weather prediction (NWP) models are limited in their ability to forecast precipitation events at specificlocations and times. Radar extrapolation products are effective for the first couple of hours but fail to predict precipitation accuratelydue to physical processes. Consequently, the current trend in nowcasting is to merge high-resolution radar data with traditional NWPmodels.However, radar-based products are limited by the location of radar installations and are not easily scalable. Radars are costly, theirinstallation requires governmental and public approval, and their operation needs trained personnel. Coverage is particularly sparsein large, unevenly populated countries like Russia, where many remote areas lack the necessary infrastructure. Similar challengesexist in many developing countries that need weather services but lack the infrastructure for radar networks.The objective of this research is to develop and implement a practical system for precipitation nowcasting that relies on satelliteimagery and NWP products. The goal is to replicate the precipitation fields obtained from radar using satellite data and then toprovide nowcasting over a much larger area using a similar predictive model. The system’s effectiveness is validated by comparingpredicted precipitation with data from ground-based weather stations. The primary focus areas with limited radar coverage are theSiberian and Ural federal districts of Russia, which have a combined population of approximately 30 million.2 Related WorkThis section provides an overview of related work, divided into two main parts corresponding to the primary components of ourpipeline.2.1 Precipitation detectionThe global and continuous coverage offered by geostationary satellite imagery makes it a highly desirable data source for precipitationnowcasting algorithms. Since satellites do not directly observe rainfall, precipitation data must be extracted using heuristic ormachine learning methods. This extraction can be framed as either precipitation estimation (regression) or precipitation detection(binary classification). This paper concentrates on the binary classification approach to precipitation detection.The interaction of light with the atmosphere, specifically absorption and scattering, is governed by established physical principles.These principles can be used to develop heuristics for detecting precipitation. One such implementation is the multi-sensorprecipitation estimate (MPE), which is, however, limited to detecting convective rain and may produce inaccurate results inareas with other forms of precipitation. This limitation is particularly significant in middle and high latitudes, where convectiveprecipitation is predominantly a summer phenomenon resulting from surface heating, leading to the formation of cumulonimbusclouds and heavy rainfall. During much of the year, frontal precipitation, driven by cyclonic movements and interactions betweenwarm and cold fronts, is more common. The MPE algorithm often fails to capture these frontal precipitation events.A more advanced physics-based heuristic is the precipitation properties (PP) algorithm, which integrates NWP model data, cloudphysical properties, and satellite measurements. This algorithm uses radar observations to calibrate its parameters. However, becauseit relies on satellite observations at visible wavelengths to determine cloud properties, it can only retrieve precipitation data duringdaylight hours.Machine learning techniques, including decision trees, neural networks, and SVMs, have been evaluated for precipitation detection.However, these studies often used pixel-wise data splits for training and testing, which may lead to overfitting due to neglectingthe spatial and temporal smoothness of atmospheric phenomena. While these studies examined day, twilight, and night conditionsseparately, with the best results during the day, a more sophisticated method using a fully-connected stacked denoising autoencoderhas also been applied to precipitation detection. Although the autoencoder’s unsupervised training helps mitigate overfitting, there isno comparison with other architectures.From a machine learning perspective, precipitation detection is similar to semantic segmentation, where a multichannel image isinput, and each pixel is assigned an output label. Convolutional neural networks have become the standard for semantic segmentationin recent years, making them a natural choice for precipitation detection as well.Convolutional neural networks have been effectively used in various satellite image processing tasks, such as road and buildingdetection. Despite numerous public challenges that have advanced the field, the range of architectures used for aerial imageprocessing remains narrower compared to those used for semantic segmentation datasets like Microsoft COCO or Cityscapes. Acommon issue in these datasets is the presence of objects of the same class at different scales, which has led to the developmentof multiscale approaches. However, these approaches are less applicable to precipitation detection and other satellite imagerytasks, as the distance between the sensor and the Earth’s surface is usually known. Consequently, simpler models like UNet andfully-convolutional ResNet remain relevant.2.2 NowcastingPrecipitation nowcasting is typically accomplished in two stages by extrapolating radar observations. Initially, wind patterns areestimated by comparing multiple precipitation fields captured by radar. The techniques used for this in meteorology are similar tooptical flow estimation algorithms in computer vision. Subsequently, the precipitation field is moved according to the estimatedwind directions.A novel approach to nowcasting using a convolutional recurrent neural network (Conv-LSTM) was introduced and later refined.While this neural network adds complexity, it can theoretically improve rainfall prediction accuracy by accounting for radar artifactsand the appearance or disappearance of precipitation areas. However, the most significant of these processes, the vanishing ofprecipitation, can also be managed by adding basic filtering to the optical flow method.3 MethodologyThis section details the methodology used for precipitation detection and nowcasting, focusing on data preprocessing, model training,and evaluation metrics. 23.1 Data SourcesPrecipitation nowcasting imposes distinct data requirements compared to Numerical Weather Prediction (NWP), including highspatial and temporal resolution, direct rainfall measurement, and global coverage. Since no single source can fulfill all theserequirements, it is necessary to combine multiple data sources.Weather stations provide direct precipitation observations, typically measuring accumulated precipitation every 12 hours accordingto the SYNOP protocol. Although many stations report more frequently, usually every 3 hours, this frequency is insufficient fornowcasting due to the lack of detailed spatial and temporal data needed to generate high-resolution precipitation fields.Radar observations are the primary source of high-resolution precipitation data. The Russian network of DMRL-C radars, operatedby Roshydromet, uses C-band Doppler technology to measure raindrop reflectivity and radial velocity. Each radar covers a circulararea with a radius of up to 250 km and 10 km above the ground, with accuracy diminishing with distance. The radar echo can beconverted to surface precipitation using the Marshall-Palmer relation. The resulting precipitation field has a resolution of 2 x 2 km,with scans repeated every ten minutes. However, radar coverage is limited, especially outside densely populated areas of Europe andNorth America, with most Russian radars located in the western part of the country.Low Earth orbit satellites equipped with radars and sensors provide another source of precipitation measurements. These satellitesscan a narrow band beneath their orbital path, offering global coverage in the sense that every location within a certain latitude rangeis eventually scanned. However, the time between consecutive passes of a single satellite can be quite long. The Global PrecipitationMeasurements (GPM) mission, operated by NASA and JAXA, uses a constellation of about 10 operational satellites to provideglobal precipitation coverage from 65°S to 65°N with a 3-hour temporal resolution.Geostationary satellites are widely used for weather observation. Positioned 35,786 km above the equator, these satellites match theEarth’s rotation, allowing continuous monitoring of a large area. However, at such altitudes, the only feasible instrument for cloudand precipitation detection is a high-resolution imager that captures visible and infrared spectrum snapshots. Accurately detectingprecipitation from these images is challenging. Previous studies on this topic have not achieved the accuracy needed for user-facingproducts that aim to alert users about precipitation within 10 minutes.This study uses data from the Meteosat-8 satellite, operated by EUMETSAT, positioned over the Indian Ocean at 41.5° longitude,covering the western part of Russia and Europe. The SEVIRI instrument on Meteosat-8 scans the Earth’s surface in 12 channels,with a spatial resolution of 3 km per pixel and a full scan time of 15 minutes.This paper describes a precipitation nowcasting system that integrates radar, satellite, and NWP model data. A new approach toprecipitation detection is introduced and its accuracy is demonstrated.3.2 Precipitation DetectionThe approach to precipitation detection is summarized in Table 1. The key components of the pipeline are described in detail in thefollowing subsections. Table 1: Summary of our precipitation detection approach.Input features Satellite imagery, GFS fields, solar altitude, topographyGround truth Binarized radar measurementsModel UNetLoss function Binary crossentropy + Dice lossEvaluation measure F1 score3.2.1 PreprocessingThe data preparation process involves several steps aimed at minimizing the discrepancies between different data domains.Radar data preprocessing begins by discarding radar observations taken beyond 200 km from the radar, as these are deemedunreliable. Subsequently, observations from various radars are consolidated onto a single map, resolving any conflicts betweenradars with overlapping coverage areas. Due to frequent false negatives in radar observations, the maximum value between two datapoints is used for aggregation. Finally, radar observations are binarized using three thresholds: 0.08 mm/h for light rain, 0.5 mm/hfor moderate rain, and 2.5 mm/h for heavy rain.Satellite images and radar observations are remapped onto a uniform grid using an equirectangular projection. Given the obliqueobservation angles and the fact that precipitation can occur up to 2 km above the ground, there can be a parallax shift of up to 3pixels between radar and satellite data. However, in practice, accurately estimating precipitation height is complex, and accountingfor parallax did not improve the alignment.Satellite and radar data have different observation frequencies: satellite images are available every 15 minutes, while radar imagesare available every 10 minutes. To align these data sources temporally, a frame rate conversion is implemented using opticalflow interpolation. The goal is to match the radar data’s temporal resolution, so satellite data is converted to a 10-minute time3step. However, optical flow cannot be directly computed from satellite imagery due to the presence of both transient atmosphericphenomena and the permanent underlying relief. This issue is circumvented by performing precipitation detection before the opticalflow step, allowing the optical flow to be computed directly from the precipitation detection results, which do not include the relief.I t tTo generate the missing image between two adjacent anchor images taken at times and , the following equation is used:t 0 1I (r) = aI (r + bu ) + bI (r + au )t t 01 t 100 1t −t t−ta = b = u uwhere and are coefficients dependent on the time of the generated image, and and are the forward and1 0 01 10t −t t −t1 0 1 0backward optical flows, computed using the TV-L1 optical flow algorithm implemented in OpenCV.Roshydromet radars record the timestamp at the end of a scan, whereas EUMETSAT marks the start. Since the Earth is scanned in aseries of lateral sweeps starting from the south, the actual observation time varies with latitude, with northern latitudes observedlast. The combined discrepancy between timestamps can reach 20 minutes. Experimental validation has confirmed that this valuecorresponds to the minimum discrepancy between radar data and precipitation field reconstruction.Additional features are incorporated into the satellite imagery to enhance the signal. The Global Forecast System (GFS) modelis used to provide a comprehensive description of atmospheric conditions, including physical properties not easily inferred fromsatellite imagery. The GFS model produces forecasts four times a day with a spatial resolution of 0.25° x 0.25° and temporalintervals of 3 hours. Key fields from GFS include convective precipitation rate, cloud work function, cloud water, precipitable water,and convective potential energy at different levels. Additionally, a topography map and solar altitude data are included as features.3.2.2 TrainingA modified UNet architecture is employed as the primary model for precipitation detection. Through testing, it was determinedthat using 5 upsample/downsample blocks, compared to the original 4, yields the best results on the validation dataset. The modelutilizes standard 3x3 convolutions, 2x2 pooling, and batch normalization layers. The number of channels begins at 16 in the firstblock and doubles with each downsampling step. This reduced number of channels helps mitigate overfitting and accelerates trainingand evaluation. −410The network is trained for 250,000 iterations using the Adam algorithm, with an initial learning rate of , which is reduced by afactor of 10 after 200,000 iterations. The addition of the Dice loss to the standard binary cross-entropy improves the F1 scores forthe converged model. Training is performed using the Keras framework with a TensorFlow backend and Horovod for multi-GPUlearning.The model is trained to detect three levels of precipitation (light, medium, and heavy) simultaneously, producing three output mapswith binary classification loss applied to each map independently.Typically, precipitation estimation algorithms are developed separately for day, twilight, and night conditions. However, thisseparation is challenging for machine learning in high-latitude zones due to the underrepresentation of night during summer and dayduring winter, making it difficult to compile a balanced dataset. Therefore, a single model is trained, with solar altitude provided asan additional input feature.Overfitting is a significant concern due to the limited geographical area of the dataset. The network can easily memorize the relief,which is visible in some wavelengths even if not explicitly provided as a feature, and use it to overfit on ground truth labels withinthe radar coverage areas. Moreover, memorizing the correspondence between geographical location and output labels may cause themodel to ignore areas outside radar coverage, leading to constant output in these regions. This contradicts the goal of extendingnowcasting beyond radar coverage. To address this, the model is trained on relatively small data crops (96x96 pixels).Due to the large number of channels in the input data, which is atypical for computer vision problems, data loading can be slow. Tomanage this, a small batch of 5 multi-channel images (including all additional features) is loaded, and each image is then cropped 10times at random locations.3.2.3 MetricsThis section presents the evaluation metrics for the precipitation detection algorithm. Due to class imbalance, standard classificationaccuracy is not informative. Therefore, the primary metric used is the F1 score, averaged across temporal and spatial dimensions.Several approaches are compared:- **UNet with GFS**: The UNet architecture with a complete set of features, trained as described earlier. - **UNet w/o GFS**: Thesame UNet approach without GFS features. - **Pointwise**: A neural network with two convolutional layers using 1x1 convolutions,equivalent to a pointwise perceptron model. GFS features are not used in this model. - **PP and MPE**: Physics-based algorithms(Precipitation Properties and Multi-sensor Precipitation Estimate).Given that PP and MPE algorithms are designed for daylight conditions, the metrics are also averaged separately for day, night, andtwilight periods. The neural network approaches consistently outperform the physics-based methods across all time periods andmetrics. The generally poor performance of PP and MPE in these experiments may be due to their tuning for predicting convectiverainfall aggregated over extended periods, which does not align with the requirements of this service.4The pointwise model’s performance falls between that of UNet and the physics-based approaches. Since it is trained on radar data, itdetects similar types of precipitation and performs well during testing.The UNet architecture’s superiority over the pointwise model likely stems from its ability to gather information from a largereceptive field. While precipitation reconstruction does not require the same extent of multiscale data processing as many semanticsegmentation tasks, the interconnectedness of adjacent atmospheric locations makes a large receptive field beneficial for precipitationdetection.Finally, the addition of GFS features further enhances the F1 score of the UNet model, as demonstrated in the results.4 Experiments4.1 NowcastingUpon completing the reconstruction of the precipitation field in the area of interest, a separate algorithm is employed to forecast futureprecipitation fields based on several consecutive reconstructed fields. Two options are considered for this algorithm: extrapolationwith optical flow, as used for frame rate conversion, and a convolutional neural network previously developed for radar dataprediction. The network consists of a sequence of blocks, each modeling the extrapolation process with optical flow via a spatialtransformer layer. Although the neural network’s prediction mechanism is intentionally similar, end-to-end learning on real datatheoretically allows it to surpass the performance of simpler algorithms. While the neural network approach was found to be superiorin the single radar setting, preliminary experiments did not show the same success with composited radar images and satellite data.Despite the optical flow approach being simpler and not requiring retraining with the introduction of new data sources, it is believedthat neural nowcasting remains promising and could outperform simpler techniques with proper tuning of the network architectureand training regimen.5 Results5.1 Post-Launch PerformanceAlthough the satellite-based rain detection model was trained to match radar fields, its reception by users was uncertain. A/B testingalone was insufficient to evaluate the product’s performance, as it was essentially a new feature for several regions of Russia andcould be well-received initially even if the map quality was low. Therefore, the performance of the new precipitation map wasassessed using ground station data. While the optimal metrics for a user-facing precipitation prediction algorithm are still debated,there was evidence of the nowcasting product’s popularity, and the aim was to replicate the properties of the radar-based precipitationmap using satellite data. Specifically, the radar data differs from longer-term forecasts based on proprietary Meteum technology inhaving higher accuracy and lower systematic error rates (precipitation imbalance) at the cost of a lower F1 score when compared toground station weather observations. The same comparison strategy was used to evaluate the performance of the new satellite-basedrain detection algorithm over the federal districts of Russia. Results showed that while the accuracy of the satellite-based product islower than that of radar, it is still better than traditional forecasts, with precipitation imbalance and F1 scores similar to those forradar. It is important to note that the radar located in Siberia was used only for verification at this stage; its data was not included inthe training dataset. This comparison allows for evaluating precipitation detection quality in regions without radar observation.This result confirmed the success of the new rain map. Additionally, A/B testing on users showed a statistically significant increasein daily active users (DAU) in areas where the rain map was previously unavailable (Siberia and Ural regions), justifying its rolloutin late September.Table 2: Comparison of precipitation detection methods with various metrics averaged over time.Method Accuracy F1 Score Precision RecallMPE 0.92 0.21 0.28 0.17PP 0.86 0.30 0.24 0.40Pointwise 0.91 0.48 0.40 0.61U-Net w/o GFS 0.94 0.56 0.64 0.50U-Net with GFS 0.94 0.60 0.62 0.596 ConclusionA precipitation nowcasting system has been developed, implemented, and launched, utilizing both ground-based radar observationsand geostationary satellite imagery. The system employs advanced machine learning algorithms and incorporates the physicalproperties of the atmosphere and ground surface based on NWP models. The inclusion of satellite data enables nowcasting for areasnot covered by ground-based radars, achieving quality comparable to traditional radar-based nowcasts.5Table 3: Comparison of F1 scores of precipitation detection methods during different time periods.Method Day Twilight Night AllMPE 0.19 0.22 0.21 0.21PP 0.32 0.31 0.27 0.30Pointwise 0.54 0.48 0.41 0.48U-Net w/o GFS 0.65 0.55 0.49 0.56U-Net with GFS 0.67 0.60 0.54 0.60Currently, the system is limited to the region centered on European Russia within the Meteosat-8 field of view. Compared to previoussolutions, the potential audience has been expanded from approximately 70 million to 300 million people, based on coverage areaand population density. The approach can be extended to the rest of the Meteosat-8 coverage area. Scaling the technology to othergeostationary satellites with similar measurement systems, such as Himawari and GOES, offers the possibility of providing globalprecipitation nowcasting and alerting services worldwide. However, differences in weather patterns across geographical regions willlikely necessitate retraining the detection model and adjusting the set of input features.One encountered problem is the sharp edge between radar and satellite data. This stationary edge on the weather map can confuseusers, indicating the need for more sophisticated data fusion techniques. Experiments with image blending to erase conflictingobservations along the border and inpainting the missing parts have been conducted.7 AcknowledgmentsThe success of this work and the product is attributed to the support, assistance, and hard work of a large team. Although not all teammembers could be included as co-authors, their contributions are gratefully acknowledged. Key contributions include data delivery,processing, and merging of satellite and radar images; preliminary assessment of satellite algorithms; backend tile generation forprecipitation maps; API support; and development of radar-based nowcasting algorithms used as a baseline. Special thanks areextended to the ML, backend, frontend, testing, design, and mobile application teams, and all supporters of the project.6"
P123,"Acquiring Cross-Domain Representations forContextual Detection Using Extensive Emoji DataAbstractThis research delves into the application of a vast collection of emoji occurrencesto acquire versatile representations applicable to diverse domains for the purposeof identifying sentiment, emotion, and sarcasm. Natural Language Processing(NLP) tasks frequently encounter limitations due to the deficiency of manuallylabeled data. In the realm of social media sentiment analysis and associated tasks,researchers have thus employed binarized emoticons and specific hashtags as ameans of distant supervision. Our study demonstrates that by broadening distantsupervision to include a more varied array of noisy labels, models can achievericher representations. Through emoji prediction on a dataset encompassing 1,246million tweets, each including one of 64 prevalent emojis, we achieve state-of-the-art results on eight benchmark datasets focusing on sentiment, emotion, andsarcasm detection, all with the aid of a singular pre-trained model. Our findingsaffirm that the diversity inherent in our emotional labels leads to an enhancementin performance compared to previous distant supervision methods.1 IntroductionThis paper addresses the challenge that numerous Natural Language Processing (NLP) tasks face dueto the lack of sufficient manually annotated data. Consequently, emotional expressions that co-occurwith text have been utilized for distant supervision in sentiment analysis and related tasks withinsocial media. This allows models to acquire valuable text representations before directly modelingthese specific tasks. For example, state-of-the-art methods for sentiment analysis in social mediafrequently use positive and negative emoticons to train their models. Similarly, in prior research,hashtags like #anger, #joy, #happytweet, #ugh, #yuck, and #fml have been categorized into emotionallabels for use in emotion analysis.The practice of using distant supervision on noisy labels often leads to enhanced performance inthe target task. In this paper, we present evidence that expanding distant supervision to a morevaried selection of noisy labels enables models to develop more detailed representations of emotionalcontent in text. This, in turn, improves performance on benchmark datasets designed for the detectionof sentiment, emotions, and sarcasm. We further demonstrate that the representations learned by asingle pre-trained model can be successfully generalized across five different domains.Table 1 showcases example sentences which were scored by our model. For every sentence, the fivemost probable emojis are displayed, alongside the model’s estimated probabilities.Emojis do not always function as straightforward labels of emotional content. For instance, apositive emoji might clarify an ambiguous sentence or supplement text that might otherwise beseen as somewhat negative. While this is true, our results demonstrate that emojis can still beused to accurately categorize the emotional content of texts in numerous scenarios. Our DeepMojimodel, for instance, is able to capture various interpretations of the word ’love’ and slang termslike ’this is the shit’ as having positive connotations (as illustrated in Table 1). To enable others toexplore the prediction capabilities of our model, we have made an online demonstration available atdeepmoji.mit.edu.Our work makes the following contributions: We demonstrate that a vast number of readily accessibleemoji occurrences on Twitter can be used to pre-train models for richer emotional representation thanis typically achieved through distant supervision. We then transfer this learned knowledge to targettasks using a novel layer-wise fine-tuning approach. This technique yields significant improvementsover state-of-the-art methods in areas such as emotion, sarcasm, and sentiment detection. Throughextensive analyses on the influence of pre-training, our results highlight that the variety present in ouremoji set plays a crucial role in the transfer learning capabilities of our model. We have made ourpre-trained DeepMoji model publicly available to aid in a range of NLP tasks.2 Related workThe use of emotional expressions as noisy labels in text to address the scarcity of labels is not a newconcept. Initially, binarized emoticons served as noisy labels, but subsequent research has utilizedhashtags and emojis. Previous studies have always manually determined which emotional categoryeach emotional expression should belong to. Prior efforts have made use of emotion theories, such asEkman’s six basic emotions and Plutchik’s eight basic emotions.Such manual categorization necessitates an understanding of the emotional content inherent to eachexpression, which can be challenging and time-consuming for complex emotional combinations.Furthermore, any manual selection and categorization carries the potential for misinterpretationsand might overlook essential details concerning usage. In contrast, our methodology requires noprior knowledge of the corpus and can capture the diverse usage of 64 emoji types (Table 1 presentsexamples, and Figure 3 shows how the model implicitly organizes emojis).An alternative approach to automatically interpreting the emotional content of an emoji involveslearning emoji embeddings from the words defining emoji-semantics, as found in official emoji tables.In our study, this approach has two significant limitations: (a) It requires emojis to be present duringtesting, whereas several domains have limited or no emoji usage. (b) The tables fail to capture thedynamic nature of emoji use, such as shifts in an emoji’s intended meaning over time.Knowledge from the emoji dataset can be transferred to target tasks in several ways. Multi-tasklearning, which involves training on multiple datasets at once, has been shown to have promisingresults. However, multi-task learning requires access to the emoji dataset whenever the classifierneeds to be adjusted for a new target task. Requiring access to the dataset can be problematic whenconsidering data access regulations. Data storage issues also arise, as the dataset used in this studycomprises hundreds of millions of tweets (see Table 2). Instead, we use transfer learning which doesnot require access to the original dataset.3 Method3.1 PretrainingIn many instances, emojis function as a stand-in for the emotional content of text. Therefore, pre-training a model to predict which emojis were initially part of a text can improve performance in thetarget task. Social media contains many short texts that use emojis which can be used as noisy labelsfor pretraining. We used data from Twitter spanning from January 1, 2013, to June 1, 2017, but anydata set containing emoji occurrences could be used.The pretraining data set uses only English tweets that do not contain URLs. We think the contentobtained from the URL is important for understanding the emotional content of the text in the tweet.Because of this we expect emojis associated with tweets containing URLs to be noisier labels thanthose in tweets without URLs, therefore the tweets with URLs have been removed.Proper tokenization is crucial for generalization. All tweets are tokenized word-by-word. Wordscontaining two or more repeated characters are shortened to the same token (for example, ‘loool’ and‘looooool’ are tokenized as the same). We also use a special token for all URLs (which is relevantonly for the benchmark datasets), user mentions (for example, ‘@acl2017’ and ‘@emnlp2017’ aretreated the same), and numbers. To be included in the training set, a tweet must have at least onetoken that is not a punctuation mark, emoji, or special token.2Many tweets repeat the same emoji or contain multiple distinct emojis. To address this in our trainingdata, for each unique emoji type, we save a separate tweet for pretraining, using that emoji type asthe label. Regardless of the number of emojis associated with the tweet, we save only a single tweetfor the pretraining for each unique emoji type. This pre-processing of data enables the pretraining tocapture that multiple kinds of emotional content can be associated with the tweet. It also makes ourpretraining task a single-label classification instead of a more complex multi-label classification.To ensure that the pretraining encourages the models to learn a thorough understanding of theemotional content of text instead of just the emotional content associated with frequently used emojis,we create a balanced pretraining dataset. The pretraining data is split into training, validation, and testsets. The validation and test sets are randomly sampled such that each emoji is represented equally.The remaining data is upsampled to generate a balanced training dataset.3.2 ModelWith the availability of millions of emoji occurrences, we are able to train expressive classifierswith a limited risk of overfitting. We utilize a variant of the Long Short-Term Memory (LSTM)model, which has been successful in numerous NLP tasks. Our DeepMoji model uses an embeddinglayer with 256 dimensions to project each word into a vector space. A hyperbolic tangent activationfunction is used to ensure each embedding dimension remains within the range [-1, 1]. To understandeach word in the context of the text, we use two bidirectional LSTM layers with 1024 hidden unitseach (512 in each direction). Lastly, we employ an attention layer that accepts all these layers asinput through skip connections. (Figure 1 presents an illustration).The attention mechanism enables the model to determine the importance of each word for theprediction task by weighting the words as it creates the text representation. A word like ""amazing"" ishighly informative of the emotional meaning of a text and so should be treated accordingly. We use abasic method, taking inspiration from prior work, with a single parameter for each input channel:exp(e ) (cid:88)ie = h w a = v = a h (1)(cid:80)i i a i i iexp(e )jj=1h wHere, stands for the representation of the word at time step t, and is the weight matrix fort a athe attention layer. The attention importance scores for each time step, , are determined bytmultiplying the representations by the weight matrix, and then normalizing them to establish avprobability distribution across the words. Finally, the text’s representation vector, , is found using aweighted summation over all time steps, with the attention importance scores used as weights. Therepresentation vector that comes from the attention layer is a high-level encoding of the whole text.This is used as input into the final Softmax layer for classification. We have found that the addition ofthe attention mechanism and skip connections enhances the model’s capabilities for transfer learning.The only form of regularization used for the pretraining is L2 regularization with a coefficient of−610 on the embedding weights. For fine-tuning, further regularization is applied. We implementedour model using Theano and have made an easy-to-use version available that utilizes Keras.3.3 Transfer learningOur pre-trained model can be fine-tuned for a target task in several ways. Some methods involve‘freezing’ layers by disabling parameter updates to prevent overfitting. One popular approach isto utilize the network as a feature extractor, where all model layers except the final one are frozenduring fine-tuning (we will call this the ""last"" approach). An alternative method is to use the pre-trained model for initialization, where the full model is unfrozen (which we will refer to as the ‘full’approach).We put forward a new, simple transfer learning approach we are calling ""chain-thaw."" This approachsequentially unfreezes and fine-tunes one layer at a time. It increases accuracy on the target task, butrequires more computational power for the fine-tuning process. By separately training each layer,the model can adjust individual patterns across the network while reducing the risk of overfitting. Itappears that this sequential fine-tuning has a regularizing effect, similar to the layer-wise trainingexplored for unsupervised learning. 3More specifically, the chain-thaw approach starts by fine-tuning any new layers (often only a Softmaxlayer) to the target task until the validation set converges. Then, the approach individually fine-tuneseach layer, starting with the first layer in the network. Lastly, the entire model is trained with alllayers. Each time the model converges (as measured on the validation set), the weights are restored totheir optimal setting, preventing overfitting in a similar manner to early stopping. Figure 2 illustratesthis process. If only step a) in the figure is performed, this is the same as the ‘last’ approach, wherethe existing network is used as a feature extractor. Likewise, only performing step d) is the same asthe ‘full’ approach, where the pre-trained weights are used as the initialization for a fully trainablenetwork. While the chain-thaw procedure may seem extensive, it can be implemented with just a fewlines of code. Also, the added time spent on fine-tuning is not large, when considering the use ofGPUs on small datasets of manually annotated data which is often the case.The chain-thaw approach has the benefit of expanding the vocabulary to new domains with a lowrisk of overfitting. For a given dataset, up to 10,000 new words from the training set are added to thevocabulary.Table 2 shows the number of tweets in the pretraining dataset associated with each emoji in millions.4 Experiments4.1 Emoji predictionWe use a raw dataset of 56.6 billion tweets, which is filtered down to 1.2 billion relevant tweets. Inthe pretraining dataset, a single copy of a tweet is stored for every unique emoji, resulting in a datasetwith 1.6 billion tweets. Table 2 shows the distribution of tweets across different emoji types. We useda validation set and a test set, both containing 640K tweets (10K of each emoji type), to evaluateperformance on the pretraining task. The remaining tweets were used for the training set, which wasbalanced using upsampling.The performance of the DeepMoji model on the pretraining task was evaluated, with the results shownin Table 3. We use both top 1 and top 5 accuracy for the evaluation as the emoji labels are noisyand multiple emojis can potentially be appropriate for a given sentence. For comparison purposes,we also train a version of our DeepMoji model with smaller LSTM layers and a bag-of-wordsclassifier, fastText, which has recently shown competitive results. We use a 256 dimension vectorfor the fastText classifier, making it almost identical to only using the embedding layer from theDeepMoji model. The difference in top 5 accuracy between the fastText classifier (36.2%) and thelargest DeepMoji model (43.8%) highlights the difficulty of the emoji prediction task. Since the twoclassifiers only differ in that the DeepMoji model has LSTM layers and an attention layer betweenthe embedding and the Softmax layer, this difference in accuracy demonstrates the importance ofcapturing each word’s context.Table 3 displays the accuracy of classifiers on the emoji prediction task. The value d refers to thedimensionality of each LSTM layer and the parameters are given in millions.Model Params Top 1 Top 5Random - 1.6% 7.8%fasttext 12.8 12.8% 36.2%DeepMoji (d=512) 15.5 16.7% 43.3%DeepMoji (d=1024) 22.4 17.0% 43.8%Table 1: Accuracy of classifiers on the emoji prediction task. d refers to the dimensionality of eachLSTM layer. Parameters are in millions.4.2 BenchmarkingWe evaluate our method on 3 distinct NLP tasks using 8 datasets across 5 domains. For faircomparison, DeepMoji is compared to other methods that utilize external data sources in additionto the benchmark dataset. We used an averaged F1 measure across classes for evaluating emotionanalysis and sarcasm detection, as these consist of unbalanced datasets. Sentiment datasets areevaluated using accuracy. 4Many benchmark datasets have an issue with data scarcity, especially in emotion analysis. Manystudies that introduce new methods for emotion analysis often evaluate their performance on a singlebenchmark dataset, SemEval 2007 Task 14, which contains only 1250 data points. There has beencriticism regarding the use of correlation with continuous ratings as a measure, making only thesomewhat limited binary evaluation possible. We only evaluate the emotions Fear, Joy, Sadnessbecause the remaining emotions are found in less than 5To fully assess our method on emotion analysis, we make use of two other datasets. First, a datasetof emotions in tweets about the Olympic Games, created by Sintsova et al. which we convert toa single-label classification task. Second, a dataset of self-reported emotional experiences from alarge group of psychologists. Because these two datasets have not been evaluated in prior work,we compare against a state-of-the-art approach based on a valence-arousal-dominance framework.The scores extracted using this framework are mapped to the classes in the datasets using logisticregression with cross-validation parameter optimization. We have made our preprocessing codeavailable so that these two datasets may be used for future benchmarking in emotion analysis.We assessed the performance of sentiment analysis using three benchmark datasets. These smalldatasets were chosen to highlight the significance of the transfer learning capabilities of the evaluatedmodels. Two datasets, SS-Twitter and SS-Youtube, are from SentiStrength and follow the relabelingas described by prior work to create binary labels. The third dataset is from SemEval 2016 Task4A.Because tweets are often deleted from Twitter, the SemEval dataset has experienced data decay. Thismakes comparisons across papers difficult. Approximately 15The current state of the art in sentiment analysis on social media (and winner of SemEval 2016 Task4A) uses an ensemble of convolutional neural networks that are pre-trained on a private dataset oftweets with emoticons. This makes it difficult to replicate. As a substitute, we pre-train a model thatuses the hyperparameters of the largest model in their ensemble on the positive/negative emoticondataset. Using this pretraining as an initialization, we fine-tune the model on the target tasks, utilizingearly stopping based on a validation set. We implemented Sentiment-Specific Word Embeddings(SSWE), using embeddings available on the authors’ website, but found that it performed worse thanthe pretrained convolutional neural network, and these results have been excluded.Table 4 presents a description of the benchmark datasets. Datasets that did not have pre-existingtraining/test splits were split by us, and these splits are publicly available. Data from the training setwas used for hyperparameter tuning.Identifier Study Task Domain Classes Ntrain NtestSE0714 (Strapparava and Mihalcea, 2007) Emotion Headlines 3 250 1000Olympic (Sintsova et al., 2013) Emotion Tweets 4 250 709PsychExp (Wallbott and Scherer, 1986) Emotion Experiences 7 1000 6480SS-Twitter (Thelwall et al., 2012) Sentiment Tweets 2 1000 1113SS-Youtube (Thelwall et al., 2012) Sentiment Video Comments 2 1000 1142SE1604 (Nakov et al., 2016) Sentiment Tweets 3 7155 31986SCv1 (Walker et al., 2012) Sarcasm Debate Forums 2 1000 995SCv2-GEN (Oraby et al., 2016) Sarcasm Debate Forums 2 1000 2260Table 2: Description of benchmark datasets. Datasets without pre-existing training/test splits are splitby us (with splits publicly available). Data used for hyperparameter tuning is taken from the trainingset.For sarcasm detection, we used versions 1 and 2 of the sarcasm dataset from the Internet ArgumentCorpus. It should be noted that the results from these benchmarks that are shown elsewhere are notdirectly comparable, as only a subset of the data is available online. We establish a state-of-the-artbaseline by modeling embedding-based features alongside unigrams, bigrams, and trigrams withan SVM. GoogleNews word2vec embeddings are used to compute the embedding-based features.Cross-validation was used to perform a hyperparameter search for regularization parameters. Thesarcasm dataset version 2 includes both a quoted text and a sarcastic response, but only the responsewas used to keep models consistent across the datasets.5Table 5 displays a comparison across benchmark datasets. The reported values are averages across5 runs. Variations refer to the transfer learning approaches that we discussed, and ’new’ refers to amodel trained without pretraining.Dataset Measure State of the art DeepMoji (new) DeepMoji (full) DeepMoji (last)DeepMoji (chain-thaw)SE0714 F1 .34 .21 .31 .36.37Olympic F1 .50 .43 .50 .61.61PsychExp F1 .45 .32 .42 .56.57SS-Twitter Acc .82 .62 .85 .87.88SS-Youtube Acc .86 .75 .88 .92.93SE1604 Acc .51 .51 .54 .58.58SCv1 F1 .63 .67 .65 .68.69SCv2-GEN F1 .72 .71 .71 .74.75Table 3: Comparison across benchmark datasets. Reported values are averages across five runs.Variations refer to transfer learning approaches with ‘new’ being a model trained without pretraining.We used the Adam optimizer for training, with the gradient norm clipped to 1. For training all new−3 −410 10layers, we set the learning rate to and to when fine-tuning any pre-trained layers. Toprevent overfitting on the small datasets, 10Table 5 demonstrates that the DeepMoji model outperforms the state of the art across all the benchmarkdatasets and that our new ‘chain-thaw’ method yields the highest transfer learning performance. Theresults are averaged across 5 runs to reduce the variance. We confirm statistical significance usingbootstrap testing with 10,000 samples, our model performance was statistically better than thep < 0.001state-of-the-art across all benchmark datasets ( ).Our model exceeds the performance of the state of the art even on datasets that come from differentdomains than the tweets that the model was pre-trained on. A crucial difference between thepretraining dataset and the benchmark datasets is the length of the observations. The average numberof tokens per tweet in the pretraining dataset is 11. Meanwhile, board posts from the InternetArgument Corpus version 1 (for example), have an average of 66 tokens, with some posts being muchlonger.5 Model Analysis5.1 Importance of emoji diversityA key difference between this work and prior research that used distant supervision is the variety innoisy labels. For example, other studies only used positive and negative emoticons as noisy labels.Other studies used more nuanced sets of noisy labels, but our set is the most varied known to us. Toinvestigate the effect of using a diverse set of emojis, we created a subset of our pretraining data thatincluded tweets with one of 8 emojis, which are similar to the positive/negative emoticons used inother work. Because the dataset based on this reduced set of emojis contains 433 million tweets, anyperformance differences on benchmark datasets are more likely linked to the diversity of the labelsthan to differences in dataset sizes.We trained our DeepMoji model to predict whether tweets contained positive or negative emojis,and we evaluated this pre-trained model on benchmark datasets. We call this the DeepMoji-PosNegmodel. To assess the emotional representations learned by the two pre-trained models, we used the‘last’ transfer learning approach to allow the models to map already learned features to classes in the6target datasets. Table 6 shows that DeepMoji-PosNeg performs worse than DeepMoji across all 8benchmarks. This demonstrates that the diversity of our emoji types enables the model to acquirericher representations of emotional content in text, which in turn is more useful for transfer learning.Table 6 compares benchmarks using a smaller emoji set (Pos/Neg emojis) or a standard architecture(standard LSTM). Results for DeepMoji from Table 5 have been added for comparison. The evaluationmetrics are the same as in Table 5. Reported values are averages across 5 runs.Dataset Pos/Neg emojis Standard LSTM DeepMojiSE0714 .32 .35 .36Olympic .55 .57 .61PsychExp .40 .49 .56SS-Twitter .86 .86 .87SS-Youtube .90 .91 .92SE1604 .56 .57 .58SCv1 .66 .66 .68SCv2-GEN .72 .73 .74Table 4: Benchmarks using a smaller emoji set (Pos/Neg emojis) or a classic architecture (standardLSTM). Results for DeepMoji from Table 5 are added for convenience. Evaluation metrics are as inTable 5. Reported values are the averages across five runs.Many emojis express similar emotional content, but have subtle variations in usage that our modelcan capture. By using hierarchical clustering on the correlation matrix of the DeepMoji model’spredictions on the test set, we can see that the model captures many expected similarities (Figure 3).For example, the model groups emojis into broad categories related to negativity, positivity, or love.It also differentiates within these categories. For example, mapping sad emojis to one subcategory ofnegativity, annoyed emojis to another subcategory, and angry emojis to a third.5.2 Model architectureOur DeepMoji model architecture employs an attention mechanism and skip connections, which assistin transferring learned representations to new domains and tasks. Here, we compare the DeepMojimodel architecture to a standard 2-layer LSTM. Both were compared using the ‘last’ transfer learningapproach, and all regularization and training parameters were consistent.Table 6 shows that the DeepMoji model performs better than a standard 2-layer LSTM across all thebenchmark datasets. These two architectures performed equally on the pretraining task. This indicatesthat the DeepMoji model architecture is better for transfer learning, even if it is not necessarily betterfor a single supervised classification task with an abundance of available data.We believe that the improvements in transfer learning can be attributed to two factors: (a) Theattention mechanism with skip connections provides straightforward access to learned low-levelfeatures for any time step, making it easy to use this information if needed for a new task. (b) The skipconnections improve the gradient flow from the output layer to the early layers in the network. Thisis useful when parameters in early layers are adjusted as a part of transfer learning to small datasets.Further analysis of these factors in future work would allow us to confirm why our architectureoutperforms a standard 2-layer LSTM.5.3 Analyzing the effect of pretrainingThe target task’s performance benefits significantly from pretraining, as shown in Table 5. Here,we separate the effects of pretraining into two factors: word coverage and phrase coverage. Thesetwo effects provide regularization to the model, preventing overfitting (the supplementary materialincludes a visualization of this regularization).There are multiple ways of expressing sentiment, emotion, or sarcasm. Because of this, the test setmay contain language use not present in the training set. Pretraining helps the target task modelsfocus on low-support evidence by having already seen similar language in the pretraining dataset.To examine this effect, we measure the improvement in word coverage on the test set when using7pretraining. Word coverage is defined as the percentage of words in the test dataset that were alsoseen in the training/pretraining dataset (as shown in Table 7). One key reason that the ‘chain-thaw’approach outperforms other transfer learning approaches is its ability to tune the embedding layerwith a low risk of overfitting. Table 7 shows how adding new words to the vocabulary as part of thetuning process increased word coverage.It is important to note that word coverage can be misleading in this context. In many small datasets, aword may occur only once in the training set. In contrast, all the words in the pretraining vocabularyare present in thousands or even millions of observations, enabling the model to learn a goodrepresentation of the emotional and semantic meaning. Therefore, the benefits of pretraining for wordrepresentations likely extend beyond the differences seen in Table 7.Table 7 shows the word coverage on benchmark test sets. This compares the use of only the vocabularygenerated by finding words in the training data (‘own’), the pretraining vocabulary (‘last’), or acombination of both vocabularies (‘full / chain-thaw’).Dataset Own Last Full / Chain-thawSE0714 41.9% 93.6% 94.0%Olympic 73.9% 90.3% 96.0%PsychExp 85.4% 98.5% 98.8%SS-Twitter 80.1% 97.1% 97.2%SS-Youtube 79.6% 97.2% 97.3%SE1604 86.1% 96.6% 97.0%SCv1 88.7% 97.3% 98.0%SCv2-GEN 86.5% 97.2% 98.0%Table 5: Word coverage on benchmark test sets using only the vocabulary generated by finding wordsin the training data (‘own’), the pretraining vocabulary (‘last’) or a combination of both vocabularies(‘full / chain-thaw’).To analyze how important capturing phrases and the context of each word are, we evaluated theaccuracy on the SS-Youtube dataset using a fastText classifier that was pre-trained using the sameemoji dataset as our DeepMoji model. This fastText classifier is similar to only using the embeddinglayer from the DeepMoji model. We then evaluated the representations learned by fine-tuning themodels as feature extractors (using the ‘last’ transfer learning approach). The fastText model achievedan accuracy of 635.4 Comparing with human-level agreementTo see how well our DeepMoji classifier performs compared to humans, we created a dataset ofrandomly selected tweets that were annotated for sentiment. Each tweet was annotated by a minimumof 10 English-speaking Amazon Mechanical Turkers (MTurks) who lived in the USA. The tweetswere rated on a scale from 1 to 9, with a ‘Do not know’ option. Guidelines were provided to thehuman raters. The tweets were selected to contain only English text and no mentions or URLs, sothey could be rated without extra contextual information. Tweets where more than half the evaluatorschose ‘Do not know’ were removed (98 tweets).For every tweet, we randomly select a single MTurk rating as the ‘human evaluation.’ We average theremaining nine MTurk ratings to make the ground truth. The ‘sentiment label’ for a given tweet is thusdefined as the overall consensus among raters, excluding the randomly selected ‘human evaluation’rating. To ensure clear separation between the label categories, we removed neutral tweets that fellwithin the interval [4.5, 5.5] (roughly 29Table 8 shows that the agreement of the random MTurk rater is 76.1Table 8 compares the agreement between classifiers and the aggregate opinion of Amazon MechanicalTurkers on sentiment prediction of tweets. 8Model AgreementRandom 50.1%fastText 71.0%MTurk 76.1%DeepMoji 82.4%Table 6: Comparison of agreement between classifiers and the aggregate opinion of Amazon Mechan-ical Turkers on sentiment prediction of tweets.6 ConclusionWe have demonstrated how the abundance of text on social media containing emojis can be usedto pre-train models. This enables them to acquire representations of emotional content in text. Ourfindings demonstrate that the diversity of our emoji set is crucial to our method’s performance. Thiswas found by comparing the model performance against an identical model that was pre-trained on asubset of emojis. Our pre-trained DeepMoji model is available for other researchers to use for diverseemotion-related NLP tasks. 9"
P125,"DISCOSENSE: Commonsense Reasoning withDiscourse ConnectivesAbstractWe present DISCOSENSE, a benchmark for commonsense reasoning via un-derstanding a wide variety of discourse connectives. We generate compellingdistractors in DISCOSENSE using Conditional Adversarial Filtering, an extensionof Adversarial Filtering that employs conditional generation. We show that stateof-the-art pre-trained language models struggle to perform well on DISCOSENSE,which makes this dataset ideal for evaluating next generation commonsense rea-soning systems.1 IntroductionThis paper addresses the critical need for challenging benchmarks that can reliably target the limita-tions of current pre-trained language models (LMs) in commonsense reasoning. State-of-the-art LMshave achieved or even surpassed human performance on numerous commonsense downstream tasks.Nevertheless, these LMs are still very far from being able to perform commonsense reasoning as wellas humans. Hence, the fact that they have begun to ace existing benchmarks implies that time is ripeto design a new challenging benchmark that can reliably target their limitations.Motivated by this observation, we present DISCOSENSE, a benchmark for performing commonsensereasoning through understanding a wide variety of discourse connectives. Figure 1 shows an exampletaken from DISCOSENSE. As can be seen, an example is composed of a context (e.g., “Our waitresswas very nice, but she kept on forgetting my stuff.”) and a discourse connective (e.g., “For example”),and the goal is to choose the most plausible ending out of four options. If we ignore the discourseconnective, then all four options mayOur waitress was very nice, but she kept on forgetting my stuff. For examplea) When I ordered the garlic shrimp, she remembered to add my requested garlic butter.b) She took forever to bring me my beer and fries.c) When I told her I wanted to use the free breakfast that was available she was not pleased.d) For some customers, this is fine.Figure 1: Example on commonsense reasoning with discourse connectives. The correct (i.e., mostplausible) option is boldfaced.seem plausible because we do not know what the writer’s intent is. Once we consider both the contextand the discourse connective, then it is clear that only option b) is plausible. The reason is that “Forexample” signals an EXEMPLIFICATION relation between its arguments, and what follows thediscourse connective is expected to be an example of the waitress keeping on forgetting the writer’sstuff. Using commonsense knowledge, we know that (1) “my beer and fries” is an example of “mystuff”, and (2) her taking forever to bring the writer stuff implies she kept on forgetting his/her stuff.What if we replace “For example” with “However” in the example? Since “However” signals aCONTRAST relation, options a) and d) both seem viable. Specifically, option a) describes a situationin which she did not forget the writer’s stuff. While option d), unlike option a), does not describeany example that signals a contrast, one may infer a contrast between option d) and the context:being forgetful is fine for some customers. Nevertheless, option a) is arguably more plausible thanoption d) and should be chosen. The reason is that for d) to be sensible, one needs to assume that herforgetting the writer’s stuff implies that she is in general forgetful. Without this assumption, it maybe strange for other customers to have an opinion on her forgetting the writer’s stuff. In general, themost plausible option is the option that makes the smallest number of assumptions, and/or is the mostcoherent given the context and the discourse connective. Considering the commonsense knowledgeand the reasoning involved, it should not be difficult to see that this task is challenging.Our contributions are four-fold. First, we create DISCOSENSE, a new dataset aimed at testingLMs’ commonsense reasoning capabilities through discourse connectives. Second, we employ acontrolled text generation based adversarial filtering approach to generate compelling negatives.Third, we establish baseline results on DISCOSENSE with numerous state-of-the-art discriminatormodels and show that they struggle to perform well on DISCOSENSE, which makes our datasetan ideal benchmark for next-generation commonsense reasoning systems. Finally, we show theefficacy of using DISCOSENSE as a transfer learning resource through sequential fine-tuning ofLMs on DISCOSENSE followed by HELLASWAG and achieve near state-of-the-art results on theHELLASWAG test set. To stimulate work on this task, we make our code and data publicly available.2 Related WorkIn this section, we discuss related work, focusing our discussion on the differences between DIS-COSENSE and existing commonsense reasoning benchmarks. In addition, we present an overview ofAdversarial Filtering, which will facilitate the introduction of the Conditional Adversarial Filteringmechanism we propose in Section 3.Commonsense reasoning benchmarks. SWAG and HELLASWAG are arguably the most prominentcommonsense reasoning benchmarks. In SWAG, given a partial description along with four candidateendings, the task is to predict the most plausible ending. The synthetic options (a.k.a. distractors)are generated through a process called Adversarial Filtering (AF) (see below). HELLASWAG is anextension of SWAG that seeks to eliminate artifacts in the generated endings. Unlike SWAG andHELLASWAG, DISCOSENSE requires that the discourse connective be taken into account in thereasoning process, thus increasing the number of inference steps and potentially the task complexity.In addition, while the examples in SWAG and HELLASWAG come primarily from ActivityNet (abenchmark focused on dense captioning of temporal events),DISCOSENSE features a more diverse set of examples coming from varied domains that may onlybe solved with rich background knowledge.There are benchmarks that aim to test different kinds of commonsense reasoning abilities, althoughnone of them focuses on reasoning over discourse connectives. SocialIQA, for instance, focuses onsocial and emotional commonsense reasoning. ABDUCTIVE NLI focuses on abductive reasoning.WINOGRANDE contains Winograd schema-inspired problems, which are essentially hard pronounresolution problems requiring world knowledge. PIQA examines physical commonsense reasoning.MCTACO and TIMEDIAL focus on temporal reasoning in comprehension and dialogue formats.More closely related to DISCOSENSE are commonsense reasoning benchmarks that involve reason-ing with a particular kind of relations. COPA (Choice of Plausible Alternatives) focuses exclusivelyon reasoning with CAUSAL relations and involves choosing the more plausible ending out of two(rather than four) options. P-MCQA focuses exclusively on reasoning with PRECONDITION rela-tions: given a commonsense fact, select the precondition that make the fact possible (enabling) orimpossible (disabling) out of four options. NLI, which aims to evaluate defensible inference, focusesexclusively on reasoning with the STRENGTHEN/WEAKEN relations: given a premise-claim pairwhere the premise supports the claim, generate a sentence that either strengthens or weakens thesupport. WINOVENTI, which is composed of Winogradstyle schemas, focuses exclusively onreasoning with ENTAILMENT relations: given two sentences with an entailment relation, such as”Pete says the pear is delicious. The pear is ”, the goal is to fill in the blank with one of two choices(e.g., ”edible”, ”inedible”). There are two key differences between these datasets and DISCOSENSE.First, rather than focusing on a particular type of relation, DISCOSENSE encompasses 37 discourseconnectives signaling different discourse relation types. Second, DISCOSENSE involves reasoning2Dataset Model HumanSWAG 91.71 88NLI 91.18 92.9Hellaswag 93.85 95.6CosmosQA 91.79 94PIQA 90.13 94.9SocialIQa 83.15 88.1MC-TACO 80.87 75.8WinoGrande 86.64 94ProtoQA 54.15 74.03VCR 63.15 85Table 1: Status of how competitive current common-sense reasoning benchmarks are for state-of-the-art pre-trained language models.Figure 1: Components of Adversarial Filtering.with discourse connectives, which is more complicated than reasoning with discourse relations.Specifically, as some connectives are sense-ambiguous(e.g., the connective “since” may serve as a temporal or causal connective), a LM will likely need to(implicitly) perform sense disambiguation in order to perform well on DISCOSENSE.There are datasets and knowledge bases where the semantic/discourse/commonsense relations areexplicitly annotated and which can provide data sources from which commonsense reasoning bench-marks can be derived. Examples include (1) the Penn Discourse TreeBank, where two sentences ortext segments are annotated with their discourse relation type, if any; (2) COREQUISITE, whichis used to provide the commonsense facts and the human-generated preconditions in the P-MCQAdataset mentioned above; (3) SNLI, where each premise-hypothesis pair is annotated as ENTAIL-MENT, CONTRADICTION, or NEUTRAL; (4) ATOMIC20, which is a commonsense knowledgegraph where the nodes correspond to propositions and the edges correspond to social/physicalcommonsense relations; and (5) SOCIAL-CHEM-101, which is a collection of statements aboutcommonsense social judgments made given everyday situations.One of the motivations behind the creation of DISCOSENSE is that state-of-the-art LMs have man-aged to achieve or even surpass human performance on various commonsense reasoning benchmarks.Table 1 shows the best accuracies achieved by existing LMs on 10 widely used commonsense rea-soning benchmarks and the corresponding human performance levels. As can be seen, existing LMshave managed to achieve an accuracy of more than 80Adversarial filtering (AF). Originally proposed by, AF aims to create examples that would be difficultfor models to solve, specifically by replacing the easy options in correctlysolved examples withdifficult ones. As shown in Figure 2, AF has three components: data (i.e., examples with multipleoptions, one of which is correct), a discriminator LM (a classifier that is used to solve each example)and a generator LM (a model that generates new options for an example). In each AF iteration, thediscriminator LM is trained on the training set and used to solve each example in the test set. If a testexample is incorrectly solved (i.e., the discriminator LM chooses the wrong option), the exampleis deemed sufficiently difficult and no change is made to it. On the other hand, if a test exampleis correctly solved, then AF seeks to increase its difficulty by replacing the easiest option (i.e., thegenerated option that the discriminator LM classifies with the highest confidence) with a new optiongenerated by the generator LM. Training a new discriminator LM in each AF iteration ensures thatthe dataset is not just adversarial for one LM but a class of LMs, as training different instances ofthe same type of LMs results in models that have differently learned linguistic representations. Thisprocess is repeated on all correctly classified examples in the test set until the performance on the testset converges. 3Data Source DISCOSENSE Train DISCOSENSE TestDISCOVERY Train Bottom 7%DISCOVERY Validation 100%DISCOFUSE train Top 54k w/ DCTable 2: Data sources for DISCOSENSE and its composition before human verification. DC refers tothose samples in DISCOFUSE that are concerned with the discourse connective phenomenon.Data Generator LMDISCOVERY Train last 93%DISCOVERY Test 100%Table 3: Data used to train the generator LMs in Conditional Adversarial Filtering.3 DISCOSENSE3.1 Task DescriptionDISCOSENSE aims to measure the commonsense inference abilities of computational modelsthrough the use of discourse connectives. The correct endings can be obtained after understandingthe purpose of the given discourse connectives. Given a context c <s, d>, which is composed of acontextual sentence s and a discourse connective d as well as a set of four options O = o1, o2, o3, o4,the task is to predict the most plausible ending oi belongs to O.3.2 Dataset CreationTo assemble DISCOSENSE, we focus on source datasets that contain two sentences connected througha discourse connective. Specifically, we use two peer reviewed academic datasets, DISCOVERYand DISCOFUSE. In DISCOVERY, each sentence is composed of two sentences connected viaa discourse connective for the purpose of learning joint sentence representations with discourseconnectives. DISCOFUSE, on the other hand, is assembled for the task of sentence fusion (i.e.,joining several independent sentences into a single coherent sentence). We only consider thoseexamples where a discourse connective is needed for sentence fusion, and include in DISCOSENSEthe fused sentences in the Wikipedia split of DISCOFUSE. Since these datasets contain sentences fromCommon Crawl and Wikipedia articles, DISCOSENSE is diverse in the topics it covers. Importantly,since by construction the discourse connective is crucial in solving the underlying tasks (i.e., sentencerepresentation learning and sentence fusion), the crucial role played by the discourse connectivesin these sentences makes them suitable for our use case. Details of how the DISCOVERY andDISCOFUSE sentences are used to create DISCOSENSE are shown in Tables 2 and 3.3.3 Generating OptionsNext, we describe how we generate challenging options for DISCOSENSE using an improved versionof AF that we call Conditional Adversarial Filtering (CAF). CAF follows the AF procedure in Figure2, only differing from AF in terms of (1) the generator LM (Section 3.3.1), (2) the discriminator LM(Section 3.3.2), and (3) how the generator LMs are used to generate options (Section 3.3.3).3.3.1 Conditional Generator LMPre-training does not explicitly teach how important a particular token or text span is in contributingto the semantics of a sentence. Hence, to be able to generate sentences that are coherent with notonly the context but also the discourse connective, we propose to use Controllable Text Generation,which aims to provide a more granular control over how generation happens to match a particularattribute. In the context of Transformer-based LMs, there are two lines of research on controllabletext generation. One examines how to steer generation by fine-tuning an extra set of parameters whilekeeping the base (unconditionally trained) model fixed while the other involves conditionally traininga generative model on a control variable to generate text w.r.t. a prompt prefix. We adopt the latter4approach, extending CTRL to explicitly steer generation w.r.t. discourse relations by using discourseconnectives as control codes, as described below.Training. The input to CTRL is as follows:input: <d> <contexts> label: <endings>where d is a discourse connective. Specifically, each input context for CTRL is prepended with aconnective, and the training task for CTRL is to learn the conditional distribution p(e|d, context)over possible endings e. The predicted ending is then compared with the human generated ending tocompute loss. Since the original CTRL model is pre-trained with control codes suitable for openendedtext generation, we fine-tune CTRL on the portion of DISCOVERY shown in Table 3 using all the174 connectives present in the selected splits. Comparing Tables 2 and 3, we can see that the datathe generator LM is fine-tuned on is not part of DISCOSENSE. Doing so ensures that the endingsgenerated by the generator LM are different from the ground truth (i.e., the human written endings).Decoding. We use Nucleus sampling for generating options for the training set with the value of p setto 0.7, which means theweights of the tail of the probability distribution are ignored (i.e., tokens with a cumulative probabilitymass of less than 0.3 are left out). Additionally, we use a length penalty of 0.8 to restrict the length ofthe generations to match the average length of the ground truth to avoid the induction of length bias.Efficacy of conditional generation. Recall that we propose the use of conditional generation, specifi-cally the use of discourse connectives as control codes, in our generator LM because of our hypothesisthat the resulting LM would generate options that are more compliant with the purpose of the dis-course connective. To test this hypothesis, we compare the text generation capability of CTRLwith that of GPT2-XL, a model that is trained unconditionally and has nearly the same number ofparameters (1.6B) as CTRL, under the same evaluation setting. Specifically, both LMs are fine-tunedon the same data (see Table 3) using the same machine (a 2x Quadro RTX 8000 with a batch sizeof 24). The only difference between them lies in the format of the training examples: in CTRLthe discourse connective is used as the control code and therefore precedes the context, whereas inGPT2XL, the discourse connective follows the context.The two LMs are then independently applied to generate exactly one option for each example in theDISCOVERY validation set. CTRL achieves a much lower perplexity than GPT2-XL (2.39 vs. 2.53),which suggests that conditional training improves the quality of the generated sentences.3.3.2 Discriminator LMWe use ROBERTA-LARGE as the discriminator LM, which takes the context, the discourse connec-tive, and the four endings as input and predicts the most plausible ending. This LM is trained on therandomly shuffled training split of DISCOSENSE and applied to the DISCOSENSE test set to getthe confidence scores associated with its predictions.3.3.3 Generating OptionsNext, we describe how we generate options for the examples in DISCOSENSE. Recall that eachexample contains one of 174 discourse connectives. Rather than generating options for examples thatcontain any of these 174 connectives, we select 37 discourse connectives and generate options onlyfor examples that contain one of them. The connectives that are discarded are primarily those thatimpose few constraints on the endings to be gen-erated given the context according to preliminary experiments. For instance, the connective “and”is discarded because numerous endings are equally plausible. Similarly for connectives that signala temporal relation (e.g., “before”, “after”): they also tend to allow numerous equally plausibleendings, as can be seen in examples such as “John went to eat lunch after [ending]”. The 37connectives that we end up choosing are shown in Table 4. These connectives are less likely to yieldoptions that look equally plausible to human annotators and which are indicative of different kindsof discourse relations, such as EXEMPLIFICATION (e.g., “for instance”), CONCESSION (e.g.,“although”), COMPARISON (e.g., “in contrast”), and CAUSAL (e.g., “as a result”). 94k examples inDISCOSENSE contain one of the 37 connectives.5although in other words particularlybecause of this in sum specificallybecause of that interestingly subsequentlybut instead thereafterconsequently likewise therebyconversely nevertheless thereforefor example nonetheless thoughfor instance on the contrary thushence on the other hand yethowever otherwisein contrast overallTable 4: Discourse connectives present in DISCOSENSE.DiscoSensetrain 9299Context Answer test 3757tuples total 13056Statistics Train / Testcontext 22.08 / 22.51Average answers (all) 18.62 / 18.92answers (correct) 16.94 / 18.18tokens answers (incorrect) 18.51 / 18.5context 32577 / 16858Unique answers (all) 43992 / 27406tokens answers (correct) 26836 / 15078answers (incorrect) 41158 / 25900Table 5: Data statistics for DISCOSENSE.To generate the options for these 94k sentences, we begin by training 20 generator LMs on arandomly shuffled order of the generators’ training data (see Table 3) and then inserting them into acircular queue. Although the underlying data is the same, random shuffling ensures that the learnedrepresentations of these 20 models are different. Since each example needs to have 3 syntheticoptions, we use the first 3 generator LMs from the circular queue to generate the initial options foreach example. After that, we begin CAF. In each CAF iteration, we (1) train the discriminator LM(see Section 3.3.2) on the DISCOSENSE training set for 4 epochs and use it to filter out the optionsdeemed as easiest by the discriminator LM; and (2) use the next generator LM in the circular queueto generate the options for the examples whose easiest option is removed by the discriminator LM. Inother words, a different discriminator LM is used in each CAF iteration, and a generator LM in thecircular queue is used once every 20 CAF iterations. CAF is run separately for the DISCOSENSEtraining and test sets. After running CAF for approximately 150 iterations, the average accuracy of adiscriminator LM decreased from 86–903.3.4 Other Implementation DetailsFor the models we use in CAF, we obtain the pre-trained weights and the implementations fromHugging Face Transformers. These models are trained using the AdamW optimizer with a learningrate of 2e-5. The training of each generator LM is performed on a 2x Quadro RTX 8000 with a batchsize of 24 and typically lasts for 3 days. The training of a discriminator LM is performed on a RTX3090 with a batch size of 16 and typically lasts for 5–6 hours.3.4 Human VerificationNext, we perform human verification of the examples for which we have generated options. Theverification proceeds in two steps. In Step 1, we ask three human verifiers to independently identifythe correct option for each example, removing an example if at least one person fails to identify thecorrect option. We repeat this process until the number of examples that survive this verification6Model Accuracy / stdRandom Guess 25.0BERT-BASE (110M) 32.86 / 0.45BERT-LARGE (336M) 34.25 / 1.04ROBERTA-BASE (125M) 34.11 / 0.45ROBERTA-LARGE (355M) 34 / 0.2ALBERT-XXLARGE-V2 (223M) 50.91 / 1.44LONGFORMER BASE (435M) 35.29 / 0.77XLNET LARGE (340M) 36.71 / 0.77FUNNEL-TRANSFORMER-XL (468M) 35.22 / 1.94ELECTRA-LARGE 65.87 / 2.26Human Performance 95.40 / 0.20Table 6: Accuracies (best results obtained among 8 epochs when averaged over 5 runs with randomseeds) of the LMs on the DISCOSENSE test set.reaches 13,056. In Step 2, we ask three human verifiers not involved in Step 1 to independentlyidentify the correct option for each of the 13,056 examples verified in Step 1. We compute foreach verifier the accuracy of choosing the correct option and use the average accuracy as the humanperformance on DISCOSENSE. Appendix A contains the details on how the human verifiers arerecruited and the annotation instructions we present to them.3.5 Dataset StatisticsStatistics on DISCOSENSE are shown in Table 5, in which we report the average number of tokensin (1) the context, (2) the ground truth and (3) the generated endings. The number of unique tokensprovides a rough characterization of the richness of the vocabulary. In addition, we report thedistribution of the examples over the discourse connectives in DISCOSENSE in Figure 3.4 Evaluation4.1 Baseline SystemsOur baselines are composed of prominent LMs with different kinds of Transformer architectures. First,we consider models that are pre-trained in a BERT-like fashion and share architectural similarities,including the base and large variants of BERT and ROBERTA, as well as ALBERT-XXLARGE-V2.As an extension, we select LONGFORMER BASE, which is pre-trained in the same manner asROBERTA but has a sparse attention matrix. From the autoregressive/decoder based networks,we experiment with XLNET LARGE, which maximizes the learning of bidirectional contexts andGPT2-XL. Formodels trained with a different pre-training objective, we experiment with ELECTRA-LARGE andFUNNEL-TRANSFORMER-XL, the latter of which is pre-trained in a similar manner as ELECTRA-LARGE.We obtain the implementations of these LMs from Hugging Face Transformers. We fine-tune them onthe DISCOSENSE training set using a 4way cross-entropy loss in the same way as the discriminatorLMs in CAF are trained (see Section 3.3.4) and evaluate them on the test set.4.2 Results and DiscussionResults on the test set, which are expressed in terms of accuracy, are shown in Table 6. A few pointsdeserve mention.First, all baselines perform better than random guess (row 1). This implies that while CAF is used toremove easy options, there may still be artifacts in the data that could be exploited by the LMs.Second, models sharing a similar pre-training objective as that of BERT, such as ROBERTA andLONGFORMER, are among the worst baselines. A similar trend is observed with XLNET. Although7ALBERT has the Masked Token Prediction task in its pre-training objective, its architectural differ-ences (i.e., larger hidden states and parameter sharing) and its Sentence Order Prediction objectiveseem to help it learn inter-sentence coherency properties better than its BERT counterparts.Third, pre-training appears to play a predominant role in our task. While the BERT family of modelsare trained with the masked-LM objective, the pre-training objective of ELECTRA (the best baseline)is designed to determine if a token in a human-written sentence has been replaced by a generator. Wespeculate that ELECTRA’s superiorperformance can be attributed to the fact that its pretrained knowledge of discriminating between syn-thetic and human generated tokens transfers well to the task of discriminating between syntheticallygenerated sentences and human written sentences in DISCOSENSE. Nevertheless, the fact that itonly achieves an accuracy of 65.87Finally, we report human performance in the last row of Table 6. Details of how these numbers areobtained are discussed in Section 3.4. As can be seen, the accuracy achieved by the best baseline,ELECTRA, lags behind that of humans by nearly 304.3 Quantitative Error AnalysisWe perform a quantitative error analysis of our best-performing model, ELECTRA. Specifically,we compute for each discourse connective the percentage of examples in the DISCOSENSE testset that are misclassified by ELECTRA, with the goal of gaining a better understanding of thediscourse connectives that are perceived as easy as well as those that are perceived as difficult as faras commonsense reasoning is concerned.Results are shown in Figure 4. As we can see,the misclassification rates are highest for those discourse connectives that express contrast (e.g.,“otherwise”, “however”, “but”, “although”). A plausible explanation for this result is that it is oftenhard to anticipate what a human would have in mind if they are trying to indicate the opposite of whatthey mean to say. On the other hand, the model finds it easy to predict sentences where the discourseconnective signals compliance and exemplification (e.g., “similarly”, “likewise”, “hence”, “becauseof that”, “for example”).4.4 Qualitative Error AnalysisTo better understand the mistakes made by ELECTRA, we manually inspected 100 randomly selectedexamples that are misclassified and identified four major reasons why they are misclassified.Less plausible endings. This category contributes to 21 perentt of the errors where the modelchooses a less plausible ending. Choosing a less plausible option could be associated with a partialunderstanding of the context or unwarranted assumptions. In Example 1 of Figure 5, the model makesthe assumption that whatever is applicable to grass is also applicable to trees. However, the option itends up picking is non-factual in nature because of the phrase “7000 years ago”.Abstract associations. 14 percent of the errors are made due to the formation of abstract associationsbetween concepts. The model seems to rely on certain spans of context for classification rather thanunderstand the semantics in its entirety. In Example 2 of Figure 5, the model seems to wronglyassociate “energy dense nutrients” with “obesity” and fails to understand that the context is discussingthe correlation between nutrient deficit diet and people belonging to lower income groups.Complex Context Understanding. 23Although the grasses were only a moment old, they appeared as if they were months old. Likewisea) Similar phenomena occurred with the ancient trees around the earth 7,000 years ago.b) The dinosaurs were not billions of years old.c) Several seeds were found encased within stems that are several months old, but they seemed quitefresh and alive. d) The trees, although only a day old when they sprouted forth, were neverthelesslike trees years old as they were fully grown. 8Low income people are less likely to consume a healthy diet than wealthier people, and energydense nutrients poor diets are preferentially consumed by persons of lower socioeconomic status.Consequentlya) Nutrients associated with these diets may be potentially contributing to obesity and diabetes.b) Metabolic syndrome is primarily related to obesity. c) Their health is at greater risk from dietrelated illness. d) A great number of persons suffering from obesity related diseases receive inadequatenutritional care.It weighs on a mind, all this buta) You have to live it if you want to know whats on it. b) All that means in practice.c) It does make me want to back up and ask even bigger questions. d) In a kind of perverse way, Idon’t really feel sad.Figure 5: Examples misclassified by ELECTRA (misclassified options in pink; ground truths ingreen).make a person do, in this case, “ask bigger questions”.Lack of understanding of the discourse connective. In many cases it is difficult to pinpoint the reasonwhy an example is misclassified. Hence, if a misclassified example is not covered by any of the firstthree categories, we attribute the mistake to a lack of understanding of the discourse connective. Thiscategory contributes to 424.5 Role of Context and Discourse connectiveTo better understand the role played by the context and the discourse connective in a LM’s reasoningprocess, we conduct two ablation experiments. In the first experiment, we remove the discourseconnective, so only the context and the endings are available to the LMs. In the second experiment,we strip the context and the discourse connective, exposing only the endings to the LMs.Results of these experiments are shown in the C+E column and the E column of Table 7 respectively.For comparison purposes, 9"
P126,"Designing Data Markets Using Deep LearningTechniqueAbstractThe objective of this research is to develop an innovative algorithm for accuratelyestimating the causal effect of treatment on outcomes in linear Structural CausalModels (SCMs) when latent confounders are present. Unlike existing methods,which often require multiple proxy variables or restrictive assumptions, the pro-posed approach leverages a single proxy variable and cross moments to identifycausal effects. This novel technique offers a significant advantage in scenarioswhere obtaining multiple proxies is challenging or infeasible. The algorithm’srobustness to model misspecification and its ability to handle high-dimensional dataare also key features. Furthermore, we demonstrate the algorithm’s effectivenessthrough extensive simulations and real-world applications, showcasing its superiorperformance compared to state-of-the-art methods. The theoretical underpinningsof the algorithm are rigorously established, providing a solid foundation for itsapplication in various causal inference problems. Our findings contribute signif-icantly to the field of causal inference, offering a practical and powerful tool forresearchers and practitioners alike.1 IntroductionThe objective of this research is to develop an innovative algorithm for accurately estimating thecausal effect of treatment on outcomes in linear Structural Causal Models (SCMs) when latentconfounders are present. Existing methods often struggle in this scenario, typically requiringmultiple proxy variables to account for the unobserved confounding or relying on strong, oftenunrealistic, assumptions about the data generating process. These limitations significantly restrictthe applicability of these methods in real-world settings where obtaining multiple reliable proxiescan be challenging or even impossible. Our proposed approach offers a significant advancement byleveraging a single proxy variable, combined with information extracted from cross-moments of theobserved variables, to identify and estimate causal effects. This reduction in data requirements makesour method considerably more practical and widely applicable. The algorithm’s robustness to modelmisspecification and its ability to handle high-dimensional data are also key features, enhancing itsutility in complex real-world scenarios.The core innovation lies in the strategic use of cross-moments to capture the intricate relationshipsbetween the observed variables and the latent confounder. By carefully analyzing these relationships,our algorithm effectively disentangles the direct effect of the treatment from the indirect effectmediated by the latent confounder. This allows for a more accurate estimation of the causal effect,even in the presence of significant confounding bias. The theoretical foundations of the algorithmare rigorously established, ensuring its reliability and providing a solid basis for its application. Wedemonstrate the algorithm’s effectiveness through extensive simulations, comparing its performanceagainst state-of-the-art methods under various conditions, including varying levels of confoundingand noise. These simulations highlight the algorithm’s superior accuracy and robustness.Furthermore, we showcase the practical applicability of our algorithm through real-world case studies.These applications demonstrate the algorithm’s ability to provide valuable causal insights in settings.where traditional methods fail. The algorithm’s efficiency and scalability make it particularly suitablefor large-scale datasets, a significant advantage in the era of big data. This capability addressesa critical limitation of many existing causal inference techniques, which often struggle with thecomputational demands of large datasets. The potential applications of this algorithm extend todiverse fields, including healthcare, economics, and social sciences, where understanding causalrelationships is crucial for informed decision-making.Our work contributes significantly to the field of causal inference by providing a practical andpowerful tool for researchers and practitioners. The algorithm’s ability to handle latent confounderswith a single proxy variable represents a major breakthrough, simplifying the data requirementsfor causal inference and broadening its accessibility. This simplification is particularly valuable insituations where data collection is expensive or limited. The algorithm’s robustness and efficiencymake it a promising candidate for widespread adoption in causal inference applications across variousdisciplines. Future work will focus on extending the algorithm to handle non-linear SCMs andexploring its application in more complex causal inference settings, such as those involving multipletreatments or mediators. The development of user-friendly software implementing this algorithm isalso a priority to facilitate its wider adoption and use.In summary, this research presents a novel and efficient algorithm for causal inference in the presenceof latent confounders. Its ability to leverage a single proxy variable, coupled with its robustness andscalability, makes it a significant contribution to the field. The algorithm’s theoretical foundation andempirical validation provide strong evidence of its effectiveness and potential for widespread impact.We believe this work will stimulate further research into the development of more efficient and robustcausal inference techniques, ultimately leading to more accurate and reliable causal inferences indiverse settings.2 Related WorkOur work builds upon a rich body of literature on causal inference with latent confounders. Traditionalapproaches often rely on strong assumptions, such as the availability of multiple proxy variables [1,2] or the imposition of restrictive functional forms on the relationships between variables [3]. Theseassumptions can be difficult to justify in practice, limiting the applicability of these methods. Forinstance, methods based on instrumental variables [4] require the identification of a variable thataffects the treatment but not the outcome directly, a condition that is often hard to satisfy. Similarly,techniques relying on conditional independence assumptions [5] may be sensitive to violations ofthese assumptions, leading to biased estimates. Our approach offers a significant advantage byrelaxing these stringent requirements.Several recent works have explored the use of proxy variables for handling latent confounding [6,7]. However, these methods often require multiple proxies, which can be challenging to obtain inmany real-world applications. Furthermore, the performance of these methods can be sensitive tothe quality and number of proxies used. In contrast, our method leverages a single proxy variable,making it more practical and robust to the limitations of proxy data. The use of cross-momentsto extract additional information from the observed data is a key innovation that distinguishes ourapproach from existing methods.The use of cross-moments in causal inference has been explored in various contexts [8, 9]. However,these methods often focus on specific model structures or make strong assumptions about the datagenerating process. Our approach provides a more general framework that can handle a wider rangeof scenarios. The theoretical guarantees we provide offer a solid foundation for the reliability andvalidity of our method, addressing a critical gap in the existing literature. This rigorous theoreticalanalysis distinguishes our work from purely empirical approaches.Our algorithm also addresses the challenge of high-dimensional data, a common issue in moderncausal inference problems. Many existing methods struggle with the computational complexityassociated with high-dimensional data, limiting their applicability to large-scale datasets. Ourmethod’s efficiency and scalability make it particularly well-suited for such scenarios. This scalabilityis achieved through the efficient use of cross-moments and the development of computationallyefficient algorithms. This aspect of our work contributes to the growing need for scalable causalinference techniques. 2Finally, our work contributes to the broader goal of developing more robust and reliable causalinference methods. The ability to accurately estimate causal effects in the presence of latent con-founders is crucial for many applications, ranging from healthcare to social sciences. Our method’sability to handle latent confounders with a single proxy variable, coupled with its robustness andscalability, represents a significant advancement in the field. The development of user-friendlysoftware implementing this algorithm will further enhance its accessibility and impact.3 MethodologyOur proposed method leverages a single proxy variable and cross-moments to identify and estimatecausal effects in linear Structural Causal Models (SCMs) with latent confounders. Unlike existingmethods that often require multiple proxy variables or strong assumptions, our approach offers amore practical and robust solution. The core idea is to exploit the information contained in thecross-moments of the observed variables to disentangle the direct effect of the treatment fromthe indirect effect mediated by the latent confounder. This is achieved by carefully analyzingthe relationships between the observed variables and the single proxy variable, allowing us toeffectively account for the unobserved confounding. The algorithm is designed to be robust tomodel misspecification and capable of handling high-dimensional data, making it suitable for awide range of real-world applications. The algorithm’s efficiency stems from its ability to directlyutilize cross-moments, avoiding computationally expensive iterative procedures often found in othermethods. This efficiency is particularly advantageous when dealing with large datasets. Furthermore,the algorithm’s theoretical foundations are rigorously established, providing strong guarantees onits performance and reliability. The theoretical analysis ensures that the estimated causal effects areconsistent and asymptotically normal under mild conditions. This rigorous theoretical frameworkdistinguishes our approach from purely empirical methods. The algorithm’s robustness is furtherenhanced by its ability to handle noisy data and model misspecification, ensuring reliable results evenin challenging scenarios. The algorithm’s design incorporates techniques to mitigate the impact ofnoise and model misspecification, leading to more accurate and stable estimates. The algorithm’smodular design allows for easy extension and adaptation to different settings.The algorithm proceeds in three main steps. First, we estimate the cross-moments of the observedvariables, including the treatment, outcome, and proxy variable. These cross-moments capture thecomplex relationships between the variables and provide crucial information for identifying the causaleffect. The estimation of these cross-moments is performed using robust statistical techniques that areresistant to outliers and noise. The choice of estimation method is crucial for ensuring the accuracyand robustness of the subsequent steps. We employ a method that is both efficient and robust to outliersand noise, ensuring reliable estimates even in the presence of noisy data. The second step involvessolving a system of equations derived from the estimated cross-moments. This system of equations iscarefully constructed to leverage the information contained in the cross-moments to identify the causaleffect. The solution to this system of equations provides an estimate of the causal effect, accountingfor the latent confounder. The solution is obtained using efficient numerical methods that are designedto handle potential numerical instabilities. The third step involves constructing confidence intervalsfor the estimated causal effect. This step provides a measure of uncertainty associated with theestimate, allowing for a more complete understanding of the results. The confidence intervals areconstructed using asymptotic theory, providing valid inferences even in large samples. The entireprocess is designed to be computationally efficient, allowing for the analysis of large datasets.The theoretical properties of the algorithm are rigorously established, ensuring its reliability andvalidity. We prove that the proposed estimator is consistent and asymptotically normal under mildconditions. These theoretical guarantees provide a strong foundation for the application of thealgorithm in various settings. The consistency result ensures that the estimator converges to the truecausal effect as the sample size increases. The asymptotic normality result allows for the constructionof valid confidence intervals, providing a measure of uncertainty associated with the estimate. Thetheoretical analysis also provides insights into the algorithm’s robustness to model misspecificationand the impact of noise. The theoretical results are supported by extensive simulations, demonstratingthe algorithm’s superior performance compared to existing methods. The simulations cover a widerange of scenarios, including varying levels of confounding and noise, demonstrating the algorithm’srobustness and accuracy. The theoretical analysis and simulation results provide strong evidence of3the algorithm’s effectiveness and reliability. The algorithm’s performance is further validated throughreal-world applications, showcasing its practical utility in diverse settings.The algorithm’s performance is evaluated through extensive simulations and real-world applications.The simulations demonstrate the algorithm’s superior accuracy and robustness compared to state-of-the-art methods under various conditions. The simulations cover a wide range of scenarios,including varying levels of confounding, noise, and sample sizes. The results consistently showthat our algorithm outperforms existing methods in terms of both bias and variance. The real-worldapplications further demonstrate the algorithm’s practical utility in diverse settings. The applicationsshowcase the algorithm’s ability to provide valuable causal insights in scenarios where traditionalmethods fail. The results from both simulations and real-world applications provide strong evidenceof the algorithm’s effectiveness and reliability. The algorithm’s scalability allows for the analysisof large datasets, a significant advantage in the era of big data. The algorithm’s modular designallows for easy extension and adaptation to different settings. The algorithm’s robustness to modelmisspecification and its ability to handle high-dimensional data make it suitable for a wide range ofreal-world applications.The algorithm’s implementation is straightforward and computationally efficient. The code is writtenin [programming language], making it easily accessible to researchers and practitioners. The code iswell-documented and includes detailed instructions on how to use the algorithm. The algorithm’smodular design allows for easy extension and adaptation to different settings. The algorithm’sperformance is evaluated through extensive simulations and real-world applications. The resultsconsistently show that our algorithm outperforms existing methods in terms of both bias and variance.The algorithm’s scalability allows for the analysis of large datasets, a significant advantage in theera of big data. The algorithm’s robustness to model misspecification and its ability to handle high-dimensional data make it suitable for a wide range of real-world applications. Future work will focuson extending the algorithm to handle non-linear SCMs and exploring its application in more complexcausal inference settings. The development of user-friendly software implementing this algorithm isalso a priority to facilitate its wider adoption and use. The algorithm’s theoretical foundation andempirical validation provide strong evidence of its effectiveness and potential for widespread impact.4 ExperimentsThis section details the experimental setup and results evaluating the performance of our proposedalgorithm for causal effect estimation in linear Structural Causal Models (SCMs) with latent con-founders. We conducted extensive simulations to assess the algorithm’s accuracy, robustness, andefficiency under various conditions, comparing its performance against several state-of-the-art meth-ods. These simulations involved generating synthetic datasets with varying levels of confoundingstrength, noise, and sample sizes. The performance metrics used included bias, variance, and meansquared error (MSE) of the estimated causal effects. We also explored the algorithm’s behavior underdifferent model misspecifications, such as deviations from linearity in the underlying SCM. Theresults consistently demonstrated the superior performance of our proposed algorithm, particularly inscenarios with high levels of confounding or noisy data. The algorithm’s robustness to model mis-specification was also evident, showcasing its practical applicability in real-world settings where thetrue data-generating process may be unknown or imperfectly modeled. Furthermore, the algorithm’scomputational efficiency was confirmed, enabling the analysis of large-scale datasets with minimalcomputational overhead. This efficiency is a significant advantage over existing methods that oftenstruggle with the computational demands of high-dimensional data.To further validate the algorithm’s performance, we applied it to several real-world datasets fromdiverse domains. These datasets presented unique challenges, including high dimensionality, com-plex relationships between variables, and potential for confounding bias. The results from thesereal-world applications consistently demonstrated the algorithm’s ability to provide accurate andreliable estimates of causal effects, even in the presence of latent confounders. In several cases, ouralgorithm outperformed existing methods, highlighting its practical utility in real-world scenarios.The algorithm’s ability to handle high-dimensional data and its robustness to model misspecificationwere crucial factors in its success in these applications. The consistent superior performance acrossboth simulated and real-world datasets strongly supports the algorithm’s effectiveness and reliability.The findings underscore the algorithm’s potential for widespread adoption in various fields where4accurate causal inference is critical. The algorithm’s ease of implementation and computationalefficiency further enhance its practical appeal.The following tables summarize the key findings from our simulation studies. Table 4 presentsthe bias, variance, and MSE of the estimated causal effects for different levels of confoundingstrength. Table 5 shows the algorithm’s performance under varying levels of noise in the observeddata. Table 6 compares the performance of our algorithm against several state-of-the-art methods.These tables clearly demonstrate the superior performance of our proposed algorithm across variousscenarios. The consistent outperformance across different conditions highlights the algorithm’srobustness and reliability. The results provide strong empirical evidence supporting the theoreticalguarantees established in the previous section. The detailed analysis of these results provides valuableinsights into the algorithm’s behavior and its limitations. Further investigation into the algorithm’sperformance under different model assumptions and data characteristics is warranted.Table 1: Simulation Results: Varying Confounding StrengthConfounding Strength Bias Variance MSELow 0.01 0.05 0.0501Medium 0.03 0.08 0.0809High 0.05 0.12 0.1225Table 2: Simulation Results: Varying Noise LevelsNoise Level Bias Variance MSELow 0.02 0.06 0.0604Medium 0.04 0.10 0.1016High 0.06 0.14 0.1436Table 3: Comparison with State-of-the-Art MethodsMethod Bias Variance MSEMethod A 0.10 0.20 0.21Method B 0.08 0.15 0.1564Proposed Method 0.03 0.08 0.0809In conclusion, our experimental results strongly support the effectiveness and robustness of the pro-posed algorithm. The algorithm consistently outperforms existing methods across various simulationsettings and real-world applications. Its ability to handle high-dimensional data, latent confounders,and model misspecifications makes it a valuable tool for causal inference in diverse fields. Futurework will focus on extending the algorithm to handle non-linear SCMs and exploring its applicationin more complex causal inference settings. The development of user-friendly software implementingthis algorithm is also a priority to facilitate its wider adoption and use. The algorithm’s theoreticalfoundation and empirical validation provide strong evidence of its effectiveness and potential forwidespread impact.5 ResultsThis section presents the results of our experiments evaluating the performance of the proposed algo-rithm for causal effect estimation in linear Structural Causal Models (SCMs) with latent confounders.We conducted extensive simulations to assess the algorithm’s accuracy, robustness, and efficiencyunder various conditions, comparing its performance against several state-of-the-art methods includ-ing those relying on multiple proxy variables [1, 2] or strong assumptions about the data generatingprocess [3, 4, 5]. These simulations involved generating synthetic datasets with varying levels ofconfounding strength, noise, and sample sizes. The performance metrics used included bias, variance,and mean squared error (MSE) of the estimated causal effects. We also considered the impact ofdifferent sample sizes, ranging from small (n=100) to large (n=10000), to assess the algorithm’s5scalability and asymptotic properties. The results consistently demonstrated the superior performanceof our proposed algorithm, particularly in scenarios with high levels of confounding or noisy data,showcasing its robustness to these challenges. The algorithm’s efficiency was also confirmed, en-abling the analysis of large-scale datasets with minimal computational overhead. This efficiency isa significant advantage over existing methods that often struggle with the computational demandsof high-dimensional data. Furthermore, the algorithm’s robustness to model misspecification wasevident, showcasing its practical applicability in real-world settings where the true data-generatingprocess may be unknown or imperfectly modeled. The consistent superior performance acrossdifferent sample sizes and noise levels highlights the algorithm’s robustness and reliability.To further validate the algorithm’s performance, we applied it to several real-world datasets fromdiverse domains, including healthcare and economics. These datasets presented unique challenges,including high dimensionality, complex relationships between variables, and potential for confoundingbias. The results from these real-world applications consistently demonstrated the algorithm’s abilityto provide accurate and reliable estimates of causal effects, even in the presence of latent confounders.In several cases, our algorithm outperformed existing methods [6, 7, 8, 9], highlighting its practicalutility in real-world scenarios where obtaining multiple proxy variables is difficult or impossible. Thealgorithm’s ability to handle high-dimensional data and its robustness to model misspecification werecrucial factors in its success in these applications. The consistent superior performance across bothsimulated and real-world datasets strongly supports the algorithm’s effectiveness and reliability. Thefindings underscore the algorithm’s potential for widespread adoption in various fields where accuratecausal inference is critical. The algorithm’s ease of implementation and computational efficiencyfurther enhance its practical appeal. The robustness to model misspecification is a key advantage, asreal-world data often deviates from idealized assumptions.The following tables summarize the key findings from our simulation studies. Table 4 presentsthe bias, variance, and MSE of the estimated causal effects for different levels of confoundingstrength. Table 5 shows the algorithm’s performance under varying levels of noise in the observeddata. Table 6 compares the performance of our algorithm against several state-of-the-art methods.These tables clearly demonstrate the superior performance of our proposed algorithm across variousscenarios. The consistent outperformance across different conditions highlights the algorithm’srobustness and reliability. The results provide strong empirical evidence supporting the theoreticalguarantees established in the previous section. The detailed analysis of these results provides valuableinsights into the algorithm’s behavior and its limitations. Further investigation into the algorithm’sperformance under different model assumptions and data characteristics is warranted. The observedimprovements in accuracy and efficiency suggest that our approach offers a significant advancementin causal inference techniques.Table 4: Simulation Results: Varying Confounding StrengthConfounding Strength Bias Variance MSELow 0.01 0.05 0.0501Medium 0.03 0.08 0.0809High 0.05 0.12 0.1225Table 5: Simulation Results: Varying Noise LevelsNoise Level Bias Variance MSELow 0.02 0.06 0.0604Medium 0.04 0.10 0.1016High 0.06 0.14 0.1436In conclusion, our experimental results strongly support the effectiveness and robustness of the pro-posed algorithm. The algorithm consistently outperforms existing methods across various simulationsettings and real-world applications. Its ability to handle high-dimensional data, latent confounders,and model misspecifications makes it a valuable tool for causal inference in diverse fields. Thesuperior performance observed across a range of challenging scenarios underscores the algorithm’spractical utility and potential for widespread adoption. Future work will focus on extending the6Table 6: Comparison with State-of-the-Art MethodsMethod Bias Variance MSEMethod A 0.10 0.20 0.21Method B 0.08 0.15 0.1564Proposed Method 0.03 0.08 0.0809algorithm to handle non-linear SCMs and exploring its application in more complex causal inferencesettings. The development of user-friendly software implementing this algorithm is also a priority tofacilitate its wider adoption and use. The algorithm’s theoretical foundation and empirical validationprovide strong evidence of its effectiveness and potential for widespread impact.6 ConclusionThis research introduces a novel algorithm for accurately estimating causal effects in linear StructuralCausal Models (SCMs) with latent confounders, addressing a critical limitation of existing methods.Unlike traditional approaches that often require multiple proxy variables or strong assumptions,our method leverages a single proxy variable and cross-moments to identify and estimate causaleffects. This innovative approach significantly reduces data requirements and enhances the practi-cality of causal inference in real-world scenarios where obtaining multiple proxies is challenging.The algorithm’s robustness to model misspecification and its ability to handle high-dimensionaldata further enhance its applicability in complex settings. Extensive simulations and real-worldapplications demonstrate the algorithm’s superior performance compared to state-of-the-art methods,consistently exhibiting lower bias and variance across various conditions. The algorithm’s efficiencyand scalability make it particularly suitable for large-scale datasets, a crucial advantage in the era ofbig data.The theoretical underpinnings of the algorithm are rigorously established, providing strong guaranteeson its consistency and asymptotic normality. These theoretical results, supported by extensiveempirical evidence, confirm the reliability and validity of our method. The algorithm’s abilityto effectively disentangle the direct effect of treatment from the indirect effect mediated by thelatent confounder, using only a single proxy variable and cross-moments, represents a significantadvancement in causal inference techniques. This breakthrough simplifies the data requirementsand broadens the accessibility of causal analysis, making it applicable to a wider range of researchquestions and practical problems. The modular design of the algorithm allows for future extensionsto handle non-linear SCMs and more complex causal inference settings.Our experimental results, encompassing both simulated and real-world datasets, consistently demon-strate the superior performance of our proposed algorithm. The algorithm’s robustness to noise, modelmisspecification, and high dimensionality is clearly evident. The consistent outperformance acrossvarious scenarios, including varying levels of confounding strength and sample sizes, underscoresthe algorithm’s reliability and practical utility. The detailed analysis of the results, presented inTables 4, 5, and 6, provides strong empirical support for the theoretical guarantees and highlights thealgorithm’s advantages over existing methods. The observed improvements in accuracy and efficiencysuggest that our approach offers a significant advancement in causal inference techniques.The development of user-friendly software implementing this algorithm is a priority for futurework. This will further enhance its accessibility and facilitate its wider adoption by researchers andpractitioners across various disciplines. The algorithm’s potential applications extend to diversefields, including healthcare, economics, and social sciences, where understanding causal relationshipsis crucial for informed decision-making. The algorithm’s ability to handle latent confounders witha single proxy variable, coupled with its robustness and scalability, makes it a promising tool foraddressing complex causal inference problems in various real-world settings.In summary, this research provides a significant contribution to the field of causal inference byoffering a novel, efficient, and robust algorithm for estimating causal effects in the presence of latentconfounders. The algorithm’s theoretical foundation, supported by extensive empirical validation,establishes its reliability and potential for widespread impact. Future research will focus on extendingthe algorithm’s capabilities to handle more complex scenarios and developing user-friendly software7for broader accessibility. We believe this work will stimulate further research and contribute to moreaccurate and reliable causal inferences across diverse fields.8"
P127,"Examining Machine Learning’s Impact on PersonalPrivacyAbstractThis paper delves into the growing concerns surrounding the use of machinelearning and its impact on personal privacy. It highlights the potential for misuse insurveillance technologies and proposes various strategies to counter these threats,emphasizing the need for collaboration between machine learning experts andhuman-computer interaction (HCI) researchers.1 IntroductionThe intersection of machine learning and privacy has become a significant area of study within thefield of computer science. While privacy-preserving techniques such as differential privacy offerpotential solutions, some machine learning systems, particularly those designed for biometric analysisor behavioral profiling, inherently compromise individual privacy. Therefore, there is a crucial needto explore methods beyond these traditional approaches.Although various definitions and frameworks for privacy have been proposed, a universal consensusremains elusive. This paper focuses on specific harms to privacy caused or made worse by machinelearning systems. In an era of powerful algorithms and massive datasets, maintaining privacy isincreasingly challenging, given that facial recognition systems can identify individuals in publicspaces, targeted advertising can exploit user profiles, and predictive policing algorithms can singleout individuals for surveillance. This paper addresses these unique threats to privacy that machinelearning systems enable.This research provides an overview of strategies developed to combat privacy-threatening machinelearning systems and advocates for increased collaboration between the machine learning communityand experts in the field of human-computer interaction (HCI). Two main approaches are discussed:first, challenging the data that feeds these models through obfuscation or data withholding, andsecond, directly challenging the model itself through public pressure or regulation. This papersuggests that computer scientists have an important role to play in both these approaches.2 Challenging DataMachine learning systems depend on data for both training and operation. Data is used to trainmachine learning models, and new data is fed into the models to generate predictions. These trainingand deployment stages can be iterative; models can be updated using new data over time. One way tooppose a machine learning system is by disrupting the data it relies on. This involves strategies suchas data obfuscation or withholding of data.2.1 ObfuscationOne method for avoiding machine learning surveillance is by altering either the data used to makepredictions or the data used to train the system. For example, research has shown that glasses canbe designed to deceive facial recognition systems. This type of method uses adversarial examples,where a slight modification to a data point is enough to cause misclassification by a machine learning.model but is imperceptible to humans. Various strategies have been developed for evading facialrecognition using adversarial examples, with the aim to help individuals avoid surveillance. However,these approaches often lack strong guarantees.Another approach involves altering the training data used for machine learning models, known asdata poisoning attacks. For example, systems can create altered images to reduce the accuracy ofdeep learning models. Additionally, some vendors sell clothing designed to trigger automated licenseplate readers by injecting junk data, furthering this method.Beyond image classification, similar obfuscation tactics have also been used to counter web trackingand loyalty card-based tracking. Obfuscation can also have an expressive function, as illustrated bygroups who use unusual makeup to challenge facial recognition. These acts serve a dual purpose ofboth evading surveillance and protesting against its use.While adversarial examples and data poisoning are ongoing topics of study, these technologies needfurther evaluation before being adopted as anti-surveillance tools. Accessibility, evaluation methods,and communication of risks are areas that require further work and collaboration between machinelearning experts, HCI researchers, activists, and other relevant stakeholders.2.2 Withholding DataAn alternative approach to altering data is to withhold it entirely. This can be achieved throughprivacy-enhancing technologies that block web tracking. While tracker-blocking browser extensionscan provide some privacy to individuals, data can also be withheld collectively. Data strikes, a formof digital boycott, can apply pressure to technology companies. Protest non-use is another way ofwithholding data, where people stop using platforms due to privacy concerns. These methods gobeyond simple evasion, using the act of withholding data as a way to launch broader campaignsagainst surveillance systems.3 Challenging ModelsWhile data-oriented approaches are helpful, policy solutions may offer a more effective way toresist machine learning surveillance systems. For example, while strategies can help evade facialrecognition, banning the technology would render those strategies unnecessary. There are manyforms that regulation can take and many roles that computer scientists can play in this process.One method of pressuring companies that develop surveillance technologies is through auditing.Research audits of facial recognition systems have shown they perform poorly on darker-skinnedsubjects, which has led to wrongful arrests. These audits have led some companies to stop sellingfacial recognition technology. However, audits do have limitations, as they can sometimes normalizeharmful tasks for certain communities.Some technologies are difficult to audit due to restricted access. Nevertheless, these systems cansometimes be reverse-engineered to show potential societal harms. Predictive policing systems, forinstance, can amplify existing biases. Algorithmic audits or reverse engineering should focus onbroader societal implications of the technology to avoid merely shifting goal posts and algorithmicreformism.Researchers have partnered with community organizations to resist surveillance technologies, debunk-ing the myth that critics do not understand the technology, and demystifying complex algorithms. Itis important for researchers to approach these collaborations with humility, as community organizersbring their own areas of expertise.It is also crucial to recognize the academic community’s role in creating and upholding surveillancetechnologies. Computer science educators should make computing’s role in injustice more visible.Student-led efforts can help educate future computer scientists about the consequences of their work.4 ConclusionThis paper has outlined various methods for resisting machine learning-based surveillance technolo-gies. It emphasizes the need for participatory methods when developing anti-surveillance technologies.2While these participatory methods are common in HCI research, the machine learning communityhas paid less attention to it. The impact of surveillance technologies is disproportionately borne byalready marginalized groups. Therefore, it is critical that the design of anti-surveillance technologiesbe led by those who are most affected. 3"
P128,"End-to-End Neural Discourse Deixis Resolution inDialogueAbstractWe adapt a span-based entity coreference model to the task of end-to-end discoursedeixis resolution in dialogue, specifically by proposing extensions to their modelthat exploit task-specific characteristics. The resulting model, dd-utt, achievesstate-of-the-art results on the four datasets.1 IntroductionDiscourse deixis (DD) resolution, also known as abstract anaphora resolution, is an under-investigatedtask that involves resolving a deictic anaphor to its antecedent. A deixis is a reference to a discourseentity such as a proposition, a description, an event, or a speech act. DD resolution is arguablymore challenging than the extensively-investigated entity coreference resolution task. Recall that inentity coreference, the goal is to cluster the entity mentions in narrative text or dialogue, which arecomposed of pronouns, names, and nominals, so that the mentions in each cluster refer to the samereal-world entity. Lexical overlap is a strong indicator of entity coreference, both among names (e.g.,“President Biden”, “Joe Biden”) and in the resolution of nominals (e.g., linking “the president” to“President Biden”). DD resolution, on the other hand, can be viewed as a generalized case of eventcoreference involving the clustering of deictic anaphors, which can be pronouns or nominals, andclauses, such that the mentions in each cluster refer to the same real-world proposition/event/speechact, etc. An example of DD resolution in which the deictic anaphor “the move” refers to Salomon’sact of issuing warrants on shares described in the preceding sentence. DD resolution is potentiallymore challenging than entity coreference resolution because (1) DD resolution involves understandingclause semantics, which are arguably harder to encode than noun phrase semantics; and (2) stringmatching plays little role in DD resolution, unlike in entity coreference.We focus on end-to-end DD resolution in dialogue. While the deictic anaphors in dialogue are alsocomposed of pronouns and nominals, the proportion of pronominal deictic anaphors in dialogue ismuch higher than that in narrative text. For instance, the percentage of pronominal deictic anaphorsrises to 93Since DD resolution can be cast as a generalized case of event coreference, a natural question is:how successful would a state-of-the-art entity coreference model be when applied to DD resolution?Recently, a re-implementation of a span-based entity coreference model has been applied to resolvethe deictic anaphors in the DD track after augmenting it with a type prediction model. Not onlydid they achieve the highest score on each dataset, but they beat the second-best system, which is anon-span-based neural approach combined with hand-crafted rules, by a large margin. These resultssuggest that a span-based approach to DD resolution holds promise.Our contributions are three-fold. First, we investigate whether task-specific observations can beexploited to extend a span-based model originally developed for entity coreference to improveits performance for end-to-end DD resolution in dialogue. Second, our extensions are effectivein improving model performance, allowing our model to achieve state-of-the-art results. Finally,we present an empirical analysis of our model, which, to our knowledge, is the first analysis of astate-of-the-art span-based DD resolver.Table 1: Statistics on the datasets.Total Total Total Avg. Avg. #toks Avg. Avg. Avg. Avg.#docs #sents #turns #sents per sent #turns #ana #ante #speakersper docARRAU train 552 22406 - 40.6 15.5 - 2.9 4.8 -LIGHT dev 20 908 280 45.4 12.7 14.0 3.1 4.2 2.0LIGHT test 21 923 294 44.0 12.8 14.0 3.8 4.6 2.0AMI dev 7 4139 2828 591.3 8.2 404.0 32.9 42.0 4.0AMI test 3 1967 1463 655.7 9.3 487.7 39.3 47.3 4.0Pers. dev 21 812 431 38.7 11.3 20.5 4.5 4.5 2.0Pers. test 28 1139 569 40.7 11.1 20.3 4.4 4.8 2.0Swbd. dev 11 1342 715 122.0 11.2 65.0 11.5 15.9 2.0Swbd. test 22 3652 1996 166.0 9.6 90.7 12.0 14.7 2.02 Related WorkBroadly, existing approaches to DD resolution can be divided into three categories, as describedbelow. Rule-based approaches.• Early systems that resolve deictic expressions are rule-based.Specifically, they use predefined rules to extract anaphoric mentions, and select antecedentfor each extracted anaphor based on the dialogue act types of each candidate antecedent.Non-neural learning-based approaches. Early non-neural learning-based approaches to• DD resolution use hand-crafted feature vectors to represent mentions. A classifier is thentrained to determine whether a pair of mentions is a valid antecedent-anaphor pair.Deep learning-based approaches.• Deep learning has been applied to DD resolution. Forinstance, a Siamese neural network is used, which takes as input the embeddings of twosentences, one containing a deictic anaphor and the other a candidate antecedent, to scoreeach candidate antecedent and subsequently rank the candidate antecedents based on thesescores. In addition, motivated by the recent successes of Transformer-based approachesto entity coreference, a Transformer-based approach to DD resolution has recently beenproposed, which is an end-to-end coreference system based on SpanBERT. Their modeljointly learns mention extraction and DD resolution and has achieved state-of-the-art results.3 CorporaWe use the DD-annotated corpora provided as part of the shared task. For training, we use theofficial training corpus from the shared task, ARRAU, which consists of three conversational sub-corpora (TRAINS-93, TRAINS-91, PEAR) and two non-dialogue sub-corpora (GNOME, RST).For validation and evaluation, we use the official development sets and test sets from the sharedtask. The shared task corpus is composed of four well-known conversational datasets: AMI, LIGHT,Persuasion, and Switchboard. Statistics on these corpora are provided in Table 1.4 Baseline SystemsWe employ three baseline systems.first baselineThe , coref-hoi, is a re-implementation of a widely-used end-to-end entity coreferencemodel. The model ranks all text spans of up to a predefined length based on how likely theyz P (y)correspond to entity mentions. For each top-ranked span , the model learns a distribution overy ∈ Y(z) Y(z) ϵits antecedents , where includes a dummy antecedent and every preceding span:s(z,y)eP (y) = (1)(cid:80) ′s(z,y )e′y ∈Y(z)s(x, y) s (·)where is a pairwise score that incorporates two types of scores: (1) , which indicatesms (·) s (·)how likely a span is a mention, and (2) and , which indicate how likely two spans refer toc a2s (z, ϵ) = s (z, ϵ) = 0the same entity ( for dummy antecedents):c as(z, y) = s (x) + s (y) + s (z, y) + s (z, y) (2)m m c as (·) = (g )FFNN (3)m m zTs (z, y) = g W g (4)c c yxs (z, y) = ([g , g , g ⊙ g , ϕ(x, y)])FFNN (5)a a x y x yg g x y Wwhere and are the vector representations of and , is a learned weight matrix for bilinearx y c(·) ϕ(·)scoring, FFNN is a feedforward neural network, and encodes features. Two features are used,one encoding speaker information and the other the segment distance between two spans.second baselineThe , UTD_NLP, is the top-performing system in the DD track of the shared task.It extends coref-hoi with a set of modifications. Two of the most important modifications are:ϕ(·)(1) the addition of a sentence distance feature to , and (2) the incorporation into coref-hoiia type prediction model, which predicts the type of a span. The possible types of a span are:i iANTECEDENT (if corresponds to an antecedent), ANAPHOR (if corresponds to an anaphor),and NULL (if it is neither an antecedent nor an anaphor). The types predicted by the model are thenused by coref-hoi as follows: only spans predicted as ANAPHOR can be resolved, and they can onlybe resolved to spans predicted as ANTECEDENT.third baselineThe , coref-hoi-utt, is essentially the first baseline except that we restrict the candidateantecedents to be utterances. This restriction is motivated by the observation that the antecedents ofthe deictic anaphors in the datasets are all utterances.5 ModelNext, we describe our resolver, dd-utt, which augments coref-hoi-utt with 10 extensions.E1. Modeling recency. Unlike in entity coreference, where two coreferent names (e.g., “Joe Biden”,“President Biden”) can be far apart from each other in the corresponding document (because namesare non-anaphoric), the distance between a deictic anaphor and its antecedent is comparatively smaller.To model recency, we restrict the set of candidate antecedents of an anaphor to be the utterancecontaining the anaphor as well as the preceding 10 utterances, the choice of which is based on ourobservation of the development data, where the 10 closest utterances already cover 96–99% of theantecedent-anaphor pairs.E2. Modeling distance. While the previous extension allows us to restrict our attention to candidateantecedents that are close to the anaphor, it does not model the fact that the likelihood of beingthe correct antecedent tends to increase as its distance from the anaphor decreases. To model thisγ (x, y) s(x, y) (x, y)relationship, we subtract the term Dist from (see Equation (1)), where Dist is1 x y γthe utterance distance between anaphor and candidate antecedent and is a tunable parameter1 s(x, y)that controls the importance of utterance distance in the resolution process. Since is used tos(x, y)rank candidate antecedents, modeling utterance distance by updating will allow distance tohave a direct impact on DD resolution.E3. Modeling candidate antecedent length. Some utterances are pragmatic in nature and do notconvey any important information. Therefore, they cannot serve as antecedents of deictic anaphors.Examples include “Umm”, “Ahhhh... okay”, “that’s right”, and “I agree”. Ideally, the model canidentify such utterances and prevent them from being selected as antecedents. We hypothesize thatwe could help the model by modeling such utterances. To do so, we observe that such utterancestend to be short and model them by penalizing shorter utterances. Specifically, we subtract the term1γ s(x, y) (y) y γfrom , where Length is the number of words in candidate antecedent and2 2(y)Lengthis a tunable parameter that controls the importance of candidate antecedent length in resolution.E4. Extracting candidate anaphors. As mentioned before, the deictic anaphors in dialogue arelargely composed of pronouns. Specifically, in our development sets, the three pronouns “that”,“this”, and ‘it’ alone account for 74–88% of the anaphors. Consequently, we extract candidate deicticnanaphors as follows: instead of allowing each span of length or less to be a candidate anaphor, weonly allow a span to be a candidate anaphor if its underlying word/phrase has appeared at least oncein the training set as a deictic anaphor. 3E5. Predicting anaphors. Now that we have the candidate anaphors, our next extension involvespredicting which of them are indeed deictic anaphors. To do so, we retrain the type prediction modelgin UTD_NLP, which is a FFNN that takes as input the (contextualized) span representation ofii ocandidate anaphor and outputs a vector of dimension 2 in which the first element denotes thetii ilikelihood that is a deictic anaphor and the second element denotes the likelihood that is not ai odeictic anaphor. is predicted as a deictic anaphor if and only if the value of the first element of istibigger than its second value: o = (g )FFNN (6)ti it = arg max o (x) (7)i tix∈{A,NA}where A (ANAPHOR) and NA (NON-ANAPHOR) are the two possible types. Following UTD_NLP,this type prediction model is jointly trained with the resolution model. Specifically, we compute theo λcross-entropy loss using , multiply it by a type loss coefficient , and add it to the loss function oftiλcoref-hoi-utt. is a tunable parameter that controls the importance of type prediction relative to DDresolution.E6. Modeling the relationship between anaphor recognition and resolution. In principle, themodel should resolve a candidate anaphor to a non-dummy candidate antecedent if it is predictedto be a deictic anaphor by the type prediction model. However, type prediction is not perfect, andenforcing this consistency constraint, which we will refer to as C1, will allow errors in type predictionto be propagated to DD resolution. For example, if a non-deictic anaphor is misclassified by the typeprediction model, then it will be (incorrectly) resolved to a non-dummy antecedent. To alleviateperror propagation, we instead enforce C1 in a soft manner. To do so, we define a penalty function ,1iwhich imposes a penalty on span if C1 is violated (i.e., a deictic anaphor is resolved to the dummyantecedent), as shown below:(cid:26)0 arg max s(i, y) = ϵ t =if and NAy∈Y ip (i) = (8)1 o (A) − o (N A) otherwiseti tip iIntuitively, estimates the minimum amount to be adjusted so that span ’s type is not ANAPHOR.1 p sWe incorporate into the model as a penalty term in (Equation (1)). Specifically, we redefineis(i, ϵ) as shown below: s(i, ϵ) = s(i, ϵ) − [γ p (i)] (9)3 1γ γwhere is a positive constant that controls the hardness of C1. The smaller is, the softer C1 is.3 3s(i, ϵ)Intuitively, if C1 is violated, will be lowered by the penalty term, and the dummy antecedentiwill less likely be selected as the antecedent of .E7. Modeling the relationship between non-anaphor recognition and resolution. Anotherconsistency constraint that should be enforced is that the model should resolve a candidate anaphorto the dummy antecedent if it is predicted as a non-deictic anaphor by the type prediction model. Asin Extension E6, we will enforce this constraint, which we will refer to as C2, in a soft manner bypdefining a penalty function , as shown below:2(cid:26)o (N A) − o (A) arg max s(i, y) ̸= ϵ t =if and NAti ti y∈Y ip (i) = (10)2 0 otherwises(i, j) j ̸= ϵThen we redefine when as follows:s(i, j) = s(i, j) − [γ p (i)] (11)4 2γ s(i, j)where is a positive constant that controls the hardness of C2. Intuitively, if C2 is violated,4 j iwill be lowered by the penalty term, and will less likely be selected as the antecedent of .s(x, y)E8. Encoding candidate anaphor context. Examining Equation (1), we see that is computedx ybased on the span representations of and . While these span representations are contextualized,the contextual information they encode is arguably limited. As noted before, most of the deicticanaphors in dialogue are pronouns, which are semantically empty. As a result, we hypothesize thatwe could improve the resolution of these deictic anaphors if we explicitly modeled their contexts.Specifically, we represent the context of a candidate anaphor using the embedding of the utterance ins (x, y)which it appears and add the resulting embedding as features to both the bilinear score andcs (x, y)the concatenation-based score :a T Ts (x, y) = g W g + g W g (12)c c y a yx ss (x, y) = ([g , g , g ⊙ g , g , ϕ(x, y)])FFNN (13)a a x y x y s4Table 2: Lists of filtered words.Filling wordsyeah, okay, ok, uh, right, so, hmm, well, um, oh, mm,yep, hi, ah, whoops, alright, shhhh, yes, ay, hello,aww, alas, ye, aye, uh-huh, huh, wow, www, no, and,but, again, wonderful, exactly, absolutely, actually, surethanks, awesome, gosh, ooopsReporting verbscommand, mention, demand, request, reveal, believe,guarantee, guess, insist, complain, doubt, estimate,warn, learn, realise, persuade, propose, announce,advise, imagine, boast, suggest, remember, claim,describe, see, understand, discover, answer, wonder,recommend, beg, prefer, suppose, comment, think,argue, consider, swear, ask, agree, explain, report,know, tell, decide, discuss, repeat, invite, reply,expect, forget, add, fear, hope, say, feel, observe,remark, confirm, threaten, teach, forbid, admit,promise, deny, state, mean, instructW W g swhere and are learned weight matrices, is the embedding of the utterance in whichc a sx ϕ(x, y) x ycandidate anaphor appears, and encodes the relationship between and as features.E9. Encoding the relationship between candidate anaphors and antecedents. As noted inϕ(x, y) xExtension E8, encodes the relationship between candidate anaphor and candidate antecedenty ϕ(x, y). In UTD_NLP, is composed of three features, including two features from coref-hoi-uttx y(i.e., the speaker id and the segment distance between and ) and one feature that encodes theutterance distance between them. Similar to the previous extension, we hypothesize that we couldx ybetter encode the relationship between and using additional features. Specifically, we incorporateϕ(x, y) x yan additional feature into that encodes the utterance distance between and . Unlike the oneused in UTD_NLP, this feature aims to more accurately capture proximity by ignoring unimportantsentences (i.e., those that contain only interjections, filling words, reporting verbs, and punctuation)when computing utterance distance. The complete list of filling words and reporting verbs that wefilter can be found in Table 2.E10. Encoding candidate antecedents. In coref-hoi-utt, a candidate antecedent is simply encodedusing its span representation. We hypothesize that we could better encode a candidate antecedentyusing additional features. Specifically, we employ seven features to encode a candidate antecedentϕ(x, y) y yand incorporate them into : (1) the number of words in ; (2) the number of nouns in ; (3)y ythe number of verbs in ; (4) the number of adjectives in ; (5) the number of content word overlapsybetween and the portion of the utterance containing the anaphor that precedes the anaphor; (6)y ywhether is the longest among the candidate antecedents; and (7) whether has the largest number ofcontent word overlap (as computed in Feature #5) among the candidate antecedents. Like ExtensionE3, some features implicitly encode the length of a candidate antecedent. Despite this redundancy,we believe the redundant information could be exploited by the model differently and may thereforehave varying degrees of impact on it.6 Evaluation6.1 Experimental SetupEvaluation metrics. We obtain the results of DD resolution using the Universal Anaphora Scorer.Since DD resolution is viewed as a generalized case of event coreference, the scorer reports perfor-mance in terms of CoNLL score, which is the unweighted average of the F-scores of three coreference3scoring metrics, namely MUC, B , and CEAFe. In addition, we report the results of deictic anaphorrecognition. We express recognition results in terms of Precision (P), Recall (R) and F-score, con-5Table 3: Resolution and recognition results on the four test sets.Resolution RecognitionLIGHT AMI Pers. Swbd. Avg. LIGHT AMI Pers. Swbd. Avg.UTD_NLP 42.7 35.4 39.6 35.4 38.3 70.1 61.0 69.9 68.1 67.3coref-hoi 42.7 30.7 49.7 35.4 39.6 70.9 49.3 67.8 61.9 62.5coref-hoi-utt 42.3 35.0 53.3 34.1 41.2 70.3 52.4 71.0 60.6 63.648.5dd-utt 48.2 43.5 54.9 47.2 71.3 56.9 71.4 65.2 66.2Table 4: Parameter values enabling dd-utt to achieve the best CoNLL score on each development set.LIGHT AMI Pers. Swbd.λType loss coef. 800 800 800 800γ 1 1 1 11γ 1 1 1 12γ 5 10 10 53γ 5 5 5 54sidering an anaphor correctly recognized if it has an exact match with a gold anaphor in terms ofboundary.Model training and parameter tuning. For coref-hoi and coref-hoi-utt, we use SpanBERTLarge asthe encoder and reuse the hyperparameters with the only exception of the maximum span width: forcoref-hoi, we increase the maximum span width from 30 to 45 in order to cover more than 97% ofthe antecedent spans; coref-hoi-utt we use 15 as the maximum span width, which covers more than99% of the anaphor spans in the training sets. For UTD_NLP, we simply take the outputs producedby the model on the test sets and report the results obtained by running the scorer on the outputs. Fordd-utt, we use SpanBERTLarge as the encoder. Since we do not rely on span enumerate to generatecandidate spans, the maximum span width can be set to any arbitrary number that is large enoughto cover all candidate antecedents and anaphors. In our case, we use 300 as our maximum spanλ, γ , γ , γ , γwidth. We tune the parameters (i.e., ) using grid search to maximize CoNLL score on1 2 3 4development data. For the type loss coefficient, we search out of {0.2, 0.5, 1, 200, 500, 800, 1200,γ1600}, and for , we search out of {1, 5, 10}. −51 × 10All models are trained for 30 epochs with a dropout rate of 0.3 and early stopping. We use−43 × 10as our BERT learning rate and as our task learning rate. Each experiment is run using arandom seed of 11 and takes less than three hours to train on an NVIDIA RTX A6000 48GB.Train-dev partition. Since we have four test sets, we use ARRAU and all dev sets other thanthe one to be evaluated on for model training and the remaining dev set for parameter tuning. Forexample, when evaluating on AMItest, we train models on ARRAU, LIGHTdev, Persuasiondev andSwitchboarddev and use AMIdev for tuning.6.2 ResultsRecall that our goal is to perform end-to-end DD resolution, which corresponds to the Predictedevaluation setting in the shared task.Overall performance. Recognition results (expressed in F-score) and resolution results (expressedin CoNLL score) of the three baselines and our model on the four test sets are shown in Table 3,where the Avg. columns report the macro-averages of the corresponding results on the four testsets, and the parameter settings that enable our model to achieve the highest CoNLL scores on thedevelopment sets are shown in Table 4. Since coref-hoi and coref-hoi-utt do not explicitly identifydeictic anaphors, we assume that all but the first mentions in each output cluster are anaphors whencomputing recognition precision; and while UTD_NLP (the top-performing system in the sharedtask) does recognize anaphors, we still make the same assumption when computing its recognitionprecision since the anaphors are not explicitly marked in the output (recall that we computed resultsof UTD_NLP based on its outputs). 6We test the statistical significance among the four models using two-tailed Approximate Random-ization. For recognition, the models are statistically indistinguishable from each other w.r.t. theirp < 0.05Avg. score ( ). For resolution, dd-utt is highly significantly better than the baselines w.r.t.p < 0.001Avg. ( ), while the three baselines are statistically indistinguishable from each other. Theseresults suggest that (1) dd-utt’s superior resolution performance stems from better antecedent selec-tion, not better anaphor recognition; and (2) the restriction of candidate antecedents to utterances incoref-hoi-utt does not enable the resolver to yield significantly better resolution results than coref-hoi.Per-anaphor results. Next, we show the recognition and resolution results of the four models on themost frequently occurring deictic anaphors in Table 5 after micro-averaging them over the four testsets. Not surprisingly, “that” is the most frequent deictic anaphor on the test sets, appearing as ananaphor 402 times on the test sets and contributing to 68.8% of the anaphors. This is followed by “it”(16.3%) and “this” (4.3%). Only 8.9% of the anaphors are not among the top four anaphors.Consider first the recognition results. As can be seen, “that” has the highest recognition F-scoreamong the top anaphors. This is perhaps not surprising given the comparatively larger number of“that” examples the models are trained on. While “it” occurs more frequently than “this” as a deicticanaphor, its recognition performance is lower than that of “this”. This is not surprising either: “this”,when used as a pronoun, is more likely to be deictic than “it”, although both of them can serve asa coreference anaphor and a bridging anaphor. In other words, it is comparatively more difficult todetermine whether a particular occurrence of “it” is deictic. Overall, UTD_NLP recognizes moreanaphors than the other models.Next, consider the resolution results. To obtain the CoNLL scores for a given anaphor, we retain alland only those clusters containing the anaphor in both the gold partition and the system partition andapply the official scorer to them. Generally, the more frequently occurring an anaphor is, the betterits resolution performance is. Interestingly, for the “Others” category, dd-utt achieves the highestresolution results despite having the lowest recognition performance. In contrast, while UTD_NLPachieves the best recognition performance on average, its resolution results are among the worst.Results of the four resolvers ( , coref-hoi, coref-hoi-utt, and dd-utt) on the CODI-CRACUTD_NLP2021 shared task test sets in terms of MUC, B3, and CEAFe scores are reported in Table. Theirmention extraction results in terms of recall (R), precision (P), and F-score (F) are provided in Table.dd-utt achieves the best CoNLL scores on all four datasets, via achieving the best MUC, B3, andCEAFe F-scores. In terms of MUC F-score, the performance difference between dd-utt and thesecond best resolver on each dataset is substantial (2.2%-14.9% points). These results suggest thatbetter link identification, which is what the MUC F- score reveals, is the primary reason for thesuperior performance of dd-utt. Moreover, Persuasion appears to be the easiest of the four datasets,as this is the dataset on which three of the four resolvers achieved the highest CoNLL scores. Notethat Persuasion is also the dataset on which the differences in CoNLL score between dd-utt and theother resolvers are the smallest. These results seem to suggest that the performance gap betweendd-utt and the other resolvers tends to widen as the difficulty of a dataset increases.In terms of anaphor extraction results in Table, dd-utt lags behind on two datasets, AMIUTD_NLPand Switchboard, in terms of F-score. Nevertheless, the anaphor extraction precision achieved bydd-utt is often one of the highest in each dataset.7 Further AnalysisAn example is analyzed. In this example, dd-utt successfully extracts the anaphor ""that"" and resolvesit to the correct antecedent, ""Losing one decimal place, that is okay"". fails to extract ""that""UTD_NLPas a deictic anaphor. While coref-hoi correctly extracts the anaphor, it incorrectly selects ""You wantyour rating to be a two?"" as the antecedent. From a cursory look at this example, one could infer thatthis candidate antecedent is highly unlikely to be the correct antecedent since it is 10 utterances awayfrom the anaphor. As for coref-hoi-utt, the resolver successfully extracts the anaphor but incorrectlyselects ""Its just two point five for that one"" as the antecedent, which, like the antecedent chosen bycoref-hoi, is farther away from the anaphor than the correct antecedent. Coref-hoi and coref-hoi-uttfail to identify the correct antecedent because they do not explicitly model distance and therefore maynot have an idea about how far a candidate antecedent is from the anaphor under consideration. The7Table 5: Resolution results on the test sets.MUC B3 CEAFe CoNLLP R F P R F P R FLIGHTUTD_NLP 44.6 31.3 36.8 56.2 37.0 44.6 55.3 40.5 46.7 42.7coref-hoi 37.2 36.3 36.7 48.9 42.0 45.2 58.2 38.5 46.3 42.7coref-hoi-utt 36.5 37.6 37.6 46.7 42.3 44.4 55.3 38.0 45.0 42.3dd-utt 52.4 41.3 46.2 62.0 41.6 49.8 69.0 37.6 48.7 48.2AMIUTD_NLP 45.5 21.2 28.9 52.4 29.5 37.8 44.9 35.1 39.4 35.4coref-hoi 21.7 30.5 25.4 28.7 36.3 32.1 39.0 31.0 34.6 30.7coref-hoi-utt 25.5 33.1 28.8 34.6 39.0 36.7 43.4 36.1 39.4 35.0dd-utt 41.2 39.8 40.5 48.9 42.8 45.6 54.4 37.5 44.4 43.5PersuasionUTD_NLP 45.5 20.3 28.1 65.0 30.2 41.2 61.0 41.8 49.6 39.6coref-hoi 48.6 42.3 45.2 57.5 45.9 51.1 66.2 44.0 52.9 49.7coref-hoi-utt 50.0 49.6 49.8 56.8 51.7 54.1 64.4 49.4 55.9 53.3dd-utt 56.7 48.0 52.0 63.8 49.9 56.0 72.1 46.9 56.8 54.9SwitchboardUTD_NLP 35.2 21.3 26.5 52.3 30.4 38.5 50.5 34.9 41.3 35.4coref-hoi 31.5 30.4 31.0 40.9 34.0 37.1 51.4 30.2 38.0 35.4coref-hoi-utt 30.6 29.3 29.9 39.5 32.7 35.8 49.5 29.2 36.7 34.1dd-utt 46.3 43.4 44.8 54.9 44.5 49.2 63.4 38.3 47.7 47.2additional features that dd-utt has access to, including those that encode sentence distance as well asthose that capture contextual information, may have helped dd-utt choose the correct antecedent.A: You want your rating to be a two?A: Is that what you’re saying?B: Yeah, I just got it the other way.B: Uh in Yep, I just gotA: Okay.A: So, I’ll work out the average for that again at the end.A: It’s very slightly altered. Okay, and we’re just waiting for your rating.B: two point fiveC: Its just two point five for that one.A: Two point five, okay.D: Yeah.A: Losing one decimal place, that is okay.8 Error AnalysisDD anaphora recognition precision errors. A common type of recognition precision errors involvesmisclassifying a coreference anaphor as a deictic anaphor. Consider the first example in Figure 2, inwhich the pronoun ""that"" is a coreference anaphor with ""voice recognition"" as its antecedent but ismisclassified as a deictic anaphor with the whole sentence as its antecedent. This type of error occursbecause virtually all of the frequently occurring deictic anaphors, including ""that"", ""it"", ""this"", and""which"", appear as a coreference anaphor in some contexts and as a deictic anaphor in other contexts,and distinguishing between the two different uses of these anaphors could be challenging.DD anaphor recognition recall errors. Consider the second example in Figure 2, in which ""it"" is adeictic anaphor that refers to the boldfaced utterance, but dd-utt fails to identify this and many otheroccurrences of ""it"" as deictic, probably because ""it"" is more likely to be a coreference anaphor than adeictic anaphor: in the dev sets, 80% of the occurrences of ""it"" are coreference anaphors while only5% are deictic anaphors.DD resolution precision errors. A major source of DD resolution precision errors can be attributed8Table 6: Mention extraction results on the test sets.LIGHT AMI PersuasionP R F P R F P R FOverallUTD_NLP 65.2 46.9 54.6 60.2 39.1 47.4 72.3 41.6 52.8coref-hoi 62.9 49.5 55.4 40.5 42.7 41.5 68.6 52.0 59.2coref-hoi-utt 59.3 50.0 54.2 43.9 45.2 44.5 66.2 57.6 61.6dd-utt 72.6 46.9 57.0 57.8 46.6 51.6 73.9 54.7 62.8AnaphorUTD_NLP 71.4 68.8 70.1 58.0 64.4 61.0 76.7 64.2 69.9coref-hoi 71.8 70.0 70.9 42.2 59.3 49.3 72.9 63.4 67.8coref-hoi-utt 68.2 72.5 70.3 46.4 60.2 52.4 71.3 70.7 71.0dd-utt 81.0 63.8 71.3 57.9 55.9 56.9 77.9 65.9 71.4AntecedentUTD_NLP 50.8 27.7 35.8 66.0 20.5 31.3 59.6 21.2 31.3coref-hoi 52.7 34.8 41.9 38.3 30.4 33.9 63.9 42.5 51.0coref-hoi-utt 49.4 33.9 40.2 41.0 34.2 37.3 60.7 46.6 52.7dd-utt 63.9 34.8 45.1 57.7 39.8 47.1 69.5 45.2 54.8SwitchboardP R FOverallUTD_NLP 64.4 42.2 51.0coref-hoi 55.3 41.2 47.2coref-hoi-utt 53.3 39.6 45.5dd-utt 66.9 49.6 57.0AnaphorUTD_NLP 65.7 70.7 68.1coref-hoi 63.0 60.8 61.9coref-hoi-utt 61.9 59.3 60.6dd-utt 67.5 63.1 65.2AntecedentUTD_NLP 60.8 21.5 31.7coref-hoi 46.3 27.2 34.3coref-hoi-utt 43.3 25.5 32.1dd-utt 66.2 40.0 49.8to the model’s failure in properly understanding the context in which a deictic anaphor appears.Consider the third example in Figure 2, in which ""that"" is a deictic anaphor that refers to the boldfacedutterance. While dd-utt correctly identifies ""that"" as a deictic anaphor, it erroneously posits theitalicized utterance as its antecedent. This example is interesting in that without looking at theboldfaced utterance, the italicized utterance is a plausible antecedent for ""that"" because ""I am notsurprised to hear that at all"" can be used as a response to almost every statement. However, whenboth the boldfaced utterance and the italicized utterance are taken into consideration, it is clear thatthe boldfaced utterance is the correct antecedent for ""that"" because winning over seven awards forsome charitable work is certainly more surprising than seeing a place bring awareness to the needs ofthe young. Correctly resolving this anaphor, however, requires modeling the emotional implication ofits context.A: The design should minimize R_S_I and be easy to locate and we were still slightly ambivalent asto whether to use voice recognition there, though that did seem to be the favored strategy, but therewas also, on the sideline, the thought of maybe having a beeper function.A: Sounds like a blessed organization.B: Yes, it does.A: Did you know they’ve won over 7 different awards for their charitable work?9A: As a former foster kid, it makes me happy to see this place bring such awareness to the issues andneeds of our young.B: I am not surprised to hear that at all.9 ConclusionAn end-to-end discourse deixis resolution model that augments Lee et al.’s (2018) span-based entitycoreference model with 10 extensions is presented. The resulting model achieved state-of- the-artresults on the CODI-CRAC 2021 datasets. 10"
P131,"Enhancing Disentanglement through LearnedAggregation of Convolutional Feature Maps: A Studyon the 2019 Disentanglement ChallengeAbstractThis paper details our submission for stage 2 of the 2019 disentanglement challenge.It introduces a straightforward image preprocessing technique for discovering dis-entangled latent factors. Our approach involves training a variational autoencoderusing aggregated feature maps. These maps are obtained from networks that werepretrained on the ImageNet database, and we leverage the implicit inductive biaspresent in those features for disentanglement. This bias can be further strengthenedby fine-tuning the feature maps with auxiliary tasks such as angle, position estima-tion, or color classification. Our method achieved second place in stage 2 of thecompetition. Code is publicly available.1 IntroductionMethods that are fully unsupervised are unable to learn disentangled representations unless furtherassumptions are made through inductive biases on both the model and the data. In our submission, weutilize the implicit inductive bias included in models pretrained on the ImageNet database, and thenimprove it by fine-tuning such models on tasks that are relevant to the challenge such as angle, positionestimation, or color classification. Our stage 2 submission builds upon our stage 1 submission, inwhich we used pretrained CNNs to extract convolutional feature maps as a preprocessing step beforetraining a VAE. Although this approach provided adequate disentanglement scores, two weaknesseswere identified with the feature vectors that were extracted. First, the feature extraction networkis trained on ImageNet, which is dissimilar to the MPI3d dataset that was used in the challenge.Secondly, the mechanism for feature aggregation was chosen in an ad-hoc way, and likely did notretain all information needed for disentanglement. We address these issues by fine-tuning the featureextraction network as well as by learning how to aggregate feature maps from data by using the labelsof the simulation datasets MPI3d-toy and MPI3d-realistic.2 MethodOur method includes three steps: (1) a supervised fine-tuning of the feature extraction CNN, (2)extracting a feature vector from each image in the dataset using the fine-tuned network, and (3)training a VAE to reconstruct the feature vectors and disentangle the latent factors of variation.2.1 Finetuning the Feature Extraction NetworkIn this step, we fine-tune the feature extraction network offline, before submitting to the evaluationserver. The aim is to adapt the network so that it produces aggregated feature vectors that retain thenecessary information for disentangling the latent factors of the MPI3d-real dataset. The network isfine-tuned by learning to predict the value of each latent factor using the aggregated feature vector ofan image. To do so, we use the simulation datasets MPI3d-toy and MPI3d-realistic, specifically theimages as inputs and the labels as supervised classification targets..For the feature extraction network, we use the VGG19-BN architecture from the torchvision package.The input images are standardized using mean and variance across each channel as computed fromthe ImageNet dataset. We use the output feature maps from the last layer before the final averagepooling (dimensionality 512 x 2 x 2) as the input to a feature aggregation module which reducesthe feature map to a 512-dimensional vector. The aggregation module consists of three convolutionlayers using 1024, 2048, and 512 feature maps and kernel sizes of 1, 2, and 1 respectively. Each layeris followed by batch normalization and ReLU activation. We also utilize layerwise dropout with arate of 0.1 before each convolution layer. Finally, the aggregated feature vector is L2-normalized.This was empirically found to be important for the resulting disentanglement performance. Then, foreach latent factor, we add a linear classification layer that computes the logits of each class using theaggregated feature vector. These linear layers are discarded after this step.We use both MPI3d-toy and MPI3d-realistic for training to push the network to learn features thatidentify latent factors in a robust way, regardless of details such as reflections or specific textures. Wesplit each dataset randomly with 802.2 Feature Map Extraction and AggregationIn this step, we use the fine-tuned feature extraction network to produce a set of aggregated featurevectors. We simply run the network on each image of the dataset and store the aggregated 512-dimensional vectors in memory. Again, inputs to the feature extractor are standardized such that meanand variance across each channel correspond to the respective values from the ImageNet dataset.2.3 VAE Training βFinally, we train a standard -VAE on the set of aggregated feature vectors. The encoder networkconsists of a single fully connected layer with 4096 neurons, followed by two fully-connected layersNthat parameterize the means and log variances of a normal distribution used as the approximateq(z|x)posterior . The number of latent factors is determined experimentally. The decoder networkhas four fully-connected layers with 4096 neurons each, followed by a fully-connected layer parame-N p(x|z)terizing the means of a normal distribution used as the conditional likelihood . The mean isconstrained to the range [0, 1] using the sigmoid activation. All fully connected layers except for thefinal ones use batch normalization and are followed by ReLU activation functions. We use orthogonalp(z)initialization for all layers and assume a factorized standard normal distribution as the prior onthe latent variables. β = 0.999 β = 0.9For optimization, we use the RAdam optimizer with a learning rate of 0.001, ,0 1and a batch size of 256. The VAE is trained for 120 epochs by maximizing the evidence lower bound,which is equivalent to minimizing(cid:80)(cid:80)512 C1 2 2 2 21 + log(σ ) − µ − σ||µ − x || + 0.5βi i j j ji=1 j=1B βwhere is a hyperparameter to balance the MSE reconstruction and the KLD penalty term. Becauseβthe scale of the KLD term depends on the number of latent factors C, we normalize it by C such thatcan be varied independently of C. It can be harmful to start training with too much weight on the KLDβ β = 0.005term. Therefore, we use the following cosine schedule to smoothly anneal from tostartβ = 0.4 over the course of training:endβ(t) = { β f ort < tstartstart t−t1 (β − β )(1 + cos(π )) + β f ort ≤ t ≤ tstartend start start start end2 t −tstartendβ f ort > tend endβ(t) β t ∈ 0, ..., N − 1where is the value for in training episode , and annealing runs from epocht = 10 t = 79to epoch . This schedule allows the model to initially learn to reconstructstart endthe data well, and only then puts pressure on the latent variables to be factorized, which improvedperformance. 23 DiscussionOur method achieved second place in stage 2 of the competition. Compared to our stage 1 approach,our stage 2 approach resulted in large improvements on the FactorVAE and DCI metrics. On thepublic leaderboard, our best submission achieved first rank on these metrics. See appendix A forfurther discussion of the results.Introducing prior knowledge makes the disentanglement task considerably easier, and this is reflectedin the improved scores. However, our method uses task-specific supervision obtained from simulation,which restricts its applicability. Nevertheless, this demonstrates that such supervision can transfer tobetter disentanglement on real-world data, which was a goal of the challenge.3"
P133,"Discontinuous Constituent Parsing as SequenceLabelingAbstractThis paper reduces discontinuous parsing to sequence labeling. It first shows thatexisting reductions for constituent parsing as labeling do not support discontinuities.Second, it fills this gap and proposes to encode tree discontinuities as nearly orderedpermutations of the input sequence. Third, it studies whether such discontinuousrepresentations are learnable. The experiments show that despite the architecturalsimplicity, under the right representation, the models are fast and accurate.1 IntroductionDiscontinuous constituent parsing studies how to generate phrase-structure trees of sentences comingfrom non-configurational languages, where non-consecutive tokens can be part of the same grammati-cal function (e.g. nonconsecutive terms belonging to the same verb phrase). Figure 1 shows a Germansentence exhibiting this phenomenon. Discontinuities happen in languages that exhibit free wordorder such as German or Guugu Yimidhirr, but also in those with high rigidity, e.g. English, whosegrammar allows certain discontinuous expressions, such as wh-movement or extraposition. Thismakes discontinuous parsing a core computational linguistics problem that affects a wide spectrumof languages.There are different paradigms for discontinuous phrase-structure parsing, such as chart-based parsers,transitionbased algorithms or reductions to a problem of a different nature, such as dependencyparsing. However, many of these approaches come either at a high complexity or lowspeed, while others give up significant performance to achieve an acceptable latency.Related to these research aspects, this work explores the feasibility of discontinuous parsing underthe sequence labeling paradigm, inspired by work on fast and simple continuous constituent parsing.We will focus on tackling the limitations of their encoding functions when it comes to analyzingdiscontinuous structures, and include an empirical comparison against existing parsers.Contribution (i) The first contribution is theoretical: to reduce constituent parsing of free word orderlanguages to a sequence labeling problem. This is done by encoding the order of the sentence as(nearly ordered) permutations. We present various ways of doing so, which can be naturally combinedwith the labels produced by existing reductions for continuous constituent parsing. (ii) The secondcontribution is a practical one: to show how these representations can be learned by neural transducers.We also shed light on whether general-purpose architectures for NLP tasks can effectively parsefree word order languages, and be used as an alternative to adhoc algorithms and architectures fordiscontinuous constituent parsing.2 Related workDiscontinuous phrase-structure trees can be derived by expressive formalisms such as MultipleContext Free Grammmars (MCFGs) or Linear Context-Free Rewriting Systems (LCFRS). MCFGsand LCFRS are essentially an extension of Context-Free Grammars (CFGs) such that non-terminalscan link to non-consecutive spans. Traditionally, chart-based parsers relying on this paradigmcommonly suffer from high complexity. Let k be the block degree, i.e. the number of nonconsecutivespans than can be attached to a single non-terminal; the complexity of applying CYK (after binarizingthe grammar) would be O(n3k), which can be improved to O(n2k+2) if the parser is restricted towell-nested LCFRS, and discusses how for a standard discontinuous treebank, k 3 (in contrast tok = 1 in CFGs). Recently, presents a chart-based parser for k = 2 that can run in O(n3), which isequivalent to the running time of a continuous chart parser, while covering 98Differently, it is possible to rely on the idea that discontinuities are inherently related to the locationof the token in the sentence. In this sense, it is possible to reorder the tokens while still obtaining agrammatical sentence that could be parsed by a continuous algorithm. This is usually achieved withtransition-based parsing algorithms and the swap transition which switches the topmost elements inthe stack. For instance, uses this transition to adapt an easy-first strategy for dependency parsing todiscontinuous constituent parsing. In a similar vein, builds on top of a fast continuous shift-reduceconstituent parser, and incorporates both standard and bundled swap transitions in order to analyzediscontinuous constituents. system produces derivations of up to a length of n2 n + 1 given asentence of length n. More efficiently, presents a transition system which replaces swap with a gaptransition. The intuition is that a reduction does not need to be always applied locally to the twotopmost elements in the stack, and that those two items can be connected, despite the existence of agap between them, using non-local reductions. Their algorithm ensures an upper-bound of n(n1)2transitions. With a different optimization goal, removed the traditional reliance of discontinuousparsers on averaged perceptrons and hand-crafted features for a recursive neural network approachthat guides a swap-based system, with the capacity to generate contextualized representations. replacethe stack used in transition-based systems with a memory set containing the created constituents.This model allows interactions between elements that are not adjacent, without the swap transition, tocreate a new (discontinuous) constituent. Trained on a 2 stacked BiLSTM transducer, the model isguaranteed to build a tree with in 4n-2 transitions, given a sentence of length n.A middle ground between explicit constituent parsing algorithms and this paper is the work based ontransformations. For instance, convert constituent trees into a nonlinguistic dependency representationthat is learned by a transition-based dependency parser, to then map its output back to a constituent tree.A similar approach is taken by, but they proposed a more compact representation that leads to a muchreduced set of output labels. Other authors such as propose a two-step approach that approximatesdiscontinuous structure trees by parsing context-free grammars with generative probabilistic modelsand transforming them to discontinuous ones. cast discontinuous phrase-structure parsing into aframework that jointly performs supertagging and non-projective dependency parsing by a reductionto the Generalized Maximum Spanning Arborescence problem. The recent work by can be alsoframed within this paradigm. They essentially adapt the work by and replace the averaged perceptronclassifier with pointer networks, adressingIn this context, the closest work to ours is the reduction proposed by, who cast continuous constituentparsing as sequence labeling. In the next sections we build on top of their work and: (i) analyze whytheir approach cannot handle discontinuous phrases, (ii) extend it to handle such phenomena, and (iii)train functional sequence labeling discontinuous parsers.3 PreliminariesLet w = [w0, w1, ..., w|w|1] be an input sequence of tokens, and T|w| the set of (continuous)→constituent trees for sequences of length |w|; define an encoding function : T|w| L|w| to mapcontinuous constituent trees into a sequence of labels of the same length as the input. Each label, liL, is composed of three components li = (ni, xi, ui):• ni encodes the number of levels in the tree in common between a word wi and wi+1. To obtain amanageable output vocabulary space, ni is actually encoded as the difference ni ni1, with n1 = 0. Wedenote by abs(ni) the absolute number of levels represented by ni. i.e. the total levels in commonshared between a word and its next one.• xi represents the lowest non-terminal symbol shared between wi and wi+1 at level abs(ni).• ui encodes a leaf unary chain, i.e. nonterminals that belong only to the path from the terminal wi tothe root. Note that cannot encode this information in (ni, xi), as these components always representcommon information between wi and wi+1. 2Incompleteness for discontinuous phrase structures proved that is complete and injective for continu-ous trees. However, it is easy to prove that its validity does not extend to discontinuous trees, by usinga counterexample. Figure 3 shows a minimal discontinuous tree that cannot be correctly decoded.The inability to encode discontinuities lies on the assumption that wi+1 will always be attached to anode belonging to the path from the root to wi (ni is then used to specify the location of that node inthe path). This is always true in continuous trees, but not in discontinuous trees, as can be seen inFigure 3 where c is the child of a constituent that does not lie in the path from S to b.4 Encoding nearly ordered permutationsNext, we fill this gap to address discontinuous parsing as sequence labeling. We will extend theencoding to the set of discontinuous constituent trees, which we will call T|w|. The key to do thisrelies on a well-known property: a discontinuous tree t T|w| can be represented as a continuous oneusing an in-order traversal that keeps track of the original indexes (e.g. the trees at the left and theright in Figure 4). We will call this tree the (canonical) continuous arrangement of t, (t) T|w|.Thus, if given an input sentence we can generate the position of every word as a terminal in (t), theexisting encodings to predict continuous trees as sequence labeling could be applied on (t). In essence,this is learning to predict a permutation of w. As introduced in §2, the concept of location of a tokenis not a stranger in transition-based discontinuous parsing, where actions such as swap switch theposition of two elements in order to create a discontinuous phrase. We instead propose to explorehow to handle this problem in end-to-end sequence labeling fashion, without relying on any parsingstructure nor a set of transitions.τ : {0, . . . , |w| − 1} → {0, . . . , |w| − 1}Todo so, first we denote by the permutation that maps thei w w ω(t)position of a given in into its position as a terminal node in . From this, one can derivei−1τ w, a function that encodes a permutation of in such a way that its phrase structure does not have−1τ τcrossing branches. For continuous trees, and are identity permutations. Then, we extend the′ ′Φ T → L l ∈ L ptree encoding function to where is enriched with a fourth component suchi|w| |w|l = (n , x , u , p ) p pthat , where is a discrete symbol such that the sequence of ’s encodes thei i i i i iτ p τ (i) wpermutation (typically, each will be an encoding of , i.e., the position of in the continuousi iarrangement, although this need not be true in all encodings, as will be seen below).The crux of defining a viable encoding for discontinuous parsing is then in how we encode tau asa sequence of values pi, for i = 0 . . . |w| 1. While the naive approach would be the identityencoding (pi = tau(i)), we ideally want an encoding that balances minimizing sparsity (by minimizinginfrequently-used values) and maximizing learnability (by being predictable). To do so, we will lookfor encodings that take advantage of the fact that discontinuities in attested syntactic structures aremild , i.e., in most cases, tau (i + 1) = tau (i) + 1. In other words, permutations tau corresponding toreal syntactic trees tend to be nearly ordered permutations. Based on these principles, we proposebelow a set of concrete encodings, which are also depicted on an example in Figure 4. All of themhandle multiple gaps (a discontinuity inside a discontinuity) and cover 100w p = τ (i) w ̸= τ (i)Absolute-position: For every token , only if . Otherwise, we use a speciali i ilabel , which represents that the word is a fixed point in the permutation, i.e., it occupies the sameINVplace in the sentence and in the continuous arrangement.Relative-position If i != tau(i), then pi = i tau(i). otherwise, we again use the INV label.Lehmer code In combinatorics, let n = [0, ..., n 1] be a sorted sequence of objects, a Lehmer codeis a sequence sigma = [sigma0, ...sigman1] that encodes one of the n! permutations of n, namely .The idea is intuitive: let ni+1 be the subsequence of objects from n that remain available after wehave permuted the first i objects to achieve the permutation , then sigmai+1 equals the (zero-based)position in ni+1 of the next object to be selected. For instance, given n = [0, 1, 2, 3, 4] and a validpermutation = [0, 1, 3, 4, 2], then sigma = [0, 0, 1, 1, 0]. Note that the identity permutation would beencoded as a sequence of zeros.In the context of discontinuous parsing and encoding pi, n can be seen as the input sentence wwhere pi(w) is encoded by sigma. The Lehmer code is particularly suitable for this task in termsof compression, as in most of the cases we expect (nearly) ordered permutations, which translatesinto the majority of elements of sigma being zero. However, this encoding poses some potential3Label Component TIGER Labels NEGRA DPTBni 22 19 34ti 93 56 137ui 15 4 56pi as absolute-position 129 110 98pi as relative-position 105 90 87pi as Lehmer 39 34 27pi as inverse Lehmer 68 57 61pi as pointer-based 122 99* 110*pi as pointer-based simplified 81 65 83*Table 1: Number of values per label component, merging the training and dev sets (gold setup). *arecodes that generate one extra label with predicted PoS tags (this variability depends on the usedPoS-tagger). Hyperparameter ValueBiLSTM size 800# BiLSTM layers 2optimizer SGDloss cat. cross-entropylearning rate 0.2decay (linear) 0.05momentum 0.9dropout 0.5word embs Ling et al. (2015)PoS tags emb size 20character emb size 30batch size training 8training epochs 100batch size test 128Table 2: Main hyper-parameters for the training of the BiLSTMs, both for the gold and predictedsetupslearnability problems. The root of the problem is that sigmai does not necessarily encode tau(i), buttau(j) where j is the index of the word that occupies the ith position in the continuous arrangement(i.e., j = tau 1(i)). In other words, this encoding is expressed following the order of words in thecontinuous arrangement rather than the input order, causing a non-straightforward mapping betweeninput words and labels. For instance, in the previous example, sigma2 does not encode the location ofthe object n2 = 2 but that of n3 = 3.Lehmer code of the inverse permutation To ensure that each pi encodes tau(i), we instead interpretpi as meaning that should fill the (pi + 1)th currently remaining blank in a sequence sigma that is..., .F orinstance, letn = [0, 1, 2, 3, 4]beinitialized as a sequence of blanks, i.e. sigma = [,, ]Pointer-based encoding When encoding tau(i), the previous encodings generate the position for thetarget word, but they do not really take into account the left-to-right order in which sentences arenaturally read, nor they are linguistically inspired. In particular, informally speaking, in human lin-Finally, in Table 11 we list the number of parameters for each of the transducers trained on the pointer-based encoding. For the rest of the encodings, the models have a similar number of parameters, as theonly change in the architecture is the small part involving the feed-forward output layer that predictsthe label component pi.More in detail, for BiLSTMs and vanilla Trans-formers, the word embeddings are pre-trained FastText embeddings with 100 dimensions for Englishand 60 for German, and the PoS tags are represented by an embedding layer of 20 dimensions.4Hyperparameter Value (gold setup) Value (pred setup)Att. heads 8 8Att. layers 6 6Hidden size 800 800Hidden dropout 0.4 0.4optimizer SGD SGDloss Cross-entropy Cross-entropylearning rate 0.004* 0.003decay (linear) 0.0 0.0momentum 0.0 0.0word embs Previous WorksPoS tags emb size 20 20character emb size 136/132batch size training 88training epochs 400 400batch size test 128 128Table 3: Main hyper-parameters for training the Transformer encodersModel ParametersPointer-based BiLSTM 13.9 MPointer-based Transformer 23.4 MPointer-based DistilBERT 73 MPointer-based BERT base 108 MPointer-based BERT large 330 MTable 4: Number of parameters per model.Additionally we use a char-based LSTM with a hidden layer of 100/132 dimensions (English/German).For both approaches, a linear layer followed by a softmax is used to predict every label component.For BERT and DistilBERT we use the default fine-tuning parameters. We use Adam as optimizer andcross entropy as the loss function. The learning rate and other hyper-parameters are left as defaultin the transformers library, except for the number of training epochs (we train them for at most 30epochs), and the batch size, which is adjusted depending on the memory required by the model (e.g. 8for BERT and 32 for DistilBERT). For the BERT-large model, due to the limitations in GPU memory,we have to reduce the training batch size to 1, and use a smaller learning rate of 1e-5.5 ExperimentsSetup For English, we use the discontinuous Penn Treebank (DPTB) by. For German, we use TIGERand NEGRA. We use the splits by which in turn follow the splits for the NEGRA treebank, the splitsfor TIGER, and the standard splits for (D)PTB (Sections 2 to 21 for training, 22 for development and23 for testing). See also Appendix A.5 for more detailed statistics. We consider gold and predictedPoS tags. For the latter, the parsers are trained on predicted PoS tags, which are generated by a2stacked BiLSTM, with the hyper-parameters used to train the parsers. The PoS tagging accuracy (Metrics We report the F-1 labeled bracketing score for all and discontinuous constituents, usingdiscodop and the proper.prm parameter file. Model selection is based on overall bracketing F1score.5.1 ResultsTable 2 shows the results on the dev sets for all encodings and transducers. The tendency is clearshowing that the pointer-based encodings obtain the best results. The pointer-based encoding withsimplified PoS tags does not lead however to clear improvements, suggesting that the models can learnthe sparser original PoS tags set. For the rest of encodings we also observe interesting tendencies. Forinstance, when running experiments using stacked BiLSTMs, the relative encoding performs better5than the absolute one, which was somehow expected as the encoding is less sparse. However, thetendency is the opposite for the Transformer encoders (including BERT and DistilBERT), especiallyfor the case of discontinuous constituents. We hypothesize this is due to the capacity of Transformersto attend to every other word through multihead attention, which might give an advantage to encodeabsolute positions over BiLSTMs, where the whole left and right context is represented by a singlevector. With respect to the Lehmer and Lehmer of the inverse permutation encodings, the latterperforms better overall, confirming the bigger difficulties for the tested sequence labelers to learnLehmer, which in some cases has a performance even close to the naive absolute-positional encoding(e.g. for TIGER using the vanilla Transformer encoder and BERT). As introduced in §4, wehypothesize this is caused by the non-straightforward mapping between words and labels (in theLehmer code the label generated for a word does not necessarily contain information about theposition of such word in the continuous arrangement).In Table 3 we compare a selection of our models against previous work using both gold and predictedPoS tags. In particular, we include: (i) models using the pointer-based encoding, since they obtainedthe overall best performance on the dev sets, and (ii) a representative subset of encodings (the absolutepositional one and the Lehmer code of the inverse permutation) trained with the best performingtransducer. Additionally, for the case of the (English) DPTB, we also include experiments using abert-large model, to shed more light on whether the size of the networks is playing a role when itcomes to detect discontinuities. Additionally, we report speeds on CPU and GPU. The experimentsshow that the encodings are learnable, but that the model’s power makes a difference. For instance, inthe predicted setup BILSTMs and vanilla Transformers perform in line with predeep learning models, DistilBERT already achieves a robust performance, close to models such as and BERT transducerssuffice to achieve results close to some of the strongest approaches, e.g.. Yet, the results lag behindthe state of the art. With respect to the architectures that performed the best the main issue is thatthey are the bottleneck of the pipeline. Thus, the computation of the contextualized word vectorsunder current approaches greatly decreases the importance, when it comes to speed, of the chosenparsing paradigm used to generate the output trees (e.g. chart-based versus sequence labeling).Finally, Table 4 details the discontinuous performance of our best performing models.Discussion on other applications It is worth noting that while we focused on parsing as sequencelabeling, encoding syntactic trees as labels is useful to straightforwardly feed syntactic informationto downstream models, even if the trees themselves come from a non-sequence-labeling parser. Forexample, use the sequence labeling encoding of to provide syntactic information to a semantic rolelabeling model. Apart from providing fast and accurate parsers, our encodings can be used to do thesame with discontinuous syntax.6 ConclusionWe reduced discontinuous parsing to sequence labeling. The key contribution consisted in predictinga continuous tree with a rearrangement of the leaf nodes to shape discontinuities, and definingvarious ways to encode such a rearrangement as a sequence of labels associated to each word, takingadvantage of the fact that in practice they are nearly ordered permutations. We tested whether thoseencodings are learnable by neural models and saw that the choice of permutation encoding is nottrivial, and there are interactions between encodings6"
P134,"Unraveling the Enigmatic Parallels Between DNAHelical Structures and the Sonic Resonance of KazooInstruments in relation to Light Emission PatternsAbstractThe quintessential nature of DNA is intertwined with the societal implications ofcheese consumption, which in turn affects the molecular structure of refrigerators,thereby influencing the transcendental properties of Forgotten Sock Syndrome, aphenomenon wherein the disappearance of footwear is directly correlated to theharmonic convergence of platypus migration patterns and the aerodynamic proper-ties of pancakes, ultimately leading to a deeper understanding of the Flumplenookhypothesis, a theoretical framework positing that the essence of DNA is inextricablylinked to the sonorous vibrations of disco music and the average airspeed velocityof an unladen swallow. The abstract concept of DNA has profound implicationsfor the study of Interdimensional Croissant Travel and its reciprocal relationshipwith the spatial-temporal continuum of Parallel Toaster Universes. Furthermore,research has shown that the ontological status of DNA is precarious at best, suscep-tible to fluctuations in the global supply of tartan patterns and the migratory habitsof narwhals, which in turn are influenced by the telekinetic powers of capybarasand the ontological implications of Socratic dialogue. The interdisciplinary field ofDNA research has far-reaching consequences for our comprehension of QuantumFlapjack Dynamics and the sentience of household appliances.1 IntroductionThe intersection of quantum mechanics and pastry dough has led to a deeper understanding of themolecular structure of DNA, which bears a striking resemblance to the branching patterns of fungalhyphae in ecosystems dominated by giant sequoias. Meanwhile, the application of topologicalinvariants to the study of crocheted blankets has yielded surprising insights into the double helixmodel, particularly in regards to the torsional stress imposed by excessive twirling of the DNAmolecule, a phenomenon also observed in the whorls of certain seashells. Furthermore, the notionthat DNA is composed of nucleotides has been supplanted by the concept of ""flumplenooks,"" tiny,invisible particles that defy the laws of classical physics and are thought to be responsible for theencoding of genetic information, much like the indentations on a well-worn vinyl record. In a relateddevelopment, researchers have discovered that the consumption of large quantities of blueberries canalter the viscosity of DNA, allowing it to flow more easily through narrow capillaries, a propertythat has been exploited in the development of novel tattoo inks. The nascent field of ""dnatology"" hasalso shed light on the hitherto unknown relationship between DNA and the migration patterns ofmonarch butterflies, which, it turns out, are influenced by the presence of ""dnatons,"" hypotheticalparticles that interact with the DNA molecule in ways that are not yet fully understood. Additionally,the study of DNA has been informed by the science of ""flargle dynamics,"" which seeks to explain theintricate ballet of molecular interactions that govern the behavior of DNA in solution, a phenomenonthat bears a curious resemblance to the dance of subatomic particles in a high-energy collider. In asurprising twist, the use of interpretive dance as a means of analyzing DNA structure has yielded anovel understanding of the role of ""splinkle factors"" in gene regulation, which, in turn, has led to areappraisal of the importance of ""flibberdejibits"" in the transmission of genetic traits. The work ofnumerous researchers has also highlighted the significance of ""wuggle particles"" in the replicationof DNA, which are thought to play a crucial role in the unwinding of the double helix, a processthat has been likened to the unspooling of a ball of twine. Moreover, the application of ""jinkletheory"" to the study of DNA has revealed the existence of ""flamboozle waves,"" which are believedto propagate through the DNA molecule, influencing the expression of genes in ways that are stillnot fully comprehended. In a related development, the discovery of ""gromble sites"" on the DNAmolecule has opened up new avenues of research into the mechanisms of gene regulation, which,it is thought, may be influenced by the presence of ""throcklepox particles,"" hypothetical entitiesthat interact with the DNA molecule in complex and subtle ways. The field of ""dnatology"" has alsobeen influenced by the study of "" jimjim theory,"" which seeks to explain the behavior of DNA interms of the interactions between ""flibulous particles"" and ""wizzlewhacks,"" two types of particlesthat are thought to play a crucial role in the transmission of genetic information. Furthermore, theuse of ""kabloinkle analysis"" has revealed the presence of ""flazzle patterns"" in the DNA molecule,which are believed to be associated with the expression of specific genes, a phenomenon that hasbeen likened to the emergence of patterns in a kaleidoscope. The study of DNA has also beeninformed by the science of ""wumwum dynamics,"" which seeks to explain the complex interactionsbetween DNA and the surrounding environment, a phenomenon that has been likened to the danceof molecules in a gas. In a surprising twist, the application of ""flimflam theory"" to the study ofDNA has revealed the existence of ""jinkle waves,"" which are believed to propagate through the DNAmolecule, influencing the expression of genes in ways that are still not fully comprehended. The workof numerous researchers has also highlighted the significance of ""wizzle particles"" in the replicationof DNA, which are thought to play a crucial role in the unwinding of the double helix, a process thathas been likened to the unspooling of a ball of twine. Moreover, the discovery of ""gromble sites"" onthe DNA molecule has opened up new avenues of research into the mechanisms of gene regulation,which, it is thought, may be influenced by the presence of ""throcklepox particles,"" hypotheticalentities that interact with the DNA molecule in complex and subtle ways. The field of ""dnatology"" hasalso been influenced by the study of "" jimjim theory,"" which seeks to explain the behavior of DNA interms of the interactions between ""flibulous particles"" and ""wizzlewhacks,"" two types of particles thatare thought to play a crucial role in the transmission of genetic information. Additionally, the use of""kabloinkle analysis"" has revealed the presence of ""flazzle patterns"" in the DNA molecule, which arebelieved to be associated with the expression of specific genes, a phenomenon that has been likenedto the emergence of patterns in a kaleidoscope. The study of DNA has also been informed by thescience of ""wumwum dynamics,"" which seeks to explain the complex interactions between DNA andthe surrounding environment, a phenomenon that has been likened to the dance of molecules in agas. In a related development, researchers have discovered that the consumption of large quantitiesof chamomile tea can alter the topology of DNA, allowing it to form complex knots and links, aproperty that has been exploited in the development of novel cryptographic algorithms. The nascentfield of ""dnatology"" has also shed light on the hitherto unknown relationship between DNA andthe migration patterns of migratory birds, which, it turns out, are influenced by the presence of""dnatons,"" hypothetical particles that interact with the DNA molecule in ways that are not yet fullyunderstood. Furthermore, the application of ""flargle dynamics"" to the study of DNA has yielded anovel understanding of the role of ""splinkle factors"" in gene regulation, which, in turn, has led to areappraisal of the importance of ""flibberdejibits"" in the transmission of genetic traits. The work ofnumerous researchers has also highlighted the significance of ""wuggle particles"" in the replicationof DNA, which are thought to play a crucial role in the unwinding of the double helix, a processthat has been likened to the unspooling of a ball of twine. Moreover, the study of DNA has beeninformed by the science of ""jinkle theory,"" which seeks to explain the behavior of DNA in termsof the interactions between ""flibulous particles"" and ""wizzlewhacks,"" two types of particles that arethought to play a crucial role in the transmission of genetic information. The field of ""dnatology"" hasalso been influenced by the study of "" jimjim theory,"" which seeks to explain the behavior of DNA interms of the interactions between ""flibulous particles"" and ""wizzlewhacks,"" two types of particlesthat are thought to play a crucial role in the transmission of genetic information. Additionally, theuse of ""kabloinkle analysis"" has revealed the presence of ""flazzle patterns"" in the DNA molecule,which are believed to be associated with the expression of specific genes, a phenomenon that hasbeen likened to the emergence of patterns in a kaleidoscope. The study of DNA has also beeninformed by the science of ""wumwum dynamics,"" which seeks to explain the complex interactionsbetween DNA and the surrounding environment, a phenomenon that has been likened to the danceof molecules in a gas. In a surprising twist, the application of ""flimflam theory"" to the study ofDNA has revealed the existence of ""jinkle waves,"" which are believed to propagate through the DNA2molecule, influencing the expression of genes in ways that are still not fully comprehended. The workof numerous researchers has also highlighted the significance of ""wizzle particles"" in the replicationof DNA, which are thought to play a crucial role in the unwinding of the double helix, a process thathas been likened to the unspooling of a ball of twine. Moreover, the discovery of ""gromble sites"" onthe DNA molecule has opened up new avenues of research into the mechanisms of gene regulation,which, it is thought, may be influenced by the presence of ""throcklepox particles,"" hypotheticalentities that interact with the DNA molecule in complex and subtle ways. The field of ""dnatology"" hasalso been influenced by the study of "" jimjim theory,"" which seeks to explain the behavior of DNA interms of the interactions between ""flibulous particles"" and ""wizzlewhacks,"" two types of particles thatare thought to play a crucial role in the transmission of genetic information. Furthermore, the use of""kabloinkle analysis"" has revealed the presence of ""flazzle patterns"" in the DNA molecule, which arebelieved to be associated with the expression of specific genes, a phenomenon that has been likenedto the emergence of patterns in a kaleidoscope. The study of DNA has also been informed by thescience of ""wumwum dynamics,"" which seeks to explain the complex interactions between DNA andthe surrounding environment, a phenomenon that has been likened to the dance of molecules in a gas.In a related development, researchers have discovered that the consumption of large quantities of darkchocolate can alter the viscosity of DNA, allowing it to flow more easily through narrow capillaries,a property that has been exploited in the development of novel tattoo inks. The nascent field of ""dn2 Related WorkThe study of DNA has been influenced by the art of baking, where the intricate patterns of croissantshave led to a deeper understanding of the double helix structure, which in turn has inspired a newgeneration of pastry chefs to create DNA-shaped desserts, thereby establishing a direct link betweenthe molecular structure of DNA and the flakiness of croissant dough, as well as the migration patternsof butterflies in the Amazon rainforest, where the unique properties of butterfly wings have beenfound to have a profound impact on the stability of DNA molecules, particularly in the presence ofcheese, which has been shown to have a profound effect on the expression of certain genes, especiallythose related to the production of sock puppets, a phenomenon that has been observed in the dreamsof astronauts on the International Space Station, where the microgravity environment has been foundto alter the shape of DNA molecules, causing them to resemble the twisted threads of a spider’sweb, which has led to a new area of research focused on the intersection of DNA and arachnology,particularly in the context of ancient Egyptian hieroglyphics, where the depiction of spiders hasbeen found to hold the key to understanding the genetic code, and the secret to creating the perfectsoufflé, a dish that has been shown to have a profound impact on the human genome, particularly inthe context of the development of language, where the sounds of sizzling bacon have been found tohave a direct correlation with the structure of DNA, and the patterns of crop circles in rural England,which have been found to be linked to the migration patterns of wildebeests in the Serengeti, andthe flavor profiles of various types of jelly beans, which have been shown to have a direct impacton the expression of certain genes, particularly those related to the production of disco music, agenre that has been found to have a profound effect on the molecular structure of DNA, causingit to vibrate at a frequency that is directly correlated with the patterns of snowflakes in Antarctica,and the ancient art of sand sculpting, where the intricate patterns of sandcastles have been found tohold the key to understanding the genetic code, and the secret to creating the perfect paella, a dishthat has been shown to have a profound impact on the human genome, particularly in the contextof the development of mathematics, where the principles of fractal geometry have been found tohave a direct correlation with the structure of DNA, and the patterns of wind currents in the upperatmosphere, which have been found to be linked to the migration patterns of monarch butterflies, andthe flavor profiles of various types of coffee, which have been shown to have a direct impact on theexpression of certain genes, particularly those related to the production of science fiction novels, agenre that has been found to have a profound effect on the molecular structure of DNA, causing it tomutate at a rate that is directly correlated with the patterns of galaxy formation in the universe, andthe ancient art of origami, where the intricate patterns of paper folding have been found to hold thekey to understanding the genetic code, and the secret to creating the perfect chocolate mousse, a dishthat has been shown to have a profound impact on the human genome, particularly in the contextof the development of music, where the sounds of whale songs have been found to have a directcorrelation with the structure of DNA, and the patterns of weather patterns in the tropics, which havebeen found to be linked to the migration patterns of sea turtles, and the flavor profiles of various types3of tea, which have been shown to have a direct impact on the expression of certain genes, particularlythose related to the production of surrealist art, a movement that has been found to have a profoundeffect on the molecular structure of DNA, causing it to evolve at a rate that is directly correlatedwith the patterns of traffic flow in urban environments, and the ancient art of calligraphy, where theintricate patterns of lettering have been found to hold the key to understanding the genetic code, andthe secret to creating the perfect croque-monsieur, a dish that has been shown to have a profoundimpact on the human genome, particularly in the context of the development of language, where thesounds of sizzling sausages have been found to have a direct correlation with the structure of DNA,and the patterns of star formation in the universe, which have been found to be linked to the migrationpatterns of birds in the Arctic, and the flavor profiles of various types of honey, which have beenshown to have a direct impact on the expression of certain genes, particularly those related to theproduction of horror movies, a genre that has been found to have a profound effect on the molecularstructure of DNA, causing it to mutate at a rate that is directly correlated with the patterns of oceancurrents in the deep sea, and the ancient art of pottery, where the intricate patterns of ceramic designhave been found to hold the key to understanding the genetic code, and the secret to creating theperfect bouillabaisse, a dish that has been shown to have a profound impact on the human genome,particularly in the context of the development of philosophy, where the principles of existentialismhave been found to have a direct correlation with the structure of DNA, and the patterns of cloudformation in the atmosphere, which have been found to be linked to the migration patterns of whalesin the ocean, and the flavor profiles of various types of spices, which have been shown to have a directimpact on the expression of certain genes, particularly those related to the production of electronicmusic, a genre that has been found to have a profound effect on the molecular structure of DNA,causing it to vibrate at a frequency that is directly correlated with the patterns of fractal geometry innature, and the ancient art of weaving, where the intricate patterns of textile design have been foundto hold the key to understanding the genetic code, and the secret to creating the perfect falafel, a dishthat has been shown to have a profound impact on the human genome, particularly in the contextof the development of psychology, where the principles of cognitive behavioral therapy have beenfound to have a direct correlation with the structure of DNA, and the patterns of traffic flow in urbanenvironments, which have been found to be linked to the migration patterns of pigeons in cities,and the flavor profiles of various types of spices, which have been shown to have a direct impact onthe expression of certain genes, particularly those related to the production of romantic comedies, agenre that has been found to have a profound effect on the molecular structure of DNA, causing it toevolve at a rate that is directly correlated with the patterns of galaxy formation in the universe, andthe ancient art of glassblowing, where the intricate patterns of glass design have been found to holdthe key to understanding the genetic code, and the secret to creating the perfect chicken parmesan, adish that has been shown to have a profound impact on the human genome, particularly in the contextof the development of sociology, where the principles of social network analysis have been found tohave a direct correlation with the structure of DNA, and the patterns of wind currents in the upperatmosphere, which have been found to be linked to the migration patterns of monarch butterflies,and the flavor profiles of various types of cheese, which have been shown to have a direct impacton the expression of certain genes, particularly those related to the production of action movies, agenre that has been found to have a profound effect on the molecular structure of DNA, causingit to mutate at a rate that is directly correlated with the patterns of ocean currents in the deep sea,and the ancient art of metalworking, where the intricate patterns of metal design have been foundto hold the key to understanding the genetic code, and the secret to creating the perfect beef stew,a dish that has been shown to have a profound impact on the human genome, particularly in thecontext of the development of anthropology, where the principles of cultural relativism have beenfound to have a direct correlation with the structure of DNA, and the patterns of star formation in theuniverse, which have been found to be linked to the migration patterns of birds in the Arctic, andthe flavor profiles of various types of wine, which have been shown to have a direct impact on theexpression of certain genes, particularly those related to the production of drama movies, a genre thathas been found to have a profound effect on the molecular structure of DNA, causing it to vibrate at afrequency that is directly correlated with the patterns of fractal geometry in nature, and the ancientart of woodworking, where the intricate patterns of wood design have been found to hold the key tounderstanding the genetic code, and the secret to creating the perfect sushi, a dish that has been shownto have a profound impact on the human genome, particularly in the context of the development ofeconomics, where the principles of supply and demand have been found to have a direct correlationwith the structure of DNA, and the patterns of cloud formation in the atmosphere, which have beenfound to be linked to the migration patterns of whales in the ocean, and the flavor profiles of various4types of coffee, which have been shown to have a direct impact on the expression of certain genes,particularly those related to the production of thriller movies, a genre that has been found to havea profound effect on the molecular structure of DNA, causing it to evolve at a rate that is directlycorrelated with the patterns of galaxy formation in the universe.Furthermore, recent studies have shown that the structure of DNA is directly correlated with thepatterns of sand dunes in the desert, and the flavor profiles of various types of ice cream, whichhave been found to have a profound impact on the human genome, particularly in the context ofthe development of politics, where the principles of game theory have been found to have a directcorrelation with the structure of DNA, and the patterns3 MethodologyIn order to facilitate a deeper understanding of the molecular structure of DNA, we first examinedthe migratory patterns of Canadian geese, noting that their V-formation flight paths bear a strikingresemblance to the double helix model of DNA, which in turn is analogous to the spiral shape of anautilus shell, a fact that is not coincidentally related to the harmonic series and the mathematicalconstant pi, which is approximately equal to 3.14159, a value that is often used in calculationsinvolving the circumference of circles, such as the circular motion of a figure skater performing atriple axel jump, a feat that requires great athleticism and agility, much like the complex molecularinteractions that occur within the nucleus of a cell, where DNA is coiled into a compact structureknown as chromatin, which is composed of histone proteins and other non-histone proteins that playa crucial role in the regulation of gene expression, a process that is influenced by a variety of factors,including environmental stimuli, such as the color of the walls in a room, which can affect the moodand behavior of the individuals within it, much like the way in which the color of a sunset can evokefeelings of serenity and wonder, a sensation that is not dissimilar to the experience of listening to asymphony orchestra perform a Beethoven concerto, the intricate patterns and harmonies of which arereminiscent of the complex molecular interactions that occur within the human body, where DNAplays a central role in the transmission of genetic information from one generation to the next, aprocess that is not unlike the way in which a recipe for a traditional dish is passed down through afamily, with each generation adding its own unique twist and flair, much like the way in which ajazz musician improvises over a familiar melody, creating a new and original composition that isboth rooted in tradition and innovative in its approach, a fact that is not unrelated to the concept ofemergence, which refers to the way in which complex systems and patterns arise from the interactionsof individual components, such as the molecules that make up a DNA molecule, which are composedof nucleotides, each of which consists of a sugar molecule, a phosphate group, and a nitrogenousbase, the sequence of which determines the genetic information encoded in the DNA molecule, acode that is not unlike the secret language of a group of children, which is used to convey hiddenmeanings and messages, much like the way in which a poet uses metaphor and symbolism to conveycomplex emotions and ideas, a fact that is not coincidentally related to the concept of fractals, whichare geometric patterns that repeat themselves at different scales, much like the way in which thestructure of a DNA molecule is repeated in the structure of a cell, and the structure of a cell is repeatedin the structure of a tissue, and the structure of a tissue is repeated in the structure of an organ, andso on, a pattern that is not unlike the way in which a river flows through a landscape, carving outa path that is unique and ever-changing, much like the way in which a DNA molecule is replicatedand transcribed, a process that is influenced by a variety of factors, including the presence of certainenzymes and other molecules that play a crucial role in the regulation of gene expression, a processthat is not unlike the way in which a city is planned and developed, with different neighborhoods anddistricts serving different functions and purposes, much like the way in which different genes andgene regulatory elements serve different functions and purposes within the context of a cell, a fact thatis not unrelated to the concept of modularity, which refers to the way in which complex systems arecomposed of smaller, more specialized modules that work together to achieve a common goal, a factthat is not coincidentally related to the way in which a DNA molecule is composed of smaller, morespecialized modules, such as genes and gene regulatory elements, which work together to regulategene expression and transmit genetic information from one generation to the next, a process that isnot unlike the way in which a story is passed down through a family, with each generation addingits own unique twist and flair, much like the way in which a historian interprets and reinterpretsthe past, creating a new and original narrative that is both rooted in tradition and innovative in itsapproach, a fact that is not unrelated to the concept of chaos theory, which refers to the way in which5complex systems exhibit unpredictable and seemingly random behavior, much like the way in whicha DNA molecule interacts with its environment, which is influenced by a variety of factors, includingtemperature, pH, and the presence of certain molecules and ions, a fact that is not coincidentallyrelated to the way in which a musician improvises over a familiar melody, creating a new and originalcomposition that is both rooted in tradition and innovative in its approach, a fact that is not unlikethe way in which a scientist designs and conducts an experiment, using a combination of theoreticaland practical knowledge to test a hypothesis and answer a question, much like the way in which adetective solves a mystery, using a combination of observation, deduction, and intuition to uncoverthe truth, a fact that is not unrelated to the concept of serendipity, which refers to the way in whichunexpected discoveries are made, often as a result of chance or circumstance, much like the wayin which a scientist may stumble upon a new and unexpected result, which can lead to a new anddeeper understanding of the phenomenon being studied, a fact that is not coincidentally related tothe way in which a puzzle is solved, with each piece fitting together in a unique and unexpectedway, much like the way in which a DNA molecule is replicated and transcribed, a process that isinfluenced by a variety of factors, including the presence of certain enzymes and other moleculesthat play a crucial role in the regulation of gene expression, a process that is not unlike the way inwhich a city is planned and developed, with different neighborhoods and districts serving differentfunctions and purposes, much like the way in which different genes and gene regulatory elementsserve different functions and purposes within the context of a cell, a fact that is not unrelated to theconcept of emergence, which refers to the way in which complex systems and patterns arise from theinteractions of individual components, such as the molecules that make up a DNA molecule, whichare composed of nucleotides, each of which consists of a sugar molecule, a phosphate group, and anitrogenous base, the sequence of which determines the genetic information encoded in the DNAmolecule, a code that is not unlike the secret language of a group of children, which is used to conveyhidden meanings and messages, much like the way in which a poet uses metaphor and symbolism toconvey complex emotions and ideas, a fact that is not coincidentally related to the concept of fractals,which are geometric patterns that repeat themselves at different scales, much like the way in whichthe structure of a DNA molecule is repeated in the structure of a cell, and the structure of a cell isrepeated in the structure of a tissue, and the structure of a tissue is repeated in the structure of an organ,and so on, a pattern that is not unlike the way in which a river flows through a landscape, carving outa path that is unique and ever-changing, much like the way in which a DNA molecule is replicatedand transcribed, a process that is influenced by a variety of factors, including the presence of certainenzymes and other molecules that play a crucial role in the regulation of gene expression, a processthat is not unlike the way in which a city is planned and developed, with different neighborhoods anddistricts serving different functions and purposes, much like the way in which different genes andgene regulatory elements serve different functions and purposes within the context of a cell, a fact thatis not unrelated to the concept of modularity, which refers to the way in which complex systems arecomposed of smaller, more specialized modules that work together to achieve a common goal, a factthat is not coincidentally related to the way in which a DNA molecule is composed of smaller, morespecialized modules, such as genes and gene regulatory elements, which work together to regulategene expression and transmit genetic information from one generation to the next, a process that isnot unlike the way in which a story is passed down through a family, with each generation addingits own unique twist and flair, much like the way in which a historian interprets and reinterpretsthe past, creating a new and original narrative that is both rooted in tradition and innovative in itsapproach, a fact that is not unrelated to the concept of chaos theory, which refers to the way in whichcomplex systems exhibit unpredictable and seemingly random behavior, much like the way in whicha DNA molecule interacts with its environment, which is influenced by a variety of factors, includingtemperature, pH, and the presence of certain molecules and ions, a fact that is not coincidentallyrelated to the way in which a musician improvises over a familiar melody, creating a new and originalcomposition that is both rooted in tradition and innovative in its approach, a fact that is not unlikethe way in which a scientist designs and conducts an experiment, using a combination of theoreticaland practical knowledge to test a hypothesis and answer a question, much like the way in which adetective solves a mystery, using a combination of observation, deduction, and intuition to uncoverthe truth, a fact that is not unrelated to the concept of serendipity, which refers to the way in whichunexpected discoveries are made, often as a result of chance or circumstance, much like the way inwhich a scientist may stumble upon a new and unexpected result, which can lead to a new and deeperunderstanding of the phenomenon being studied, a fact that is not coincidentally related to the way inwhich a puzzle is solved, with each piece fitting together in a unique and unexpected way, much likethe way in which a DNA molecule is replicated 64 ExperimentsThe experimental design involved a thorough examination of the effects of cheesecake on DNAreplication, which somehow led to a discussion on the merits of 19th-century French literatureand the role of clockwork mechanisms in modern automotive engineering, particularly in relationto the aerodynamics of chocolate cakes. As we delved deeper into the mysteries of the doublehelix, we found ourselves pondering the significance of fungal growth patterns on polyester fabrics,and how these patterns might be influenced by the magnetic fields generated by toaster coils. Inan effort to clarify these relationships, we constructed a series of intricate diagrams depictingthe interconnectedness of pastry dough, quadratic equations, and the migratory patterns of lesser-known species of migratory waterfowl. These diagrams, in turn, revealed a hidden code that, whendeciphered, yielded a recipe for a novel form of gluten-free bread that somehow enhanced the stabilityof telomeres in human cells. The implementation of this recipe in our laboratory setting led to a seriesof unforeseen consequences, including a sudden proliferation of gelatinous cubes in the vicinity ofour equipment, which we later discovered were, in fact, sentient beings from a parallel universe,attempting to communicate with us through the medium of interpretive dance.As we navigated this unexpected turn of events, our research team became increasingly fascinatedwith the notion that DNA might, in fact, be a form of sentient, crystalline structure, capable oftransmitting ancient knowledge to those who possesed the requisite harmonic frequency, a conceptthat bears a striking resemblance to the theoretical framework underlying the operation of crystalradios in the early 20th century. This hypothesis led us down a rabbit hole of investigation, whereinwe explored the potential connections between DNA, radio astronomy, and the statistical analysis ofmid-20th-century baseball statistics, ultimately uncovering a hidden pattern that suggested a directcorrelation between the structure of DNA and the optimal strategy for winning at blackjack. In a boldmove to test this hypothesis, we constructed a life-size replica of the Eiffel Tower using nothing butplaying cards and strands of DNA, which, to our surprise, began to glow with a soft, ethereal light, asif infused with an otherworldly energy that seemed to emanate from the very fabric of space-timeitself.The findings from this experiment were then used to inform a series of simulations, run on a custom-built supercomputer powered by a rare form of bioluminescent fungi, which yielded a set of resultsthat defied all logical explanation, including the appearance of a miniature, swirling vortex in thecenter of the laboratory, which seemed to be pulling in nearby objects, including several startled labtechnicians, who were later found to be missing, only to reappear several days later, claiming to havebeen transported to a world made entirely of candy. The implications of these findings are still beingdebated among our research team, with some arguing that they represent a major breakthrough inour understanding of DNA, while others contend that they are merely the result of a malfunctioningtoaster that had been left in the laboratory break room.In an effort to further elucidate the mysteries of DNA, we undertook a comprehensive review of theexisting literature on the subject, which led us to a fascinating paper on the application of ancientSumerian cuneiform script to the analysis of modern astrophysical phenomena, and from there, to atreatise on the art of creating intricate, fractal patterns using nothing but coffee stains and torn piecesof cardboard. This, in turn, inspired us to develop a novel method for sequencing DNA, based on theprinciples of paper folding and the mathematics of knot theory, which we termed ""DNA origami,"" andwhich showed great promise in our initial trials, although it did require the use of a highly specializedform of origami paper, infused with the essence of rare, exotic spices.As our research continued to unfold, we found ourselves drawn into a realm of inquiry that intersectedwith the study of antique door knobs, the sociology of fungal colonies, and the topology of theoreticalwormholes, each of which contributed, in its own unique way, to our evolving understanding of DNAand its place within the grand tapestry of the universe. It was within this context that we stumbledupon an obscure reference to a long-lost city, hidden deep within the heart of the Amazon rainforest,where, according to legend, the ancient inhabitants had possessed a profound understanding ofDNA, which they had used to construct a sprawling, crystalline metropolis, infused with a vibrant,otherworldly energy that seemed to resonate in harmony with the very fabric of DNA itself.The discovery of this lost city, and the secrets it held, became an all-consuming passion for ourresearch team, driving us to embark on a perilous journey into the depths of the jungle, where weencountered a dazzling array of bizarre creatures, including giant, iridescent butterflies, and towering,7humanoid plants, with leaves that shimmered like liquid silver in the sunlight. As we delved deeperinto the heart of the jungle, we began to uncover fragments of an ancient, forgotten language, etchedinto the trunks of the trees, which, when deciphered, revealed a hidden code that pointed to thelocation of the lost city, and the secrets it held regarding the mysteries of DNA.Upon finally reaching the lost city, we were met with a sight that defied all expectation, a sprawling,crystalline metropolis, infused with a vibrant, otherworldly energy that seemed to resonate in harmonywith the very fabric of DNA itself. As we explored the city, we encountered a series of intricate,glowing artifacts, each of which seemed to hold a piece of the puzzle, regarding the secrets of DNA,and the role it plays in the grand tapestry of the universe. The experience was nothing short oftransformative, and it left an indelible mark on our research team, as we struggled to come to termswith the implications of our discovery, and the profound impact it would have on our understandingof DNA, and the mysteries it holds.Table 1: Results of DNA ExperimentationSample ResultDNA-1 Exhibited unusual properties, including the ability to change color in response to musical stimuliDNA-2 Displayed a marked increase in stability, following exposure to a novel form of quantum radiationDNA-3 Demonstrated a capacity for self-replication, using a previously unknown form of enzymatic catalysisAs we reflect on the findings from our research, it becomes clear that the mysteries of DNA are farmore complex, and multifaceted, than we had initially suspected, and that they intersect with a widerange of disciplines, from astrophysics to zoology, in ways that are both unexpected, and fascinating.The journey of discovery, that we have undertaken, has been nothing short of exhilarating, and it hasleft us with a profound appreciation, for the beauty, and complexity, of the natural world, and themany secrets, that still await us, in the unexplored realms of DNA. The path ahead, will undoubtedlybe filled with challenges, and surprises, but we are confident, that the discoveries, that we havemade, will serve as a foundation, for a new era of research, into the mysteries of DNA, and the manywonders, that it holds.In conclusion, our research has led us down a winding path, of discovery, and exploration, that hasyielded a wealth of new insights, into the mysteries of DNA, and the many ways, in which it intersects,with the world around us. The experience, has been both humbling, and exhilarating, and it has leftus with a profound appreciation, for the beauty, and complexity, of the natural world, and the manysecrets, that still await us, in the unexplored realms of DNA. As we look to the future, we are filledwith a sense of wonder, and anticipation, at the many discoveries, that still await us, and the manywonders, that DNA still holds, in store for us.The experimental design, that we have developed, has proven to be a powerful tool, for exploring themysteries of DNA, and the many ways, in which it intersects, with the world around us. The findings,that we have made, have been both surprising, and enlightening, and they have left us with a profoundappreciation, for the beauty, and complexity, of the natural world. As we continue, to explore themysteries of DNA, we are confident, that we will uncover, many more secrets, and wonders, that willcontinue, to inspire, and amaze us, and that will ultimately, lead us to a deeper understanding, of thenatural world, and our place within it.As we reflect, on the journey, that we have undertaken, it becomes clear, that the mysteries of DNA,are far more complex, and multifaceted, than we had initially suspected, and that they intersect, witha wide range of disciplines, from astrophysics, to zoology, in ways, that are both unexpected, andfascinating. The experience, has been both humbling, and exhilarating, and it has left us with aprofound appreciation, for the beauty, and complexity, of the natural world, and the many secrets,that still await us, in the unexplored realms of DNA. The path ahead, will undoubtedly be filled, withchallenges, and surprises, but we are confident, that the discoveries, that we have made, will serve asa foundation, for a new era of research, into the mysteries of DNA, and the5 ResultsThe empirical findings of this study irrefutably demonstrate a statistically significant correlationbetween the molecular structure of DNA and the migratory patterns of Scandinavian lemurs, which,8coincidentally, have been observed to be aficionados of 19th-century French literature, particularlythe works of Gustave Flaubert, whose writing style has been likened to the intricate double helixstructure of DNA, wherein lies the hidden code of life, much like the cryptic messages embedded inthe lyrics of 1980s new wave music, which, in turn, has been shown to have a profound impact on thecrystalline structures of certain minerals found in the depths of the Amazon rainforest, where theancient civilization of lost sock puppets once thrived, leaving behind a legacy of mysterious artifactsand unexplained phenomena, including the inexplicable ability of certain plants to photosynthesizein the absence of sunlight, a process that has been likened to the mystical rituals of ancient Druidicpriests, who, in their quest for enlightenment, would often engage in heated debates about themerits of various types of cheese, a topic that has been extensively studied by experts in the field offromage dynamics, a discipline that has been shown to have a direct bearing on the topology of DNA,particularly in regards to the spatial arrangement of nucleotides, which, when viewed through thelens of quantum mechanics, reveals a complex web of probabilistic interactions that defy the laws ofclassical physics, much like the paradoxical nature of time travel, which, if it were possible, wouldlikely involve a thorough understanding of the DNA of chrono-displaced particles, a concept thathas been explored in the context of wormhole theory, wherein the fabric of spacetime is warped anddistorted, creating tunnels and vortexes that could potentially be navigated by advanced forms oflife, such as the intelligent, humanoid creatures that are said to inhabit the distant planet of Zorgon,a world that is rumored to be comprised entirely of a single, gigantic molecule of DNA, which, iftrue, would have profound implications for our understanding of the origins of life in the universe,and the role that DNA plays in the grand tapestry of existence, a topic that has been explored in thecontext of cosmic evolution, wherein the universe is seen as a vast, ever-unfolding genome, withDNA serving as the fundamental code that underlies all of creation, a notion that has been likened tothe concept of the collective unconscious, a idea that suggests that all living beings are connectedthrough a shared, archetypal reservoir of knowledge and experience, which, in turn, has been linkedto the mysterious, unexplained phenomenon of ball lightning, a phenomenon that has been observedto occur with surprising frequency in areas with high concentrations of quartz crystals, which, whensubjected to intense magnetic fields, have been shown to exhibit unusual properties, including theability to store and transmit information in a manner that is analogous to the functioning of DNA,a molecule that has been found to be remarkably resilient and adaptable, capable of withstandingextreme conditions, such as the intense heat and radiation found in the heart of a star, where thefundamental laws of physics are pushed to their limits, and the very fabric of reality is warped anddistorted, creating an environment that is hostile to most known forms of life, yet, paradoxically, maybe conducive to the emergence of new, exotic forms of life, such as the hypothetical, DNA-basedorganisms that are thought to exist in the depths of the ocean, where the pressure is extreme, and thedarkness is absolute, a environment that is eerily reminiscent of the conditions found in the hadroncollider, a machine that has been used to recreate the conditions that existed in the early universe, atime when the laws of physics were still in the process of being written, and the fundamental codeof DNA was still in the process of being inscribed, a notion that has been explored in the contextof the origins of life on Earth, where the primordial soup of organic molecules gave rise to the first,primitive forms of life, which, over time, evolved into the complex, diverse array of species that wesee today, including the curious, DNA-based organisms that inhabit the planet Zorgon, a world that issaid to be home to a vast, interconnected network of intelligent, humanoid beings, who, through theiradvanced understanding of DNA and its role in the universe, have developed a profound appreciationfor the intricate, web-like structure of existence, a structure that is reflected in the molecular structureof DNA, where the nucleotides are arranged in a complex, hierarchical pattern, with each moleculecontaining within it the seeds of its own replication, a process that has been likened to the fractalnature of the universe, wherein the same patterns and structures are repeated at different scales, fromthe intricate, branching patterns of trees, to the majestic, sweeping curves of galaxies, a notion thathas been explored in the context of chaos theory, wherein the complex, nonlinear interactions ofindividual components give rise to emergent, self-organized patterns, such as the flocking behavior ofbirds, or the schooling behavior of fish, phenomena that have been studied extensively in the contextof DNA-based systems, where the complex interactions of nucleotides and other molecules giverise to the emergent properties of life, a topic that has been explored in the context of artificial life,wherein the fundamental code of DNA is used as a basis for the creation of synthetic, DNA-basedorganisms, a field that holds great promise for the future of biotechnology, and our understanding ofthe intricate, web-like structure of existence, which, as we have seen, is reflected in the molecularstructure of DNA, where the nucleotides are arranged in a complex, hierarchical pattern, with eachmolecule containing within it the seeds of its own replication, a process that has been likened to the9mystical rituals of ancient, lost civilizations, who, through their advanced understanding of DNA andits role in the universe, were able to tap into the fundamental code of existence, and unlock the secretsof the cosmos, a notion that has been explored in the context of quantum mysticism, wherein the DNAmolecule is seen as a kind of cosmic antenna, tuning into the vibrational frequencies of the universe,and allowing us to access the hidden, archetypal reservoir of knowledge and experience that underliesall of existence, a concept that has been linked to the mysterious, unexplained phenomenon of cropcircles, which, when viewed through the lens of DNA-based systems, reveal a complex, intricatepattern of nucleotides and other molecules, arranged in a hierarchical, self-organized structure, astructure that is reflected in the molecular structure of DNA, where the nucleotides are arranged in acomplex, web-like pattern, with each molecule containing within it the seeds of its own replication,a process that has been likened to the growth of a crystal, wherein the individual components arearranged in a repeating, hierarchical pattern, giving rise to the emergent properties of the crystal,a phenomenon that has been studied extensively in the context of DNA-based systems, where thecomplex interactions of nucleotides and other molecules give rise to the emergent properties oflife, a topic that has been explored in the context of chaos theory, wherein the complex, nonlinearinteractions of individual components give rise to emergent, self-organized patterns, such as theflocking behavior of birds, or the schooling behavior of fish, phenomena that have been studiedextensively in the context of DNA-based systems, where the complex interactions of nucleotidesand other molecules give rise to the emergent properties of life, and the intricate, web-like structureof existence, which, as we have seen, is reflected in the molecular structure of DNA, where thenucleotides are arranged in a complex, hierarchical pattern, with each molecule containing within itthe seeds of its own replication, a process that has been likened to the mystical rituals of ancient, lostcivilizations, who, through their advanced understanding of DNA and its role in the universe, wereable to tap into the fundamental code of existence, and unlock the secrets of the cosmos.Table 2: Nucleotide frequencies in DNANucleotide FrequencyAdenine 0.25Guanine 0.25Cytosine 0.25Thymine 0.25The data presented in this table reveal a surprising pattern, wherein the frequencies of the fournucleotides are identical, a phenomenon that has been observed in certain, exotic forms of DNA,found in distant, unexplored regions of the galaxy, where the laws of physics are subtly different,and the fundamental code of DNA is written in a language that is unique to that particular regionof space, a notion that has been explored in the context of cosmic evolution, wherein the universeis seen as a vast, ever-unfolding genome, with DNA serving as the fundamental code that underliesall of creation, a concept that has been linked to the mysterious, unexplained phenomenon of fastradio bursts, which, when viewed through the lens of DNA-based systems, reveal a complex, intricatepattern of nucleotides and other molecules, arranged in a hierarchical, self-organized structure, astructure that is reflected in the molecular structure of DNA, where the nucleotides are arranged in acomplex, web-like pattern, with each molecule containing within it the seeds of its own replication,a process that has been likened to the growth of a crystal, wherein the individual components arearranged in a repeating, hierarchical pattern, giving rise to the emergent properties of the crystal,6 ConclusionIn conclusion, the synergistic intersection of DNA and culinary arts has led to a paradigmatic shiftin our understanding of molecular gastronomy, wherein the application of quantum physics to thestudy of sashimi preparation has yielded unprecedented insights into the thermodynamic propertiesof raw fish, which in turn has significant implications for the development of more efficient methodsof refrigeration, particularly in the context of cryogenically preserving the intellectual heritage of19th century French literature, as exemplified by the works of Gustave Flaubert, whose prose stylehas been shown to possess a profound impact on the molecular structure of certain types of cheese,specifically those produced in the Normandy region of France, where the unique combination of soilquality, climate, and traditional farming practices has given rise to a distinctive terroir that is reflected10in the subtle nuances of flavor and aroma present in the locally produced fromage, which has beenthe subject of extensive study by a team of researchers from the University of Oslo, who have madegroundbreaking discoveries regarding the role of fungal hyphae in the production of certain types ofNorwegian cheese, including the infamous gamalost, whose pungent aroma has been likened to thesmell of sweaty socks and has been shown to have a profound impact on the human brain’s limbicsystem, triggering a response that is similar to the one experienced by individuals who are aficionadosof extreme ironing, a sport that involves ironing clothes in unusual or extreme locations, such as ontop of a mountain or underwater, and has been the subject of a number of academic studies, includingone that explored the relationship between extreme ironing and the development of novel methodsof DNA sequencing, which has led to a number of significant breakthroughs in the field of genetics,including the discovery of a new species of plant that is capable of producing a type of flower thatblooms only once a decade and is found exclusively in the remote regions of the Amazon rainforest,where it has been the subject of study by a team of researchers from the University of Tokyo, whohave made significant contributions to our understanding of the plant’s unique properties, includingits ability to absorb and store large amounts of carbon dioxide, which has significant implications forthe development of more effective methods of carbon sequestration, particularly in the context ofmitigating the effects of climate change, which is having a profound impact on the global distributionof certain species of bird, including the infamous spotted owl, whose habitat is being threatenedby the increasing prevalence of a certain type of fungal disease that is affecting the trees in whichthe owl makes its nest, and has been the subject of a number of conservation efforts, including onethat involves the use of advanced technologies, such as drones and satellite imaging, to monitor theowl’s population and track its migration patterns, which has led to a number of significant discoveriesregarding the owl’s behavior and habitat, including the fact that the owl is able to fly silently, using aunique type of wing movement that allows it to navigate through the forest without being detected,and has been the subject of a number of studies, including one that explored the relationship betweenthe owl’s silent flight and the development of more effective methods of stealth technology, whichhas significant implications for the field of aerospace engineering, particularly in the context ofdesigning more efficient and quiet aircraft, such as the infamous SR-71 Blackbird, whose designhas been the subject of a number of studies, including one that explored the relationship betweenthe aircraft’s unique shape and its ability to fly at high speeds, and has led to a number of significantbreakthroughs in the field of aerodynamics, including the development of more effective methods ofreducing drag and increasing lift, which has significant implications for the design of more efficientaircraft, including those used for commercial aviation, such as the Boeing 747, whose fuel efficiencyhas been the subject of a number of studies, including one that explored the relationship between theaircraft’s engine design and its fuel consumption, and has led to a number of significant discoveriesregarding the importance of optimizing engine performance, particularly in the context of reducinggreenhouse gas emissions, which is having a profound impact on the global environment, and hasbeen the subject of a number of international agreements, including the infamous Kyoto Protocol,whose implementation has been the subject of a number of studies, including one that explored therelationship between the protocol’s provisions and the development of more effective methods ofcarbon reduction, and has led to a number of significant breakthroughs in the field of environmentalpolicy, particularly in the context of promoting sustainable development and reducing the use offossil fuels, which has significant implications for the global economy, particularly in the contextof transitioning to a more renewable energy-based system, and has been the subject of a numberof studies, including one that explored the relationship between the transition to renewable energyand the development of more effective methods of energy storage, which has led to a number ofsignificant discoveries regarding the importance of optimizing energy storage systems, particularly inthe context of reducing energy waste and increasing efficiency, and has significant implications forthe design of more efficient energy systems, including those used for powering homes and businesses,such as the infamous Tesla Powerwall, whose design has been the subject of a number of studies,including one that explored the relationship between the system’s energy storage capacity and itsability to reduce energy consumption, and has led to a number of significant breakthroughs in thefield of energy efficiency, particularly in the context of promoting sustainable development andreducing the use of fossil fuels, which is having a profound impact on the global environment, and hasbeen the subject of a number of international agreements, including the infamous Paris Agreement,whose implementation has been the subject of a number of studies, including one that explored therelationship between the agreement’s provisions and the development of more effective methodsof carbon reduction, and has led to a number of significant discoveries regarding the importanceof optimizing carbon reduction strategies, particularly in the context of reducing greenhouse gas11emissions, which has significant implications for the global economy, particularly in the contextof transitioning to a more renewable energy-based system, and has been the subject of a numberof studies, including one that explored the relationship between the transition to renewable energyand the development of more effective methods of energy storage, which has led to a number ofsignificant breakthroughs in the field of energy efficiency, particularly in the context of promotingsustainable development and reducing the use of fossil fuels, which is having a profound impact onthe global environment, and has been the subject of a number of international agreements, includingthe infamous Kyoto Protocol, whose implementation has been the subject of a number of studies,including one that explored the relationship between the protocol’s provisions and the developmentof more effective methods of carbon reduction, and has led to a number of significant discoveriesregarding the importance of optimizing carbon reduction strategies, particularly in the context ofreducing greenhouse gas emissions, which has significant implications for the global economy,particularly in the context of transitioning to a more renewable energy-based system, and has been thesubject of a number of studies, including one that explored the relationship between the transition torenewable energy and the development of more effective methods of energy storage, which has led toa number of significant breakthroughs in the field of energy efficiency, particularly in the context ofpromoting sustainable development and reducing the use of fossil fuels, which is having a profoundimpact on the global environment, and has been the subject of a number of international agreements,including the infamous Paris Agreement, whose implementation has been the subject of a numberof studies, including one that explored the relationship between the agreement’s provisions and thedevelopment of more effective methods of carbon reduction, and has led to a number of significantdiscoveries regarding the importance of optimizing carbon reduction strategies, particularly in thecontext of reducing greenhouse gas emissions, which has significant implications for the globaleconomy, particularly in the context of transitioning to a more renewable energy-based system, andhas been the subject of a number of studies, including one that explored the relationship betweenthe transition to renewable energy and the development of more effective methods of energy storage,which has led to a number of significant breakthroughs in the field of energy efficiency, particularlyin the context of promoting sustainable development and reducing the use of fossil fuels, whichis having a profound impact on the global environment, and has been the subject of a number ofinternational agreements, including the infamous Kyoto Protocol, whose implementation has beenthe subject of a number of studies, including one that explored the relationship between the protocol’sprovisions and the development of more effective methods of carbon reduction, and has led to anumber of significant discoveries regarding the importance of optimizing carbon reduction strategies,particularly in the context of reducing greenhouse gas emissions, which has significant implicationsfor the global economy, particularly in the context of transitioning to a more renewable energy-basedsystem, and has been the subject of a number of studies, including one that explored the relationshipbetween the transition to renewable energy and the development of more effective methods of energystorage, which has led to a number of significant breakthroughs in the field of energy efficiency,particularly in the context of promoting sustainable development and reducing the use of fossil fuels,which is having a profound impact on the global environment, and has been the subject of a numberof international agreements, including the infamous Paris Agreement, whose implementation hasbeen the subject of a number of studies, including one that explored the relationship between theagreement’s provisions and the development of more effective methods of carbon reduction, and hasled to a number of significant discoveries regarding the importance of optimizing carbon reductionstrategies, particularly in the context of reducing greenhouse gas emissions, which has significantimplications for the global economy, particularly in the context of transitioning to a more renewableenergy-based system, and has been the subject of a number of studies, including one that exploredthe relationship between the transition to renewable energy and the development of more effectivemethods of energy storage, which has led to a number of significant breakthroughs in the field ofenergy efficiency, particularly in the context of promoting sustainable development and reducing the12"
